[{"categories":null,"contents":" 谷歌浏览器\nclash https://github.com/Dreamacro/clash/releases\nterminal 应用商店\ndraw.io 应用商店\npowershell https://github.com/PowerShell/PowerShell\nwsl2 ubuntu 子系统\nchoco\noh-my-posh\nchoco install neovim\nchoco install lsd\nchoco install hugo\nHach Nerd 字体 链接: https://pan.baidu.com/s/1Cts0PCJeeB8Nb3gO7laeRA 提取码: t49r\nwindows docker desktop https://www.docker.com/products/docker-desktop/\nvscode\njava\nruby\ngo\nlocalsend 局域网文件传输，最好用，没有之一 https://localsend.org/#/download\ncpu 内存等监控\nqq 音乐\n洛雪音乐 https://github.com/lyswhut/lx-music-desktop\nBandzip 压缩软件 https://www.bandisoft.com/bandizip/\nBCUninstall 卸载工具 https://github.com/Klocman/Bulk-Crap-Uninstaller\nutool https://u.tools/\neverything 文件搜索 https://www.voidtools.com/zh-cn/downloads/\nIDM 下载破解版 链接: https://pan.baidu.com/s/146O6MdD_yd1bvfQ6HmN-EQ 提取码: 3i2j\nPotPlayer https://potplayer.daum.net/\ntelegram https://desktop.telegram.org/\n","date":"2023-04-15T10:32:37Z","permalink":"https://dccmmtop.github.io/posts/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E6%B8%85%E5%8D%95/","section":"posts","tags":["windows"],"title":"常用软件清单"},{"categories":null,"contents":" 简单示例 项目结构 依赖 配置 生产者 消费者 消息的可靠投递示例 confirm return 消费者确认机制 消费端限流 TTL 单条消息 整个队列设置 TTL 死信队列 死信队列的实现步骤 延迟队列 消息幂等设计 简单示例 项目结构 依赖 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;RELEASE\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 配置 spring: rabbitmq: host: 127.0.0.1 port: 5672 username: dc password: Aa111111 virtual-host: /dc0407 # 让发布者可以收到消息是否投递成功的回调 # 默认是 false publisher-confirms: true listener: simple: # 消费者开启手动确认模式，默认是 none acknowledge-mode: manual # 每次消费一个 prefetch: 1 package io.dc.rabbitmqinspringboot.config; import org.springframework.amqp.core.*; import org.springframework.amqp.rabbit.config.SimpleRabbitListenerContainerFactory; import org.springframework.amqp.rabbit.connection.CachingConnectionFactory; import org.springframework.amqp.rabbit.connection.ConnectionFactory; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class RabbitMQConfig { public static final String EXCHANGE_NAME = \u0026#34;boot_topic_exchange\u0026#34;; public static final String QUEUE_NAME = \u0026#34;boot_queue\u0026#34;; // 1. 声明交换机 @Bean(\u0026#34;bootTopicExchange\u0026#34;) public Exchange bootExchange(){ return ExchangeBuilder.topicExchange(EXCHANGE_NAME).durable(false).build(); } // 2. 声明队列 @Bean(\u0026#34;bootQueue\u0026#34;) public Queue bootQueue(){ return QueueBuilder.durable(QUEUE_NAME).build(); } // 3. 将队列与交换器进行绑定 @Bean public Binding bindQueueExchange(@Qualifier(\u0026#34;bootQueue\u0026#34;) Queue queue, @Qualifier(\u0026#34;bootTopicExchange\u0026#34;) Exchange exchange){ return BindingBuilder.bind(queue).to(exchange).with(\u0026#34;boot.#\u0026#34;).noargs(); } // 自定义连接工厂，自由程度高，本次演示没用到 @Bean(\u0026#34;customConnectionFactory\u0026#34;) public ConnectionFactory connectionFactory( @Value(\u0026#34;${spring.rabbitmq.host}\u0026#34;) String host, @Value(\u0026#34;${spring.rabbitmq.port}\u0026#34;) int port, @Value(\u0026#34;${spring.rabbitmq.username}\u0026#34;) String username, @Value(\u0026#34;${spring.rabbitmq.password}\u0026#34;) String password, @Value(\u0026#34;${spring.rabbitmq.virtual-host}\u0026#34;) String vhost ){ CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setHost(host); factory.setPort(port); factory.setUsername(username); factory.setPassword(password); factory.setVirtualHost(vhost); return factory; } // 自定义消息监听器工厂，自由程度高 @Bean(name = \u0026#34;customListenFactory\u0026#34;) public SimpleRabbitListenerContainerFactory listenerContainerFactory(@Qualifier(\u0026#34;customConnectionFactory\u0026#34;) ConnectionFactory connectionFactory){ SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); // 设置手动签收 factory.setAcknowledgeMode(AcknowledgeMode.MANUAL); // 设置预处理数量，同时没有确认的消息不能超过 N 个，起到消费者限流的作用，详细解释见下 factory.setPrefetchCount(5); return factory; } } 生产者 在测试类中发消息\npackage io.dc.rabbitmqinspringboot; import io.dc.rabbitmqinspringboot.config.RabbitMQConfig; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.amqp.core.Message; import org.springframework.amqp.rabbit.connection.CorrelationData; import org.springframework.amqp.rabbit.core.RabbitTemplate; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; @SpringBootTest @RunWith(SpringJUnit4ClassRunner.class) public class RabbitMqInSpringBootApplicationTests { @Autowired RabbitTemplate rabbitTemplate; // 发一次消息 @Test public void sendMsg() { rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,\u0026#34;boot.dc\u0026#34;,\u0026#34;你好 rabbitMQ\u0026#34;); } // 批量发消息 @Test public void sendBatchMsg() { for (int i = 0; i \u0026lt; 10; i++) { rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,\u0026#34;boot.dc\u0026#34;,\u0026#34;你好 rabbitMQ\u0026#34;); } } } 消费者 简单版，自动 ack\npackage io.dc.rabbitmqinspringboot.consumer; import io.dc.rabbitmqinspringboot.config.RabbitMQConfig; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.stereotype.Component; import org.springframework.amqp.core.Message; @Component public class Consumer1 { @RabbitListener(queues = RabbitMQConfig.QUEUE_NAME) public void listenerQueue(Message message){ System.out.println(\u0026#34;消费者 1 接收到消息：\u0026#34;+ message); } } 手动 ack:\n需要在配置文件中额外设置为手动确认：manual\npackage io.dc.rabbitmqinspringboot.consumer; import com.rabbitmq.client.Channel; import io.dc.rabbitmqinspringboot.config.RabbitMQConfig; import org.springframework.amqp.core.Message; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.amqp.rabbit.listener.api.ChannelAwareMessageListener; import org.springframework.stereotype.Component; @Component public class Consumer2 { // 也可以采用自定义监听工厂 // @RabbitListener(queues = RabbitMQConfig.QUEUE_NAME, containerFactory = \u0026#34;customListenFactory\u0026#34;) @RabbitListener(queues = RabbitMQConfig.QUEUE_NAME) public void onMessage(Message message, Channel channel) throws Exception { System.out.println(\u0026#34;消费者 2 收到消息：\u0026#34; + new String(message.getBody())); long deliveryTag = message.getMessageProperties().getDeliveryTag(); System.out.println(\u0026#34;消息 Id: \u0026#34; + deliveryTag); // 进行消息签收，如果不签收，将只会收到 ${listener.simple.prefetch} 个消息，因为设置了手动签收模式 channel.basicAck(deliveryTag, true); // 拒绝签收 // 最后一个参数： 是否重回队列 // channel.basicNack(deliveryTag,false, false); } } 消息的可靠投递示例 在 RabbitMQ 的基本概念和五种模式使用示例 中已经介绍了两种实现可靠投递的机制，这里仅作为一个完整的补充：\nconfirm @Test public void testConfirm(){ rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() { @Override public void confirm(CorrelationData correlationData, boolean b, String s) { System.out.println(\u0026#34;执行了 confirm 方法。..\u0026#34;); if(b){ System.out.println(\u0026#34;发送成功\u0026#34;); }else{ System.out.println(\u0026#34;发送失败：\u0026#34; + s); } } }); // 正常 rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,\u0026#34;boot.dc\u0026#34;,\u0026#34;你好 rabbitMQ\u0026#34;); // 错误的交换器。会打印发送失败 rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME + \u0026#34;111\u0026#34;,\u0026#34;boot.dc\u0026#34;,\u0026#34;你好 rabbitMQ\u0026#34;); try { Thread.sleep(5000); } catch (InterruptedException e) { throw new RuntimeException(e); } } return @Test public void testReturn(){ // 只有设置 true，消息无法到达队列时，才会退回给生产者 rabbitTemplate.setMandatory(true); rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() { /** * * @param message the returned message. * @param replyCode the reply code. * @param replyText the reply text. * @param exchange the exchange. * @param routingKey the routing key. */ @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) { System.out.println(\u0026#34;消息退回了~\u0026#34;); System.out.println(\u0026#34;message: \u0026#34; + message.toString()); System.out.println(\u0026#34;replyCode: \u0026#34; + replyCode); System.out.println(\u0026#34;replyText: \u0026#34; + replyText); System.out.println(\u0026#34;exchange: \u0026#34; + exchange); System.out.println(\u0026#34;routingKey: \u0026#34; + routingKey); } }); // 正常 rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,\u0026#34;boot.dc\u0026#34;,\u0026#34;你好 rabbitMQ\u0026#34;); // 错误的路由 key。消息会被退回 rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,\u0026#34;1111_boot.dc\u0026#34;,\u0026#34;你好 rabbitMQ\u0026#34;); try { Thread.sleep(5000); } catch (InterruptedException e) { throw new RuntimeException(e); } } 消费者确认机制 Ack 指的是 ack 指 Acknowledge，确认。 表示消费端收到消息后的确认方式。\n有三种确认方式：\n自动确认 acknowledge = none 手动确认 acknowledge = manual 根据异常情况确认 acknowledge = auto 这种情况使用麻烦，一般不用 其中自动确认是指，当消息一旦被 Consumer 接收到，则自动确认收到，并将相应 message 从 RabbitMQ 的消息缓存中移除。但是在实际业务处理中，很可能消息接收到，业务处理出现异常，那么该消息就会丢失。如果设置了手动确认方式，则需要在业务处理成功后，调用 channel.basicAck()，手动签收，如果出现异常，则调用 channel.basicNack() 方法，让其自动重新发送消息。\n消费者调用 basicAck 或者 basicNack 签收或拒绝\n消息可靠性总结：\n持久化： exchange 要持久化，queue 要持久化， message 要持久化 生产者要 confirm 消费者要 ack Broker 要高可用 消费端限流 假设有一个场景：服务端挤压了大量的消息消息，此时启动消费者客户端，大量的消息会瞬间流入该客户端，可能会让客户端宕机。\n当数据量特别大的时候，对生产者限制肯定是不科学的，这是用户的行为，我们应该对消费端限流。\nRabbitMQ 提供了一种 qos（服务质量保证）功能，在非自动确认消息的前提下，如果一定数目的消息未被确认前，不消费新得消息\n设置方法：\n/** * Request specific \u0026#34;quality of service\u0026#34; settings. * * These settings impose limits on the amount of data the server * will deliver to consumers before requiring acknowledgements. * Thus they provide a means of consumer-initiated flow control. * @see com.rabbitmq.client.AMQP.Basic.Qos * @param prefetchSize maximum amount of content (measured in * octets) that the server will deliver, 0 if unlimited * @param prefetchCount maximum number of messages that the server * will deliver, 0 if unlimited * @param global true if the settings should be applied to the * entire channel rather than each consumer * @throws java.io.IOException if an error is encountered */ void basicQos(int prefetchSize, int prefetchCount, boolean global) throws IOException; prefetchSize: 每条消息得大小。0： 不限制 注意： RabbitMQ 没有实现这个功能 prefetchCount: 一次性消费消息得数量，会告诉 RabbitMQ 不要同时个同一个客户端推送对于 N 个消息，也就是一旦超过 N 个消息美欧被 ack，则该客户端就会阻塞，知道有消息 ack global: true / false 是否将上面得设置应用到 channel，简单得说上面得限制是 channel 级别还是某个消费者客户端级别。设置 false 的时候生效，因为 rabbitmq 没有实现 channel 级别的控制 在 sprinBoot 中，对客户端限流只需配置 prefetch 即可。和调用 basicQos(0, N, false) 效果一样\nTTL TTL 全称是 Time To Live （存活时间）, 当消息到达存活时间后还没有被消费就会自动清除，这与 redis 中的过期时间概念类似，我们应该合理应用 TTL，可以有效的处理过期的垃圾信息，从而降低服务器的负载。\nRabbitMQ 既可以对单条消息设置存活时间，也可以对整个队列设置\n单条消息 @Test public void sendMsgWithTtl() { MessageProperties properties1 = new MessageProperties(); properties1.setExpiration(\u0026#34;6000\u0026#34;); Message message = new Message(\u0026#34;你好\u0026#34;.getBytes(),properties1); rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,\u0026#34;boot.dc\u0026#34;,message); } 整个队列设置 TTL 可以在后管设置：\n也可以在代码中设置：\n// 2. 声明队列 @Bean(\u0026#34;bootQueueTTL\u0026#34;) public Queue bootQueue(){ Map\u0026lt;String, Object\u0026gt; arg = new HashMap\u0026lt;\u0026gt;(); arg.put(\u0026#34;x-message-ttl\u0026#34;,10000); return QueueBuilder.durable(\u0026#34;bootQueueTTL\u0026#34;).withArguments(arg).build(); } 后管查看结果：\n如果两者都设置了 TTL 以短的时间为准\n死信队列 没有被及时消费的消息将被投放到一个特殊队列，被称为死信队列\n没有被及时消费的原因:\n消息被拒绝(basic.reject, basic.nack)， 并且不再重新投递，（requeue = false） 消息超时未被消费 队列长度达到最大 死信队列的实现步骤 声明队列Q的时候，在附加参数 x-dead-letter-exchange 指定交换器E的名称，只是声明，并非绑定。 它的含义是，当在队列Q上产生死信时，该消息会通过交换器 E 发走。\n就这么简单，至于 E 会把消息发送发送到哪里，就看交换器E绑定了哪些队列。\n代码示例:\n// 带有死信的队列 public static final String QUEUE_NAME_WITH_DEAD = \u0026#34;boot_queue_with_dead\u0026#34;; // 死信消息从正常队列中移除，通过该交换机进入死信队列 // 和正常交换机没有差别，只不过被带有死信的队列指定了 public static final String deadExchange = \u0026#34;dead_exchange\u0026#34;; // 死信队列,接收死信交换机过来的消息 public static String deadQueue = \u0026#34;dead_queue\u0026#34;; // 声明死信交换器,和普通交换器一样 @Bean(\u0026#34;bootDeadExchange\u0026#34;) public Exchange bootDeadExchange() { return ExchangeBuilder.topicExchange(deadExchange).durable(false).build(); } // 声明接收死信的队列 @Bean(\u0026#34;bootDeadQueue\u0026#34;) public Queue bootDeadQueue(){ return QueueBuilder.durable(deadQueue).build(); } // 把接收死信的队列与死信交换器绑定 @Bean public Binding bindWithDeadQueueExchange(@Qualifier(\u0026#34;bootDeadQueue\u0026#34;) Queue queue, @Qualifier(\u0026#34;bootDeadExchange\u0026#34;) Exchange exchange){ return BindingBuilder.bind(queue).to(exchange).with(\u0026#34;boot.#\u0026#34;).noargs(); } //********* 这才是死信队列的重要步骤 *************** // 声明带有死信的队列 @Bean(\u0026#34;bootWithDeadQueue\u0026#34;) public Queue bootWithDeadQueue(){ Map\u0026lt;String, Object\u0026gt; arg = new HashMap\u0026lt;\u0026gt;(); // 声明接收死信消息的交换器 // *****这一步很重要，可以让死信通过 deadExchange 发走**** arg.put(\u0026#34;x-dead-letter-exchange\u0026#34;,deadExchange); return QueueBuilder.durable(QUEUE_NAME_WITH_DEAD).withArguments(arg).build(); } // 将带有死信的队列绑定到交换机上 @Bean public Binding bindDeadQueueExchange(@Qualifier(\u0026#34;bootWithDeadQueue\u0026#34;) Queue queue, @Qualifier(\u0026#34;bootTopicExchange\u0026#34;) Exchange exchange){ return BindingBuilder.bind(queue).to(exchange).with(\u0026#34;boot.#\u0026#34;).noargs(); } // 发消息 @Test public void sendDeadMsg() { MessageProperties properties = new MessageProperties(); properties.setExpiration(\u0026#34;6000\u0026#34;); Message message = new Message(\u0026#34;你好,6秒后我将要从列中移除，并进入死信队列\u0026#34;.getBytes(),properties); rabbitTemplate.convertAndSend(RabbitMQConfig.EXCHANGE_NAME,\u0026#34;boot.hei\u0026#34;,message); } 效果如下：\n六秒后:\n延迟队列 RabbitMQ 没有自己实现延迟队列，但是我们可以借助TTL 和死信队列完成延迟队列的功能。\n把需要延时处理的消息设置TTL,并发送到带有死信的队列中。 消费者监听死信队列 消息幂等设计 幂等设计是一种设计思想，一次或多次请求同一个资源，对资源本身应该有同样的结果，也就是说任意执行多次对资源本身产生的影响与执行一次的影响相同\n在MQ中，消费多条相同的消息应该与只消费一次带来的效果相同\n可以采用乐观锁的方式实现消息幂等：\n以sql 更新语句为例：\n-- version 某时刻等于 1 update account set price = price - 100, version = version + 1 where id = 1 and version = 1; -- 把版本号作为更新语句的条件，同时版本号自增 ","date":"2023-04-12T11:33:58Z","permalink":"https://dccmmtop.github.io/posts/springboot%E4%B8%8Erabbitmq%E9%9B%86%E6%88%90%E5%92%8C%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/","section":"posts","tags":["rabbitMQ"],"title":"SpringBoot 与 RabbitMQ 集成和高级用法"},{"categories":null,"contents":"概念 MQ 全称 Message Queue（消息队列），是在消息的传输过程中保存消息的容器。多用于分布式系统之间进行通信。\n无 MQ\n有 MQ\n优势 应用解耦 系统的耦合性越高，容错性就越低，可维护性就越低\n异步提速 如图所示，在没有 MQ 的情况下，订单系统需要等待其他下游系统的反馈。总耗时： 20 + 300 + 300 + 300 = 920ms\n加入 MQ 后，只需把任务交给 MQ, 然后让 MQ 给下游系统分发任务：\n用户点击完下单按钮后，只需等待 25ms 就能得到下单响应 (20 + 5 = 25ms)。\n提升用户体验和系统吞吐量（单位时间内处理请求的数目）。\n削峰填谷 假设系统每秒最大能接受 1000 个请求，突然来了 5000 个请求，高并发下可能会导致服务不可用：\n在用户请求和服务器之间加入一层 MQ:\n使用了 MQ 之后，限制消费消息的速度为 1000，这样一来，高峰期产生的数据势必会被积压在 MQ 中，高峰就被“削”掉了，但是因为消息积压，在高峰期过后的一段时间内，消费消息的速度还是会维持在 1000，直到消费完积压的消息，这就叫做“填谷”。\n使用 MQ 后，可以提高系统稳定性。\n劣势 降低系统的可用性 系统引入的外部依赖越多，系统的稳定性越差，一旦 MQ 宕机，机会对业务造成影响，需要额外可虑 MQ 的高可用\n提高系统的复杂性 MQ 的加入大大增加了系统的复杂度，以前系统间是同步的远程调用，现在是通过 MQ 进行异步调用。需要考虑消息丢失的情况\n常见的 MQ 产品 RabbitMQ 简介 2007 年，Rabbit 技术公司基于 AMQP 标准开发的 RabbitMQ 1.0 发布。RabbitMQ 采用 Erlang 语言开发。Erlang 语言由 Ericson 设计，专门为开发高并发和分布式系统的一种语言，在电信领域使用广泛。\nAMQP 即 Advanced Message Queuing Protocol（高级消息队列协议），是一个网络协议，是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制\n架构图 图中涉及的几个概念：\nBroker 接收和分发消息的应用，RabbitMQ Server 就是 Message Broker\nVirtual host 虚拟机\n出于多租户和安全因素考虑而设计的，把 AMQP 的基本组件划分到一个虚拟的分组中，类似与网络中的 namespace 概念，当不同的用户使用同一个 MQ 服务时，可以划分多个 vhost\nConnection 就是生产者和消费者与 MQ 服务器之间的 TCP 链接\nChannel 如果每一次访问都建立一个 connection, 在消息量大的时候建立 TCP 链接对系统资源的消耗将是巨大的，Channel 是 Connection 内部建立的逻辑链接，通常每个线程会单独创建 Channel 与消息队列服务器进行通讯，每个 channel 都有唯一标识，他们之间是完全隔离的，可以把 channel 理解为轻量级的 connection。极大的减少了操作系统建立 TCP 链接的开销。\nExchange 交换器\n是消息到达消息队列服务的第一站，可以根据分发规则把消息分发不同的队列中去。\n交换器不像队列那样有真实的进程， 它只是一张名称与队列进程 PID 的关系表\n当你将消息发布到交换器时，实际上是由你所连接到的信道将消息上的路由键同交换器的绑定列表进行比较，然后路由消息。正是信道（channel）按照绑定匹配的结果，将消息路由到队列。 信道才是真正的路由器\n由于交换器只是一张表，因此将交换器在整个集群中进行复制，更加简单\n举例来说，当创建一个新的交换器时，RabbitMQ 所要做的是将查询表添加到集群中的所有节点上。这时，每个节点上的每条信道都可以访问到新的交换器了。因此，相对于默认情况下队列的完整信息存在于集群中的单一节点来说，集群中的每个节点拥有每个交换器的所有信息。就可用性来讲，这非常棒，因为这意味着你不用担心在节点故障时重新声明交换器。只需让故障节点上的生产者重新连接到集群上，它们立即就能开始往交换器上发布消息了。\nQueue 队列\n消息最终被送达到这里，等待被消费者取走。\nBinding 绑定\n交换器和队列之间的虚拟连接，其实就是这两者之间的对应关系。\n常用的五工作模式 简单模式 最简单的工作模式，只包含一个生产者，一个消费者。这种模式下不要指定交换器，使用默认的即可，如下图：\njava 代码展示：\n依赖\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.rabbitmq\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;amqp-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; rabbitmq 连接工具类\nimport com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; public class RabbitUtils { private static ConnectionFactory connectionFactory = new ConnectionFactory(); static { connectionFactory.setHost(\u0026#34;127.0.0.1\u0026#34;); // 默认端口号 connectionFactory.setPort(5672); connectionFactory.setUsername(\u0026#34;dc\u0026#34;); connectionFactory.setPassword(\u0026#34;Aa111111\u0026#34;); // 设置要连接的虚机 connectionFactory.setVirtualHost(\u0026#34;/dc0407\u0026#34;); } public static Connection getConnection(){ Connection connection = null; try { connection = connectionFactory.newConnection(); return connection; } catch (Exception e) { throw new RuntimeException(e); } } } 生产者\nimport com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import org.example.util.RabbitUtils; import java.io.IOException; import java.util.concurrent.TimeoutException; /** * 生产者 */ public class Producer { public static void main(String[] args) throws IOException, TimeoutException, InterruptedException { // 获得连接 Connection connection = RabbitUtils.getConnection(); // 获得 channel Channel channel = connection.createChannel(); String queueName = \u0026#34;hello\u0026#34;; // 声明队列并创建一个队列 // 第一个参数： 队列名称 // 第二个参数：是否持久化队列，不持久化队列时，MQ 重启后，队列中的消息会丢失 // 第三个参数：是否私有化队列，false 代表所有消费者都可以访问，true 代表只有第一次拥有它的消费者才能访问 // 第四个参数：是否自动删除，false 代表连接停掉后不自动删除这个队列 // 其他额外参数： null channel.queueDeclare(queueName,false,false,false, null); // 发送消息 String msg = \u0026#34;你好 dc\u0026#34;; // 交换机： 简单模式下用不到，后面的发布订阅模式下用到 // 队列名称 // 额外的参数 // 发送的消息，字节形式 channel.basicPublish(\u0026#34;\u0026#34;, queueName,null,msg.getBytes()); System.out.println(\u0026#34;消息已发送\u0026#34;); channel.close(); // 关闭连接 connection.close(); } } 消费者\nimport com.rabbitmq.client.*; import org.example.util.RabbitUtils; import java.io.IOException; /** * 消费者 */ public class Consumer { public static void main(String[] args) throws IOException { // 获取连接 Connection connection = RabbitUtils.getConnection(); // 获取 chanel Channel channel = connection.createChannel(); // 要绑定的队列，参数同消费者 String queueName = \u0026#34;hello\u0026#34;; channel.queueDeclare(queueName,false,false,false,null); // 接受并处理消息 // 队列名称 // 是否自动确认收到消息，false 代表需要编程手动来确认，这是 MQ 推荐的做法 // 用来处理接收到的消息，是 DefaultConsumer 的实现类 channel.basicConsume(queueName,false,new Reciver(channel)); // 不要关闭连接，要持续等待消息的到来 } } class Reciver extends DefaultConsumer { private Channel channel; public Reciver(Channel channel) { super(channel); this.channel = channel; } @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String msg = new String(body); System.out.println(\u0026#34;消费者收到消息：\u0026#34; + msg); System.out.println(\u0026#34;消息的 TagId: \u0026#34; + envelope.getDeliveryTag()); // 第二个参数： false 代表只确认签收当前的消息，true: 代表签收该消费者所有未签收的消息 channel.basicAck(envelope.getDeliveryTag(),false); } } woker queues 工作队列\n与简单模式相比消费从单个变成了多个，当单个消费者能力不足时，可以增加多个消费者同时去处理队列中的任务。\n消息队列服务对任务的分发有两种方式：1. 轮询机制，挨个给每个消费者分发任务，100 个任务，2 个消费者，每个消费者依次处理 50 个任务。 2. 公平模式： 消费者处理完一个任务时立刻派发新的任务，不关系上个任务是不是该消费者处理。也就是能者多劳。\n展示\n生产者：\nimport com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import org.example.util.RabbitUtils; import java.io.IOException; import java.util.concurrent.TimeoutException; /** * 生产者 */ public class Producer { public static void main(String[] args) throws IOException, TimeoutException, InterruptedException { // 获得连接 Connection connection = RabbitUtils.getConnection(); // 获得 channel Channel channel = connection.createChannel(); String queueName = \u0026#34;hello\u0026#34;; // 声明队列并创建一个队列 // 第一个参数： 队列名称 // 第二个参数：是否持久化队列，不持久化队列时，MQ 重启后，队列中的消息会丢失 // 第三个参数：是否私有化队列，false 代表所有消费者都可以访问，true 代表只有第一次拥有它的消费者才能访问 // 第四个参数：是否自动删除，false 代表连接停掉后不自动删除这个队列 // 其他额外参数： null channel.queueDeclare(queueName,false,false,false, null); // 发送消息 String msg = \u0026#34;你好 dc\u0026#34;; // 交换机： 简单模式下用不到，后面的发布订阅模式下用到 // 队列名称 // 额外的参数 // 发送的消息，字节形式 for (int i = 0; i \u0026lt; 100; i++) { String m1 = msg + i; channel.basicPublish(\u0026#34;\u0026#34;, queueName,null,m1.getBytes()); } System.out.println(\u0026#34;消息已发送\u0026#34;); channel.close(); // 关闭连接 connection.close(); } } 消费者 1\nimport com.rabbitmq.client.*; import org.example.util.RabbitUtils; import java.io.IOException; /** * 消费者 */ public class Consumer1 { public static void main(String[] args) throws IOException { // 获取连接 Connection connection = RabbitUtils.getConnection(); // 获取 chanel Channel channel = connection.createChannel(); // 要绑定的队列，参数同消费者 String queueName = \u0026#34;hello\u0026#34;; channel.queueDeclare(queueName,false,false,false,null); // 处理完一个再取一个。公平模式，默认是轮询机制 // channel.basicQos(1); // 接受并处理消息 // 队列名称 // 是否自动确认收到消息，false 代表需要编程手动来确认，这是 MQ 推荐的做法 // 用来处理接收到的消息 channel.basicConsume(queueName, false, new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String msg = new String(body); System.out.println(\u0026#34;消费者 1 收到消息：\u0026#34; + msg + \u0026#34;。TagId: \u0026#34; + envelope.getDeliveryTag()); try { // 模拟处理任务的耗时 Thread.sleep(100); } catch (InterruptedException e) { throw new RuntimeException(e); } // 第二个参数： false 代表只确认签收当前的消息，true: 代表签收该消费者所有未签收的消息 channel.basicAck(envelope.getDeliveryTag(),false); } }); // 不要关闭连接，要持续等待消息的到来 } } 消费者 2\nimport com.rabbitmq.client.*; import org.example.util.RabbitUtils; import java.io.IOException; /** * 消费者 */ public class Consumer2 { public static void main(String[] args) throws IOException { // 获取连接 Connection connection = RabbitUtils.getConnection(); // 获取 chanel Channel channel = connection.createChannel(); // 要绑定的队列，参数同消费者 String queueName = \u0026#34;hello\u0026#34;; channel.queueDeclare(queueName,false,false,false,null); // 处理完一个再取一个。公平模式，默认是轮询机制 // channel.basicQos(1); // 接受并处理消息 // 队列名称 // 是否自动确认收到消息，false 代表需要编程手动来确认，这是 MQ 推荐的做法 // 用来处理接收到的消息 channel.basicConsume(queueName, false, new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String msg = new String(body); System.out.println(\u0026#34;消费者 2 收到消息：\u0026#34; + msg + \u0026#34;。TagId: \u0026#34; + envelope.getDeliveryTag()); try { // 模拟处理任务的耗时 Thread.sleep(500); } catch (InterruptedException e) { throw new RuntimeException(e); } // 第二个参数： false 代表只确认签收当前的消息，true: 代表签收该消费者所有未签收的消息 channel.basicAck(envelope.getDeliveryTag(),false); } }); // 不要关闭连接，要持续等待消息的到来 } } 轮询机制输出结果：\n不管两个消费者处理消息能力的强弱，被分配的任务一样多：\n消费者 1 收到消息：你好 dc0。TagId: 1 消费者 1 收到消息：你好 dc2。TagId: 2 消费者 1 收到消息：你好 dc4。TagId: 3 消费者 1 收到消息：你好 dc6。TagId: 4 消费者 1 收到消息：你好 dc8。TagId: 5 消费者 1 收到消息：你好 dc10。TagId: 6 消费者 1 收到消息：你好 dc12。TagId: 7 消费者 1 收到消息：你好 dc14。TagId: 8 消费者 1 收到消息：你好 dc16。TagId: 9 消费者 1 收到消息：你好 dc18。TagId: 10 消费者 2 收到消息：你好 dc1。TagId: 1 消费者 2 收到消息：你好 dc3。TagId: 2 消费者 2 收到消息：你好 dc5。TagId: 3 消费者 2 收到消息：你好 dc7。TagId: 4 消费者 2 收到消息：你好 dc9。TagId: 5 消费者 2 收到消息：你好 dc11。TagId: 6 消费者 2 收到消息：你好 dc13。TagId: 7 消费者 2 收到消息：你好 dc15。TagId: 8 消费者 2 收到消息：你好 dc17。TagId: 9 消费者 2 收到消息：你好 dc19。TagId: 10 公平模式下的输出结果：\n消费者 1 处理能力强，单位时间内处理任务数多\n消费者 1 收到消息：你好 dc0。TagId: 1 消费者 1 收到消息：你好 dc2。TagId: 2 消费者 1 收到消息：你好 dc3。TagId: 3 消费者 1 收到消息：你好 dc4。TagId: 4 消费者 1 收到消息：你好 dc5。TagId: 5 消费者 1 收到消息：你好 dc7。TagId: 6 消费者 1 收到消息：你好 dc8。TagId: 7 消费者 1 收到消息：你好 dc9。TagId: 8 消费者 1 收到消息：你好 dc10。TagId: 9 消费者 1 收到消息：你好 dc11。TagId: 10 消费者 1 收到消息：你好 dc13。TagId: 11 消费者 1 收到消息：你好 dc14。TagId: 12 消费者 1 收到消息：你好 dc15。TagId: 13 消费者 1 收到消息：你好 dc16。TagId: 14 消费者 1 收到消息：你好 dc17。TagId: 15 消费者 1 收到消息：你好 dc19。TagId: 16 消费者 1 收到消息：你好 dc20。TagId: 17 消费者 2 处理能力弱，单位时间内处理任务数少\n消费者 2 收到消息：你好 dc1。TagId: 1 消费者 2 收到消息：你好 dc6。TagId: 2 消费者 2 收到消息：你好 dc12。TagId: 3 消费者 2 收到消息：你好 dc18。TagId: 4 消费者 2 收到消息：你好 dc23。TagId: 5 消费者 2 收到消息：你好 dc29。TagId: 6 消费者 2 收到消息：你好 dc35。TagId: 7 消费者 2 收到消息：你好 dc40。TagId: 8 消费者 2 收到消息：你好 dc46。TagId: 9 消费者 2 收到消息：你好 dc52。TagId: 10 消费者 2 收到消息：你好 dc58。TagId: 11 消费者 2 收到消息：你好 dc63。TagId: 12 消费者 2 收到消息：你好 dc69。TagId: 13 消费者 2 收到消息：你好 dc75。TagId: 14 消费者 2 收到消息：你好 dc81。TagId: 15 消费者 2 收到消息：你好 dc86。TagId: 16 消费者 2 收到消息：你好 dc92。TagId: 17 Publish/Subscribe 发布订阅模式\n目前为止，系统中只出现了一个队列，队列中的消息一般只能被消费一次，当一个消费者从队列中取出消息后，其他消费者便无法再次处理该消息了。\n假如有一个天气预报发布系统，有一个气象发布中心以及各大气象软件和门户网站，这些软件和网站都要从气象中心获取天气信息。如下图：\n气象发布中心不可能一个个去对接不同的展示平台，他们是把天气信息按照一定的格式发布到某个地方，其他各大展示平台都从这里获取。现在我们用 RabbitMQ 来设计这套系统，上面介绍的工作队列模式就无法适用了，因为如果一个地方的天气信息被“墨迹天气”取走后，这条消息就没了，肯定不合适。我们自然想到每个天气展示平台都去绑定一个队列，各自从队列中接收气象中发布的消息。而气象中心每次都要向这些队列中依次发布消息。就像下面这样：\n这样是可行，只不过在生产者一端增加了几行代码而已，但是这样做大大的增加了系统的耦合度。每当新增加一个展示平台时，气象中心都要修改代码新增一个推送消息的队列。\n这种一端发布，多端都能收到的场景很像广播站与收音机的关系。只要用户把收音机调到固定频道，就都可以接收到来自该频道的信息。在 RabbitMQ 中也有这种广播模式，而这种广播模式就是依靠交换器来实现的。 如下图：\n这种模式下，生产者只需把消息发送指定交换器上，消费者绑定队列的时候，声明一下队列与交换器的关系即可。生产者在发布消息时，交换器会根据这种绑定关系把消息向所有队列都发送一遍。\n下面来看一下代码实现：\n需要先在RabbitMQ 管理端创建``weather_routing_exchange`交换器 , 类型是 fanout\n气象站\nimport com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import org.example.util.RabbitUtils; import java.io.IOException; import java.util.concurrent.TimeoutException; /** * 气象站 */ public class WeatherStation { public static void main(String[] args) throws IOException, TimeoutException { Connection connection = RabbitUtils.getConnection(); Channel channel = connection.createChannel(); String weather= \u0026#34;{\u0026#39;date\u0026#39;: \u0026#39;2023-04-10\u0026#39;,\u0026#39;location\u0026#39;:\u0026#39;郑州\u0026#39;,\u0026#39;text\u0026#39;: \u0026#39;晴，16℃~28℃\u0026#39;}\u0026#34;; // 第一个参数：指定交换器 // 第二个参数：路由 key, 暂时用不到。 // 第三个参数：额外信息 // 第四个参数：消息内容，字节形式 // 与工作队列模式不同，这里直接发布消息了，没有指定发布到哪个队列中，这是由 RabbitMQ 中的交换器来决定的 channel.basicPublish(\u0026#34;weather_exchange\u0026#34;,\u0026#34;\u0026#34;,null,weather.getBytes()); weather= \u0026#34;{\u0026#39;date\u0026#39;: \u0026#39;2023-04-10\u0026#39;,\u0026#39;location\u0026#39;:\u0026#39;新乡\u0026#39;,\u0026#39;text\u0026#39;: \u0026#39;多云，10℃~26℃\u0026#39;}\u0026#34;; channel.basicPublish(\u0026#34;weather_exchange\u0026#34;,\u0026#34;\u0026#34;,null,weather.getBytes()); channel.close(); connection.close(); } } 墨迹 APP\nimport com.rabbitmq.client.*; import org.example.util.RabbitUtils; import java.io.IOException; /** * 墨迹 APP */ public class MoJiApp { public static void main(String[] args) throws IOException { Connection connection = RabbitUtils.getConnection(); Channel channel = connection.createChannel(); String queueName = \u0026#34;moji\u0026#34;; // 声明一个队列 channel.queueDeclare(queueName,false,false,false, null); // 绑定队列与交换器的关系 channel.queueBind(queueName,\u0026#34;weather_exchange\u0026#34;,\u0026#34;\u0026#34;); channel.basicQos(1); // 监听消息 channel.basicConsume(queueName,false, new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(\u0026#34;墨迹 APP 收到天气信息：\u0026#34; + new String(body)); channel.basicAck(envelope.getDeliveryTag(),false); } }); } } 彩云 APP\nimport com.rabbitmq.client.*; import org.example.util.RabbitUtils; import java.io.IOException; /** * 彩云 APP */ public class CaiYunApp { public static void main(String[] args) throws IOException { Connection connection = RabbitUtils.getConnection(); Channel channel = connection.createChannel(); String queueName = \u0026#34;caiyun\u0026#34;; // 声明一个队列 channel.queueDeclare(queueName,false,false,false, null); // 绑定队列与交换器的关系 channel.queueBind(queueName,\u0026#34;weather_exchange\u0026#34;,\u0026#34;\u0026#34;); channel.basicQos(1); // 监听消息 channel.basicConsume(queueName,false, new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(\u0026#34;彩云 APP 收到天气信息：\u0026#34; + new String(body)); channel.basicAck(envelope.getDeliveryTag(),false); } }); } } 输出：\n墨迹 APP 收到天气信息：{\u0026#39;date\u0026#39;: \u0026#39;2023-04-10\u0026#39;,\u0026#39;location\u0026#39;:\u0026#39;郑州\u0026#39;,\u0026#39;text\u0026#39;: \u0026#39;晴，16℃~28℃\u0026#39;} 墨迹 APP 收到天气信息：{\u0026#39;date\u0026#39;: \u0026#39;2023-04-10\u0026#39;,\u0026#39;location\u0026#39;:\u0026#39;新乡\u0026#39;,\u0026#39;text\u0026#39;: \u0026#39;多云，10℃~26℃\u0026#39;} 彩云 APP 收到天气信息：{\u0026#39;date\u0026#39;: \u0026#39;2023-04-10\u0026#39;,\u0026#39;location\u0026#39;:\u0026#39;郑州\u0026#39;,\u0026#39;text\u0026#39;: \u0026#39;晴，16℃~28℃\u0026#39;} 彩云 APP 收到天气信息：{\u0026#39;date\u0026#39;: \u0026#39;2023-04-10\u0026#39;,\u0026#39;location\u0026#39;:\u0026#39;新乡\u0026#39;,\u0026#39;text\u0026#39;: \u0026#39;多云，10℃~26℃\u0026#39;} 这种发布订阅模式实现了一端发布，多端订阅的功能，一旦客户端绑定了队列与交换器的关系，只要发布在交换器上的消息，该客户端都可以收到。假如墨迹 APP 用户只想关注郑州的天气，彩云 APP 用户只想关注新乡的天气。这又该如何做呢？最先想到的可能就是让气象站按照地区给交换器分类，郑州的天气往郑州的交换器上发，新乡的天气往新乡的交换器上发，等等。但是这样会很麻烦，假如以后国家多了一个省，或者行政区域有变动，岂不是又要气象站修改代码。\nRouting 这时候交换器的路由模式，就派上用场了，生产者在发布一条消息时，可以给该消息指定一个路由 key, 消费者在绑定队列与交换器的关系时，可以指定具有哪一种的路由 key的消息应该发布到该队列里面，而不是一股脑的全部接收该交换器的所有消息。如下图；\n下面看一下代码实现；\n需要先在RabbitMQ 管理端创建``weather_routing_exchange`交换器 , 类型是 direct\n气象站\nimport com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import org.example.util.RabbitUtils; import java.io.IOException; import java.util.ArrayList; import java.util.List; import java.util.concurrent.TimeoutException; /** * 气象站 */ public class WeatherStation { public static void main(String[] args) throws IOException, TimeoutException { Connection connection = RabbitUtils.getConnection(); Channel channel = connection.createChannel(); List\u0026lt;Weather\u0026gt; weatherList = new ArrayList\u0026lt;\u0026gt;(); weatherList.add(new Weather(\u0026#34;zhengzhou\u0026#34;,\u0026#34;郑州晴，16℃~28℃\u0026#34;)); weatherList.add(new Weather(\u0026#34;xinxiang\u0026#34;,\u0026#34;新乡多云，10℃~26℃\u0026#34;)); for (Weather weather : weatherList) { // 指定了路由key， 路由key的格式是地区 channel.basicPublish(\u0026#34;weather_routing_exchange\u0026#34;,weather.getKey(),null,weather.getMsg().getBytes()); } channel.close(); connection.close(); } } class Weather{ private String key; private String msg; public Weather(String key, String msg) { this.key = key; this.msg = msg; } public String getKey() { return key; } public String getMsg() { return msg; } } 墨迹APP\nimport com.rabbitmq.client.*; import org.example.util.RabbitUtils; import java.io.IOException; /** * 墨迹APP */ public class MoJiApp { public static void main(String[] args) throws IOException { Connection connection = RabbitUtils.getConnection(); Channel channel = connection.createChannel(); String queueName = \u0026#34;moji\u0026#34;; // 声明一个队列 channel.queueDeclare(queueName,false,false,false, null); // 绑定队列与交换器的关系,同时指定了路由key是郑州 channel.queueBind(queueName,\u0026#34;weather_routing_exchange\u0026#34;,\u0026#34;zhengzhou\u0026#34;); channel.basicQos(1); // 监听消息 channel.basicConsume(queueName,false, new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(\u0026#34;墨迹APP 收到天气信息: \u0026#34; + new String(body)); channel.basicAck(envelope.getDeliveryTag(),false); } }); } } 彩云APP\nimport com.rabbitmq.client.*; import org.example.util.RabbitUtils; import java.io.IOException; /** * 彩云APP */ public class CaiYunApp { public static void main(String[] args) throws IOException { Connection connection = RabbitUtils.getConnection(); Channel channel = connection.createChannel(); String queueName = \u0026#34;caiyun\u0026#34;; // 声明一个队列 channel.queueDeclare(queueName,false,false,false, null); // 绑定队列与交换器的关系 channel.queueBind(queueName,\u0026#34;weather_routing_exchange\u0026#34;,\u0026#34;xinxiang\u0026#34;); channel.basicQos(1); // 监听消息 channel.basicConsume(queueName,false, new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(\u0026#34;彩云APP 收到天气信息: \u0026#34; + new String(body)); channel.basicAck(envelope.getDeliveryTag(),false); } }); } } 输出内容\n墨迹APP 收到天气信息: 郑州晴，16℃~28℃ 彩云APP 收到天气信息: 新乡多云，10℃~26℃ Topics 这种模式就比较好理解了，上面的 Routing 模式是要求key完全一致，消息才能被交换器投递到队列。而Topics 模式支持统配符的key，在设计key的时候，一般由多个关键词组成，并用.连接，而通配符支持两种模式:\n#: 匹配一个或者多个单词 *: 只匹配一个单词 例如 item.# 可以匹配 item.insert item.insert.abc\nitem.* 只能匹配 item.insert\n这种模式最灵活，用的最多\n总结 简单模式: 一个生产者，一个队列，一个消费者。 工作队列模式: 一个队列，多个消费者。队列种的消息只能被消费一次，可以增加消费者的数量快速处理队列中的任务。队列中的任务派发给消费者的模式有两种: 1. 轮询。 2. 能者多劳 发布订阅模式: 引入交换器，多个队列，多个消费者。实现一端发布，多端都能收到消息 路由模式: 在发布订阅的基础上，给消费者增加了自由选择消息的能力。通过全值匹配路由key实现的 Topics 模式: 在路由模式的基础上，路由key采用通配符的匹配方式， 这种模式最灵活，使用最多 消息的可靠投递 RabbitMQ 通过两个回调方法来让生产者确认消息是否正确投递到队列中，\n消息是否投递到交换器中，调用 confirmListener 消息是否投递到队列中，调用 returnListener 注意：上面两种状态只是生产者与Broker之间的关系，和消费者的是否接收确认消息无关\nconfirmListener 代表生产者将消息投递到Broker时的状态，后续会出现两种情况：\nack 代表数据已经被broker接收， nack 代表broker拒收消息，原因有多种：队列已满，限流，IO异常等\u0026hellip; 示例代码:\n//测试 Confirm 模式 @Test public void testConfirm() { //定义回调 rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() { /** * * @param correlationData 相关配置信息 * @param ack exchange交换机 是否成功收到了消息。true 成功，false代表失败 * @param cause 失败原因 */ @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) { System.out.println(\u0026#34;confirm方法被执行了....\u0026#34;); //ack 为 true表示 消息已经到达交换机 if (ack) { //接收成功 System.out.println(\u0026#34;接收成功消息\u0026#34; + cause); } else { //接收失败 System.out.println(\u0026#34;接收失败消息\u0026#34; + cause); //做一些处理，让消息再次发送。 } } }); //进行消息发送 rabbitTemplate.convertAndSend(\u0026#34;test_exchange_confirm\u0026#34;,\u0026#34;confirm\u0026#34;,\u0026#34;message Confirm...\u0026#34;); } returnListener 代表消息正常被broker接收(ack)，但broker没有对应的队列对消息进行投递，消息被退回给生产者。\n//测试 return模式 @Test public void testReturn() { //设置交换机处理失败消息的模式 为true的时候，消息达到不了 队列时，会将消息重新返回给生产者 rabbitTemplate.setMandatory(true); //定义回调 rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() { /** * * @param message 消息对象 * @param replyCode 错误码 * @param replyText 错误信息 * @param exchange 交换机 * @param routingKey 路由键 */ @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) { System.out.println(\u0026#34;return 执行了....\u0026#34;); System.out.println(\u0026#34;message:\u0026#34;+message); System.out.println(\u0026#34;replyCode:\u0026#34;+replyCode); System.out.println(\u0026#34;replyText:\u0026#34;+replyText); System.out.println(\u0026#34;exchange:\u0026#34;+exchange); System.out.println(\u0026#34;routingKey:\u0026#34;+routingKey); //处理 } }); //进行消息发送 rabbitTemplate.convertAndSend(\u0026#34;test_exchange_confirm\u0026#34;,\u0026#34;confirm\u0026#34;,\u0026#34;message return...\u0026#34;); } ","date":"2023-04-07T23:27:52Z","permalink":"https://dccmmtop.github.io/posts/mq%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/","section":"posts","tags":["rabbitMQ"],"title":"MQ 的基本概念和五种模式使用示例"},{"categories":null,"contents":" 三种清触策略 针对设置了过期时间的 key 针对所有的 key 不处理 LRU 算法（Least Recently Used，最近最少使用） LFU 算法（Least Frequently Used，最不经常使用） 实际应用 三种清触策略 被动清除\n当读写一个已经过期的 key 时，会触发惰性删除策略，直接删除掉这个过期的 key 主动删除\n由于惰性删除无法保证冷数据及时清理，所以 redis 会定期主动淘汰已经过期的部分 key，默认是每 100ms 一次。这里只是部分已过期的 key，所以可能会出现部分 key 已经过期，但没有清理掉的情况，导致内存并没有释放 maxmemory 限定\n当前内存使用超过 maxmemory 限定时，触发主动清理策略 主动清理策略在 redis 4.0 之前一共实现了 6 种内存淘汰算法，4.0 之后，又增加了 2 中，共 8 种。可以按照针对 key 是否设置过期时间分为两大类：\n针对设置了过期时间的 key volatile-ttl: 会针对设置了过期时间的 key，根据过期时间的先后进行清理，越早过期的，越先被删除 volatile-random: 在设置了过期时间的 key 中，随机选择删除 volatile-lru: 会使用 lru 算法来选择设置了过期时间的 key 进行删除 volatile-lfu: 会使用 lfu 算法来选择设置了过期时间的 key 进行删除 针对所有的 key allkeys-random: 从所有的键值对中随机选择并删除 allkeys-lru: 从所有的键值对中使用 lru 算法选择并删除 allkeys-lfu: 从所有的键值对中使用 lfu 算法选择并删除 不处理 noeviction：不会剔除任何数据，拒绝所有写入操作并返回客户端错误信息\u0026quot;(error) OOM command not allowed when used memory\u0026quot;，此时 Redis 只响应读操作。 LRU 算法（Least Recently Used，最近最少使用） 淘汰很久没被访问过的数据，以最近一次访问时间作为参考。\nLFU 算法（Least Frequently Used，最不经常使用） 淘汰最近一段时间被访问次数最少的数据，以次数作为参考。\n实际应用 当存在热点数据时，LRU 的效率很好，但偶发性的、周期性的批量操作会导致 LRU 命中率急剧下降，缓存污染情况比较严重。这时使用 LFU 可能更好点。\n根据自身业务类型，配置好 maxmemory-policy（默认是 noeviction)，推荐使用 volatile-lru。如果不设置最大内存，当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)，会让 Redis 的性能急剧下降。\n当 Redis 运行在主从模式时，只有主结点才会执行过期删除策略，然后把删除操作”del key”同步到从结点删除数据。\n","date":"2023-03-30T08:14:20Z","permalink":"https://dccmmtop.github.io/posts/redis%E5%86%85%E5%AD%98%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5/","section":"posts","tags":["redis"],"title":"Redis 内存清理策略"},{"categories":null,"contents":"目录\n键的设计 value 设计 拒绝 bigkey （强制） bigkey 的危害： bigKey 的产生 如何优化 bigkey 选择合适的数据类型 控制 key 的生命周期，redis 不是垃圾桶 命令的使用 关注元素数量 禁用危险命令 批量操作提升效率 不建议使用自带事务 客户端的使用 避免多个业务使用一个 redis 服务（推荐） 使用带有连接池的客户端 连接池参数含义： 优化建议 maxTotal: 最大连接数，早期叫做 maxActive maxIdle 和 minIdel 连接池预热 系统内核参数优化 vm.swapniess vm.overcommit_memory 合理设置文件句柄数 慢日志查询 键的设计 兼顾可读性和可管理性\n以业务名（或数据库名）为前缀（防止 key 冲突），用冒号分隔，比如业务名：表名：id 简洁性\n保证语义的前提下，控制 key 的长度，当 key 较多时，内存占用也不容忽视，例如： user:{uid}:friends:messages:{mid} 简化为 u:{uid}🇫🇷m:{mid} 不要包含特殊字符（强制） value 设计 拒绝 bigkey （强制） 在 Redis 中，一个字符串最大 512MB，一个二级数据结构（例如 hash、list、set、zset）可以存储大约 40 亿个 (2^32-1) 个元素，但实际中如果下面两种情况，就会认为它是 bigkey。\n字符串类型：它的 big 体现在单个 value 值很大，一般认为超过 10KB 就是 bigkey。 非字符串类型：哈希、列表、集合、有序集合，它们的 big 体现在元素个数太多。 一般来说，string 类型控制在 10KB 以内，hash、list、set、zset 元素个数不要超过 5000。 反例：一个包含 200 万个元素的 list。\n非字符串的 bigkey，不要使用 del 删除，使用 hscan、sscan、zscan 方式渐进式删除，同时要注意防止 bigkey 过期时间自动删除问题（例如一个 200 万的 zset 设置 1 小时过期，会触发 del 操作，造成阻塞）\nbigkey 的危害： 导致 redis 阻塞 网络拥塞\nbigkey 也就意味着每次获取要产生的网络流量较大，假设一个 bigkey 为 1MB，客户端每秒访问量为 1000，那么每秒产生 1000MB 的流量，对于普通的千兆网卡（按照字节算是 128MB/s） 的服务器来说简直是灭顶之灾，而且一般服务器会采用单机多实例的方式来部署，也就是说一个 bigkey 可能会对其他实例也造成影响，其后果不堪设想。 过期删除\n有个 bigkey，它安分守己（只执行简单的命令，例如 hget、lpop、zscore 等），但它设置了过期时间，当它过期后，会被删除，如果没有使用 Redis 4.0 的过期异步删除 (lazyfree-lazy-expire yes)，就会存在阻塞 Redis 的可能性。\nbigkey 的产生： bigKey 的产生 一般来说，bigkey 的产生都是由于程序设计不当，或者对于数据规模预料不清楚造成的，来看几个例子：\n社交类：粉丝列表，如果某些明星或者大 v 不精心设计下，必是 bigkey。 统计类：例如按天存储某项功能或者网站的用户集合，除非没几个人用，否则必是 bigkey。 缓存类：将数据从数据库 load 出来序列化放到 Redis 里，这个方式非常常用，但有两个地方需要注意，第一，是不是有必要把所有字段都缓存；第二，有没有相关关联的数据，有的同学为了图方便把相关数据都存一个 key 下，产生 bigkey。 如何优化 bigkey 拆\nbig list： list1、list2、\u0026hellip;listN 就是把大列表分成几个小列表存储\nbig hash：可以讲数据分段存储，比如一个大的 key，假设存了 1 百万的用户数据，可以拆分成 200 个 key，每个 key 下面存放 5000 个用户数据\n如果 bigkey 不可避免，也要思考一下要不要每次把所有元素都取出来（例如有时候仅仅需要 hmget，而不是 hgetall)，删除也是一样，尽量使用优雅的方式来处理。\n选择合适的数据类型 控制 key 的生命周期，redis 不是垃圾桶 建议使用 expire 设置过期时间（条件允许可以打散过期时间，防止集中过期）。\n命令的使用 关注元素数量 例如 hgetall、lrange、smembers、zrange、sinter 等并非不能使用，但是需要明确 N 的值。有遍历的需求可以使用 hscan、sscan、zscan 代替。\n禁用危险命令 禁止线上使用 keys、flushall、flushdb 等，通过 redis 的 rename 机制禁掉命令，或者使用 scan 的方式渐进式处理。\n批量操作提升效率 原生命令：例如 mget、mset。 非原生命令：可以使用 pipeline 提高效率。 不建议使用自带事务 Redis 事务功能较弱，不建议过多使用，可以用 lua 替代\n客户端的使用 避免多个业务使用一个 redis 服务（推荐） 多个不相干的业务应该使用不同的 redis 服务，公共数据做服务化\n使用带有连接池的客户端 可以有效控制连接数量，同时提高效率，标准使用方式如下：\nJedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMaxTotal(5); jedisPoolConfig.setMaxIdle(2); jedisPoolConfig.setTestOnBorrow(true); JedisPool jedisPool = new JedisPool(jedisPoolConfig, \u0026#34;192.168.0.60\u0026#34;, 6379, 3000, null); Jedis jedis = null; try { jedis = jedisPool.getResource(); //具体的命令 jedis.executeCommand() } catch (Exception e) { logger.error(\u0026#34;op key {} error: \u0026#34; + e.getMessage(), key, e); } finally { //注意这里不是关闭连接，在 JedisPool 模式下，Jedis 会被归还给资源池。 if (jedis != null) jedis.close(); } 连接池参数含义： 序号 参数名 含义 默认值 使用建议 1 maxTotal 资源池中最大连接数 8 设置建议见下面 2 maxIdle 资源池允许最大空闲的连接数 8 设置建议见下面 3 minIdle 资源池确保最少空闲的连接数 0 设置建议见下面 4 blockWhenExhausted 当资源池用尽后，调用者是否要等待。只有当为 true 时，下面的 maxWaitMillis 才会生效 true 建议使用默认值 5 maxWaitMillis 当资源池连接用尽后，调用者的最大等待时间（单位为毫秒） -1：表示永不超时 不建议使用默认值 6 testOnBorrow 向资源池借用连接时是否做连接有效性检测 (ping)，无效连接会被移除 false 业务量很大时候建议设置为 false（多一次 ping 的开销）。 7 testOnReturn 向资源池归还连接时是否做连接有效性检测 (ping)，无效连接会被移除 false 业务量很大时候建议设置为 false（多一次 ping 的开销）。 8 jmxEnabled 是否开启 jmx 监控，可用于监控 true 建议开启，但应用本身也要开启 优化建议 maxTotal: 最大连接数，早期叫做 maxActive 如何设置这个值是比较难回答的，没有固定的计算方式，考虑的因素比较多：\n业务希望 redis 的并发量 客户端执行命令的时间 redis 资源，应用的个数 * maxTotal \u0026lt; redis 的最大连接数 maxclients 资源开销，例如虽然希望控制空闲连接（连接池此刻可马上使用的连接），但是不希望因为连接池的频繁释放创建连接造成不必靠开销。 例子：\n假设：一次命令时间（borrow|return resource + Jedis 执行命令（含网络） ）的平均耗时约为 1ms，一个连接的 QPS 大约是 1000, 业务期望的 QPS 是 50000\n那么理论上需要的资源池大小是 50000 / 1000 = 50 个。但事实上这是个理论值，还要考虑到要比理论值预留一些资源，通常来讲 maxTotal 可以比理论值大一些。\n但这个值不是越大越好，一方面连接太多占用客户端和服务端资源，另一方面对于 Redis 这种高 QPS 的服务器，一个大命令的阻塞即使设置再大资源池仍然会无济于事。\nmaxIdle 和 minIdel maxIdle 实际上才是业务需要的最大连接数，maxTotal 是为了给出结余量，所以 maxIdele 不要设置的太小，否则会不断的发生新建连接，释放连接的开销。\n连接池的最佳性能是 maxTotal = maxIdle 这样就避免连接池伸缩带来的性能干扰，但是在并发量不大的时候，或者 maxTotal 设置过高，会导致不必要的连接资源浪费，一般推荐 maxIdle 按照上面的计算方式设置，maxTotal 可以再放大一倍。\nminIdle（最小空闲连接数），与其说是最小空闲连接数，不如说是\u0026quot;至少需要保持的空闲连接数\u0026quot;，在使用连接的过程中，如果连接数超过了 minIdle，那么继续建立连接，如果超过了 maxIdle，当超过的连接执行完业务后会慢慢被移出连接池释放掉\n连接池预热 如果系统刚启动完，就马上有很多请求过来，那么可以给连接池做预热，比如快速的创建一些 redis 连接，执行简单命令，如 ping, 快速的将连接池中的连接提升到 minIdel 的数量\n示例代码：\nList\u0026lt;Jedis\u0026gt; minIdleJedisList = new ArrayList\u0026lt;Jedis\u0026gt;(jedisPoolConfig.getMinIdle()); for (int i = 0; i \u0026lt; jedisPoolConfig.getMinIdle(); i++) { Jedis jedis = null; try { jedis = pool.getResource(); minIdleJedisList.add(jedis); jedis.ping(); } catch (Exception e) { logger.error(e.getMessage(), e); } finally { //注意，这里不能马上 close 将连接还回连接池，否则最后连接池里只会建立 1 个连接。 //jedis.close(); } } //统一将预热的连接还回连接池 for (int i = 0; i \u0026lt; jedisPoolConfig.getMinIdle(); i++) { Jedis jedis = null; try { jedis = minIdleJedisList.get(i); //将连接归还回连接池，注意这里是规范连接，而不是关闭 jedis.close(); } catch (Exception e) { logger.error(e.getMessage(), e); } finally { } } 系统内核参数优化 vm.swapniess swap 对操作系统比较重要，当物理内存不足时，可以将一部分内存也 swap 到硬盘上，已解燃眉之急，对于需要高并发，高吞吐的应用来说，磁盘 IO 通常会成为系统的瓶颈，再 Linux 中，并不是等所有的物理内存使用完后才用到 swap。系统参数 swapniess 决定了操作系统使用 swap 的倾向程度，swapniess 取值范围是 0-100，值越大，说明操作系统使用 swapniess 的概率越高。值越低，表示操作系统更加倾向于使用物理内存。\n如果内核版本 \u0026lt; 3.5, 那么 swapniess 的值为 0，系统宁愿 swap 也不会 oom killer （杀掉进程）\n如果内核版本 \u0026gt;= 3.5, 那么 swapniess 的值为 1，系统宁愿 swap 也不会 oom killer （杀掉进程）\nOOM killer 机制是指 Linux 操作系统发现可用内存不足时，强制杀死一些用户进程（非内核进程），来保证系统有足够的可用内存进行分配。\n一般要保证 redis 不会被 kill\n先查看系统版本，再设值\ncat /proc/version echo 1 \u0026gt; /proc/sys/vm/swapniess echo vm.swapniess=1 \u0026gt;\u0026gt; /etc/sysctl.conf vm.overcommit_memory 默认值是 0\n0: 表示内核将检查是否有足够的物理内存供进程使用（而不是检查是否用尽），如果有足够的内存，内存申请允许，否则内存申请失败，并把错误返回给应用进程 1: 表示内核允许分配所有的物理内存，而不管当前内存状态如何 如果是 0 的话，可能导致类似于 fork 等操作失败，申请不到足够的内存空间\nRedis 建议把这个值设置为 1，就是为了让 fork 再低内存下也能运行\ncat /proc/sys/vm/overcommit_memory echo \u0026#34;vm.overcommit_memory=1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl vm.overcommit_memory=1 合理设置文件句柄数 操作系统进程试图打开一个文件（或者叫句柄），但是现在进程打开的句柄数已经达到了上限，继续打开会报错：“Too many open files”\nulimit -a #查看系统文件句柄数，看 open files 那项 ulimit -n 65535 #设置系统文件句柄数 慢日志查询 Redis 慢日志命令说明：\nconfig get slow* #查询有关慢日志的配置信息 config set slowlog-log-slower-than 20000 #设置慢日志使时间阈值，单位微秒，此处为 20 毫秒，即超过 20 毫秒的操作都会记录下来，生产环境建议设置 1000，也就是 1ms，这样理论上 redis 并发至少达到 1000，如果要求单机并发达到 1 万以上，这个值可以设置为 100 config set slowlog-max-len 1024 #设置慢日志记录保存数量，如果保存数量已满，会删除最早的记录，最新的记录追加进来。记录慢查询日志时 Redis 会对长命令做截断操作，并不会占用大量内存，建议设置稍大些，防止丢失日志 config rewrite #将服务器当前所使用的配置保存到 redis.conf slowlog len #获取慢查询日志列表的当前长度 slowlog get 5 #获取最新的 5 条慢查询日志。慢查询日志由四个属性组成：标识 ID，发生时间戳，命令耗时，执行命令和参数 slowlog reset #重置慢查询日志 ","date":"2023-03-27T23:11:51Z","permalink":"https://dccmmtop.github.io/posts/redis%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83%E4%B8%8E%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","section":"posts","tags":["redis"],"title":"redis 使用规范与性能优化"},{"categories":null,"contents":"为了能够在 Windows 上使用系统剪贴板，您需要安装win32yank应用程序。您可以从这里下载。之后，您需要将剪贴板设置\nvim 脚本 set clipboard+=unnamedplus lua 脚本 vim.opt.clipboard = \u0026#34;unnamedplus\u0026#34; ","date":"2023-03-27T22:29:41Z","permalink":"https://dccmmtop.github.io/posts/%E4%B8%8E%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%89%AA%E8%B4%B4%E6%9D%BF%E4%BA%92%E9%80%9A/","section":"posts","tags":["vim"],"title":"nvim与操作系统剪贴板互通"},{"categories":null,"contents":" 缓存穿透 缓存穿透问题解决方案 缓存空对象 布隆过滤器 redisson 实现布隆过滤器 引入依赖 预先放入数据 使用 缓存击穿 缓存雪崩 预防和解决方案 热点key的重建优化 缓存穿透 缓存穿透指的是查询一个根本不存在的数据，缓存层和存储层都不会命中，通常处于容错的考虑，如果存储层不存在数据也不会写入缓存层。这样就导致每次请求都要去存储层查询，失去了缓存保护后端存储的意义。\n造成缓存穿透的原因有两个:\n自身业务代码或者数据出现问题 一些恶意攻击，爬虫等造成大量的空命中 缓存穿透问题解决方案 缓存空对象 String get(String key) { // 从缓存中获取数据 String cacheValue = cache.get(key); // 缓存为空 if (StringUtils.isBlank(cacheValue)) { // 从存储中获取 String storageValue = storage.get(key); cache.set(key, storageValue); // 如果存储数据为空， 需要设置一个过期时间(300秒) if (storageValue == null) { cache.expire(key, 60 * 5); } return storageValue; } else { // 缓存非空 return cacheValue; } } 布隆过滤器 对于恶意攻击，向服务器请求大量不存在的数据造成的缓存穿透可以先用布隆过滤器做一次筛选，对于不存在数据，布隆过滤器一般都能够过滤掉，不让请求再继续往后端走。\n布隆过滤器的特性是：当它说这个值存在，那么这个值可能存在，也可能不存在；当它说不存在时，这个值一定不存在\n它的这种特性是由它的算法和数据结构决定的：\n如上图所示，布隆过滤器就是一个大型的位数组和几个不一样的无偏hash函数，所谓无偏，就是能够把元素的hash值计算的比较均匀。工作原理如下：\n向布隆过滤器添加key时，会使用多个hash函数对key进行hash运算，得出一个整数索引值，然后对数组长度进行取模运算，得到在数组中的位置。每个hash函数都会算的一个不同的位置，再把为位数组的这几个位置都置为1，就完成了add操作\n那么为什么需要多个hash函数呢？先带着问题看一下布隆过滤器如何判断一个值是否存在:\n当检查一个元素是否在布隆过滤器中时，同样会将该元素哈希成多个不同的位数组下标，然后检查这些下标对应的位是否都为1。如果有任何一个位为0，则可以确定该元素一定不存在于布隆过滤器中；如果所有位都为1，则该元素可能存在于布隆过滤器中，但也可能是一个假阳性（false positive）。\n设置多个hash函数主要是为了减少假阳性的概率。当只使用一个哈希函数时，会有可能出现多个元素被哈希到同一个位数组下标的情况，从而导致误判。而使用多个哈希函数可以让元素分布在更多的位数组下标上，从而减少假阳性的概率。当使用k个哈希函数时，假阳性的概率为(1 - \\frac{1}{m})^{kn}，其中m是位数组的长度，n是元素数量。\n因此，使用多个哈希函数可以提高布隆过滤器的准确性和可靠性。\n综上述可以得知，布隆过滤器的可靠性大致由 hash 函数的个数，以及位数组的长度决定。hash 函数的个数计算有些复杂，暂按下不表。而位数组的长度一般我们不能直接设定，而是先设置成业务中待查找样本的总量，布隆过滤器会自动调整位数组的长度。这个长度一般很大，过亿都是正常的，因为它是按位存储的，一亿的长度所占的空间也就 12M。\n这种方法适用于数据命中不高、 数据相对固定、 实时性低（通常是数据集较大） 的应用场景， 代码维护较为复杂， 但是缓存空间占用很少。\nredisson 实现布隆过滤器 引入依赖 dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.6.5\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 预先放入数据 使用布隆过滤器需要把所有数据提前放入布隆过滤器，并且在增加数据时也要往布隆过滤器里放，但是布隆过滤器不支持删除数据，如果原始数据删除比较多，需要定期重置布隆过滤器，以保持较高的正确性\n/初始化布隆过滤器 RBloomFilter\u0026lt;String\u0026gt; bloomFilter = redisson.getBloomFilter(\u0026#34;nameList\u0026#34;); //初始化布隆过滤器：预计元素为100000000L,误差率为3% bloomFilter.tryInit(100000000L,0.03); //把所有数据存入布隆过滤器 void init(){ keys = [\u0026#34;zhangsan\u0026#34;,\u0026#34;lisi\u0026#34;,\u0026#34;wanger\u0026#34; ....] for (String key: keys) { bloomFilter.put(key); } } 使用 public class RedissonBloomFilter { public static void main(String[] args) { Config config = new Config(); config.useSingleServer().setAddress(\u0026#34;redis://localhost:6379\u0026#34;); //构造Redisson RedissonClient redisson = Redisson.create(config); RBloomFilter\u0026lt;String\u0026gt; bloomFilter = redisson.getBloomFilter(\u0026#34;nameList\u0026#34;); //初始化布隆过滤器：预计元素为100000000L,误差率为3%,根据这两个参数会计算出底层的bit数组大小 bloomFilter.tryInit(100000000L,0.03); //判断下面号码是否在布隆过滤器中 System.out.println(bloomFilter.contains(\u0026#34;sa\u0026#34;));//false System.out.println(bloomFilter.contains(\u0026#34;jlsd\u0026#34;));//false System.out.println(bloomFilter.contains(\u0026#34;zhangsan\u0026#34;));//true } } 缓存击穿 由于大批量缓存在同一时间失效可能导致大量得请求同时穿透缓存直达访问数据库，可能会造成数据库瞬间压力过大甚至挂掉。为应对这种情况，我们一般将某批缓存在过期的时间基础上再加一个短暂的随机值，避免同时失效, 伪代码如下:\nString get(String key) { // 从缓存中获取数据 String cacheValue = cache.get(key); // 缓存为空 if (StringUtils.isBlank(cacheValue)) { // 从存储中获取 String storageValue = storage.get(key); cache.set(key, storageValue); //设置一个过期时间(300到600之间的一个随机数) int expireTime = new Random().nextInt(300) + 300; if (storageValue == null) { cache.expire(key, expireTime); } return storageValue; } else { // 缓存非空 return cacheValue; } } 缓存雪崩 缓存雪崩指的是缓存层支撑不住请求或者宕机了，流量会直接冲击到后端存储层。\n由于缓存层承载这大量的请求，有效的保护了存储层，但是如果某些原因导致缓存层不能提供正常的服务，如：\n超大并发，缓存层支撑不住 缓存设计不好，类似于大量请求 bigkey,导致缓存层服务性能急剧下降\n这样就导致大量的请求都会打到后端，造成存储层宕机 预防和解决方案 保证缓存服务的高可用，使用哨兵架构，或者集群架构 依赖隔离组件为后端请求限流熔断并降级，比如使用 sentinel 或者 Hystrix 限流降级组件 比如服务降级，我们可以针对不同的数据采取不同的处理方式。当业务应用访问的是非核心数据（例如电商商品属性，用户信息等）时，暂时停止从缓存中查询这些数据，而是直接返回预定义的默认降级信息、空值或是错误提示信息；当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。\n提前演练。在项目上线前， 演练缓存层宕掉后， 应用以及后端的负载情况以及可能出现的问题， 在此基础上做一些预案设定。 热点key的重建优化 对于缓存的使用一般是 \u0026ldquo;缓存 + 过期时间\u0026quot;的方式，既能提高接口的并发，也能在一定的时间内保证数据最新。这种方式基本能满足大部分需求，但是下面两个问题同时出现时，可能会对应用造成致命的伤害:\n当前key热度很高，并发量很大 重建缓存的过程比较耗时 这种情况下缓存失效的瞬间，有大量的线程去重建缓存，造成后端负载过大，甚至导致宕机。解决这个问题的关键是要避免大量线程同时重建缓存：\n可以使用互斥锁来解决，此方法只允许一个线程重建缓存，其他线程等待，然后重新获取最新数据，伪代码如下:\nString get(String key) { // 从Redis中获取数据 String value = redis.get(key); // 如果value为空， 则开始重构缓存 if (value == null) { // 只允许一个线程重建缓存， 使用nx， 并设置过期时间ex String mutexKey = \u0026#34;mutext🔑\u0026#34; + key; if (redis.set(mutexKey, \u0026#34;1\u0026#34;, \u0026#34;ex 180\u0026#34;, \u0026#34;nx\u0026#34;)) { // 从数据源获取数据 value = db.get(key); // 回写Redis， 并设置过期时间 redis.setex(key, timeout, value); // 删除key_mutex redis.delete(mutexKey); }// 其他线程休息50毫秒后重试 else { Thread.sleep(50); get(key); } } return value; } ","date":"2023-03-27T07:59:48Z","permalink":"https://dccmmtop.github.io/posts/redis%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E5%92%8C%E7%A9%BF%E9%80%8F%E4%BB%A5%E5%8F%8A%E9%9B%AA%E5%B4%A9/","section":"posts","tags":["redis"],"title":"Redis缓存击穿和穿透以及雪崩"},{"categories":null,"contents":"Redis 的主从架构 主从架构搭建步骤 1. 添加从节点 复制一份 redis.conf 文件，将相关配置修改如下：\n# 如果在不同机器上部署，端口可以不用修改\rport 6380\r# 把 pid 进程号写入 pidfile 配置的文件\rpidfile /var/run/redis_6380.pid logfile \u0026#34;6380.log\u0026#34;\r# 指定数据存放目录\rdir /usr/local/redis-5.0.3/data/6380 # 需要注释掉 bind\r# bind 127.0.0.1（bind 绑定的是自己机器网卡的 ip，如果有多块网卡可以配多个 ip，代表允许客户端通过机器的哪些网卡 ip 去访问，内网一般可以不配置 bind，注释掉即可）\r# 配置主从复制\r# 从 192.168.2.10 6379 的 redis 实例复制数据，Redis 5.0 之前使用 slaveof\rreplicaof 192.168.2.10 6379 # 配置从节点只读\rreplica-read-only yes 2. 启动从节点 redis-server redis.conf\n3. 连接从节点 redis-cli -p 6380\n4. 测试 在主节点 6379 上写如数据，然后在 6380 上看看是否能读取到\n5. 同理，再添加一个 6381 节点 Redis 主从工作原理 如果为一个 master 配置一个 slave ，不管这个 slave 是否第一次连接上 Master， 它都会发送一个 PSYNC 命令给 master， 请求复制数据。 master 接收到 PSYNC 命令后，会在后台通过 bgsave 命令，生成最新的 rdb 快照文件，在持久化期间，master 会继续接收客户端请求，把可能修改数据的请求记录在内存中，master 会把 rdb 文件发给 slave，salve 接收到 rdb 文件后，保存在本地，然后加载到内存中。 之后，master 会把刚刚记录在内存中的修改数据的命令再发给 slave， slave 再依次执行这些命令。达到数据一致。 当 master 与 slave 之间因为网络问题而断开时， slave 能够自动连接到 master。 如果 master 收到多个 slave 的连接请求，它只会进行一次持久化动作，而不是每个连接一次，然后再把这持久化文件发送给各个 slave。 主从全量复制的流程 数据的部分复制 当主节点与从节点断开重连后，一般都会进行全量的数据复制，从 2.8 版本开始，redis 可以支持部分数据复制的命令与 master 同步，也就是断点续传。\n主节点会在内存中维护一个复制数据用的缓存队列，这个队列保存这最近一段时间的数据，master 和 slave 都会维护复制数据的下标 offset，和 master 的进程 id。因此当网络重连后，slave 会请求 master 继续未完成的复制，从所记录的下标位置开始，如果 master 的进程 id 变了，或者在 master 中的缓存队列中找不到这个下标，（意味着从节点的下标 offset 太旧了） 那么将会进行一次全量的数据复制。\n流程图如下：\n主从复制风暴 如果有很多个从节点，多个从节点同时从主节点复制数据，导致主节点压力过大，可以做如下阶梯式架构，让部分从节点从其他从节点复制数据：\nRedis 哨兵高可用架构 sentinel 哨兵是特殊的 redis 服务，不提供读写服务，主要用来监控 redis 节点。\n哨兵架构下的 client 端第一次从哨兵中找出主节点，后续就直接访问 redis 的主节点，不会每次都通过哨兵代理访问主节点，当 redis 的主节点发生变化时，哨兵会第一时间感知到，并将新的 redis 主节点通知给客户端，这里的 redis 客户端一般都实现了订阅功能，订阅哨兵发布的节点变动信息。\n哨兵架构搭建步骤 复制一份 sentinel.conf 文件\ncp sentinel.conf sentinel-26379.conf 修改相关配置 port 26379\rdaemonize yes\rpidfile \u0026#34;/var/run/redis-sentinel-26379.pid\u0026#34;\rlogfile \u0026#34;26379.log\u0026#34;\rdir \u0026#34;/usr/local/redis-5.0.3/data\u0026#34;\r# sentinel monitor \u0026lt;master-redis-name\u0026gt; \u0026lt;master-redis-ip\u0026gt; \u0026lt;master-redis-port\u0026gt; \u0026lt;quorum\u0026gt;\r# quorum 是一个数字，指明当有多少个 sentinel 认为一个 master 失效时（值一般为：sentinel 总数/2 + 1)，master 才算真正失效\rsentinel monitor mymaster 192.168.1.32 6379 2 # mymaster 这个名字随便取，客户端访问时会用到 启动哨兵实例 src/redis-sentinel sentinel-26379.conf 连接 redis 客户端执行 info 命令查看哨兵信息 再配置另外两个 sentinel, 端口为 26380， 26381 哨兵集群启动完毕后，会将哨兵集群的元信息写入所有的 sentinel 的配置文件中，例如：\nsentinel known-replica mymaster 192.168.0.60 6380 #代表 redis 主节点的从节点信息 sentinel known-replica mymaster 192.168.0.60 6381 #代表 redis 主节点的从节点信息 sentinel known-sentinel mymaster 192.168.0.60 26380 52d0a5d70c1f90475b4fc03b6ce7c3c56935760f #代表感知到的其它哨兵节点 sentinel known-sentinel mymaster 192.168.0.60 26381 e9f530d3882f8043f76ebb8e1686438ba8bd5ca6 #代表感知到的其它哨兵节点 如果主节点（6379）宕机，哨兵集群会重新选举新的主节点，同时修改所有 sentinel 节点配置文件的集群元信息， 同时还会修改 sentinel 配置文件中的 mymaster 对应的 6379 端口，改为 6380:\nsentinel monitor mymaster 192.168.0.60 6380 2 当 6379 节点再次启动时，哨兵集群根据集群元信息就可以将 6379 作为从节点加入集群中。\nRedis 集群 TODO:\njava 使用 Jedis 连接集群 public class JedisSentinelTest { public static void main(String[] args) throws IOException { JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(20); config.setMaxIdle(10); config.setMinIdle(5); String masterName = \u0026#34;mymaster\u0026#34;; Set\u0026lt;String\u0026gt; sentinels = new HashSet\u0026lt;String\u0026gt;(); sentinels.add(new HostAndPort(\u0026#34;192.168.1.32\u0026#34;,26379).toString()); sentinels.add(new HostAndPort(\u0026#34;192.168.1.32\u0026#34;,26380).toString()); sentinels.add(new HostAndPort(\u0026#34;192.168.1.32\u0026#34;,26381).toString()); //JedisSentinelPool 其实本质跟 JedisPool 类似，都是与 redis 主节点建立的连接池 //JedisSentinelPool 并不是说与 sentinel 建立的连接池，而是通过 sentinel 发现 redis 主节点并与其建立连接 JedisSentinelPool jedisSentinelPool = new JedisSentinelPool(masterName, sentinels, config, 3000, null); Jedis jedis = null; try { jedis = jedisSentinelPool.getResource(); System.out.println(jedis.set(\u0026#34;sentinel\u0026#34;, \u0026#34;dc\u0026#34;)); System.out.println(jedis.get(\u0026#34;sentinel\u0026#34;)); } catch (Exception e) { e.printStackTrace(); } finally { //注意这里不是关闭连接，在 JedisPool 模式下，Jedis 会被归还给资源池。 if (jedis != null) jedis.close(); } } } spting-boot 连接集群 引入依赖 dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置信息 server: port: 8080 spring: redis: database: 0 timeout: 3000 sentinel: #哨兵模式 master: mymaster #主服务器所在集群名称 nodes: 192.168.1.32:26379,192.168.1.32:26380,192.168.1.32:26381 lettuce: pool: max-idle: 50 min-idle: 10 max-active: 100 max-wait: 1000 访问代码 @RestController public class IndexController { private static final Logger logger = LoggerFactory.getLogger(IndexController.class); @Autowired private StringRedisTemplate stringRedisTemplate; /** * 测试节点挂了哨兵重新选举新的 master 节点，客户端是否能动态感知到 * 新的 master 选举出来后，哨兵会把消息发布出去，客户端实际上是实现了一个消息监听机制， * 当哨兵把新 master 的消息发布出去，客户端会立马感知到新 master 的信息，从而动态切换访问的 masterip * * @throws InterruptedException */ @RequestMapping(\u0026#34;/test_sentinel\u0026#34;) public void testSentinel() throws InterruptedException { int i = 1; while (true){ try { stringRedisTemplate.opsForValue().set(\u0026#34;zhuge\u0026#34;+i, i+\u0026#34;\u0026#34;); System.out.println(\u0026#34;设置 key：\u0026#34;+ \u0026#34;zhuge\u0026#34; + i); i++; Thread.sleep(1000); }catch (Exception e){ logger.error(\u0026#34;错误：\u0026#34;, e); } } } } StringRedisTemplate 与 RedisTemplate 详解 spring 封装了 RedisTemplate 对象来进行对 redis 的各种操作，它支持所有的 redis 原生的 api。在 RedisTemplate 中提供了几个常用的接口方法的使用，分别是：\nprivate ValueOperations\u0026lt;K, V\u0026gt; valueOps; private HashOperations\u0026lt;K, V\u0026gt; hashOps; private ListOperations\u0026lt;K, V\u0026gt; listOps; private SetOperations\u0026lt;K, V\u0026gt; setOps; private ZSetOperations\u0026lt;K, V\u0026gt; zSetOps; // RedisTemplate 中定义了对 5 种数据结构操作 redisTemplate.opsForValue();//操作字符串 redisTemplate.opsForHash();//操作 hash redisTemplate.opsForList();//操作 list redisTemplate.opsForSet();//操作 set redisTemplate.opsForZSet();//操作有序 set StringRedisTemplate 继承自 RedisTemplate，也一样拥有上面这些操作。\nStringRedisTemplate 默认采用的是 String 的序列化策略，保存的 key 和 value 都是采用此策略序列化保存的。\nRedisTemplate 默认采用的是 JDK 的序列化策略，保存的 key 和 value 都是采用此策略序列化保存的。\nRedis 客户端命令对应的 RedisTemplate 中的方法列表：\nString 类型结构 Redis RedisTemplate rt set key value rt.opsForValue().set(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;value\u0026rdquo;) get key rt.opsForValue().get(\u0026ldquo;key\u0026rdquo;) del key rt.delete(\u0026ldquo;key\u0026rdquo;) strlen key rt.opsForValue().size(\u0026ldquo;key\u0026rdquo;) getset key value rt.opsForValue().getAndSet(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;value\u0026rdquo;) getrange key start end rt.opsForValue().get(\u0026ldquo;key\u0026rdquo;,start,end) append key value rt.opsForValue().append(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;value\u0026rdquo;) Hash 结构 hmset key field1 value1 field2 value2\u0026hellip; rt.opsForHash().putAll(\u0026ldquo;key\u0026rdquo;,map) //map 是一个集合对象 hset key field value rt.opsForHash().put(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;field\u0026rdquo;,\u0026ldquo;value\u0026rdquo;) hexists key field rt.opsForHash().hasKey(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;field\u0026rdquo;) hgetall key rt.opsForHash().entries(\u0026ldquo;key\u0026rdquo;) //返回 Map 对象 hvals key rt.opsForHash().values(\u0026ldquo;key\u0026rdquo;) //返回 List 对象 hkeys key rt.opsForHash().keys(\u0026ldquo;key\u0026rdquo;) //返回 List 对象 hmget key field1 field2\u0026hellip; rt.opsForHash().multiGet(\u0026ldquo;key\u0026rdquo;,keyList) hsetnx key field value rt.opsForHash().putIfAbsent(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;field\u0026rdquo;,\u0026ldquo;value\u0026rdquo; hdel key field1 field2 rt.opsForHash().delete(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;field1\u0026rdquo;,\u0026ldquo;field2\u0026rdquo;) hget key field rt.opsForHash().get(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;field\u0026rdquo;) List 结构 lpush list node1 node2 node3\u0026hellip; rt.opsForList().leftPush(\u0026ldquo;list\u0026rdquo;,\u0026ldquo;node\u0026rdquo;) rt.opsForList().leftPushAll(\u0026ldquo;list\u0026rdquo;,list) //list 是集合对象 rpush list node1 node2 node3\u0026hellip; rt.opsForList().rightPush(\u0026ldquo;list\u0026rdquo;,\u0026ldquo;node\u0026rdquo;) rt.opsForList().rightPushAll(\u0026ldquo;list\u0026rdquo;,list) //list 是集合对象 lindex key index rt.opsForList().index(\u0026ldquo;list\u0026rdquo;, index) llen key rt.opsForList().size(\u0026ldquo;key\u0026rdquo;) lpop key rt.opsForList().leftPop(\u0026ldquo;key\u0026rdquo;) rpop key rt.opsForList().rightPop(\u0026ldquo;key\u0026rdquo;) lpushx list node rt.opsForList().leftPushIfPresent(\u0026ldquo;list\u0026rdquo;,\u0026ldquo;node\u0026rdquo;) rpushx list node rt.opsForList().rightPushIfPresent(\u0026ldquo;list\u0026rdquo;,\u0026ldquo;node\u0026rdquo;) lrange list start end rt.opsForList().range(\u0026ldquo;list\u0026rdquo;,start,end) lrem list count value rt.opsForList().remove(\u0026ldquo;list\u0026rdquo;,count,\u0026ldquo;value\u0026rdquo;) lset key index value rt.opsForList().set(\u0026ldquo;list\u0026rdquo;,index,\u0026ldquo;value\u0026rdquo;) Set 结构 sadd key member1 member2\u0026hellip; rt.boundSetOps(\u0026ldquo;key\u0026rdquo;).add(\u0026ldquo;member1\u0026rdquo;,\u0026ldquo;member2\u0026rdquo;,\u0026hellip;) rt.opsForSet().add(\u0026ldquo;key\u0026rdquo;, set) //set 是一个集合对象 scard key rt.opsForSet().size(\u0026ldquo;key\u0026rdquo;) sidff key1 key2 rt.opsForSet().difference(\u0026ldquo;key1\u0026rdquo;,\u0026ldquo;key2\u0026rdquo;) //返回一个集合对象 sinter key1 key2 rt.opsForSet().intersect(\u0026ldquo;key1\u0026rdquo;,\u0026ldquo;key2\u0026rdquo;)//同上 sunion key1 key2 rt.opsForSet().union(\u0026ldquo;key1\u0026rdquo;,\u0026ldquo;key2\u0026rdquo;)//同上 sdiffstore des key1 key2 rt.opsForSet().differenceAndStore(\u0026ldquo;key1\u0026rdquo;,\u0026ldquo;key2\u0026rdquo;,\u0026ldquo;des\u0026rdquo;) sinter des key1 key2 rt.opsForSet().intersectAndStore(\u0026ldquo;key1\u0026rdquo;,\u0026ldquo;key2\u0026rdquo;,\u0026ldquo;des\u0026rdquo;) sunionstore des key1 key2 rt.opsForSet().unionAndStore(\u0026ldquo;key1\u0026rdquo;,\u0026ldquo;key2\u0026rdquo;,\u0026ldquo;des\u0026rdquo;) sismember key member rt.opsForSet().isMember(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;member\u0026rdquo;) smembers key rt.opsForSet().members(\u0026ldquo;key\u0026rdquo;) spop key rt.opsForSet().pop(\u0026ldquo;key\u0026rdquo;) srandmember key count rt.opsForSet().randomMember(\u0026ldquo;key\u0026rdquo;,count) srem key member1 member2\u0026hellip; rt.opsForSet().remove(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;member1\u0026rdquo;,\u0026ldquo;member2\u0026rdquo;,\u0026hellip;) ","date":"2023-03-23T08:01:20Z","permalink":"https://dccmmtop.github.io/posts/redis%E7%9A%84%E4%B8%BB%E4%BB%8E%E5%92%8C%E5%93%A8%E5%85%B5%E4%BB%A5%E5%8F%8A%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84/","section":"posts","tags":["redis"],"title":"Redis 的主从和哨兵以及集群架构"},{"categories":null,"contents":"redis 的单线程和和高性能 redis 是单线程的吗 Redis 的单线程主要是指网络 IO 和键值对读写是由一个线程完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如：持久化，异步删除，集群数据同步等，其实是由额外的线程执行的。\nRedis 单线程为什么还能这么快？ 因为它的所有数据都在内存中，所有的运算都是内存级别的运算，而且避免了多线程上下文切换的损耗。\n正因为 Redis 是单线程的，使用的过程中一定要注意避免阻塞操作，比如 在 key 数量非常多的情况下使用 keys *， 这会阻塞后续所有 redis 对 key 的操作。\nRedis 单线程如何处理多客户端并发连接 Redis 的 IO 多路复用，redis 利用 epoll 来实现 IO 多路复用，将连接信息和事件放到队列中，依次放到文件事件分派器，事件分派器将事件分发给事件处理器。\n如图：\nredis 支持的最大连接数 在配置文件中 redis.conf :\nmaxlients 6000 也可以在 redis-cli 中查看：\n127.0.0.1:6379\u0026gt; CONFIG GET maxclients \u0026#34;maxclients\u0026#34; \u0026#34;6000\u0026#34; 查看服务运行信息 info 命令可以查看 redis 服务运行信息，分为 9 大块，分别是：\nServer 服务器运行参数 Clients 客户端相关信息 Memory 服务器运行内存统计数据 Persistence 持久化信息 Stats 通用统计数据 Replication 主从复制相关信息 CPU CPU 使用情况 Cluster 集群信息 KeySpace 键值对统计数量信息 下面简要介绍一些信息：\nconnected_clients:2 # 正在连接的客户端数量 instantaneous_ops_per_sec:789 # 每秒执行多少次指令 used_memory:929864 # Redis 分配的内存总量 (byte)，包含 redis 进程内部的开销和数据占用的内存 used_memory_human:908.07K # Redis 分配的内存总量 (Kb，human 会展示出单位） used_memory_rss_human:2.28M # 向操作系统申请的内存大小 (Mb)（这个值一般是大于 used_memory 的，因为 Redis 的内存分配策略会产生内存碎片） used_memory_peak:929864 # redis 的内存消耗峰值 (byte) used_memory_peak_human:908.07K # redis 的内存消耗峰值 (KB) maxmemory:0 # 配置中设置的最大可使用内存值 (byte), 默认 0, 不限制，一般配置为机器物理内存的百分之七八十，需要留一部分给操作系统 maxmemory_human:0B # 配置中设置的最大可使用内存值 maxmemory_policy:noeviction # 当达到 maxmemory 时的淘汰策略 Redis 持久化 RDB 快照 默认情况下，redis 将内存数据库快照保存在名字为 dump.rbd 的二进制文件中\n可以通过配置文件设置 \u0026ldquo;N 秒内至少有 M 个改动\u0026rdquo; 条件触发时，自动保存一次数据，配置项是 save, 如：\nsave 60 1000 这个配置让 redis 在满足 “60 秒内至少有 1000 个键改动” 条件时，自动保存一次数据集\n还可以手动执行命令生成 RDB 快照，进入 reids 客户端命令执行 save 或者 bgsave 可以生成 dump.rbd 文件，每次执行命令都会将内存中的数据保存到一个新的 rdb 文件中，并覆盖原有的 rdb 快照文件\n如果想关闭 RDB 快照功能，直接将此配置注释即可\nbgsave 的写时复制机制 (COW) reids 借助操作系统提供的写时复制技术 (Copy-On-Write) 在生成快照的同时，依然可以进行写命令。实现机制是：bgsave 是由子进程操作的，它有主线程 fork 生成，可以共享主线程的所有内存数据，bgsave 子进程生成后，它开始读取主线程的内存数据，并把他们写入 RDB 文件。 此时 如果主线程对数据都是读操作，那么主线程和子进程互不影响，但是如果主线程要修改一块数据，那么这个数据就会被复制一份，生成该数据的副本，bgsave 子进程会把这个副本数据写入 RDB 文件，这个过程中，主进程仍然可以直接修改原来的数据。\n在配置文件指定 save 60 1000 用的是 basave 的方式\nsave 与 bgsave 的对比 save bgsave IO 类型 同步 异步 是否阻塞 redis 其它命令 是 否 复杂度 O(n) O(n) 优点 不会消耗额外的内存 不阻塞客户端命令 缺点 阻塞客户端命令 需要 fork 子进程，消耗内存 AOF(append-only file) 快照功能并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， **那么服务器将丢失 最近写入、且仍未保存到快照中的那些数据。**从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化，将修改的每一条指令记录进文件 appendonly.aof 中（先写入 os cache，每隔一段时间 fsync 到磁盘）\n比如执行命令set dc 666，aof 文件里会记录如下数据\n1 *3 2 $3 3 set 4 $5 5 dc 6 $3 7 666 这是一种 resp 协议格式数据，星号后面的数字代表命令有多少个参数，$号后面的数字代表这个参数有几个字符，注意：如果执行带过期时间的 set 命令，aof 文件里记录的是并不是执行的原始命令，而是记录 key 过期的时间戳\n可以修改配置文件来打开 AOF 功能：\nappendonly yes 当 Redis 重新启动时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的，由于 redis 不是直接把命令写入磁盘，而是先写入 os cache, 隔一段时间才持久化到磁盘中，故而可以配置 Redis 多久才将数据 fsync 得到磁盘一次，有三种选择：\nappenfsync always # 每次有新命令追加到 AOF 文件时，就执行一次 fsync， 非常慢也非常安全 appenfsync everysec # 每秒 fsnc 一次，足够快，并且指挥丢失 1 秒中的数据。 appenfsync no # 从不 fsync 依赖操作系统处理，够快，但不够安全 推荐每秒一次，也是默认的配置， 兼顾速度和安全\nAOF 重写 加入对一个 key 进行 N 次累加： incr viewCnt, AOF 文件中就会记录 N 次这个命令，其实只需记录 incr ViewCnt N 即可。 所以 reids 会定期对 AOF 中的命令重写，以便将来可以更快的恢复数据。\n有两个配置可以控制重写的频率：\nauto‐aof‐rewrite‐min‐size 64mb # aof 文件至少要达到 64M 才会自动重写，文件太小恢复速度本来就 很快，重写的意义不大 auto‐aof‐rewrite‐percentage 100 # aof 文件自上一次重写后文件大小增长了 100%则再次触发重写 当让 AOF 重写动作还可以手动触发，在 redis-cli 中执行 bgrewriteaof, AOF 重写会 fork 出一个子进程去做，和 bgsave 类似，不会对 redis 执行正常的命令有太多的影响。\nRDB 与 AOF 的对比 RDB AOF 启动优先级 低 高 文件体积 小 大 恢复速度 快 慢 数据安全性 容易丢数据 根据策略决定 在生产环境都可以启用，如果既有 RBD 文件，也有 AOF 文件，则优先选择 AOF 文件来恢复数据，因为相对来说数据更安全一些\nRedis 4.0 混合持久化 重启 redis 时，很少会使用 rdb 文件来恢复数据，因为会丢失大量数据，通常会通过 AOF 日志重放，但是重放 AOF 日志的性能会比 RDB 来说慢的多，尤其在 redis 中数据很大的情况下启动要花费很长时间， redis 为了解决这个问题，带来了一个新的持久化选项——混合持久化。通过下面的配置可以开启混合持久化，注意：必须先开启 aof 持久化：\naof-use-rdb-premable yes 如果开启了混合持久化，AOF 在重写时，不再是单纯将内存数据转换为 RESP 命令写入 AOF 文件，而是将重写这一刻之前的内存做 RDB 快照处理，并且将 RDB 快照内容和增量的 AOF 修改内存数据的命令存在一起，都写入新的 AOF 文件，新的文件一开始不叫 appendonly.aof，等到重写完新的 AOF 文件才会进行改名，覆盖原有的 AOF 文件，完成新旧两个 AOF 文件的替换。 于是在 Redis 重启的时候，可以先加载 RDB 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，因此重启效率大幅得到提升。\n混合持久化AOF文件结构如下:\nRedis 数据备份策略 写crontab定时调度脚本，每小时都copy一份rdb或aof的备份到一个目录中去，仅仅保留最近48 小时的备份 每天都保留一份当日的数据备份到一个目录中去，可以保留最近1个月的备份 每次copy备份的时候，都把太旧的备份给删了 每天晚上将当前机器上的备份复制一份到其他机器上，以防机器损坏 ","date":"2023-03-22T22:58:08Z","permalink":"https://dccmmtop.github.io/posts/redis%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/","section":"posts","tags":["redis"],"title":"redis基础配置和持久化"},{"categories":null,"contents":"为什么需要渐进式遍历key 有时候需要从 Redis 实例成千上万的 key 中找出特定前缀的 key 列表来手动处理数据，可能是修改它的值，也可能是删除 key。这里就有一个问题，如何从海量的 key 中找出满足特定前缀的 key 列表来？\nRedis 提供了一个简单暴力的指令 keys 用来列出所有满足特定正则字符串规则的 key。\n!redis-cli keys key67* 1) \u0026#34;key6764\u0026#34; 2) \u0026#34;key6738\u0026#34; 3) \u0026#34;key6774\u0026#34; 4) \u0026#34;key673\u0026#34; 5) \u0026#34;key6710\u0026#34; 6) \u0026#34;key6759\u0026#34; 7) \u0026#34;key6715\u0026#34; 8) \u0026#34;key6746\u0026#34; 9) \u0026#34;key6796\u0026#34; 这个指令使用非常简单，提供一个简单的正则字符串即可，但是有很明显的两个缺点。 没有 offset、limit 参数，一次性吐出所有满足条件的 key，万一实例中有几百 w 个 key 满足条件， 当你看到满屏的字符串刷的没有尽头时，你就知道难受了。\nkeys 算法是遍历算法，复杂度是 O(n)，如果实例中有千万级以上的 key，这个指令就会导致 Redis 服务卡顿， 所有读写 Redis 的其它的指令都会被延后甚至会超时报错， 因为 Redis 是单线程程序，顺序执行所有指令，其它指令必须等到当前的 keys 指令执行完了才可以继续。\n建议生产环境屏蔽 keys 命令\nscan Redis 为了解决这个问题，它在 2.8 版本中加入了指令——scan。\nscan 相比 keys 具备有以下特点：\n复杂度虽然也是 O(n)，但是它是通过游标分步进行的，不会阻塞线程； 提供 limit 参数，可以控制每次返回结果的最大条数，limit 只是对增量式迭代命令的一种提示 (hint)，返回的结果可多可少； 同 keys 一样，它也提供模式匹配功能； 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数； 返回的结果可能会有重复，需要客户端去重复，这点非常重要； 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的； 单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零 scan 基础使用 SCAN cursor [MATCH pattern] [COUNT count]\n初始执行 scan 命令例如 scan 0。SCAN 命令是一个基于游标的迭代器。 这意味着命令每次被调用都需要使用上一次这个调用返回的游标作为该次调用的游标参数，以此来延续之前的迭代过程。\n当 SCAN 命令的游标参数被设置为 0 时，服务器将开始一次新的迭代，而当 redis 服务器向用户返回值为 0 的游标时， 表示迭代已结束，这是唯一迭代结束的判定方式，而不能通过返回结果集是否为空判断迭代结束。\nscan 参数提供了三个参数，第一个是 cursor 整数值，第二个是 key 的正则模式，第三个是遍历的 limit hint。\n第一次遍历时，cursor 值为 0，然后将返回结果中第一个整数值作为下一次遍历的 cursor。 一直遍历到返回的 cursor 值为 0 时结束。\n!redis-cli scan 0 match key99* count 1000 1) \u0026#34;13912\u0026#34; # 返回的游标 2) 1) \u0026#34;key997\u0026#34; 2) \u0026#34;key9906\u0026#34; 3) \u0026#34;key9957\u0026#34; 4) \u0026#34;key9902\u0026#34; 5) \u0026#34;key9971\u0026#34; 6) \u0026#34;key9935\u0026#34; 7) \u0026#34;key9958\u0026#34; 8) \u0026#34;key9928\u0026#34; 9) \u0026#34;key9931\u0026#34; 10) \u0026#34;key9961\u0026#34; 11) \u0026#34;key9948\u0026#34; 12) \u0026#34;key9965\u0026#34; 13) \u0026#34;key9937\u0026#34; !redis-cli scan 13912 match key99* count 1000 1) \u0026#34;5292\u0026#34; # 返回的游标 2) 1) \u0026#34;key996\u0026#34; 2) \u0026#34;key9960\u0026#34; 3) \u0026#34;key9973\u0026#34; 4) \u0026#34;key9978\u0026#34; 5) \u0026#34;key9927\u0026#34; 6) \u0026#34;key995\u0026#34; 7) \u0026#34;key9992\u0026#34; 8) \u0026#34;key9993\u0026#34; 9) \u0026#34;key9964\u0026#34; 10) \u0026#34;key9934\u0026#34; 返回结果分为两个部分：第一部分即 1) 就是下一次迭代游标，第二部分即 2) 就是本次迭代结果集。\n从上面的过程可以看到虽然提供的 limit 是 1000，但是返回的结果只有 10 个左右。 因为这个 limit 不是限定返回结果的数量，而是限定服务器单次遍历的字典槽位数量（约等于）。 如果将 limit 设置为 10，你会发现返回结果是空的，但是游标值不为零，意味着遍历还没结束。\n如下:\n!redis-cli scan 0 match key99* count 10 1) \u0026#34;15360\u0026#34; 2) (empty list or set) !redis-cli scan 15360 match key99* count 10 1) \u0026#34;2304\u0026#34; 2) (empty list or set) 更多的 scan 指令 scan 指令是一系列指令，除了可以遍历所有的 key 之外，还可以对指定的容器集合进行遍历。 zscan 遍历 zset 集合元素， hscan 遍历 hash 字典的元素、 sscan 遍历 set 集合的元素。 注意点：\nSSCAN 命令、 HSCAN 命令和 ZSCAN 命令的第一个参数总是一个数据库键。 而 SCAN 命令则不需要在第一个参数提供任何数据库键 —— 因为它迭代的是当前数据库中的所有数据库键。\n大 key 扫描 有时候会因为业务人员使用不当，在 Redis 实例中会形成很大的对象，比如一个很大的 hash，一个很大的 zset 这都是经常出现的。\n这样的对象对 Redis 的集群数据迁移带来了很大的问题，因为在集群环境下，如果某个 key 太大，会让数据导致迁移卡顿。 另外在内存分配上，如果一个 key 太大，那么当它需要扩容时，会一次性申请更大的一块内存，这也会导致卡顿。 如果这个大 key 被删除，内存会一次性回收，卡顿现象会再一次产生。\n在平时的业务开发中，要尽量避免大 key 的产生。 如果你观察到 Redis 的内存大起大落，这极有可能是因为大 key 导致的，这时候你就需要定位出具体是那个 key， 进一步定位出具体的业务来源，然后再改进相关业务代码设计。\n那如何定位大 key 呢？ 为了避免对线上 Redis 带来卡顿，这就要用到 scan 指令，对于扫描出来的每一个 key，使用 type 指令获得 key 的类型， 然后使用相应数据结构的 size 或者 len 方法来得到它的大小，对于每一种类型，保留大小的前 N 名作为扫描结果展示出来。\n上面这样的过程需要编写脚本，比较繁琐，不过 Redis 官方已经在 redis-cli 指令中提供了这样的扫描功能，我们可以直接拿来即用。\n!redis-cli --bigkeys # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest string found so far \u0026#39;key316\u0026#39; with 3 bytes [00.00%] Biggest string found so far \u0026#39;key7806\u0026#39; with 4 bytes [12.79%] Biggest zset found so far \u0026#39;salary\u0026#39; with 1 members [13.19%] Biggest string found so far \u0026#39;counter:__rand_int__\u0026#39; with 6 bytes [13.50%] Biggest hash found so far \u0026#39;websit\u0026#39; with 2 fields [14.37%] Biggest set found so far \u0026#39;bbs\u0026#39; with 3 members [14.67%] Biggest hash found so far \u0026#39;website\u0026#39; with 3 fields [30.41%] Biggest list found so far \u0026#39;mylist\u0026#39; with 100000 items [95.53%] Biggest zset found so far \u0026#39;page_rank\u0026#39; with 3 members -------- summary ------- Sampled 10019 keys in the keyspace! Total key length in bytes is 68990 (avg len 6.89) Biggest string found \u0026#39;counter:__rand_int__\u0026#39; has 6 bytes Biggest list found \u0026#39;mylist\u0026#39; has 100000 items Biggest set found \u0026#39;bbs\u0026#39; has 3 members Biggest hash found \u0026#39;website\u0026#39; has 3 fields Biggest zset found \u0026#39;page_rank\u0026#39; has 3 members 10011 strings with 38919 bytes (99.92% of keys, avg size 3.89) 3 lists with 100003 items (00.03% of keys, avg size 33334.33) 1 sets with 3 members (00.01% of keys, avg size 3.00) 2 hashs with 5 fields (00.02% of keys, avg size 2.50) 2 zsets with 4 members (00.02% of keys, avg size 2.00) 如果你担心这个指令会大幅抬升 Redis 的 ops 导致线上报警，还可以增加一个休眠参数。\n!redis-cli --bigkeys -i 0.1 # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest string found so far \u0026#39;key316\u0026#39; with 3 bytes [00.00%] Biggest string found so far \u0026#39;key7806\u0026#39; with 4 bytes [12.79%] Biggest zset found so far \u0026#39;salary\u0026#39; with 1 members [13.19%] Biggest string found so far \u0026#39;counter:__rand_int__\u0026#39; with 6 bytes [13.50%] Biggest hash found so far \u0026#39;websit\u0026#39; with 2 fields [14.37%] Biggest set found so far \u0026#39;bbs\u0026#39; with 3 members [14.67%] Biggest hash found so far \u0026#39;website\u0026#39; with 3 fields [30.41%] Biggest list found so far \u0026#39;mylist\u0026#39; with 100000 items [95.53%] Biggest zset found so far \u0026#39;page_rank\u0026#39; with 3 members -------- summary ------- Sampled 10019 keys in the keyspace! Total key length in bytes is 68990 (avg len 6.89) Biggest string found \u0026#39;counter:__rand_int__\u0026#39; has 6 bytes Biggest list found \u0026#39;mylist\u0026#39; has 100000 items Biggest set found \u0026#39;bbs\u0026#39; has 3 members Biggest hash found \u0026#39;website\u0026#39; has 3 fields Biggest zset found \u0026#39;page_rank\u0026#39; has 3 members 10011 strings with 38919 bytes (99.92% of keys, avg size 3.89) 3 lists with 100003 items (00.03% of keys, avg size 33334.33) 1 sets with 3 members (00.01% of keys, avg size 3.00) 2 hashs with 5 fields (00.02% of keys, avg size 2.50) 2 zsets with 4 members (00.02% of keys, avg size 2.00) 上面这个指令每隔 100 条 scan 指令就会休眠 0.1s，ops 就不会剧烈抬升，但是扫描的时间会变长。\n需要注意的是，这个 bigkeys 得到的最大，不一定是最大。\n说明原因前，首先说明 bigkeys 的原理，非常简单，通过 scan 命令遍历，各种不同数据结构的 key，分别通过不同的命令得到最大的 key：\n如果是 string 结构，通过 strlen 判断；\n如果是 list 结构，通过 llen 判断；\n如果是 hash 结构，通过 hlen 判断；\n如果是 set 结构，通过 scard 判断；\n如果是 sorted set 结构，通过 zcard 判断。\n正因为这样的判断方式，虽然 string 结构肯定可以正确的筛选出最占用缓存，也可以说最大的 key。\n但是 list 不一定，例如，现在有两个 list 类型的 key，分别是：numberlist–[0,1,2]，stringlist–[“123456789123456789”]， 由于通过 llen 判断，所以 numberlist 要大于 stringlist。 而事实上 stringlist 更占用内存。其他三种数据结构 hash，set，sorted set 都会存在这个问题。 使用 bigkeys 一定要注意这一点。\nslowlog 命令 上面提到不能使用 keys 命令，如果就有开发这么做了呢，我们如何得知？ 与其他任意存储系统例如 mysql，mongodb 可以查看慢日志一样，redis 也可以，即通过命令 slowlog。\n用法如下\nSLOWLOG subcommand [argument] subcommand 主要有：\nget，用法：slowlog get [argument]，获取 argument 参数指定数量的慢日志。\nlen，用法：slowlog len，总慢日志数量。\nreset，用法：slowlog reset，清空慢日志。\n!redis-cli slowlog get 5 1) 1) (integer) 2 2) (integer) 1537786953 3) (integer) 17980 4) 1) \u0026#34;scan\u0026#34; 2) \u0026#34;0\u0026#34; 3) \u0026#34;match\u0026#34; 4) \u0026#34;key99*\u0026#34; 5) \u0026#34;count\u0026#34; 6) \u0026#34;1000\u0026#34; 5) \u0026#34;127.0.0.1:50129\u0026#34; 6) \u0026#34;\u0026#34; 2) 1) (integer) 1 2) (integer) 1537785886 3) (integer) 39537 4) 1) \u0026#34;keys\u0026#34; 2) \u0026#34;*\u0026#34; 5) \u0026#34;127.0.0.1:49701\u0026#34; 6) \u0026#34;\u0026#34; 3) 1) (integer) 0 2) (integer) 1537681701 3) (integer) 18276 4) 1) \u0026#34;ZADD\u0026#34; 2) \u0026#34;page_rank\u0026#34; 3) \u0026#34;10\u0026#34; 4) \u0026#34;google.com\u0026#34; 5) \u0026#34;127.0.0.1:52334\u0026#34; 6) \u0026#34;\u0026#34; 命令耗时超过多少才会保存到 slowlog 中，可以通过命令 config set slowlog-log-slower-than 2000 配置并且不需要重启 redis。 注意：单位是微妙，2000 微妙即 2 毫秒。\n禁用危险命令 rename-command 为了防止把问题带到生产环境，我们可以通过配置文件重命名一些危险命令，\n例如 keys 等一些高危命令。操作非常简单，\n只需要在 conf 配置文件增加如下所示配置即可：\nrename-command flushdb flushddbb\rrename-command flushall flushallall\rrename-command keys keysys ","date":"2023-03-22T07:30:03Z","permalink":"https://dccmmtop.github.io/posts/scan/","section":"posts","tags":["redis"],"title":"Redis中渐进式遍历key"},{"categories":null,"contents":"查看项目依赖 gradle: gradle dependencies\nmaven: mvn dependency:tree\n排除依赖 不需要某个依赖或者需要替换掉某依赖时，就需要把这个依赖排除:\n以Spring Boot的Web起步依赖为例，它传递依赖了Jackson JSON库。如果你正在构建一个生产或消费JSON资源表述的REST服务，那它会很有用。但是，要构建传统的面向人类用户的Web应用程序，你可能用不上Jackson。虽然把它加进来也不会有什么坏处，但排除掉它的传递依赖，可以为你的项目瘦身。\ngradle: compile(\u0026#34;org.springframework.boot:spring-boot-starter-web\u0026#34;) { exclude group: \u0026#39;com.fasterxml.jackson.core\u0026#39; } maven \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.core\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 另一方面，也许项目需要Jackson，但你需要用另一个版本的Jackson来进行构建，而不是Web起步依赖里的那个。假设Web起步依赖引用了Jackson 2.3.4，但你需要使用2.4.33。在Maven里，你可以直接在pom.xml中表达诉求，就像这样：\nmaven: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.core\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-databind\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Maven总是会用最近的依赖，也就是说，你在项目的构建说明文件里增加的这个依赖，会覆盖传递依赖引入的另一个依赖。\ngradle\n假如要替换的版本更新。直接写新版本即可: compile(\u0026#34;com.fasterxml.jackson.core:jackson-databind:2.4.3\u0026#34;) 假如要替换的是一个旧版本,需要把原有更新的版本排除，再添加旧版本的包:\ncompile(\u0026#34;org.springframework.boot:spring-boot-starter-web\u0026#34;) { exclude group: \u0026#39;com.fasterxml.jackson.core\u0026#39; } compile(\u0026#34;com.fasterxml.jackson.core:jackson-databind:2.3.1\u0026#34;) 不管什么情况，在覆盖Spring Boot起步依赖引入的传递依赖时都要多加小心。虽然不同的版本放在一起也许没什么问题，但你要知道，起步依赖中各个依赖版本之间的兼容性都经过了精心的测试。应该只在特殊的情况下覆盖这些传递依赖（比如新版本修复了一个bug）。\n读取配置 Spring Boot能从多种属性源获得属性，包括如下几处。\n命令行参数 java:comp/env 里的JNDI属性 JVM系统属性 操作系统环境变量 随机生成的带random.* 前缀的属性（在设置其他属性时，可以引用它们，比如${random.long} ） 应用程序以外的application.properties或者appliaction.yml文件 打包在应用程序内的application.properties或者appliaction.yml文件 通过@PropertySource 标注的属性源 默认属性 这个列表按照优先级排序，也就是说，任何在高优先级属性源里设置的属性都会覆盖低优先级的相同属性。例如，命令行参数会覆盖其他属性源里的属性。\napplication.properties和application.yml文件能放在以下四个位置。\n外置，在相对于应用程序运行目录的/config子目录里。 外置，在应用程序运行的目录里。 内置，在config包内。 内置，在Classpath根目录。 ","date":"2022-12-30T14:42:18Z","permalink":"https://dccmmtop.github.io/posts/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E5%A4%87%E5%BF%98/","section":"posts","tags":["spring"],"title":"常用工具备忘"},{"categories":null,"contents":"声明切点@Poincut 把切点声明成一个方法，便于重用\n@Poincut 的使用格式如下：\n@Poincut(\u0026#34;PCD\u0026#34;) // 切点表达式 表示对哪些方法进行增强 public void pc(){} // 切点签名，返回值必须为 void 10 种切点表达式 AspectJ 的切点指示符 AspectJ pointcut designators (PCD) ，也就是俗称的切点表达式，Spring 中支持 10 种，如下表：\n表达式类型 作用 匹配规则 execution 用于匹配方法执行的连接点 within 用于匹配指定类型内的方法执行 within(x) 匹配规则 target.getClass().equals(x) this 用于匹配当前 AOP 代理对象类型的执行方法，包含引入的接口类型匹配 this(x) 匹配规则：x.getClass.isAssingableFrom(proxy.getClass) target 用于匹配当前目标对象类型的执行方法，不包括引入接口的类型匹配 target(x) 匹配规则：x.getClass().isAssignableFrom(target.getClass()); args 用于匹配当前执行的方法传入的参数为指定类型的执行方法 传入的目标位置参数。getClass().equals(@args（对应的参数位置的注解类型）)!= null @target 用于匹配当前目标对象类型的执行方法，其中目标对象持有指定的注解 target.class.getAnnotation（指定的注解类型） != null @args 用于匹配当前执行的方法传入的参数持有指定注解的执行 传入的目标位置参数。getClass().getAnnotation(@args（对应的参数位置的注解类型）)!= null @within 用于匹配所有持有指定注解类型内的方法 被调用的目标方法 Method 对象。getDeclaringClass().getAnnotation(within 中指定的注解类型） != null @annotation 用于匹配当前执行方法持有指定注解的方法 target.getClass().getMethod(\u0026ldquo;目标方法名\u0026rdquo;).getDeclaredAnnotation(@annotation（目标注解）)!=null bean Spring AOP 扩展的，AspectJ 没有对应的指示符，用于匹配特定名称的 Bean 对象的执行方法 ApplicationContext.getBean(\u0026ldquo;bean 表达式中指定的 bean 名称\u0026rdquo;) != null 简单介绍下 AspectJ 中常用的 3 个通配符：\n*：匹配任何数量字符 ..：匹配任何数量字符的重复，如任何数量子包，任何数量方法参数 +：匹配指定类型及其子类型，仅作为后缀防过载类型模式后面。 execution 用于匹配方法执行，最常用。\n格式说明 execution(modifiers-pattern? ret-type-pattern declaring-type-pattern?name-pattern(param-pattern) throws-pattern?) 其中带 ?号的 modifiers-pattern?，declaring-type-pattern?，throws-pattern?是可选项 ret-type-pattern,name-pattern, parameters-pattern是必选项 modifier-pattern? 修饰符匹配，如 public 表示匹配公有方法，*表示任意修饰符 ret-type-pattern 返回值匹配，* 表示任何返回值，全路径的类名等 declaring-type-pattern? 类路径匹配 name-pattern 方法名匹配，* 代表所有，xx*代表以 xx 开头的所有方法 (param-pattern) 参数匹配，指定方法参数（声明的类型），(..)代表所有参数，(*,String)代表第一个参数为任何值，第二个为 String 类型，(..,String)代表最后一个参数是 String 类型 throws-pattern? 异常类型匹配 举例说明 public class PointcutExecution { // com.crab.spring.aop.demo02 包下任何类的任意方法 @Pointcut(\u0026#34;execution(* com.crab.spring.aop.demo02.*.*(..))\u0026#34;) public void m1(){} // com.crab.spring.aop.demo02 包及其子包下任何类的任意方法 @Pointcut(\u0026#34;execution(* com.crab.spring.aop.demo02..*.*(..))\u0026#34;) public void m2(){} // com.crab.spring.aop 包及其子包下 IService 接口的任意无参方法 @Pointcut(\u0026#34;execution(* com.crab.spring.aop..IService.*(..))\u0026#34;) public void m3(){} // com.crab.spring.aop 包及其子包下 IService 接口及其子类型的任意无参方法 @Pointcut(\u0026#34;execution(* com.crab.spring.aop..IService+.*(..))\u0026#34;) public void m4(){} // com.crab.spring.aop.demo02.UserService 类中有且只有一个 String 参数的方法 @Pointcut(\u0026#34;execution(* com.crab.spring.aop.demo02.UserService.*(String))\u0026#34;) public void m5(){} // com.crab.spring.aop.demo02.UserService 类中参数个数为 2 且最后一个参数类型是 String 的方法 @Pointcut(\u0026#34;execution(* com.crab.spring.aop.demo02.UserService.*(*,String))\u0026#34;) public void m6(){} // com.crab.spring.aop.demo02.UserService 类中最后一个参数类型是 String 的方法 @Pointcut(\u0026#34;execution(* com.crab.spring.aop.demo02.UserService.*(..,String))\u0026#34;) public void m7(){} } within 格式说明 within（类型表达式）：目标对象 target 的类型是否和 within 中指定的类型匹配\n匹配规则： target.getClass().equals(within 表达式中指定的类型）\n举例说明 public class PointcutWithin { // 匹配 com.crab.spring.aop.demo02 包及其子包下任何类的任何方法 @Pointcut(\u0026#34;within(com.crab.spring.aop.demo02..*)\u0026#34;) public void m() { } // 匹配 m.crab.spring.aop.demo02 包及其子包下 IService 类型及其子类型的任何方法 @Pointcut(\u0026#34;within(com.crab.spring.aop.demo02..IService+)\u0026#34;) public void m2() { } // 匹配 com.crab.spring.aop.demo02.UserService 类中所有方法，不含其子类 @Pointcut(\u0026#34;within(com.crab.spring.aop.demo02.UserService)\u0026#34;) public void m3() { } } this 格式说明 this（类型全限定名）：通过 aop 创建的代理对象的类型是否和 this 中指定的类型匹配；this 中使用的表达式必须是类型全限定名，不支持通配符。\nthis(x) 的匹配规则是：x.getClass.isAssingableFrom(proxy.getClass) 举例说明 package com.crab.spring.aop.demo02.aspectj; @Aspect public class PointcutThis { interface I1{ void m(); } static class C1 implements I1{ @Override public void m() { System.out.println(\u0026#34;C1 m()\u0026#34;); } } // 匹配 I1 类型或是其子类 @Pointcut(\u0026#34;this(com.crab.spring.aop.demo02.aspectj.PointcutThis.I1)\u0026#34;) public void pc(){} @Before(\u0026#34;pc()\u0026#34;) public void before(JoinPoint joinPoint) { System.out.println(\u0026#34;before: \u0026#34; + joinPoint); } public static void main(String[] args) { C1 target = new C1(); AspectJProxyFactory proxyFactory = new AspectJProxyFactory(); proxyFactory.setTarget(target); // proxyFactory.setProxyTargetClass(true); // 获取 C1 上所有接口 spring 工具类提供的方法 Class\u0026lt;?\u0026gt;[] allInterfaces = ClassUtils.getAllInterfaces(target); // 设置代理接口 proxyFactory.setInterfaces(allInterfaces); // 添加切面 proxyFactory.addAspect(PointcutThis.class); // 获取代理 I1 proxy = proxyFactory.getProxy(); // 调用方法 proxy.m(); System.out.println(\u0026#34;JDK 代理？\u0026#34; + AopUtils.isJdkDynamicProxy(proxy)); System.out.println(\u0026#34;CGLIB 代理？\u0026#34; + AopUtils.isCglibProxy(proxy)); //判断代理对象是否是 C1 类型的 System.out.println(C1.class.isAssignableFrom(proxy.getClass())); } } 来观察下输出\nbefore: execution(void com.crab.spring.aop.demo02.aspectj.PointcutThis$C1.m()) C1 m() JDK 代理？false CGLIB 代理？true true 使用 JDK 动态代理生成的代理对象，其类型是 I1 类型。\n思考下：将切点表达式改成下面的输出结果是？\n// 匹配 C1 类型或是其子类\n@Pointcut(\u0026ldquo;this(com.crab.spring.aop.demo02.aspectj.PointcutThis.C1)\u0026rdquo;)\npublic void pc(){}\ntarget 格式说明 target（类型全限定名）：判断目标对象的类型是否和指定的类型匹配；表达式必须是类型全限定名，不支持通配符。\ntarget(x) 匹配规则：x.getClass().isAssignableFrom(target.getClass()); 举例说明 @Aspect public class PointcutTarget { interface I1{ void m(); } static class C1 implements I1{ @Override public void m() { System.out.println(\u0026#34;C1 m()\u0026#34;); } } // 匹配目标类型必须是 @Pointcut(\u0026#34;target(com.crab.spring.aop.demo02.aspectj.PointcutTarget.C1)\u0026#34;) public void pc(){} @Before(\u0026#34;pc()\u0026#34;) public void before(JoinPoint joinPoint) { System.out.println(\u0026#34;before: \u0026#34; + joinPoint); } public static void main(String[] args) { C1 target = new C1(); AspectJProxyFactory proxyFactory = new AspectJProxyFactory(); proxyFactory.setTarget(target); proxyFactory.setProxyTargetClass(true); // 获取 C1 上所有接口 spring 工具类提供的方法 Class\u0026lt;?\u0026gt;[] allInterfaces = ClassUtils.getAllInterfaces(target); // 设置代理接口 proxyFactory.setInterfaces(allInterfaces); // 添加切面 proxyFactory.addAspect(PointcutTarget.class); // 获取代理 I1 proxy = proxyFactory.getProxy(); // 调用方法 proxy.m(); System.out.println(\u0026#34;JDK 代理？\u0026#34; + AopUtils.isJdkDynamicProxy(proxy)); System.out.println(\u0026#34;CGLIB 代理？\u0026#34; + AopUtils.isCglibProxy(proxy)); //判断代理对象是否是 C1 类型的 System.out.println(C1.class.isAssignableFrom(target.getClass())); } } 输出结果\nbefore: execution(void com.crab.spring.aop.demo02.aspectj.PointcutTarget$C1.m()) C1 m() JDK 代理？false CGLIB 代理？true true args 格式说明 args（参数类型列表）匹配当前执行的方法传入的参数是否为 args 中指定的类型；参数类型列表中的参数必须是类型全限定名，不支持通配符；args 属于动态切入点，也就是执行方法的时候进行判断的，开销非常大，非特殊情况最好不要使用。\nargs(String) // 方法个数为 1，类型是 String args(*,String) // 方法参数个数 2，第 2 个是 String 类型 args(..,String) // 方法个数不限制，最后一个必须是 String 举例说明 package com.crab.spring.aop.demo02.aspectj; @Aspect public class PointcutArgs { interface I1{ void m(Object name); } static class C1 implements I1{ @Override public void m(Object name) { String type = name.getClass().getName(); System.out.println(\u0026#34;C1 m() 参数类型 \u0026#34; + type); } } // 匹配方法参数个数 1 且类型是必须是 String @Pointcut(\u0026#34;args(String)\u0026#34;) public void pc(){} @Before(\u0026#34;pc()\u0026#34;) public void before(JoinPoint joinPoint) { System.out.println(\u0026#34;before: \u0026#34; + joinPoint); } public static void main(String[] args) { C1 target = new C1(); AspectJProxyFactory proxyFactory = new AspectJProxyFactory(); proxyFactory.setTarget(target); proxyFactory.setProxyTargetClass(true); // 获取 C1 上所有接口 spring 工具类提供的方法 Class\u0026lt;?\u0026gt;[] allInterfaces = ClassUtils.getAllInterfaces(target); // 设置代理接口 proxyFactory.setInterfaces(allInterfaces); // 添加切面 proxyFactory.addAspect(PointcutArgs.class); // 获取代理 I1 proxy = proxyFactory.getProxy(); // 调用方法 proxy.m(\u0026#34;xxxx\u0026#34;); proxy.m(100L); System.out.println(\u0026#34;JDK 代理？\u0026#34; + AopUtils.isJdkDynamicProxy(proxy)); System.out.println(\u0026#34;CGLIB 代理？\u0026#34; + AopUtils.isCglibProxy(proxy)); //判断代理对象是否是 C1 类型的 System.out.println(C1.class.isAssignableFrom(target.getClass())); } } 观察下输出\nbefore: execution(void com.crab.spring.aop.demo02.aspectj.PointcutArgs$C1.m(Object)) C1 m() 参数类型 java.lang.String C1 m() 参数类型 java.lang.Long JDK 代理？false CGLIB 代理？true true\t参数类型传递是 String 时候增强了，而 Long 的时候没有执行增强方法。\n@within 格式说明 @within（注解类型）：匹配指定的注解内定义的方法。\n匹配规则： 被调用的目标方法 Method 对象。getDeclaringClass().getAnnotation(within 中指定的注解类型） != null 举例说明 package com.crab.spring.aop.demo02.aspectj; @Aspect public class PointcutAnnWithin { @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @interface MyAnn { } interface I1 { void m(); } @MyAnn static class C1 implements I1 { @Override public void m() { System.out.println(\u0026#34;C1 m()\u0026#34;); } } // 匹配目标类型必须上必须有注解 MyAnn @Pointcut(\u0026#34;@within(com.crab.spring.aop.demo02.aspectj.PointcutAnnWithin.MyAnn)\u0026#34;) public void pc() { } @Before(\u0026#34;pc()\u0026#34;) public void before(JoinPoint joinPoint) { System.out.println(\u0026#34;before: \u0026#34; + joinPoint); } public static void main(String[] args) { C1 target = new C1(); AspectJProxyFactory proxyFactory = new AspectJProxyFactory(); proxyFactory.setTarget(target); proxyFactory.setProxyTargetClass(true); // 获取 C1 上所有接口 spring 工具类提供的方法 Class\u0026lt;?\u0026gt;[] allInterfaces = ClassUtils.getAllInterfaces(target); // 设置代理接口 proxyFactory.setInterfaces(allInterfaces); // 添加切面 proxyFactory.addAspect(PointcutAnnWithin.class); // 获取代理 I1 proxy = proxyFactory.getProxy(); // 调用方法 proxy.m(); System.out.println(\u0026#34;JDK 代理？\u0026#34; + AopUtils.isJdkDynamicProxy(proxy)); System.out.println(\u0026#34;CGLIB 代理？\u0026#34; + AopUtils.isCglibProxy(proxy)); //判断代理对象是否是 C1 类型的 System.out.println(C1.class.isAssignableFrom(target.getClass())); } } 输出\nbefore: execution(void com.crab.spring.aop.demo02.aspectj.PointcutAnnWithin$C1.m()) C1 m() JDK 代理？false CGLIB 代理？true true 思考下父类上有注解，子类继承父类的方法，同时考虑下注解@Inherited 是否在切点注解的场景？\n@target 格式说明 @target（注解类型）：判断目标对象 target 类型上是否有指定的注解；@target 中注解类型也必须是全限定类型名。\n匹配规则： target.class.getAnnotation（指定的注解类型） != null 注意，如果目标注解是标注在父类上的，那么定义目标注解时候应使用@Inherited标注，使子类能继承父类的注解。\n举例说明 package com.crab.spring.aop.demo02.aspectj; @Aspect public class PointcutAnnTarget { @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Inherited // 子类能继承父类的注解 @interface MyAnn2 { } @MyAnn2 // 注解在父类上 static class P1 { void m(){} } static class C1 extends P1 { @Override public void m() { System.out.println(\u0026#34;C1 m()\u0026#34;); } } // 匹配目标类型必须上必须有注解 MyAnn @Pointcut(\u0026#34;@target(com.crab.spring.aop.demo02.aspectj.PointcutAnnTarget.MyAnn2)\u0026#34;) public void pc() { } @Before(\u0026#34;pc()\u0026#34;) public void before(JoinPoint joinPoint) { System.out.println(\u0026#34;before: \u0026#34; + joinPoint); } public static void main(String[] args) { C1 target = new C1(); AspectJProxyFactory proxyFactory = new AspectJProxyFactory(); proxyFactory.setTarget(target); proxyFactory.setProxyTargetClass(true); // 获取 C1 上所有接口 spring 工具类提供的方法 Class\u0026lt;?\u0026gt;[] allInterfaces = ClassUtils.getAllInterfaces(target); // 设置代理接口 proxyFactory.setInterfaces(allInterfaces); // 添加切面 proxyFactory.addAspect(PointcutAnnTarget.class); // 获取代理 C1 proxy = proxyFactory.getProxy(); // 调用方法 proxy.m(); System.out.println(\u0026#34;JDK 代理？\u0026#34; + AopUtils.isJdkDynamicProxy(proxy)); System.out.println(\u0026#34;CGLIB 代理？\u0026#34; + AopUtils.isCglibProxy(proxy)); // 目标类上是否有切点注解 System.out.println(target.getClass().getAnnotation(MyAnn2.class)!= null); } } 输出结果\nbefore: execution(void com.crab.spring.aop.demo02.aspectj.PointcutAnnTarget$C1.m()) C1 m() JDK 代理？false CGLIB 代理？true true 从结果最后一行看，目标对象继承了父类的注解，符合@target 的切点规则。\n@args 格式说明 @args（注解类型）：方法参数所属的类上有指定的注解；注意不是参数上有指定的注解，而是参数类型的类上有指定的注解。和args类似，不过针对的是参数类型上的注解。\n匹配规则： 传入的目标位置参数。getClass().getAnnotation(@args（对应的参数位置的注解类型）)!= null 举例说明 package com.crab.spring.aop.demo02.aspectj; @Aspect public class PointcutAnnArgs { @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Inherited // 子类能继承父类的注解 @interface MyAnn3 { } @MyAnn3 static class MyParameter{ } static class C1 { public void m(MyParameter myParameter) { System.out.println(myParameter.getClass().getAnnotation(MyAnn3.class)); System.out.println(\u0026#34;C1 m()\u0026#34;); } } // 匹配方法上最后的一个参数类型上有注解 MyAnn3 @Pointcut(\u0026#34;@args(..,com.crab.spring.aop.demo02.aspectj.PointcutAnnArgs.MyAnn3)\u0026#34;) public void pc() { } @Before(\u0026#34;pc()\u0026#34;) public void before(JoinPoint joinPoint) { System.out.println(\u0026#34;before: \u0026#34; + joinPoint); } public static void main(String[] args) { C1 target = new C1(); AspectJProxyFactory proxyFactory = new AspectJProxyFactory(); proxyFactory.setTarget(target); proxyFactory.setProxyTargetClass(true); // 获取 C1 上所有接口 spring 工具类提供的方法 Class\u0026lt;?\u0026gt;[] allInterfaces = ClassUtils.getAllInterfaces(target); // 设置代理接口 proxyFactory.setInterfaces(allInterfaces); // 添加切面 proxyFactory.addAspect(PointcutAnnArgs.class); // 获取代理 C1 proxy = proxyFactory.getProxy(); // 调用方法 MyParameter myParameter = new MyParameter(); proxy.m(myParameter); System.out.println(\u0026#34;JDK 代理？\u0026#34; + AopUtils.isJdkDynamicProxy(proxy)); System.out.println(\u0026#34;CGLIB 代理？\u0026#34; + AopUtils.isCglibProxy(proxy)); // 目标类上是否有切点注解 System.out.println(myParameter.getClass().getAnnotation(MyAnn3.class)!= null); } } 观察结果\nbefore: execution(void com.crab.spring.aop.demo02.aspectj.PointcutAnnArgs$C1.m(MyParameter)) @com.crab.spring.aop.demo02.aspectj.PointcutAnnArgs$MyAnn3() C1 m() JDK 代理？false CGLIB 代理？true true 第二行中目标方法上输出了参数的注解。\n最后一行判断参数类型上确实有注解。\n@annotation 格式说明 @annotation（注解类型）：匹配被调用的目标对象的方法上有指定的注解\n匹配规则：target.getClass().getMethod(\u0026#34;目标方法名\u0026#34;).getDeclaredAnnotation(@annotation（目标注解）)!=null 这个在针对特定注解的方法日志拦截场景下应用比较多。\n举例说明 package com.crab.spring.aop.demo02.aspectj; @Aspect public class PointcutAnnotation { @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) @interface MyAnn4 { } /** * 父类 方法上都有@MyAnn4 */ static class P1{ @MyAnn4 public void m1(){ System.out.println(\u0026#34;P1 m()\u0026#34;); } @MyAnn4 public void m2(){ System.out.println(\u0026#34;P1 m2()\u0026#34;); } } /** * 子类 * 注意重新重写了父类的 m1 方法但是没有声明注解@Ann4 * 新增了 m3 方法带注解@Ann4 */ static class C1 extends P1 { @Override public void m1() { System.out.println(\u0026#34;C1 m1()\u0026#34;); } @MyAnn4 public void m3() { System.out.println(\u0026#34;C1 m3()\u0026#34;); } } // 匹配调用的方法上必须有注解 @Pointcut(\u0026#34;@annotation(com.crab.spring.aop.demo02.aspectj.PointcutAnnotation.MyAnn4)\u0026#34;) public void pc() { } @Before(\u0026#34;pc()\u0026#34;) public void before(JoinPoint joinPoint) { System.out.println(\u0026#34;before: \u0026#34; + joinPoint); } public static void main(String[] args) throws NoSuchMethodException { C1 target = new C1(); AspectJProxyFactory proxyFactory = new AspectJProxyFactory(); proxyFactory.setTarget(target); proxyFactory.setProxyTargetClass(true); // 获取 C1 上所有接口 spring 工具类提供的方法 Class\u0026lt;?\u0026gt;[] allInterfaces = ClassUtils.getAllInterfaces(target); // 设置代理接口 proxyFactory.setInterfaces(allInterfaces); // 添加切面 proxyFactory.addAspect(PointcutAnnotation.class); // 获取代理 C1 proxy = proxyFactory.getProxy(); // 调用方法 proxy.m1(); proxy.m2(); proxy.m3(); System.out.println(\u0026#34;JDK 代理？\u0026#34; + AopUtils.isJdkDynamicProxy(proxy)); System.out.println(\u0026#34;CGLIB 代理？\u0026#34; + AopUtils.isCglibProxy(proxy)); // 目标对象的目标方法上是否直接声明了注解 MyAnn4 System.out.println(target.getClass().getMethod(\u0026#34;m1\u0026#34;).getDeclaredAnnotation(MyAnn4.class)!=null); System.out.println(target.getClass().getMethod(\u0026#34;m2\u0026#34;).getDeclaredAnnotation(MyAnn4.class)!=null); System.out.println(target.getClass().getMethod(\u0026#34;m3\u0026#34;).getDeclaredAnnotation(MyAnn4.class)!=null); } } 观察下结果\nC1 m1() before: execution(void com.crab.spring.aop.demo02.aspectj.PointcutAnnotation$P1.m2()) P1 m2() before: execution(void com.crab.spring.aop.demo02.aspectj.PointcutAnnotation$C1.m3()) C1 m3() JDK 代理？false CGLIB 代理？true false true 简单分析下：\nC1 中重写了 m1 方法，上面有没有 @Ann4，所有方法没有被拦截 其它的 m2 在父类上有注解@Ann4，m3 在子类上也有注解@Ann4，所以拦截了。 最后 3 行输出了目标对象的 3 个方法上是否有注解的情况。 bean 格式说明 bean(bean 名称）：这个用在 spring 环境中，匹配容器中指定名称的 bean。\n匹配格式：ApplicationContext.getBean(\u0026#34;bean 表达式中指定的 bean 名称\u0026#34;) != null 举例说明 定义一个 bean\npackage com.crab.spring.aop.demo02.aspectj; public class MyBean { private String beanName; public MyBean(String beanName) { this.beanName = beanName; } public void m() { System.out.println(\u0026#34;我是\u0026#34; + this.beanName); } } 切面中的切点和通知定义\n@Aspect public class PointcutBean { // 容器中 bean 名称是\u0026#34;myBean1\u0026#34;的方法进行拦截 @Pointcut(\u0026#34;bean(myBean1)\u0026#34;) public void pc() { } @Before(\u0026#34;pc()\u0026#34;) public void m(JoinPoint joinPoint) { System.out.println(\u0026#34;start \u0026#34; + joinPoint); } } 组合使用\n@Aspect @Configuration @EnableAspectJAutoProxy // 自动生成代理对象 public class PointcutBeanConfig { // 注入 myBean1 @Bean(\u0026#34;myBean1\u0026#34;) public MyBean myBean1() { return new MyBean(\u0026#34;myBean1\u0026#34;); } // myBean2 @Bean(\u0026#34;myBean2\u0026#34;) public MyBean myBean2() { return new MyBean(\u0026#34;myBean2\u0026#34;); } // 注入切面 @Bean(\u0026#34;pointcutBean\u0026#34;) public PointcutBean pointcutBean() { return new PointcutBean(); } public static void main(String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(PointcutBeanConfig.class); MyBean myBean1 = context.getBean(\u0026#34;myBean1\u0026#34;, MyBean.class); myBean1.m(); MyBean myBean2 = context.getBean(\u0026#34;myBean2\u0026#34;, MyBean.class); myBean2.m(); } } 观察下结果\nstart execution(void com.crab.spring.aop.demo02.aspectj.MyBean.m()) 我是 myBean1 我是 myBean2 myBean1 的方法被拦截了。\n上面介绍了 Spring 中 10 中切点表达式，下面介绍下切点的组合使用和公共切点的抽取。\n切点的组合 切点与切点直接支持逻辑逻辑组合操作： \u0026amp;\u0026amp; 、||、 !。使用较小的命名组件构建更复杂的切入点表达式是最佳实践。\n同一个类内切点组合 public class CombiningPointcut { /** * 匹配 com.crab.spring.aop.demo02 包及子包下任何类的 public 方法 */ @Pointcut(\u0026#34;execution(public * com.crab.spring.aop.demo02..*.*(..))\u0026#34;) public void publicMethodPc() { } /** * com.crab.spring.aop.demo02.UserService 类的所有方法 */ @Pointcut(\u0026#34;execution(* com.crab.spring.aop.demo02.UserService.*(..))\u0026#34;) public void serviceMethodPc(){} /** * 组合的切点 */ @Pointcut(\u0026#34;publicMethodPc() \u0026amp;\u0026amp; serviceMethodPc()\u0026#34;) public void combiningPc(){ } /** * 组合的切点 2 */ @Pointcut(\u0026#34;publicMethodPc() || !serviceMethodPc()\u0026#34;) public void combiningPc2(){ } } 不同类之间切点组合 切点方法的可见性会影响组合但是不影响切点的匹配。\npublic class CombiningPointcut2 { /** * com.crab.spring.aop.demo02.UserService 类的所有方法 */ @Pointcut(\u0026#34;execution(* com.crab.spring.aop.demo02.UserService.*(..))\u0026#34;) public void serviceMethodPc2(){} /** * 组合的切点，跨类组合 */ @Pointcut(\u0026#34;com.crab.spring.aop.demo02.aspectj.reuse.CombiningPointcut.publicMethodPc() \u0026amp;\u0026amp; serviceMethodPc2()\u0026#34;) public void combiningPc(){ } /** * 组合的切点，跨类组合，由于 serviceMethodPc 是 private, 此处无法组合 */ @Pointcut(\u0026#34;com.crab.spring.aop.demo02.aspectj.reuse.CombiningPointcut.serviceMethodPc() \u0026amp;\u0026amp; serviceMethodPc2()\u0026#34;) public void combiningPc2(){ } } 切点的公用 在使用企业应用程序时，开发人员通常希望从多个方面引用应用程序的模块和特定的操作集。建议为此目的定义一个捕获公共切入点表达式的 CommonPointcuts 方面。直接看案例。\n不同层的公共切点\n/** * 公用的切点 */ public class CommonPointcuts { /** * web 层的通用切点 */ @Pointcut(\u0026#34;within(com.xyz.myapp.web..*)\u0026#34;) public void inWebLayer() {} @Pointcut(\u0026#34;within(com.xyz.myapp.service..*)\u0026#34;) public void inServiceLayer() {} @Pointcut(\u0026#34;within(com.xyz.myapp.dao..*)\u0026#34;) public void inDataAccessLayer() {} @Pointcut(\u0026#34;execution(* com.xyz.myapp..service.*.*(..))\u0026#34;) public void businessService() {} @Pointcut(\u0026#34;execution(* com.xyz.myapp.dao.*.*(..))\u0026#34;) public void dataAccessOperation() {} } 程序中可以直接引用这些公共的切点\n@Aspect public class UseCommonPointcuts { /** * 直接使用公共切点 */ @Before(\u0026#34;com.crab.spring.aop.demo02.aspectj.reuse.CommonPointcuts.inWebLayer()\u0026#34;) public void before(JoinPoint joinPoint){ System.out.println(\u0026#34;before:\u0026#34; + joinPoint); } } 带参数的切点 @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = AspectConfig.class) public class AspectDemo { @Autowired private List\u0026lt;Perform\u0026gt; performList; @Test public void testPerform(){ for (Perform perform : performList) { perform.doPerform(\u0026#34;dc1\u0026#34;); } } } @Configuration @ComponentScan @EnableAspectJAutoProxy public class AspectConfig { @Bean public Audience audience() { return new Audience(); } } @Aspect public class Audience { /** * 声明一个切点 */ @Pointcut(\u0026#34;execution(* io2.dc.Perform.doPerform(..)) \u0026amp;\u0026amp; bean(poet)\u0026#34;) public void perform(){}; /** * 声明一个可以接收参数的切点 * @param name */ @Pointcut(\u0026#34;execution(* io2.dc.Perform.doPerform(String)) \u0026amp;\u0026amp; args(name)\u0026#34;) public void performWithArg(String name){}; @Before(\u0026#34;perform()\u0026#34;) public void drinkWater() { System.out.println(\u0026#34;喝口水\u0026#34;); } @Before(\u0026#34;performWithArg(name)\u0026#34;) public void printName(String name) { System.out.println((name + \u0026#34;开始表演了\u0026#34;)); } } public interface Perform{ int doPerform(String name); } @Component public class Poet implements Perform { @Override public int doPerform(String name) { System.out.println((\u0026#34;朗诵\u0026#34; + name)); return 0; } } @Component public class Singer implements Perform { @Override public int doPerform(String name) { System.out.println((\u0026#34;歌唱家\u0026#34; + name)); return 0; } } 参考资料\nhttps://www.cnblogs.com/kongbubihai/p/16017046.html 《Spring 实战第4版》 ","date":"2022-11-27T17:08:10Z","permalink":"https://dccmmtop.github.io/posts/aop%E7%9A%84%E4%BD%BF%E7%94%A8/","section":"posts","tags":["spring"],"title":"AOP 的使用"},{"categories":null,"contents":"横切关注点 散播应用中多处的功能被称为横切关注点\n安全就是一个关注点，每个方法或者类都需要注重安全 方法参数日志记录也是一个关注点 以及事务管理 横切关注点从概念上讲是与业务分离的，但往往会直接嵌入到业务中\n多个类使用到相同的功能。最常见到的就是继承或委托\n为什么不使用继承\n如果整个应用都使用相同的基类，会导致脆弱的对象体系\n为什么不使用委托\n可能需要对委托对象进行复杂的调用\n把横切关注点与业务分离是面向切面编程（AOP）要解决的重要问题\n切面 横切点可以被模块化为特殊的类，这个类被称为切面 取代继承和委托的另一种方案 一处定义功能，生声明何时何地使用 术语 AOP 已经形成了自己的术语，常见的有 1. 通知 2. 切点 3. 连接点\n通知 Advice 切面要完成的工作被称为通知，它定义了切面要做什么，何时使用\n何时使用\n前置通知 Before\n在目标方法被调用之前调用通知功能 后置通知 After\n在目标方法完成之后调用通知，不会关心方法输出是什么，也不会关心是否执行成功 返回通知 After-returning\n在目标方法成功执行之后调用通知 异常通知 After-throwing\n在目标方法抛出异常后调用通知 环绕通知 Around\n包裹被通知的方法：目标方法调用之前之后都要执行 连接点 JoinPoint 触发通知的时机叫做连接点\n调用方法时 抛出异常时 修改字段时 等 切点 PointCut 要执行通知的地点，指明了哪些方法要执行切面。 通常使用明确的类或方法名称或正则表达式匹配类或方法名指定切点 切面 Aspect 切面是通知和切点的结合，它是什么，何时 何地完成其功能 引入 Introduction 可以在现有的类中添加新的方法和属性 织入 Weaving 把切面应用到目标对象并创建新的代理对象的过程。\n织入可以在多个时期发生：\n编译期\n切面在目标类编译期被织入，需要特殊的编译器 类加载期\n切面在目标类加载到JVM时被织入，需要特殊的类加载器。他可以在目标类被引入到应用之前增强该目标类的字节码 运行期\n切面在应用运行的某时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态的创建一个代理对象，SpringAOP就是以这种方式织入切面的 Spring对AOP的支持 Spring 提供了4种类型的AOP支持 基于代理的经典AOP\n不用，过于笨重和复杂 纯POJO切面\n不怎么用，需要XML配置 @AspectJ注解驱动切面\n用的多 注入式 AspectJ切面，适用Spring各种版本\nSpring 只支持方法级别的连接点 因为Spring 基于动态代理实现的AOP\nSpring 不支持对字段和构造器连接点\n不可以拦截对象字段的修改 无法在bean创建时拦截 AspectJ 和 JBoss 除了方法切点，还支持字段和构造器\n编写切面 SpringAOP 仅支持部分Aspect指示器 Spring AOP 支持的指示器： arg()\n限制连接点的匹配参数为为指定类型的执行方法\n@args()\n限制连接点匹配参数由指定注解标注的执行方法\nexecution()\n用于匹配是连接点的执行方法\nthis()\n限制连接点匹配 AOP代理的bean引用为指定类型的类\ntarget\n限制连接点匹配目标对象为指定类型的类\n@target()\n限制连接点匹配特定的执行对象，这些对象对应的类要有指定类型的注解\nwithin()\n限制连接点匹配指定的类型\n@within()\n限制连接点匹配注解所标注的类型\n@annotation\n限定匹配 带有指定注解的连接点\nbean()\n限定beanId\n当在Spring 中使用其他指示器时，会抛出异常\n只有 execution 指示器是实际执行匹配的，其他都是限制匹配的\n切入点的编写规则 示例：\n与 within() 配合\n在切点中选择bean\n","date":"2022-11-27T15:06:05Z","permalink":"https://dccmmtop.github.io/posts/%E9%9D%A2%E5%90%91%E5%88%87%E9%9D%A2%E7%9A%84spring/","section":"posts","tags":["spring"],"title":"面向切面的Spring"},{"categories":null,"contents":"什么是循环依赖 A 类中有一个属性 B ，也就是说 A 依赖 B，同时 B 类中有一个属性 A, 也就是说 B 依赖 A. 他们之间的依赖关系形成了环。就是我们说的循环依赖，如下图：\n循环依赖示例 public class CircularDependenciesDemo { public static void main(String[] args) { new A1(); } } class A1 { private B1 b1; public A1() { this.b1 = new B1(); } } class B1 { private A1 a1; public B1() { this.a1 = new A1(); } } 结果：\nException in thread \u0026#34;main\u0026#34; java.lang.StackOverflowError at io.dc.B1.\u0026lt;init\u0026gt;(CircularDependenciesDemo.java:23) at io.dc.A1.\u0026lt;init\u0026gt;(CircularDependenciesDemo.java:16) at io.dc.B1.\u0026lt;init\u0026gt;(CircularDependenciesDemo.java:24) 如上所示，发生 栈溢出错误。下面我们试着解决这种循环依赖\n解决循环依赖 public class CircularDependenciesDemo { // 实例缓存池 public static Map\u0026lt;String, Object\u0026gt; existsObject = new HashMap\u0026lt;\u0026gt;(); public static Object loadObject (String objectName) { if(existsObject.containsKey(objectName)) { return existsObject.get(objectName); } if(\u0026#34;A1\u0026#34;.equals(objectName)) { A1 a1 = new A1(); // 先将空白的实例放入缓存 existsObject.put(\u0026#34;A1\u0026#34;,a1); // 然后将对该空白实例进行属性赋值 a1.setB1((B1)loadObject(\u0026#34;B1\u0026#34;)); return a1; } if (\u0026#34;B1\u0026#34;.equals(objectName)) { B1 b1 = new B1(); existsObject.put(\u0026#34;B1\u0026#34;,b1); b1.setA1((A1)loadObject(\u0026#34;A1\u0026#34;)); return b1; } return null; } public static void main(String[] args) { A1 a1 = (A1)loadObject(\u0026#34;A1\u0026#34;); a1.getB1().sayHello(); } } class A1 { private B1 b1; public B1 getB1() { return b1; } public void setB1(B1 b1) { this.b1 = b1; } } class B1 { private A1 a1; public A1 getA1() { return a1; } public void setA1(A1 a1) { this.a1 = a1; } public void sayHello(){ System.out.println(\u0026#34;我是 B1 , 你好\u0026#34;); } } 结果：\n我是 B1 , 你好 本来由构造方法来构造 A1 中的 B1. 现在分成两步：\n先调用无参构造器生成一个空白实例 再调用 set 方法，为空白实例赋值 为了解决循环依赖，设置了一个实例缓存池，existsObject. 用来存放已经生成的实例。但是这个实例池会存放空白对象的状态。在多线程的情况下，会取到一个空白实例。也就是对象中的字段都是 null, 引发程序错误。我们可以再添加一层二级缓存，二级缓存中存放空白实例。一级缓存中只放完整实例。\n纯净的缓存 public class CircularDependenciesDemo { // 一级缓存，只存放完整的对象 public static Map\u0026lt;String, Object\u0026gt; existsObject = new HashMap\u0026lt;\u0026gt;(); // 二级缓存，包含空白的对象 public static Map\u0026lt;String, Object\u0026gt; blankObject = new HashMap\u0026lt;\u0026gt;(); // 加载对象 public static Object loadObject (String objectName) { // 先从一级缓存取 if(existsObject.containsKey(objectName)) { return existsObject.get(objectName); } // 再从二级缓存取 if(blankObject.containsKey(objectName)) { return blankObject.get(objectName); } if(\u0026#34;A1\u0026#34;.equals(objectName)) { A1 a1 = new A1(); // 先将空白的实例放入二级缓存 blankObject.put(\u0026#34;A1\u0026#34;,a1); // 然后将对该空白实例进行属性赋值 a1.setB1((B1)loadObject(\u0026#34;B1\u0026#34;)); // 再放入一级缓存 existsObject.put(\u0026#34;A1\u0026#34;,a1); return a1; } if (\u0026#34;B1\u0026#34;.equals(objectName)) { B1 b1 = new B1(); blankObject.put(\u0026#34;B1\u0026#34;,b1); b1.setA1((A1)loadObject(\u0026#34;A1\u0026#34;)); existsObject.put(\u0026#34;B1\u0026#34;,b1); return b1; } return null; } public static void main(String[] args) { A1 a1 = (A1)loadObject(\u0026#34;A1\u0026#34;); a1.getB1().sayHello(); B1 b1 = (B1) loadObject(\u0026#34;B1\u0026#34;); b1.getA1().sayHello(); } } class A1 { private B1 b1; public B1 getB1() { return b1; } public void setB1(B1 b1) { this.b1 = b1; } public void sayHello(){ System.out.println(\u0026#34;我是 A1 , 你好\u0026#34;); } } class B1 { private A1 a1; public A1 getA1() { return a1; } public void setA1(A1 a1) { this.a1 = a1; } public void sayHello(){ System.out.println(\u0026#34;我是 B1 , 你好\u0026#34;); } } 通过添加二级缓存，把空白对象和完整对象剥离了。当从一级缓存中取实例时，要么拿到的是完整对象，要么拿到的是 null, 而不会获取到空白的对象，引发错误。\n但是上面的代码是有问题的，当实例未初始化完，并且在在多线程的情况下,仍然会取到不完整对象，如下:\n那么单单只加二级缓存并不能解决这个并发问题，同时还要加锁：\npublic static Object loadObject (String objectName) { // 先从一级缓存取,因为可以确保一级缓存中只有完整的对象 if(existsObject.containsKey(objectName)) { return existsObject.get(objectName); } synchronized (existsObject) { // 第二次判断一级缓存是否有,因为有可能在等待锁的时候，已经有其他线程把实例放入一级缓存中 if(existsObject.containsKey(objectName)) { return existsObject.get(objectName); } // 再从二级缓存取 if(blankObject.containsKey(objectName)) { return blankObject.get(objectName); } if(\u0026#34;A1\u0026#34;.equals(objectName)) { A1 a1 = new A1(); // 先将空白的实例放入二级缓存 blankObject.put(\u0026#34;A1\u0026#34;,a1); // 然后将对该空白实例进行属性赋值 a1.setB1((B1)loadObject(\u0026#34;B1\u0026#34;)); // 再放入一级缓存 existsObject.put(\u0026#34;A1\u0026#34;,a1); // 然后移除二级缓存中的 A1 blankObject.remove(\u0026#34;A1\u0026#34;); return a1; } if (\u0026#34;B1\u0026#34;.equals(objectName)) { B1 b1 = new B1(); blankObject.put(\u0026#34;B1\u0026#34;,b1); b1.setA1((A1)loadObject(\u0026#34;A1\u0026#34;)); existsObject.put(\u0026#34;B1\u0026#34;,b1); blankObject.remove(\u0026#34;B1\u0026#34;); return b1; } } return null; } 到此我们才完整的解决了循环依赖，并且保证不会取到不完整的实例.\n其实 spring 也是这样来解决bean循环依赖的，不过 spring 还添加动态代理bean的功能，为了解耦，还添加了三级缓存，保证代码整洁，优雅。\nspring 是如何解决循环依赖的 由于 spring 在处理循环依赖时考虑很多其他功能，代码非常复杂，为了便于展示，这里只模仿核心功能：\n详细说明已在注释中\n// 主类 public class MainStart { // 本应该使用 ConcurrentHashMap 类型，但是为了演示效果，确保先实例化A，使用了有序HashMap private static Map\u0026lt;String, BeanDefinition\u0026gt; beanDefinitionMap = new LinkedHashMap\u0026lt;\u0026gt;(256); /** * 读取bean定义，当然在spring中肯定是根据配置 动态扫描注册 */ public static void loadBeanDefinitions() { RootBeanDefinition aBeanDefinition=new RootBeanDefinition(InstanceA.class); RootBeanDefinition bBeanDefinition=new RootBeanDefinition(InstanceB.class); beanDefinitionMap.put(\u0026#34;instanceA\u0026#34;,aBeanDefinition); beanDefinitionMap.put(\u0026#34;instanceB\u0026#34;,bBeanDefinition); } public static void main(String[] args) throws Exception { // 加载了BeanDefinition loadBeanDefinitions(); // 循环创建Bean for (String key : beanDefinitionMap.keySet()){ // 先创建A getBean(key); } IApi instanceA = (IApi) getBean(\u0026#34;instanceA\u0026#34;); instanceA.say(); } // 一级缓存 public static Map\u0026lt;String,Object\u0026gt; singletonObjects=new ConcurrentHashMap\u0026lt;\u0026gt;(); // 二级缓存： 为了将 成熟Bean和纯净Bean分离，避免读取到不完整得Bean public static Map\u0026lt;String,Object\u0026gt; earlySingletonObjects=new ConcurrentHashMap\u0026lt;\u0026gt;(); // 三级缓存 public static Map\u0026lt;String,ObjectFactory\u0026gt; singletonFactories=new ConcurrentHashMap\u0026lt;\u0026gt;(); // 循环依赖标识 public static Set\u0026lt;String\u0026gt; singletonsCurrennlyInCreation=new HashSet\u0026lt;\u0026gt;(); // 假设A 使用了Aop @PointCut(\u0026#34;execution(* *..InstanceA.*(..))\u0026#34;) 要给A创建动态代理 // 获取Bean public static Object getBean(String beanName) throws Exception { Object singleton = getSingleton(beanName); if(singleton!=null){ return singleton; } Object instanceBean = null; synchronized (singletonObjects) { // 第二次判断 if(singletonObjects.containsKey(beanName)) { return singletonObjects.get(beanName); } // 标记正在创建,而不是用二级缓存是否包含 beanName 来判断 if(!singletonsCurrennlyInCreation.contains(beanName)){ singletonsCurrennlyInCreation.add(beanName); } // 实例化 RootBeanDefinition beanDefinition = (RootBeanDefinition) beanDefinitionMap.get(beanName); // 获取 class 对象 Class\u0026lt;?\u0026gt; beanClass = beanDefinition.getBeanClass(); // 通过无参构造函数,构造一个空白对象 instanceBean = beanClass.newInstance(); // 创建动态代理 // 只在循环依赖的情况下在实例化后创建proxy 判断当前是不是循环依赖 Object finalInstanceBean = instanceBean; // 这是一个三级缓存，三级缓存放的不是 bean, 而是一个可以创建动态代理bean的函数。（lambda 表达式。关键词： 函数接口） // 为什么不直接创建一个代理对象放入二级缓存中，而是先把 lambda 表达式放入三级缓存， // 而后面再调用这个表达式去生成代理对象，然后放入二级缓存（getSingleton 方法） // 我认为是是为了逻辑上的统一，getSingleton方法负责创建或返回对象，而不是在这里。 singletonFactories.put(beanName, () -\u0026gt; new JdkProxyBeanPostProcessor().getEarlyBeanReference(finalInstanceBean,beanName)); // 属性赋值 Field[] declaredFields = beanClass.getDeclaredFields(); for (Field declaredField : declaredFields) { Autowired annotation = declaredField.getAnnotation(Autowired.class); // 说明属性上面有Autowired if(annotation!=null){ declaredField.setAccessible(true); String name = declaredField.getName(); Object fileObject= getBean(name); declaredField.set(instanceBean,fileObject); } } // 由于递归完后A 还是原实例，， 所以要从二级缓存中拿到proxy 。 if(earlySingletonObjects.containsKey(beanName)){ instanceBean=earlySingletonObjects.get(beanName); } // 添加到一级缓存 A singletonObjects.put(beanName,instanceBean); // remove 二级缓存和三级缓存 earlySingletonObjects.remove(beanName); singletonFactories.remove(beanName); } return instanceBean; } public static Object getSingleton(String beanName){ // 先从一级缓存中拿 Object bean = singletonObjects.get(beanName); synchronized (singletonObjects) { // 说明是循环依赖 if(bean==null \u0026amp;\u0026amp; singletonsCurrennlyInCreation.contains(beanName)){ bean=earlySingletonObjects.get(beanName); // 如果二级缓存没有就从三级缓存中拿 if(bean==null) { // 从三级缓存中拿 ObjectFactory factory = singletonFactories.get(beanName); if (factory != null) { // 拿到动态代理 // 为什么需要动态代理对象，而不是我们自己原来的bean. // 因为原始bean的代理对象扩展了功能，同时还和原始 bean 有相同的类型。因为它们继承了同一个接口 // 或者 代理对象继承了原始bean. (详情可以搜索 JDK动态代理,cglib 代理) bean=factory.getObject(); // 将代理对象放入二级缓存，这个代理对象仍然是空白对象 // 所以这个方法如果不加锁仍然可能会返回一个空白对象 earlySingletonObjects.put(beanName, bean); } } } } return bean; } } // InstanceA 接口， 为了动态代理使用 public interface IApi { void say(); } // InstanceA @Component public class InstanceA implements IApi { @Autowired private InstanceB instanceB; public InstanceB getInstanceB() { return instanceB; } public void setInstanceB(InstanceB instanceB) { this.instanceB = instanceB; } public InstanceA(InstanceB instanceB) { this.instanceB = instanceB; } public InstanceA() { System.out.println(\u0026#34;InstanceA实例化\u0026#34;); } @Override public void say() { System.out.println(\u0026#34;I\u0026#39;m A\u0026#34;); } } // InstanceB @Component public class InstanceB { @Autowired private InstanceA instanceA; public InstanceA getInstanceA() { return instanceA; } public void setInstanceA(InstanceA instanceA) { this.instanceA = instanceA; } public InstanceB(InstanceA instanceA) { this.instanceA = instanceA; } public InstanceB() { System.out.println(\u0026#34;InstanceB实例化\u0026#34;); } } // JDK 动态代理使用 public class JdkDynimcProxy implements InvocationHandler { private Object target; public JdkDynimcProxy(Object target) { this.target = target; } public \u0026lt;T\u0026gt; T getProxy() { return (T) Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), this); } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\u0026#34;测试\u0026#34;); return method.invoke(target,args); } } // @Component public class JdkProxyBeanPostProcessor implements SmartInstantiationAwareBeanPostProcessor { public Object getEarlyBeanReference(Object bean, String beanName) throws BeansException { // 假设:A 被切点命中 需要创建代理 @PointCut(\u0026#34;execution(* *..InstanceA.*(..))\u0026#34;) if(bean instanceof InstanceA) { JdkDynimcProxy jdkDynimcProxy = new JdkDynimcProxy(bean); return jdkDynimcProxy.getProxy(); } return bean; } } 为什么需要三级缓存，而不是两级缓存 我认为两级缓存完全可以解决循环依赖，完全可以先创建 bean的动态代理放入二级缓存中，而不是在 getSingleton 方法中延迟调用三级缓存中的 lambda 表达式再去生成动态代理。\n我认为三级缓存作用之一是为了代码解耦，逻辑统一。\nspring 如何避免拿到不完整的bean spring 容器加载完成后，因为解决了循环依赖不会存在不完整的bean， 在 spring 容器加载过程中，通过加锁的方式可以避免取到不完整的bean spring 没有解决的循环依赖 没有解决构造函数的循环依赖\n所以不建议构造函数注入方式 没有解决多例下的循环依赖\n好像也无法解决，没有必要 ","date":"2022-11-19T18:14:12Z","permalink":"https://dccmmtop.github.io/posts/spring%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","section":"posts","tags":["java","spring"],"title":"spring 如何解决循环依赖"},{"categories":null,"contents":"获取系统中正在运行的进程。和平台无关\nimport ( ps \u0026#34;github.com/mitchellh/go-ps\u0026#34; ) func running() bool { processList, err := ps.Processes() if err != nil { fmt.Printf(\u0026#34;err %v\\n\u0026#34;,err); return false } var process ps.Process num := 0 for x := range processList { process = processList[x] if process.Executable() == \u0026#34;test.exe\u0026#34; { num ++ if num \u0026gt;= 2 { fmt.Println(\u0026#34;on running\u0026#34;) return true } } } return false } ","date":"2022-11-09T13:10:41Z","permalink":"https://dccmmtop.github.io/posts/%E8%8E%B7%E5%8F%96%E6%AD%A3%E5%9C%A8%E8%BF%90%E8%A1%8C%E7%9A%84%E8%BF%9B%E7%A8%8B/","section":"posts","tags":["go"],"title":"获取正在运行的进程"},{"categories":null,"contents":"本文从源码层面简要介绍一下 Spring IoC 加载过程，以及这个过程遇到的重点方法，重点类。\nnew AnnotationConfigApplicationContext() 一切的根源都要从 new AnnotationConfigApplicationContext() 方法开始，这是 Spring 启动的入口，先准备如下代码：\npublic class ContextDemo { public static void main(String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(Config.class); context.getBean(\u0026#34;car\u0026#34;,Car.class); } } @Configuration @ComponentScan(basePackageClasses = {Car.class}) class Config{ @Bean public User1 user(){ return new User1(10, \u0026#34;dc1\u0026#34;); } } @Component class Car { private int color; private String name; } class User1 { private int age; private String name; } 进入 new AnnotationConfigApplicationContext(Config.class); 方法可见如下：\npublic AnnotationConfigApplicationContext(Class\u0026lt;?\u0026gt;... annotatedClasses) { this(); register(annotatedClasses); refresh(); } 再进入 this() 方法，如下：\npublic AnnotationConfigApplicationContext() { this.reader = new AnnotatedBeanDefinitionReader(this); this.scanner = new ClassPathBeanDefinitionScanner(this); } 发现调用的 AnnotationConfigApplicationContext 无参构造器， 再调用一个类的无参构造器时，会先调用父类的无参构造器， 同时发现这个类的父类构造器如下：\npublic GenericApplicationContext() { this.beanFactory = new DefaultListableBeanFactory(); } 方法很简单，只是 new 了一个 beanFactory, 从名称来看，这个属性是 bean 工厂。现在只知道 AnnotationConfigApplicationContext 中有一个 beanFactory 的字段，这个字段是从父类继承来的。\n父类的构造方法已经结束，接着往下看， this.reader = new AnnotatedBeanDefinitionReader(this);, this.scanner = new ClassPathBeanDefinitionScanner(this); 。初始化 reader 和 scanner.\n找到 reader 属性，会发现它是 AnnotatedBeanDefinitionReader 类型。看一下这个类型的注释说明： /** * Convenient adapter for programmatic registration of annotated bean classes. * This is an alternative to {@link ClassPathBeanDefinitionScanner}, applying * the same resolution of annotations but for explicitly registered classes only. * * @author Juergen Hoeller * @author Chris Beams * @author Sam Brannen * @author Phillip Webb * @since 3.0 * @see AnnotationConfigApplicationContext#register */ public class AnnotatedBeanDefinitionReader {....} 翻译一下：\n简便的适配器，用于注解 bean 类的注册。这是 ClassPathBeanDefinitionScanner 的替代方案，应用相同的注解，但仅适用于显式注册的类。\n意思就是可以替代 ClassPathBeanDefinitionScanner 使用，但是它只能用于我们手动注册的类。\n找到scanner 属性，发现它正是 ClassPathBeanDefinitionScanner 类型。再看一下这个类的注释说明： /** * A bean definition scanner that detects bean candidates on the classpath, * registering corresponding bean definitions with a given registry ({@code BeanFactory} * or {@code ApplicationContext}). * * \u0026lt;p\u0026gt;Candidate classes are detected through configurable type filters. The * default filters include classes that are annotated with Spring\u0026#39;s * {@link org.springframework.stereotype.Component @Component}, * {@link org.springframework.stereotype.Repository @Repository}, * {@link org.springframework.stereotype.Service @Service}, or * {@link org.springframework.stereotype.Controller @Controller} stereotype. * * \u0026lt;p\u0026gt;Also supports Java EE 6\u0026#39;s {@link javax.annotation.ManagedBean} and * JSR-330\u0026#39;s {@link javax.inject.Named} annotations, if available. * * @author Mark Fisher * @author Juergen Hoeller * @author Chris Beams * @since 2.5 * @see AnnotationConfigApplicationContext#scan * @see org.springframework.stereotype.Component * @see org.springframework.stereotype.Repository * @see org.springframework.stereotype.Service * @see org.springframework.stereotype.Controller */ public class ClassPathBeanDefinitionScanner extends ClassPathScanningCandidateComponentProvider {...} 翻译一下：\n一个 bean 定义扫描器，用于检测类路径上的 bean 的候选者，将相应的 bean 定义注册到给定的注册表（ BeanFactory 或 ApplicationContext ）。\n通过可配置的类型过滤器检测候选类。默认过滤器包括使用 Spring 的@Component 、 @Repository 、 @Service 或@Controller 原型注释的类。\n如果可用，还支持 Java EE 6 的 javax.annotation.ManagedBean 和 JSR-330 的 javax.inject.Named 注释。\n它的意思说，把类路径中的所有类进行扫描。把类中配置的 bean 放到注册表， 注册表可能是 BeanFactory 或者 ApplicationContext\n到这里可以知道， AnnotationConfigApplicationContext 除了继承父类的属性外，还初始化了 1. bean 定义读取器 (reader) 2. bean 定义扫描器 (scanner)\n下面看一下如何初始化的：\n初始化 AnnotatedBeanDefinitionReader 一层层进入 new AnnotatedBeanDefinitionReader(this), 最终找到方法：\n/** * Register all relevant annotation post processors in the given registry. * @param registry the registry to operate on * @param source the configuration source element (already extracted) * that this registration was triggered from. May be {@code null}. * @return a Set of BeanDefinitionHolders, containing all bean definitions * that have actually been registered by this call */ public static Set\u0026lt;BeanDefinitionHolder\u0026gt; registerAnnotationConfigProcessors( BeanDefinitionRegistry registry, @Nullable Object source) { DefaultListableBeanFactory beanFactory = unwrapDefaultListableBeanFactory(registry); if (beanFactory != null) { if (!(beanFactory.getDependencyComparator() instanceof AnnotationAwareOrderComparator)) { beanFactory.setDependencyComparator(AnnotationAwareOrderComparator.INSTANCE); } if (!(beanFactory.getAutowireCandidateResolver() instanceof ContextAnnotationAutowireCandidateResolver)) { beanFactory.setAutowireCandidateResolver(new ContextAnnotationAutowireCandidateResolver()); } } Set\u0026lt;BeanDefinitionHolder\u0026gt; beanDefs = new LinkedHashSet\u0026lt;\u0026gt;(8); if (!registry.containsBeanDefinition(CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)); } if (!registry.containsBeanDefinition(AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(AutowiredAnnotationBeanPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)); } // .... 省略很多代码 return beanDefs; } 先翻译一下这个方法的注释，以及参数和返回值：\n注释：\n在给定的注册表中注册所有相关的注解后处理器\n参数：\n第一个参数是： register, 这个是从最开始传入的 AnnotationConfigApplicationContext 对象 第二个参数是：Object source 这个传入的 null, 暂不关心它 返回值：\n返回的是 Set\u0026lt;BeanDefinitionHolder\u0026gt; 集合，到这里先暂停一下，需要看 BeanDefinitionHolder 是什么，它里面都是什么属性，以及干嘛用的：\nBeanDefinition 进入这个类，查看它的注释和内部结构：\n通过注释可以知道它是 具有名称和别名的 BeanDefinition 的持有者\n由此可知道关键信息不在它，而是 BeanDefinition, BeanDefinitionHolder 只是 BeanDefinition 的持有者，只比 BeanDefinition 多了 beanName 以及别名属性。为的是可以有一个具体的名称来描述 BeanDefinition. 所以要接着看 BeanDefinition：\n先看翻译下 BeanDefinition 的注释：\nBeanDefinition 描述了一个 bean 实例，它具有属性值、构造函数参数值以及具体实现提供的更多信息。 这只是一个最小接口：主要目的是允许像 PropertyPlaceholderConfigurer 这样的 BeanFactoryPostProcessor 内省和修改属性值和其他 bean 元数据。\n它说 BeanDefinition 描述了 bean 的实例。也就是建筑图纸和建筑物的关系，BeanDefinition 会存有 某个 bean 的字段，构造方法等等。可以根据 BeanDefinition 构造出应的 bean。就如同可以通过建筑图纸还原出建筑物一样。\n这里就非常巧妙了，一个系统中的有各式各样的 bean，功能不同，类型不同。用一个相同的东西去描述，操作这些 bean，是一个非常有利的转换。因为它们都是对象，是对象就会有方法，字段，类型。把这些方法，字段，类型再抽象，聚合就形成了 BeanDefinition.\n看一下 BeanDefinition 内部结构：\n内容很多，都是设置，获取 某个对象类型信息的，挑几个看一下：\npublic interface BeanDefinition extends AttributeAccessor, BeanMetadataElement { // 标准单例范围的范围标识符：“单例”。 String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; // 标准原型范围的范围标识符：“原型”。 String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; // ... // 指定此 bean 定义的 bean 类名。 // 类名可以在 bean 工厂后期处理期间修改，通常用它的解析变体替换原始类名。 void setBeanClassName(@Nullable String beanClassName); // 覆盖此 bean 的目标范围，指定一个新的范围名称 void setScope(@Nullable String scope); //设置这个 bean 是否应该被延迟初始化。 //如果为 false ，则 bean 将在启动时由执行单例预初始化的 bean 工厂实例化。 void setLazyInit(boolean lazyInit); //返回此 bean 是否应该延迟初始化，即在启动时不急切地实例化。仅适用于单例 bean。 boolean isLazyInit(); // .... } 好了，对 BeanDefinition 的了解暂时到此。不再深入了。继续上面的 registerAnnotationConfigProcessors方法：\n第一行： DefaultListableBeanFactory beanFactory = unwrapDefaultListableBeanFactory(registry); 进入方法后，发现没有做太多的是事情，就是把注册器的 beanFactory 包装一下，再返回。这里的 register 就是最开始 main 方法中的 AnnotationConfigApplicationContext 对象。\n接着看下面：\nif (beanFactory != null) { if (!(beanFactory.getDependencyComparator() instanceof AnnotationAwareOrderComparator)) { beanFactory.setDependencyComparator(AnnotationAwareOrderComparator.INSTANCE); } if (!(beanFactory.getAutowireCandidateResolver() instanceof ContextAnnotationAutowireCandidateResolver)) { beanFactory.setAutowireCandidateResolver(new ContextAnnotationAutowireCandidateResolver()); } } 对 beanFactory 添加 依赖比较器 和 Autowire 候选解析器 暂时不知道设么作用，跳过。继续：\nSet\u0026lt;BeanDefinitionHolder\u0026gt; beanDefs = new LinkedHashSet\u0026lt;\u0026gt;(8); if (!registry.containsBeanDefinition(CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)); } 先构造了一个 BeanDefinitionHolder 的集合，也就是这个方法要返回的集合，if 语句中， 判断注册器是否包含某个 BeanDefinition, 进去看一下怎么判断的：\n// 在 DefaultListableBeanFactory 类下面 @Override public boolean containsBeanDefinition(String beanName) { Assert.notNull(beanName, \u0026#34;Bean name must not be null\u0026#34;); return this.beanDefinitionMap.containsKey(beanName); } 最终会进入 DefaultListableBeanFactory 类中的 containsBeanDefinition 方法，也就是说这个方法不是注册器提供的，而是它的 beanFactory 属性提供的。并且在 beanFactory 中有一个 Map\u0026lt;String, BeanDefinition\u0026gt; beanDefinitionMap；通过名字就可以知道，存放的是 BeanDefinition, 判断注册器是否包已经包含某个 BeanDefinition, 就是看这个 map 是否包含相应的 key 了。这个功能是由它所辖的 beanFactory 实现的。这也比较符合常理： bean 工厂去管理 bean 定义。到这里就可以猜测出注册 bean 时，也是放入这个 Map, 取出 bean 时，也是从这个 Map 中获取。\n再看一下传入的 key 是什么：\nCONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME 是写死在源码中的常量：org.springframework.context.annotation.internalConfigurationAnnotationProcessor 某个类的全名称。继续向下看\nRootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)); 将 ConfigurationClassPostProcessor.class 作为参数，构造了一个 RootBeanDefinition , 查看类关系，发现 RootBeanDefinition 是 BeanDefinition 的一个实现类。这里只需明白把一个 class 转成了 BeanDefinition 就行了。至于怎么转的，可以自行查看\ndef.setSource(source) 刚刚说过 source 参数是 null, 先跳过。不深究它是什么意思。\n再看 registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME) 这一行，方法名是 注册后置处理器， 把注册器，BeanDefinition, 和 类全名作为参数传入方法，再一路跟踪下去发现最后还是进入了 DefaultListableBeanFactory 类。执行的是 registerBeanDefinition 方法：\n@Override public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException { Assert.hasText(beanName, \u0026#34;Bean name must not be empty\u0026#34;); Assert.notNull(beanDefinition, \u0026#34;BeanDefinition must not be null\u0026#34;); // 省略很多校验性代码 BeanDefinition existingDefinition = this.beanDefinitionMap.get(beanName); if (existingDefinition != null) { // 省略很多校验性代码 this.beanDefinitionMap.put(beanName, beanDefinition); } else { // 检查这个工厂的 bean 创建阶段是否已经开始，即在此期间是否有任何 bean 被标记为已创建。 if (hasBeanCreationStarted()) { //... 省略暂不会执行的代码 } else { // Still in startup registration phase: 仍处于启动注册阶段 // beanName 作为 key, beanDefinition 是 value, 这些都是参数传入的 this.beanDefinitionMap.put(beanName, beanDefinition); // bean 工厂还维护了所有的 beanName，有序的 this.beanDefinitionNames.add(beanName); // 不懂，先跳过 this.manualSingletonNames.remove(beanName); } // 不懂。先跳过 this.frozenBeanDefinitionNames = null; } // 省略后置代码，先跳过 } 从上面代码可知，注册 BeanDefinition 也是有注册器的 beanFactory 实现的。还记得 AnnotatedBeanDefinitionReader 这个类的注释说道：用于注解 bean 类的编程注册。这是 ClassPathBeanDefinitionScanner 的替代方案，应用相同的注解，但仅适用于显式注册的类. 显示注册 就是手动把事先定义好的类添加到 beanDefinitionMap。 那么猜测一下 ClassPathBeanDefinitionScanner 是不是自动扫描我们系统自定义的类，然后把我们自定义类自动添加到 beanDefinitionMap 中的。比如被 @Service @Controller @Bean 修饰的类。我们使用 Spring 时，可没有把这些类向 bean 工厂手动注册\n这个方法的后面还注册了很多其他内置的类。就不一一罗列了。之后这个方法就结束了。\n总结一下 this.reader = new AnnotatedBeanDefinitionReader(this); 都干了哪些事情：\nnew 一个 AnnotatedBeanDefinitionReader Bean 定义读取类 在 new 的过程中，把 spring 内置的各种类注册到 beanDefinitionMap 中。 从这个方法中我们可以知道：\nAnnotationConfigApplicationContext 中有 AnnotatedBeanDefinitionReader 类型的属性 BeanDefinition 是用来描述各种 bean 的类，类似建筑图纸和建筑物的关系 BeanDefinitionHolder 只是 BeanDefinition 的持有者，相当于为 BeanDefinition 增加了名称和别名 AnnotationConfigApplicationContext 中有 beanFactory, beanDefinition 的注册都是由 beanFactory 完成的。 BeanFactory 中有 beanDefinitionMap, 用来存放 beanDefinition. BeanFactory 中还有 beanDefinitionNames, 存放所有的 beanDefinition 的名称 目前为止我们只知道向注册器中注册了一堆内置的类。还没有看到这些类的用法， 大胆猜测一下，这些内置类是一些创世纪的类，后面用到的类扫描，属性注入，切面等，可能都是由这些类实现的。\n初始化 ClassPathBeanDefinitionScanner 这里主要是初始化一个扫描器，扫描器的作用就是把某个路径下的 class 文件加载 jvm 中，然后找到这些类中可以使用的 Bean, 把这些 bean 注册进来。\n初始化 reader 后，接着就是初始化 scanner 字段了：\nthis.scanner = new ClassPathBeanDefinitionScanner(this); 一层层进入方法后，会到达这个地方：\npublic ClassPathBeanDefinitionScanner(BeanDefinitionRegistry registry, boolean useDefaultFilters, Environment environment, @Nullable ResourceLoader resourceLoader) { Assert.notNull(registry, \u0026#34;BeanDefinitionRegistry must not be null\u0026#34;); this.registry = registry; if (useDefaultFilters) { registerDefaultFilters(); } setEnvironment(environment); setResourceLoader(resourceLoader); } 这是一个构造器，第一个参数是register, 和 reader 一样，是 AnnotationConfigApplicationContext 的对象，第二参数默认传的 true， 第三个参数是环境相关信息，第四个参数是一个 resourceLoader, 这里传入的也是 AnnotationConfigApplicationContext.\n其中 registerDefaultFilters(); 是一个默认的过滤器，其方法实现是：\nprotected void registerDefaultFilters() { // 首先把 Component 注解加入 this.includeFilters.add(new AnnotationTypeFilter(Component.class)); // 获取类加载器 ClassLoader cl = ClassPathScanningCandidateComponentProvider.class.getClassLoader(); try { // 想把 javax.annotation.ManagedBean 加载到 jvm 中，如果系统中没有这个包，就会异常 this.includeFilters.add(new AnnotationTypeFilter( ((Class\u0026lt;? extends Annotation\u0026gt;) ClassUtils.forName(\u0026#34;javax.annotation.ManagedBean\u0026#34;, cl)), false)); logger.debug(\u0026#34;JSR-250 \u0026#39;javax.annotation.ManagedBean\u0026#39; found and supported for component scanning\u0026#34;); } catch (ClassNotFoundException ex) { // JSR-250 1.1 API (as included in Java EE 6) not available - simply skip. } try { // 想把 javax.inject.Named 加载到 jvm 中，如果系统中没有这个包，就会异常 this.includeFilters.add(new AnnotationTypeFilter( ((Class\u0026lt;? extends Annotation\u0026gt;) ClassUtils.forName(\u0026#34;javax.inject.Named\u0026#34;, cl)), false)); logger.debug(\u0026#34;JSR-330 \u0026#39;javax.inject.Named\u0026#39; annotation found and supported for component scanning\u0026#34;); } catch (ClassNotFoundException ex) { // JSR-330 API not available - simply skip. } } 这个方法想把 @Component 、@ManagedBean、 @Named 注解加入过滤器中，但是由于系统没有后面两个包，会走到异常处理块。所以过滤器中只会有 @Component.\n然后下面就是为扫描器设置环境信息，和资源加载信息了。这时候并没有开始扫描包。只是完成了扫描器的初始化。到此我们可以知道扫描器 scanner 中有下面几个属性：\n默认过滤器，用来判断哪些 bean 应该注册到工厂中 一些环境信息。暂时不深究 资源加载器 猜测应该是使用资源加载器把某个包下的所有 class 加载到 jvm 中，然后根据反射遍历每个 class 的信息，看看类上的注解是否在默认的过滤器中，如果在就注册到 bean 工厂中。 我们还需要了解一下资源加载器是什么？\nResourceLoader 进入 setResourceLoader 方法，最后会进入 PathMatchingResourcePatternResolver 类。这个类的根是 ResourceLoader. 进入这个接口看一下他都有哪些方法：\npublic interface ResourceLoader { /** Pseudo URL prefix for loading from the class path: \u0026#34;classpath:\u0026#34; */ String CLASSPATH_URL_PREFIX = ResourceUtils.CLASSPATH_URL_PREFIX; Resource getResource(String location); @Nullable ClassLoader getClassLoader(); } 就只有两个方法，到这里就比较明显了，不同的 ResourceLoader 实现类都会实现特定的 getResource 方法，功能就是根据一个位置，获取一个 Resource. 进入 Resource 类中可知，里面有很多和文件相关的方法。它保存着一个文件的详细信息，比如 文件所在的路径，文件的 File 对象等等。可以理解为 ResourceLoader 把某个路径的下的 class 文件封装成了 Resource 对象。\n而 PathMatchingResourcePatternResolver 正是 ResourceLoader 的一个实现，从这个类的注释可知它能够将指定的资源位置路径解析为一个或多个匹配的资源。源路径可以是与目标 Resource 一对一映射的简单路径，或者可以包含特殊的“ classpath*: ”前缀和/或内部 Ant 样式的正则表达式\n到这里已经知道 ResourceLoader 是什么了，暂不向下深究了。\n已经浏览完 AnnotationConfigApplicationContext 无参构造器中的功能了。接着往下看：\npublic AnnotationConfigApplicationContext(Class\u0026lt;?\u0026gt;... annotatedClasses) { this(); // 结束了 register(annotatedClasses); // 该它了 refresh(); } 通过 this() 这个构造方法，注册器 AnnotationConfigApplicationContext 中有了 reader, scanner, bean 工厂中有了一些内置类的 beanDefinition. scanner 中有了资源加载器。\n注册配置类 进入 register(annotatedClasses) :\npublic void register(Class\u0026lt;?\u0026gt;... annotatedClasses) { Assert.notEmpty(annotatedClasses, \u0026#34;At least one annotated class must be specified\u0026#34;); // 使用 reader 注册配置类 this.reader.register(annotatedClasses); } 上面已经说过，reader 是用来显式注册 beanDefinition 的。这里还是指定一个具体的 class 来注册，属于显式注册，所以用 reader 中的注册方法，没毛病。进入方法：\n//从给定的 bean 类注册一个 bean，从类声明的注释中派生其元数据。 //参数： //annotatedClass – bean 的类 //instanceSupplier – 创建 bean 实例的回调（可能为 null ） //name – bean 的显式名称 //qualifiers – 除了 bean 类级别的限定符之外，要考虑的特定限定符注释（如果有） //definitionCustomizers – 一个或多个用于自定义工厂 BeanDefinition 的回调，例如设置惰性初始化或主标志 \u0026lt;T\u0026gt; void doRegisterBean(Class\u0026lt;T\u0026gt; annotatedClass, @Nullable Supplier\u0026lt;T\u0026gt; instanceSupplier, @Nullable String name, @Nullable Class\u0026lt;? extends Annotation\u0026gt;[] qualifiers, BeanDefinitionCustomizer... definitionCustomizers) { // 把 class 转换成一个 BeanDefinition, 而这个 BeanDefinition 还保留了注解信息 AnnotatedGenericBeanDefinition abd = new AnnotatedGenericBeanDefinition(annotatedClass); // 如果这个类有 @Condition 注解，那么就会判断是否应该跳过 if (this.conditionEvaluator.shouldSkip(abd.getMetadata())) { return; } // 省略。... // 注册 BeanDefinition BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, this.registry); } 这里已经把配置类注册到工厂中了。 这里仅仅是把系统中的配置类进行注册，并没有开始扫描配置类中的代码。接下来就是 refresh 方法了：\nrefresh @Override public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // 刷新前做一些准备，记录开始时间等 prepareRefresh(); // 获取 bean 工厂 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 对 bean 工厂做一些预处理、先不展开 prepareBeanFactory(beanFactory); try { // 这个类是一个模板方法，给子类留了一个接口，可以做一些事情 // 当前类并没有做什么事情 是一个空方法 postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. // 调用 bean 工厂中所有 实现了 BeanFactoryPostProcessor 和 // BeanDefinitionRegistryPostProcessor 接口的 bean 的 postProcessBeanFactory 方法，或者 postProcessBeanDefinitionRegistry 方法， invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\u0026#34;Exception encountered during context initialization - \u0026#34; + \u0026#34;cancelling refresh attempt: \u0026#34; + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset \u0026#39;active\u0026#39; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring\u0026#39;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } invokeBeanFactoryPostProcessors 进入 invokeBeanFactoryPostProcessors(beanFactory) ：\n// 第一个参数 bean 工厂 // 第二参数是 BeanFactoryPostProcessor 的实现类。 这里是空集合 public static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List\u0026lt;BeanFactoryPostProcessor\u0026gt; beanFactoryPostProcessors) { // Invoke BeanDefinitionRegistryPostProcessors first, if any. Set\u0026lt;String\u0026gt; processedBeans = new HashSet\u0026lt;\u0026gt;(); if (beanFactory instanceof BeanDefinitionRegistry) { BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; List\u0026lt;BeanFactoryPostProcessor\u0026gt; regularPostProcessors = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;BeanDefinitionRegistryPostProcessor\u0026gt; registryProcessors = new ArrayList\u0026lt;\u0026gt;(); // 这个集合是空的，不会走这个循环 .. 省略 for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) { // .... 省略了 } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // Separate between BeanDefinitionRegistryPostProcessors that implement // PriorityOrdered, Ordered, and the rest. List\u0026lt;BeanDefinitionRegistryPostProcessor\u0026gt; currentRegistryProcessors = new ArrayList\u0026lt;\u0026gt;(); // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered. // 这里是个重点，通过 bean 工厂获取所有类型为 BeanDefinitionRegistryPostProcessor // 的 bean， 也就是说只要 bean 实现了这个接口，都会被获取到 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); // 遍历所有的 BeanDefinitionRegistryPostProcessor 类型的 bean for (String ppName : postProcessorNames) { // 如果这个 bean 还实现了 PriorityOrdered 接口 if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { // 把这个 bean 单独放到 currentRegistryProcessors 集合中 // 注意看这里，已经使用 getBean 方法，把该 BeanDefinition 进行实例化了 // 实例化 bean 时还要遵循 bean 的生命周期。下面会讲到 currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); // 同时把 bean 添加到 processedBeans 集合中 processedBeans.add(ppName); } } // 把这些有优先级的 bean 排序 sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); // 先调用这些有优先级的 bean 中的 postProcessBeanDefinitionRegistry 方法 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); // 清空有优先 bean 集合，注意 registryProcessors 并没有清空 currentRegistryProcessors.clear(); // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered. // 再一次获取 获取所有类型为 BeanDefinitionRegistryPostProcessor // 的 bean， 因为上面执行了 postProcessBeanDefinitionRegistry 方法 // 可能有新的 bean 被注册 postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); // 遍历 for (String ppName : postProcessorNames) { // 如果没有在集合中，并且属于 Ordered 类型，也就是说 bean 还实现了 Ordered 接口 if (!processedBeans.contains(ppName) \u0026amp;\u0026amp; beanFactory.isTypeMatch(ppName, Ordered.class)) { // 把有排序的 bean 加入 currentRegistryProcessors 集合 currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); // 同时加入 processedBeans 集合 processedBeans.add(ppName); } } // 排序 sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); // 再调用这些有排序的 bean 中的 postProcessBeanDefinitionRegistry 方法 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); // 清空 currentRegistryProcessors.clear(); // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear. // 同上，继续查找普通的 BeanDefinitionRegistryPostProcessor 的 bean. 直到没有出现新的了 boolean reiterate = true; while (reiterate) { reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); // 还有新的 bean 被加入，就不能终止，可能因为这个新的 bean 会引入更多的 bean reiterate = true; } } sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); } // Now, invoke the postProcessBeanFactory callback of all processors handled so far. invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); } else { // Invoke factory processors registered with the context instance. invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); } // 所有 BeanDefinitionRegistryPostProcessor 类型的 bean 的 postProcessBeanDefinitionRegistry 放法执行完后 // 再执行所有 BeanFactoryPostProcessor 类型的 bean 的 postProcessBeanFactory 方法 // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // 获取所有 BeanFactoryPostProcessor 类型的 bean // 因为 BeanDefinitionRegistryPostProcessor 继承了 BeanFactoryPostProcessor // 所以这里还会获取到所有 BeanDefinitionRegistryPostProcessor 的 bean // 当然这些 bean 已经处理过了，下面会跳过已经处理过的 bean String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // Separate between BeanFactoryPostProcessors that implement PriorityOrdered, // Ordered, and the rest. // 同上，分别获取有优先级的，有排序的，普通的 bean // 然后调用他们的 postProcessBeanFactory 方法 List\u0026lt;BeanFactoryPostProcessor\u0026gt; priorityOrderedPostProcessors = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; orderedPostProcessorNames = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; nonOrderedPostProcessorNames = new ArrayList\u0026lt;\u0026gt;(); for (String ppName : postProcessorNames) { // 跳过已经处理过的 bean if (processedBeans.contains(ppName)) { // skip - already processed in first phase above } // 优先级 else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); } // 有排序 else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { // 普通的 bean nonOrderedPostProcessorNames.add(ppName); } } // First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered. sortPostProcessors(priorityOrderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // Next, invoke the BeanFactoryPostProcessors that implement Ordered. List\u0026lt;BeanFactoryPostProcessor\u0026gt; orderedPostProcessors = new ArrayList\u0026lt;\u0026gt;(); for (String postProcessorName : orderedPostProcessorNames) { orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // Finally, invoke all other BeanFactoryPostProcessors. List\u0026lt;BeanFactoryPostProcessor\u0026gt; nonOrderedPostProcessors = new ArrayList\u0026lt;\u0026gt;(); for (String postProcessorName : nonOrderedPostProcessorNames) { nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // Clear cached merged bean definitions since the post-processors might have // modified the original metadata, e.g. replacing placeholders in values... beanFactory.clearMetadataCache(); } refresh 方法中的 invokeBeanFactoryPostProcessors() 作用是：\n找到所有实现了 BeanDefinitionRegistryPostProcessor 接口的 bean，然后调用他们的 postProcessBeanDefinitionRegistry 方法。 找到所有实现了 BeanFactoryPostProcessor 接口的 bean，然后调用他们的 postProcessBeanFactory 方法。 还记得在初始化 reader 时，注册了很多内置类到 bean 工厂中吗？其中第一个注册的就是：\nif (!registry.containsBeanDefinition(CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)); } 而 ConfigurationClassPostProcessor 正是实现了 BeanDefinitionRegistryPostProcessor 接口，所以会调用这个类的 postProcessBeanDefinitionRegistry 方法：\n/** * Build and validate a configuration model based on the registry of * {@link Configuration} classes. */ public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) { //.... } 翻译一下注释：\n基于 Configuration 类的注册表构建和验证配置模型。\n也就是说这个方法会解析我们传入的配置类，如果配置类上有 @ComponentScan 注解，还会扫描给定的包下的所有 class, 找到所有需要注册的 bean. 也就是在这里会把系统中我们定义的 bean 注册成 BeanDefinition 到工厂中。具体的扫描动作在 ClassPathBeanDefinitionScanner 类中的 doScan() 方法：\nBeanFactoryPostProcessor 上面我们说了。 spring IoC 容器在加载过程中会找到所有实现了 BeanFactoryPostProcessor 接口的 bean，然后调用他们的 postProcessBeanFactory 方法。而这个时机是在 bean 定义注册到工厂之后，bean 实例生成之前。所以如果我们自己的 bean 想在 bean 通过 bean 工厂做一些事情的时候，可以实现这个接口，做一些特殊的操作：下面是对加密的配置进行解密，\n数据库配置：\nmysql.host=127.0.0.1 mysql.port=13306 # 密码是加密的 mysql.password=SM@jfajoadajdfa 代码：\npublic class ContxtDemo { public static void main(String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(Config.class); // 获取解密后的密码 System.out.println(context.getEnvironment().getProperty(\u0026#34;mysql.password\u0026#34;)); } } @Configuration @ComponentScan(basePackageClasses = {Config.class}) @PropertySource(\u0026#34;classpath:/app.properties\u0026#34;) class Config{ } /** * 还实现了 EnvironmentAware 接口，可以获得 Environment 信息 */ @Component class DecryptConfig implements EnvironmentAware,BeanFactoryPostProcessor { private ConfigurableEnvironment environment; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { MutablePropertySources propertySources = environment.getPropertySources(); // 获取所有 PropertySource for (org.springframework.core.env.PropertySource\u0026lt;?\u0026gt; propertySource : propertySources) { Map\u0026lt;String,String\u0026gt; map = (Map)propertySource.getSource(); // 遍历所有配置信息， // 如果值是 SM@ 开头的，说明是加密信息，需要解密 map.forEach((k,v) -\u0026gt; { if(v.indexOf(\u0026#34;SM@\u0026#34;) == 0){ String ciphertext = v.substring(3) + \u0026#34;解密后\u0026#34;; map.put(k,ciphertext); } }); } } @Override public void setEnvironment(Environment environment) { // 得到环境信息 this.environment = (ConfigurableEnvironment)environment; } } 结果\njfajoadajdfa 解密后 finishBeanFactoryInitialization(beanFactory); 实例化剩余非懒加载的 bean, 为什么说是剩余的？ 因为在前面的 invokeBeanFactoryPostProcessors 方法中，可能有的 bean 已经调用 getBean() 方法进行实例化了。\n这里会看到 bean 实例化过程会执行留给用户的扩展点。下面列出过程重要的步骤：\n1. 实例化 bean instanceWrapper = createBeanInstance(beanName, mbd, args);//创建 bean 的实例。核心 2. 填充属性 opulateBean(beanName, mbd, instanceWrapper);//填充属性，重要 3. Aware 系列接口的回调 aware 系列接口的回调位于 initializeBean 中的 invokeAwareMethods 方法：\nprotected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) { if (System.getSecurityManager() != null) { AccessController.doPrivileged((PrivilegedAction\u0026lt;Object\u0026gt;) () -\u0026gt; { invokeAwareMethods(beanName, bean); return null; }, getAccessControlContext()); } else { // aware 系列接口回调 invokeAwareMethods(beanName, bean); } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { // BeanPostProcessor 方法回调 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { // initMethods 方法回调 invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \u0026#34;Invocation of init method failed\u0026#34;, ex); } if (mbd == null || !mbd.isSynthetic()) { // BeanPostProcessorsAfterInitialization 方法回调 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } 下面是 aware 方法：\nprivate void invokeAwareMethods(final String beanName, final Object bean) { if (bean instanceof Aware) { if (bean instanceof BeanNameAware) { ((BeanNameAware) bean).setBeanName(beanName); } if (bean instanceof BeanClassLoaderAware) { ClassLoader bcl = getBeanClassLoader(); if (bcl != null) { ((BeanClassLoaderAware) bean).setBeanClassLoader(bcl); } } if (bean instanceof BeanFactoryAware) { ((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this); } } } 4. postProcessBeforeInitialization public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) { Object current = processor.postProcessBeforeInitialization(result, beanName); if (current == null) { return result; } result = current; } return result; } 5. afterPropertiesSet 和 init-method 在 invokeInitMethods 方法中又掉了两个回调方法：\nprotected void invokeInitMethods(String beanName, final Object bean, @Nullable RootBeanDefinition mbd) throws Throwable { boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean \u0026amp;\u0026amp; (mbd == null || !mbd.isExternallyManagedInitMethod(\u0026#34;afterPropertiesSet\u0026#34;))) { if (logger.isDebugEnabled()) { logger.debug(\u0026#34;Invoking afterPropertiesSet() on bean with name \u0026#39;\u0026#34; + beanName + \u0026#34;\u0026#39;\u0026#34;); } if (System.getSecurityManager() != null) { try { AccessController.doPrivileged((PrivilegedExceptionAction\u0026lt;Object\u0026gt;) () -\u0026gt; { ((InitializingBean) bean).afterPropertiesSet(); return null; }, getAccessControlContext()); } catch (PrivilegedActionException pae) { throw pae.getException(); } } else { // 回调 afterPropertiesSet 方法 ((InitializingBean) bean).afterPropertiesSet(); } } if (mbd != null \u0026amp;\u0026amp; bean.getClass() != NullBean.class) { String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) \u0026amp;\u0026amp; !(isInitializingBean \u0026amp;\u0026amp; \u0026#34;afterPropertiesSet\u0026#34;.equals(initMethodName)) \u0026amp;\u0026amp; !mbd.isExternallyManagedInitMethod(initMethodName)) { // 回调 init 方法 invokeCustomInitMethod(beanName, bean, mbd); } } } 6. postProcessAfterInitialization public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) { // 回调 postProcessAfterInitialization 方法 Object current = processor.postProcessAfterInitialization(result, beanName); if (current == null) { return result; } result = current; } return result; } Spring Bean 的生命周期 实例化 Bean 对象，这个时候 Bean 的对象是非常低级的，基本不能够被我们使用，因为连最基本的属性都没有设置，可以理解为 连 Autowired 注解都是没有解析的； 填充属性，当做完这一步，Bean 对象基本是完整的了，可以理解为 Autowired 注解已经解析完毕，依赖注入完成了； 如果 Bean 实现了 BeanNameAware 接口，则调用 setBeanName 方法； 如果 Bean 实现了 BeanClassLoaderAware 接口，则调用 setBeanClassLoader 方法； 如果 Bean 实现了 BeanFactoryAware 接口，则调用 setBeanFactory 方法； 调用 BeanPostProcessor 的 postProcessBeforeInitialization 方法； 如果 Bean 实现了 InitializingBean 接口，调用 afterPropertiesSet 方法； 如果 Bean 定义了 init-method 方法，则调用 Bean 的 init-method 方法； 调用 BeanPostProcessor 的 postProcessAfterInitialization 方法；当进行到这一步，Bean 已经被准备就绪了，一直停留在应用的\n上下文中，直到被销毁； 如果应用的上下文被销毁了，如果 Bean 实现了 DisposableBean 接口，则调用 destroy 方法，如果 Bean 定义了 destory-method\n声明了销毁方法也会被调用。 写个程序验证一下：\npublic class ContxtDemo { public static void main(String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(Config.class); } } @Configuration @ComponentScan(basePackageClasses = {Config.class}) @PropertySource(\u0026#34;classpath:/app.properties\u0026#34;) class Config{ @Bean(initMethod = \u0026#34;init\u0026#34;, destroyMethod = \u0026#34;des\u0026#34;, name=\u0026#34;car\u0026#34;) public Car car(){ return new Car(); } } class Car implements BeanPostProcessor, InitializingBean, BeanNameAware, BeanFactoryAware, BeanClassLoaderAware, DisposableBean { int age; public Car() { System.out.println(\u0026#34;构造方法\u0026#34;); } @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { if(\u0026#34;tank\u0026#34;.equals(beanName)) { System.out.println(\u0026#34;postProcessBeforeInitialization\u0026#34;); } return bean; } public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if(\u0026#34;tank\u0026#34;.equals(beanName)) { System.out.println(\u0026#34;postProcessAfterInitialization\u0026#34;); } return bean; } @Override public void afterPropertiesSet() throws Exception { System.out.println(\u0026#34;afterPropertiesSet\u0026#34;); } @Override public void setBeanClassLoader(ClassLoader classLoader) { System.out.println(\u0026#34;setBeanClassLoader\u0026#34;); } @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { System.out.println(\u0026#34;setBeanFactory\u0026#34;); } @Override public void setBeanName(String name) { System.out.println(\u0026#34;setBeanName\u0026#34;); } @Override public void destroy() throws Exception { System.out.println(\u0026#34;destroy\u0026#34;); } private void init() { System.out.println(\u0026#34;initMe\u0026#34;); } private void des() { System.out.println(\u0026#34;desM\u0026#34;); } } @Component class Tank { } 结果:\n构造方法 setBeanName setBeanClassLoader setBeanFactory afterPropertiesSet initMe postProcessBeforeInitialization postProcessAfterInitialization ","date":"2022-11-08T14:48:39Z","permalink":"https://dccmmtop.github.io/posts/springioc%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/","section":"posts","tags":["java","spring"],"title":"spring IOC 加载过程简要介绍"},{"categories":null,"contents":"我们先不粘贴官方的晦涩定义，通过一个例子来一步一步的引出 IoC 思想，真正的体会到 IoC 思想对我们编程带来的益处。\n车，引擎，轮胎 一个汽车需要引擎和轮胎，有下面模型：\n/** * 东风汽车 */ class NissanCar{ private Engine engine; private Tyre tyre; public NissanCar() { this.engine = new EngineV1(); this.tyre = new TyreV1(); } } /** * 大众汽车 */ class VolkswagenCar{ private Engine engine; private Tyre tyre; public VolkswagenCar() { this.engine = new EngineV1(); this.tyre = new TyreV1(); } } // .... 还有几十种其他品牌的车。用的都是 V1 引擎 /** * 引擎接口，输出动力 */ interface Engine { void outputPower(); } class EngineV1 implements Engine { public void outputPower() { } } /** * 轮胎接口，可以转动 */ interface Tyre { void turn(); } class TyreV1 implements Tyre { public void turn() { } } 无论什么汽车都需要引擎和轮胎，我们构造一个汽车时，必须同时构造一个引擎和轮胎的对象，这样的汽车才能正常工作，这也是我们正常的编码方式，一切都很美好。直到有一天，发生了一起严重的交通事故，某人正在开着汽车，突然加速，无法控制。直到撞上了其他汽车才停下来，同时也受了非常严重的伤。汽车厂商的工程师排查很久发现是引擎出了问题，在某些情况下，会突然不受控制，以最大马力转动。\n生产引擎的厂商很快修复了这个问题，并推出了 v2 版本的引擎。只要所有的车都换了最新的引擎就彻底解决了这个问题。.. 这个现象对应到我们上面的模型，就是：\nclass EngineV2 implements Engine { public void outputPower() { } } 然后所有汽车的构造方法中，都要修改成：this.engine = new EngineV2()。 看起来也不是什么大问题，我们借助编辑器，来个全局替换不就行了。\n等等，别忘了我们编写的这程序只是一个示例，非常非常简单，实际应用中我们不可能有这么简单的应用，这么清晰明了的依赖关系，我们应用中的用户依赖汽车，汽车依赖引擎，依赖轮胎，依赖座椅，依赖变速系统，刹车系统，而变速系统可能会和引擎有交互，刹车系统也可能和引擎有交互等等，非常复杂的依赖关系。可不敢全局替换后直接提交到生产运行。我们必须要做详尽的测试，\n我们的程序不可能是写完后就一层不变的，我们的程序就是为了解决现实生活中的问题，而现实生活最不缺的就是变化，如果需求变化，我们的程序都要发生如此之大的改动，对程序员来说简直是秃顶之灾。必须寻求解决之道。\n工厂方法 灾难发生的原因就在与底层的引擎发生了变化，而引擎被众多汽车依赖，汽车也就必须发生变化，如果众汽车厂商在生产汽车的时候，不要主动的去创建某种类型的引擎，而是告诉引擎工厂：给我一个引擎。如果以后在需要更换引擎的时候，众多汽车厂商不再做什么改动，仍是告诉引擎工厂：给我一个引擎，引擎工厂自会把合适引擎返回给汽车厂商，如下：\n/** * 东风汽车 */ class NissanCar{ private Engine engine; private Tyre tyre; public NissanCar() { this.engine = EngineFactory.get(); this.tyre = TyreFactory.get(); } } /** * 大众汽车 */ class VolkswagenCar{ private Engine engine; private Tyre tyre; public VolkswagenCar() { this.engine = EngineFactory.get(); this.tyre = TyreFactory.get(); } } // .... 还有几十种其他品牌的车。用的都是 V1 引擎 /** * 引擎工厂 */ class EngineFactory { public static Engine get(){ return new EngineV2(); } } /** * 轮胎工厂 */ class TyreFactory { public static Tyre get(){ return new TyreV1(); } } 从原来汽车厂商自己构造引擎，然后组装到车上（赋值动作）, 简化到直接组装引擎即可，不再关注引擎如何制造了。再升级引擎时，那就是引擎工厂的事了，和汽车厂商无关，而引擎工厂只有一个，升级的工作量也比较少。代码改动少，那么对整体系统的影响就比较小。世界又再次美好了。 但是刚刚说过，这只是一个简单的示例程序，依赖关系比较清晰，真实世界中，一辆车不可能只有引擎，轮胎，还会有座椅，方向盘，灯，刹车，变速箱等等。可能会觉得，那就继续添加工厂呗，给每个零件都添加一个工厂方法。组装汽车需要的零件都从工厂方法中获取，不要自己 new 对象出来。嗯，这样当然可以，但是一辆车至少需要上百上千个零件，我们都要自己去建工厂方法，然后组装吗？这样太累了。\n我们再看一下构造汽车都做了哪些工作？\n声明需要的配件（定义属性： 如：private Engine engine) 获取配件（在构造方法中调用配件的工厂方法，如： EngineFactory.get()) 组装配件（在构造方法中赋值，如 this.engine = EngineFactory.get()) 不管是汽车厂商自己构造配件，还是通过工厂方法构造，都是我们自己 new 出来的配件对象（步骤 2)，然后由自己把配件对象赋值给汽车，解决汽车与配件的依赖关系（步骤 3), 我们再深入思考一下，可以不可以制作一个智能的汽车生产车间，我们把定义好的汽车模型和所有配件模型告诉这个生产车间（步骤 1)， 然后生产车间会根据已经定义好的配件模型和汽车模型自己生产配件（步骤 2)，把配件组装到汽车中（步骤 3)。\n其实这是完全可以的，程序员已经把所有的类定义好了，类中所依赖其他对象也是明确的，像自动 new 对象以及为对象属性赋值这种机械动作就可以由某个程序自动化执行。下面我们看下如何制造这种“车间”\n车间 首先生成一个引擎对象，轮胎对象放入一个池中。 解析汽车需要的所有配件 去池中查找配件，把找到的配件赋值为汽车对应的属性,构造出一辆新汽车 把构造好的汽车对象放入池中 这里做了一些简化：\n车间知道要先构建引擎对象,轮胎对象，然后再构建汽车对象 引擎不依赖其他配件 如果引擎依赖另外的A配件，而A配件又依赖另外的B配件。我们是不是要先构建B配件，然后再构建A配件，再构建引擎。按照这个固定顺序才可以正常工作。如果真是这样，那么这个车间太脆弱了，太死板了。可以让车间智能一点，在构建汽车对象时，发现汽车依赖引擎，就先让汽车等一等，先把引擎构建出来，在构建引擎时，发现引擎依赖A配件，就让引擎等一等，先去构建A配件。每构造出一个完整的配件，就把这个配件放入池中，下次需要时直接使用。\n上述步骤对应到代码中，我们就需要获取类中有哪些字段，字段的类型是什么，这些都需要靠反射实现，如下完整示例:\n/** * 东风汽车 */ class NissanCar{ private Engine engine; private Tyre tyre; void run(){ this.engine.outputPower(); this.tyre.turn(); System.out.println(\u0026#34;东风汽车开跑\u0026#34;); } } /** * 大众汽车 */ class VolkswagenCar{ private Engine engine; private Tyre tyre; void run(){ this.engine.outputPower(); this.tyre.turn(); System.out.println(\u0026#34;大众汽车开跑\u0026#34;); } } // .... 还有几十种其他品牌的车。用的都是 V1 引擎 class EngineFactory { public static Engine get(){ return new EngineV2(); } } class TyreFactory { public static Tyre get(){ return new TyreV1(); } } /** * 引擎接口，输出动力 */ interface Engine { void outputPower(); } class EngineV1 implements Engine { @Override public void outputPower() { System.out.println(\u0026#34;我是 Engine1\u0026#34;); } } class EngineV2 implements Engine { @Override public void outputPower() { System.out.println(\u0026#34;我是 Engine2\u0026#34;); } } /** * 轮胎接口,可以转动j */ interface Tyre { void turn(); } class TyreV1 implements Tyre { @Override public void turn() { System.out.println(\u0026#34;我是 Tyre1\u0026#34;); } } /** * Author: dc * Date: 2022/11/4 16:03 **/ public class IocDemo { /** * 存放实例化的对象，key 类类名，value 是对象 */ public static Map\u0026lt;String,Object\u0026gt; objectPool = new HashMap\u0026lt;\u0026gt;(); public static void buildObjectPool() throws InstantiationException, IllegalAccessException { // 将待实例化的类添加到集合中 // 这一步可以写一个自定义注解实现，把所有需要自动化实例的类上面添加我们自定义注解, 然后扫描所有类，把包含这个注解的类添加到集合中 List\u0026lt;Class\u0026lt;?\u0026gt;\u0026gt; classList = new ArrayList\u0026lt;\u0026gt;(); classList.add(NissanCar.class); classList.add(VolkswagenCar.class); classList.add(EngineV1.class); classList.add(EngineV2.class); classList.add(TyreV1.class); // 所有类是否都已经实例化 boolean okFlag = false; // 遍历所有待实例化的集合，逐一实例化 // 并不能一次循环就可以全部实例化完毕，假如A依赖B, 但是B还没有实例化，所以A暂时也不能实例化，等下一次循环，B 实例化后，再实例化A // 如果 A 依赖B, B 依赖A, 那么在这个循环永远不能结束。产生了循环依赖，这里仅做示例。不考虑循环依赖 while(!okFlag){ okFlag = true; for (Class\u0026lt;?\u0026gt; klass : classList) { // 如果还没有被实例化 if(!objectPool.containsKey(klass.getName())){ // 利用反射，实例化该类。 如果klass有其他未实例化的依赖，o 等于 null Object o = getInstance(klass); if(o != null){ // 类名作为 key objectPool.put(klass.getSimpleName(), o); // 如果该类实现了一些接口，那么接口对应的实例也是该类的实例 for (Class\u0026lt;?\u0026gt; klassInterface : klass.getInterfaces()) { objectPool.put(klassInterface.getSimpleName(), o); } }else{ // 还存在没有实例化的类，不能结束 okFlag = false; } } } } } private static Object getInstance(Class\u0026lt;?\u0026gt; klass) throws IllegalAccessException, InstantiationException { // new 一个空对象 Object o = klass.newInstance(); String beanName = \u0026#34;\u0026#34;; // 为对象中的所有字段赋值 for (Field field : klass.getDeclaredFields()) { // 获取字段的类型 beanName = field.getType().getSimpleName(); // 所依赖的字段的实例暂时在对象池中找不到,暂不实例化 if(!objectPool.containsKey(beanName)){ return null; } // 设置私有变量可以访问 field.setAccessible(true); // 从对象池中获取该字段的实例 Object value = objectPool.get(beanName); // 为字段赋值 field.set(o,value); } return o; } public static void main(String[] args) throws InstantiationException, IllegalAccessException { buildObjectPool(); /** * 通过类型向容器中取对象，对象中的所有依赖已经自动处理完毕。 */ NissanCar nissanCar = (NissanCar)objectPool.get(\u0026#34;NissanCar\u0026#34;); nissanCar.run(); VolkswagenCar volkswagenCar = (VolkswagenCar) objectPool.get(\u0026#34;VolkswagenCar\u0026#34;); volkswagenCar.run(); } } 结果:\n我是 Engine2 我是 Tyre1 东风汽车开跑 我是 Engine2 我是 Tyre1 大众汽车开跑 在 main方法中，以及各种car类中，都没有使用new关键词构建对象，而是靠反射技术，自动解析各类之间的依赖关系，赋值。从而把一个个完整对象放入对象池。\n上面的代码只是简单的示例，很多问题没有解决，如 循环依赖问题，实例冲突问题(一个接口多个实现类的)。虽然比较简陋，但是也足以展现自动化配置的思路了。\n再谈IoC 在最开始，直接在汽车的构造方法中 new 出来所有依赖的配件，这是汽车主动构建配件，如果某个配件发生变化，所有汽车都在做出改变。\n后来把汽车主动构建配件步骤交给一个个的配件工厂，如果某个配件发生变化，直接改动对应的工厂就行了。\n上面这两种方法都会随着配件的增加，导致模板代码越来越多。如果有上千个配件，对于1：要在构造方法中一个个构造出配件，然后赋值给汽车对应的字段。对于2: 虽然把配件的构造步骤转移到工厂方法中，避免了大批量改动代码的问题，但是我们要写很多个工厂方法，工厂方法内容都差不多。 如果我们再开发其他系统，汽车系统中所有工厂方法都不能复用。\n最后我们制造了一个智能车间(buildObjectPool)，只需把所有汽车，配件的类告诉这个车间，车间就会自动维护他们的依赖关系，创建好各个对象放入池中(objectPool)，等待我们使用。同时这个车间没有任何类型信息，我们可以把它不加改动的用于任何系统。 把1、2 和 3 进行比较，发现最大的不同在于：对象的依赖关系本来由对象自己解决变成了由外部工具解决\n这就是 IoC 的思想, 上面我们写的 buildObjectPool 工具，就是 IoC 思想的简单实现， 更出名的一个IoC思想的技术实现是——依赖注入(DI)\n切记 IoC 不是技术， 它是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。\n其实IoC对编程带来的最大改变不是从代码上，而是从思想上，发生了“主从换位”的变化。应用程序原本是老大，要获取什么资源都是主动出击，但是在IoC/DI思想中，应用程序就变成被动的了，被动的等待IoC容器来创建并注入它所需要的资源了。\nIoC很好的体现了面向对象设计法则之一—— 好莱坞法则：“别找我们，我们找你”；即由IoC容器帮对象找相应的依赖对象并注入，而不是由对象主动去找。\n关于更多的 IoC 概念，参考:\n维基百科 百度百科 ","date":"2022-11-04T15:49:47Z","permalink":"https://dccmmtop.github.io/posts/ioc%E6%80%9D%E6%83%B3%E8%AE%B2%E8%A7%A3%E5%92%8C%E5%AE%9E%E7%8E%B0/","section":"posts","tags":["java"],"title":"IoC 思想和实现"},{"categories":null,"contents":"注解也被称为元数据，它为我们在代码中添加信息提供了一种形式化的方法，使我们可以在稍后某个时刻非常方便的使用这些数据。\n注解是受c#启发，在 javaSE5 中引入的，虽然javaSE5 预先定义了一些注解，但一般来说，主要还是程序员添加新的注解，并按自己的方式使用他们。\n内置注解 java SE5 内置了三种注解：\n@Override, 表示当前的方法覆盖超类中的方法。如果拼写错误，或者方法签名对不上被覆盖的方法，编译器就会发出错误提示， @Deprecated, 标记方法被弃用了，如果使用了被它标记的元素，编译器会发出警告信息 @SuppressWarnings, 关闭不当的编译器警告信息。 之外，java还提供了四种注解，专门负责新注解的创建，也被称为元注解。\n元注解 @Target 用来定义你的注解将用于什么地方，例如用在方法上，类上 还是字段上等， 使用方式： @Target(ElementType.METHOD), 其中 ElementType 的取值有下面几种:\nCONSTRUCTOR 构造器 FIELD 域， 包括 enum 的实例 LOCAL_VARIABLE 局部变量 METHOD 方法上 PACKAGE 包 PAEAMETER 参数 TYPE 类， 接口，包括注解类型 或 enum @Retention 表示在什么级别保存该注解的信息，例如源码中，类文件中，或者运行时，使用方式： @Retention(RetentionPolicy.RUNTIME), 其中 RetentionPolicy 的取值有：\nSOURCE 注解将被编译器丢弃 CLASS 注解在class文件中可用，但会被 JVM 丢弃 RUNTIME JVM 在运行期也保留注解，因此可以通过反射机制读取注解信息 @Documented 将此注解包含在 javadoc中\n@Inherited 允许子类继承父类中的注解\n编写注解 一个注解得以工作要有三个要素：\n定义注解 使用注解 注解处理器 下面用一个对象/关系映射功能 ORM的简单例子来展现如何编写自定义注解：\n@DBTable 将此注解用于 JavaBean 上，以便生成一个数据库表 @Constrains此注解用于字段上，描述该字段有一些约束，比如不能为空，要唯一等 @SQLString 将此注解用于字段上，会生成一个 varchar 类型的列 @SQLInt 将此注解用于字段上，会生成一个 int 类型的列 定义注解 /** * 注解定义 */ @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) public @interface DBTable{ // 表名, 规定：必须要有一个默认值， String name() default \u0026#34;\u0026#34;; } /** * 字段的约束 */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) public @interface Constraints{ // 是否是主键 boolean primary() default false; // 是否可以为空 boolean allowNull() default true; // 是否唯一 boolean unique() default false; } /** * 生成 varchar 类型的列 */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) public @interface SQLString{ /** * 让value代表 varchar的长度，value 是个比较特殊的名字，在使用该注解的时候， * 如果该元素是唯一需要赋值的元素。那么可以简写，无需使用 名———值 语法，而是 * @SQLString(30) */ int value() default 0; // 列的名字 String name() default \u0026#34;\u0026#34;; } /** * 生成 int 类型的列 */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) public @interface SQLInt{ // 列的名字 String name() default \u0026#34;\u0026#34;; } 注解的使用 /** * 注解使用 */ @DBTable(name = \u0026#34;users\u0026#34;)// 表名是useres public class User { /** * 设置id是主键，唯一， 不能为空 */ @Constraints(primary = true, allowNull = false, unique = true) @SQLInt private int id; @SQLInt private int age; @SQLString(20) private String name; public int getId() { return id; } public void setId(int id) { this.id = id; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } } 注解处理器 /** * 注解处理器 */ public class TableCreator{ public String createTableSQLString(Class\u0026lt;?\u0026gt; cl) { // 获取类上的 DBTable 注解 DBTable table = cl.getAnnotation(DBTable.class); if(table == null) { return \u0026#34;\u0026#34;; } // 获取 DBTable 注解中的 name 参数 String tableName = table.name(); // 保存所有列名 List\u0026lt;String\u0026gt; columnNameList = new ArrayList\u0026lt;String\u0026gt;(); // 反射的方式遍历类中的字段 for (Field field : cl.getDeclaredFields()) { // 列名 String columnName = \u0026#34;\u0026#34;; // 遍历字段上的注解 for (Annotation annotation : field.getAnnotations()) { // 如果包含 SQLInt 注解 if(annotation instanceof SQLInt) { SQLInt sint = (SQLInt) annotation; if(sint.name().length() == 0) { columnName = field.getName().toLowerCase(); }else{ // 如果没有设置name属性，就用字段名作为列名 columnName = sint.name(); } columnName += \u0026#34; INT\u0026#34;; // 获取字段上的约束类型的注解 Constraints con = field.getAnnotation(Constraints.class); columnName += buildConstraints(con); // 否则检查是否包含 @SQLString 注解 }else if(annotation instanceof SQLString) { SQLString sString = (SQLString) annotation; if(sString.name().length() == 0) { columnName = field.getName().toLowerCase(); }else{ // 如果没有设置name属性，就用字段名作为列名 columnName = sString.name(); } // 获取字段长度属性 columnName += \u0026#34; VARCHAR(\u0026#34; + sString.value() + \u0026#34;)\u0026#34;; // 获取字段上的约束类型的注解 Constraints con = field.getAnnotation(Constraints.class); columnName += buildConstraints(con); } } columnNameList.add(columnName); } StringBuilder createConmmand = new StringBuilder(\u0026#34;\u0026#34;); createConmmand.append(\u0026#34;CREATE TABLE \u0026#34;).append(tableName).append(\u0026#34;(\u0026#34;); int length = columnNameList.size(); for (int i = 0; i \u0026lt; length; i ++) { createConmmand.append(\u0026#34;\\n \u0026#34;).append(columnNameList.get(i)); if(i != length -1){ createConmmand.append(\u0026#34;,\u0026#34;); } } createConmmand.append(\u0026#34;);\u0026#34;); return createConmmand.toString(); } public String buildConstraints(Constraints con){ if(con == null) { return \u0026#34;\u0026#34;; } if(!con.allowNull()){ return \u0026#34; NOT NULL\u0026#34;; } if(con.primary()){ return \u0026#34; PRIMARY KEY\u0026#34;; } if(con.unique()) { return \u0026#34; UNIQUE\u0026#34;; } return \u0026#34;\u0026#34;; } } public class AnnotationDemo { public static void main(String[] args) throws ClassNotFoundException { Class\u0026lt;?\u0026gt; userClass = Class.forName(\u0026#34;io.dc.User\u0026#34;); String sql = new TableCreator().createTableSQLString(userClass); // 直接下面的也行 // String sql = new TableCreator().createTableSQLString(User.class); System.out.println(sql); } } 注解使用注意 不同注解可以组合使用，但同类型的注解不能重复使用 目前注解不支持继承 注解中的属性不能为 null, 本例中的 @SQLString name() 不能为 null, 必须设置 default 注解元素只能一下几种,否则编译器会报错 所有基本类型(int, float, boolean 等) String Class enum Annotation 以上类型的数组 完整代码 为了测试方便，把所有类都放在一个文件中了：\npackage io.dc; import java.lang.annotation.*; import java.lang.reflect.Field; import java.util.ArrayList; import java.util.List; /** * 注解定义 */ @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @interface DBTable{ // 表名, 规定：必须要有一个默认值， String name() default \u0026#34;\u0026#34;; } /** * 字段的约束 */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @interface Constraints{ // 是否是主键 boolean primary() default false; // 是否可以为空 boolean allowNull() default true; // 是否唯一 boolean unique() default false; } /** * 生成 varchar 类型的列 */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @interface SQLString{ /** * 让value代表 varchar的长度，value 是个比较特殊的名字，在使用该注解的时候， * 如果该元素是唯一需要赋值的元素。那么可以简写，无需使用 名———值 语法，而是 * @SQLString(30) */ int value() default 0; // 列的名字 String name() default \u0026#34;\u0026#34;; } /** * 生成 int 类型的列 */ @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @interface SQLInt{ // 列的名字 String name() default \u0026#34;\u0026#34;; } /** * 注解使用 */ @DBTable(name = \u0026#34;users\u0026#34;)// 表名是useres class User { /** * 设置id是主键，唯一， 不能为空 */ @Constraints(primary = true, allowNull = false, unique = true) @SQLInt private int id; @SQLInt private int age; @SQLString(20) private String name; public int getId() { return id; } public void setId(int id) { this.id = id; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } } /** * 注解处理器 */ class TableCreator{ public String createTableSQLString(Class\u0026lt;?\u0026gt; cl) { // 获取类上的 DBTable 注解 DBTable table = cl.getAnnotation(DBTable.class); if(table == null) { return \u0026#34;\u0026#34;; } // 获取 DBTable 注解中的 name 参数 String tableName = table.name(); // 保存所有列名 List\u0026lt;String\u0026gt; columnNameList = new ArrayList\u0026lt;String\u0026gt;(); // 反射的方式遍历类中的字段 for (Field field : cl.getDeclaredFields()) { // 列名 String columnName = \u0026#34;\u0026#34;; // 遍历字段上的注解 for (Annotation annotation : field.getAnnotations()) { // 如果包含 SQLInt 注解 if(annotation instanceof SQLInt) { SQLInt sint = (SQLInt) annotation; if(sint.name().length() == 0) { columnName = field.getName().toLowerCase(); }else{ // 如果没有设置name属性，就用字段名作为列名 columnName = sint.name(); } columnName += \u0026#34; INT\u0026#34;; // 获取字段上的约束类型的注解 Constraints con = field.getAnnotation(Constraints.class); columnName += buildConstraints(con); // 否则检查是否包含 @SQLString 注解 }else if(annotation instanceof SQLString) { SQLString sString = (SQLString) annotation; if(sString.name().length() == 0) { columnName = field.getName().toLowerCase(); }else{ // 如果没有设置name属性，就用字段名作为列名 columnName = sString.name(); } // 获取字段长度属性 columnName += \u0026#34; VARCHAR(\u0026#34; + sString.value() + \u0026#34;)\u0026#34;; // 获取字段上的约束类型的注解 Constraints con = field.getAnnotation(Constraints.class); columnName += buildConstraints(con); } } columnNameList.add(columnName); } StringBuilder createConmmand = new StringBuilder(\u0026#34;\u0026#34;); createConmmand.append(\u0026#34;CREATE TABLE \u0026#34;).append(tableName).append(\u0026#34;(\u0026#34;); int length = columnNameList.size(); for (int i = 0; i \u0026lt; length; i ++) { createConmmand.append(\u0026#34;\\n \u0026#34;).append(columnNameList.get(i)); if(i != length -1){ createConmmand.append(\u0026#34;,\u0026#34;); } } createConmmand.append(\u0026#34;);\u0026#34;); return createConmmand.toString(); } public String buildConstraints(Constraints con){ if(con == null) { return \u0026#34;\u0026#34;; } if(!con.allowNull()){ return \u0026#34; NOT NULL\u0026#34;; } if(con.primary()){ return \u0026#34; PRIMARY KEY\u0026#34;; } if(con.unique()) { return \u0026#34; UNIQUE\u0026#34;; } return \u0026#34;\u0026#34;; } } public class AnnotationDemo { public static void main(String[] args) throws ClassNotFoundException { Class\u0026lt;?\u0026gt; userClass = Class.forName(\u0026#34;io.dc.User\u0026#34;); String sql = new TableCreator().createTableSQLString(userClass); // 直接下面的也行 // String sql = new TableCreator().createTableSQLString(User.class); System.out.println(sql); } } 结果:\nCREATE TABLE users( id INT NOT NULL, age INT, name VARCHAR(20)); Process finished with exit code 0 ","date":"2022-11-03T10:56:39Z","permalink":"https://dccmmtop.github.io/posts/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3/","section":"posts","tags":["java"],"title":"自定义注解"},{"categories":null,"contents":"代理是基本的设计模式之一，使用代理对象代替真实对象的方法调用，可以扩展真实对象某些方法的功能。\n在 java 中又分为静态代理和动态代理, 先看静态代理:\n静态代理 public class DynamicDemo { public static void main(String[] args) { /** * 静态代理示例 */ System.out.println(\u0026#34;=======静态代理========\u0026#34;); // user 真实对象 Save user = new UserDAO(); // userTransaction 是代理对象 Save userTransaction = new UserTransaction(user); // 使用代理对象执行方法 userTransaction.save(\u0026#34;dc1\u0026#34;); } } interface Save { /** * 保存 */ void save(String name); /** * 删除 * @param name */ void delete(String name); } class UserDAO implements Save { public void save(String name) { System.out.println(\u0026#34;真实对象执行 save 方法\u0026#34;); } public void delete(String name) { System.out.println(\u0026#34;真实对象执行 delete 方法\u0026#34;); } } /** * 手动编写代理类 */ class UserTransaction implements Save { /** * 真实对象 */ private Save realObject; UserTransaction(Save save) { this.realObject = save; } /** * 在真实对象前后做一些操作 */ public void save(String name) { System.out.println(\u0026#34;静态代理： 在真实对象做事之前，代理要做的事情\u0026#34;); // 真实对象开始执行save方法 realObject.save(name); System.out.println(\u0026#34;静态代理： 在真实对象做事之后，代理要做的事情\u0026#34;); } public void delete(String name) { /** * 什么都不做，直接执行真实对象的 delete 方法。 * 这里就体现了静态代理方式的弊端，哪怕我只想代理一个方法，也需要实现所有方法。 * 只用真实对象代理。有很多无意义的代码 */ realObject.save(name); } } 通过以上代码可以知道java中的静态代理有几个关键的地方：\n代理对象和真实对象实现了相同的接口，或者继承同一个类。换句话说就是可以向上转型为相同的对象 真实对象要作为代理对象中一个属性。方便真实对象调用方法 有时我们只想扩展原对象的某个方法，给这个方法添加一些功能。但是我们仍要编写一个代理类，实现真实类中的所有方法。导致很多冗余代码， java 中的动态代理可以解决这个问题\n动态代理 先看代码:\npublic class DynamicDemo { public static void main(String[] args) { /** * 动态代理示例 */ System.out.println(\u0026#34;======动态代理======\u0026#34;); // 动态生成一个代理对象 Save saveProxy = (Save) Proxy.newProxyInstance( // 真实对象的类加载器 user.getClass().getClassLoader(), // 要代理的接口，数组 new Class[]{ Save.class}, // 将 user 真实对象传入代理处理类 new DynamicHandler(user) ); // 代理对象执行方法，内部会调用真实对象执行对应方法 saveProxy.save(\u0026#34;dc1\u0026#34;); } } interface Save { /** * 保存 */ void save(String name); /** * 删除 * @param name */ void delete(String name); } class UserDAO implements Save { public void save(String name) { System.out.println(\u0026#34;真实对象执行 save 方法\u0026#34;); } public void delete(String name) { System.out.println(\u0026#34;真实对象执行 delete 方法\u0026#34;); } } /** * 代理对象要执行的方法 */ class DynamicHandler implements InvocationHandler { private Object realObject; public DynamicHandler(Object real) { this.realObject = real; } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\u0026#34;代理对象类:\u0026#34; + proxy.getClass()); System.out.println(\u0026#34;要代理的方法是: \u0026#34; + method.getName() ); System.out.println(\u0026#34;参数是:\u0026#34;); for (Object arg : args) { System.out.printf(arg + \u0026#34; \u0026#34;); } System.out.println(\u0026#34;\u0026#34;); if(method.getName().equals(\u0026#34;delete\u0026#34;)){ System.out.println(\u0026#34;执行了 delete 方法\u0026#34;); } /** * 这里的操作体现了动态代理的优点： 在需要代理的方法前后编写功能 */ if(method.getName().equals(\u0026#34;save\u0026#34;)){ System.out.println(\u0026#34;执行了 save 方法\u0026#34;); System.out.println(\u0026#34;真实对象执行save方法前\u0026#34;); } // 真实对象执行方法 method.invoke(this.realObject,args); if(method.getName().equals(\u0026#34;save\u0026#34;)){ System.out.println(\u0026#34;真实对象执行save方法后\u0026#34;); } return null; } } 从上面代码中可以发现，实现动态代理的关键点：\nInvocationHandler 的实现类，invoke 方法中编写你要扩展的功能 InvocationHandler 的实现类中要有一个属性保存真实对象。也就是例中的 realObject, 以便真实对象调用方法，如果不需要真实对象调用方法，那么也就不需要这个真实对象了。 使用 Proxy.newProxyInstance 方法创建代理对象。 动态代理实际上是JVM在运行期动态创建class字节码并加载的过程，它并没有什么黑魔法。它创建的字节码就是我们在静态代理中编写的代理类。只不过不用我们手动编写了而已。\n","date":"2022-11-02T16:57:07Z","permalink":"https://dccmmtop.github.io/posts/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E4%B8%8E%E9%9D%99%E6%80%81%E4%BB%A3%E7%90%86/","section":"posts","tags":["java"],"title":"动态代理与静态代理"},{"categories":null,"contents":"@ComponentScan 启用组件扫描 默认扫描与配置类相同的包以及子包 如何扫描指定的包 @ComponentScan(\u0026ldquo;包名\u0026rdquo;) 扫描一组包： @ComponentScan(basePackages={\u0026quot;xxx\u0026quot;,\u0026quot;xxx\u0026quot;}) 这种两种方式都是传入字符串，无法方便的重构。因为我们的编辑器一般不会解析你的字符串值，可以使用下面方式：\n扫描类和接口： @ComponentScan(basePackageClasses={xxx.class, xxxx.class}) 这些类所在的包会作为组件扫描的基础包，可以为需要扫描的包中添加一个空接口，标记该包需要扫描，这种方式避免了传入字符串，方便重构修改 @ContextConfiguration(classes=xxx.class) 需要在 xxx.class 中加载配置\n@Component 向容器中注册组件\nbean 的名字默认是类名首字母小写 也可以指定： @Component(\u0026ldquo;xxx\u0026rdquo;) @Named 可以替代 @Component @Autowired 用在构造器上 当 Spring 去创建该类的 bean 时，会调用这个构造方法，同时传入一个 满足参数的 bean\nClass User { private Car car; // 会使用这个方法去构造 bean, 而不是无 参构造器 @Autowired public User(Car car){ this.car = car; } } 用在 Setter 方法上\n其实可以用在类的任何方法上，Spring 尝试满足方法参数上声明的依赖，如果找不到对应的 bean 就会抛出异常\n@Autowired(required=false) 即使没找到也不会抛出异常，会传入 Null. 可能会导致空指针异常\n如果有多个 bean 都能满足，会抛出异常\n可以使用 Inject 替换，Inject 是 java 中依赖注入规范\n@Bean 显示配置 Bean, 可以把第三方对象设置成 Bean，因为我们无法直接在第三方类上添加@Component 注解 bean Id 默认是方法名，可以指定：@Bean(\u0026quot;xxx\u0026quot;) @Configuration @ComponentScan public class AutowiredDemoConfig { /** * User 是引入的第三方工具，无法修改 User 类，在类名上加 @Component 注解 */ @Bean public User user(){ return new User(10,\u0026#34;dc1\u0026#34;); } } @Profile(\u0026ldquo;dev\u0026rdquo;) 和@Bean 一起使用，当 dev 环境上才会创建 bean 配置\nspring.profiles.default=dev 或者 spring.profiles.active=dev 激活 dev 环境，如果没有激活的环境，只会创建那些没有定义在 profile 中的 bean 从 Spring 4 开始，依赖 @Conditional 注解 @Conditional Since 4.0 条件化的 bean 用到带有@Bean 的注解的方法上 给定条件的结果为 true 时，创建 bean 否则不创建 需要实现 Conditional 接口中的 matches 方法 @Primary 处理自动装配的歧义\n如果一个 Bean 是接口，系统中有多个实现类。注入时就会有歧义，抛出异常\n@Primary 用在 Bean 上，标记这个 bean 是首选的，发生歧义时首选它， 不会报错\n缺点： 只能使用其中一个 bean，无法指定注入哪个，@Qualifier 解决这个问题\n示例：\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = AutowiredDemoConfig.class) public class QualifierDemo { @Autowired private Dessert dessert; @Test public void eat(){ dessert.getName(); } } /** * Cake IceCream 都实现了 Dessert */ interface Dessert{ String getName(); } @Component @Primary // 会优先注入 Cake class Cake implements Dessert { public String getName() { String name = \u0026#34;Cake\u0026#34;; System.out.println(name); return name; } } @Component class IceCream implements Dessert{ public String getName() { String name = \u0026#34;IceCream\u0026#34;; System.out.println(name); return name; } } @Qualifier(\u0026ldquo;xxx bean name\u0026rdquo;) 与 @Autowired 一起使用时直接指定 bean 的名字进行装配： 与 @Component 一起使用，可以设置 bean 的名字 与 @Bean 一起使用，可以设置 bean 的名字 @Scope bean 的作用域 默认所有的 bean 都是单例的。Spring 定义了多种作用域，包括：\n单例 (Singleton): 整个应用中只创建一个 bean 实例 原型 (Prototype): 每次注入或者通过 Spring 应用上下文获取的时候，都会创建一个新的 bean。 会话 (Session): 在 Web 应用中，为每个会话创建一个实例 请求 (Request): 在 Web 应用中，为每个请求创建一个实例 示例 使用 @Scope 注解，改变bean的作用域\n@RunWith(SpringJUnit4ClassRunner.class) @ComponentScan @ContextConfiguration(classes = ScopeDemo.class) public class ScopeDemo { @Autowired @Qualifier(\u0026#34;singleSTU\u0026#34;) private Student s1; @Autowired @Qualifier(\u0026#34;singleSTU\u0026#34;) private Student s2; @Autowired @Qualifier(\u0026#34;prototypeSTU\u0026#34;) private Student s3; @Autowired @Qualifier(\u0026#34;prototypeSTU\u0026#34;) private Student s4; @Test public void scopeDemo(){ Assert.assertEquals(\u0026#34;s1 s2 是单例作用域，两者应该相等:\u0026#34;, s1.hashCode(), s2.hashCode()); Assert.assertNotEquals(\u0026#34;s3 s4 是原型作用域，两者应该不相等:\u0026#34;, s3.hashCode(), s4.hashCode()); } } @Configuration class Student { private int age; /** * 默认是单例作用域 * @return */ @Bean(\u0026#34;singleSTU\u0026#34;) public Student stu(){ Random random = new Random(); Student s1 = new Student(); s1.setAge(random.nextInt(100)); return s1; } /** * 设置原型作用域 * @return */ @Bean(\u0026#34;prototypeSTU\u0026#34;) @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) public Student stu1(){ Random random = new Random(); Student s1 = new Student(); s1.setAge(random.nextInt(100)); return s1; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 运行时注入外部值 把配置文件中的字段注入到对象中\nEnviroment 通过注解@PropertySource(\u0026ldquo;xxx\u0026rdquo;)讲配置文件中的信息注入到Enviorment 对象中:\n@RunWith(SpringJUnit4ClassRunner.class) @ComponentScan @ContextConfiguration(classes = EnviromentDemo.class) public class EnviromentDemo { @Autowired private Mysql mysql; @Test public void testDataSource(){ System.out.println(mysql.getPort()); } } @Configuration @PropertySource(\u0026#34;classpath:/app.properties\u0026#34;)// 将配置文件中的字段加载到 Environment 中，稍后可以通过 Environment 对象获取 class DataSource { @Autowired Environment env; @Bean public Mysql mysql(){ // 获取对应的字段值，getProperty 更多用法参考源码 return new Mysql(env.getProperty(\u0026#34;mysql.port\u0026#34;, Integer.class), env.getProperty(\u0026#34;mysql.host\u0026#34;)); } } class Mysql { private int port; private String host; public Mysql(int port, String host) { this.port = port; this.host = host; } public int getPort() { return port; } public void setPort(int port) { this.port = port; } public String getHost() { return host; } public void setHost(String host) { this.host = host; } } 通过属性占位符装配 还可以通过 ${} 占位符读取配置文件中的字段，注入到字段中； 如下:\n@RunWith(SpringJUnit4ClassRunner.class) @ComponentScan @ContextConfiguration(classes = ValueDemo.class) public class ValueDemo { @Autowired private Mysql1 mysql; @Test public void testDataSource(){ System.out.println(mysql.getPort()); } } @Configuration class DataSource1 { @Autowired Environment env; @Bean // 这个bean 是必须的。可以解析占位符,自动查找 public static PropertySourcesPlaceholderConfigurer propertySourcesPlaceholderConfigurer(){ return new PropertySourcesPlaceholderConfigurer(); } @Bean // ${} 占位符方式注入 public Mysql1 mysql(@Value(\u0026#34;${mysql.port}\u0026#34;) int port,@Value(\u0026#34;${mysql.host}\u0026#34;) String host){ // 获取对应的字段值，getProperty 更多用法参考源码 return new Mysql1(port, host); } } class Mysql1 { private int port; private String host; public Mysql1(int port, String host) { this.port = port; this.host = host; } public int getPort() { return port; } public void setPort(int port) { this.port = port; } public String getHost() { return host; } public void setHost(String host) { this.host = host; } } ","date":"2022-11-01T16:05:53Z","permalink":"https://dccmmtop.github.io/posts/bean%E8%A3%85%E9%85%8D%E7%9B%B8%E5%85%B3%E6%B3%A8%E8%A7%A3/","section":"posts","tags":["spring"],"title":"Bean装配相关注解"},{"categories":null,"contents":"1.从可上网的外部电脑下载需要的jar包，自行传到内部电脑maven仓库对应文件夹，比如\ncom.alibaba.easyExcel，【注意有些包会有其他的依赖jar包，需要一同下载】\n2.IDEA中添加jar包的maven依赖，比如\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;easyexcel\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.2-beta4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 3.设置IDEA的maven为离线模式，打勾。\n4.更改maven的setting.xml为离线模式\n\u0026lt;offline\u0026gt;true\u0026lt;/offline\u0026gt; 更改源 \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;central\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;central\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;file://D:\\MyRepository\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; 完成以上配置，不会在线下载依赖了，完全走本地。\n","date":"2022-10-31T12:18:31Z","permalink":"https://dccmmtop.github.io/posts/maven%E5%AE%8C%E5%85%A8%E4%BE%9D%E8%B5%96%E6%9C%AC%E5%9C%B0%E5%BA%93/","section":"posts","tags":["maven"],"title":"maven完全依赖本地库"},{"categories":null,"contents":"为什么需要 Callable 无论是继承 Thread 类，还是实现 Runnale 接口，或者使用线程池的 execute 方法去执行一个异步任务，都无法将这个任务的返回值带出来，以 Runnale 接口为例：\n/** * @since JDK1.0 */ @FunctionalInterface public interface Runnable { public abstract void run(); } 可以看到 run() 方法没有参数，没有返回值，没有抛出异常，因此不能适用于那些需要异步任务返回值得场景，由此就诞生了可以获取异步任务结果的工具： Callable\nCallable 又是 Doug Lea 大师\n/** * @see Executor * @since 1.5 * @author Doug Lea * @param \u0026lt;V\u0026gt; the result type of method {@code call} */ @FunctionalInterface public interface Callable\u0026lt;V\u0026gt; { V call() throws Exception; } Callable 是一个泛型接口，里面只有一个方法，call(), 可以返回泛型值V, Cacheable 与 Runnable 接口很是相似，下面看一下他们之间的区别\nCallable VS Runnable 两个接口都是用于多线程执行任务的，但是他们的使用场景的差别还是很大的。\n执行机制上的差别 Runnable 可以用在Thread类中，也可以在 ExecutorService 中配合线程池使用，但是 Callable 只能用于 ExecutorService 中，Thread 类中没有 Cacheable 的身影：\n异常处理的差别 Runnable 接口中的 run 方法签名上没有 throws, 自然也不能向上传播受检异常。而 Callable 接口中的 call f方法可以向上抛出异常。\n区别总结如下：\nCallable 在线程池中的应用 上面说过，Callable 接口只能在 ExecutorService 中使用，下面看一下如何使用的\n可以看到在 submit 方法中可以接收一个实现了 Callable 接口的任务， 返回的是 Future 类型。而 execute 方法无返回值。 那么 Future 是什么呢？ 如何通过 Future 拿到返回值呢？\nFuture Future 是一个接口，通过方法名就可以看出他们的用途：\n// 取消任务 boolean cancel(boolean mayInterruptIfRunning); // 获取任务执行结果 V get() throws InterruptedException, ExecutionException; // 获取任务执行结果，带有超时时间限制 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; // 判断任务是否已经取消 boolean isCancelled(); // 判断任务是否已经结束 boolean isDone(); 看一下如何使用：\npublic class FutureDemo { public static void main(String[] args) throws InterruptedException, ExecutionException { ExecutorService executorService = Executors.newSingleThreadExecutor(); // 使用 Callable ，可以获取返回值 Callable\u0026lt;String\u0026gt; callable = () -\u0026gt; { System.out.println(\u0026#34;进入 Callable 的 call 方法\u0026#34;); // 模拟子线程任务，在此睡眠 2s， // 小细节：由于 call 方法会抛出 Exception，这里不用像使用 Runnable 的run 方法那样 try/catch 了 Thread.sleep(5000); return \u0026#34;Hello from Callable\u0026#34;; }; System.out.println(\u0026#34;提交 Callable 到线程池\u0026#34;); Future\u0026lt;String\u0026gt; future = executorService.submit(callable); System.out.println(\u0026#34;主线程继续执行\u0026#34;); System.out.println(\u0026#34;主线程等待获取 Future 结果\u0026#34;); // //检查任务是否做完 // while(!future.isDone()) { // System.out.println(\u0026#34;Task is still not done...\u0026#34;); // Thread.sleep(1000); // } // Future.get() blocks until the result is available String result = future.get(); System.out.println(\u0026#34;主线程获取到 Future 结果: \u0026#34; + result); executorService.shutdown(); } } 结果如下:\n提交 Callable 到线程池 主线程继续执行 主线程等待获取 Future 结果 进入 Callable 的 call 方法 主线程获取到 Future 结果: Hello from Callable Process finished with exit code 0 如果子程序运行时间过长，或者其他原因，我们想 cancel 子程序的运行，则我们可以使用 Future 提供的 cancel 方法，继续对程序做一些修改\nwhile(!future.isDone()) { System.out.println(\u0026#34;子线程任务还没有结束...\u0026#34;); Thread.sleep(1000); double elapsedTimeInSec = (System.nanoTime() - startTime)/1000000000.0; // 如果程序运行时间大于 1s，则取消子线程的运行 if(elapsedTimeInSec \u0026gt; 1) { future.cancel(true); } } 特别注意的是，如果使用 cancel 方法取消了任务， get() 方法会抛出一个 CancellationException 异常。\n到这里已经知道和配合使用 Cacheable 和 Future 来获取异步任务的返回值了。总结就只有3点：\n待执行的任务要实现 Callable 接口中的 使用线程池中的submit 方法 通过 Future 获取任务的返回值 刚刚我们看 ExecutorService 中的 submit 方法返回的是 Future 接口，然后通过接口中的get 方法获取任务的返回值，其实 submit 返回的是 Future 的实现类： FutureTask, 至于 FutureTask 是如何配合线程池拿到任务的返回值，就需要深入源码查看底层实现了，这个以后再新写一遍博客讲解。\n参考 http://www.lllpan.top/article/102\n","date":"2022-10-29T17:15:31Z","permalink":"https://dccmmtop.github.io/posts/callable%E4%B8%8Efuture%E7%9A%84%E5%BA%94%E7%94%A8/","section":"posts","tags":["java"],"title":"Callable与Future的应用"},{"categories":null,"contents":"什么是 Fork/Join 框架 Fork/Join 是从 java7 开始提供的并行执行任务的框架，是一个把大任务分割成若干个小任务，最终汇总每个小任务的结果，得到大任务结果的框架.\n如下图：\nFork/Join 的特性 ForJoinPool 不是为了替代 ExecutorService, 而是它的补充，在一些可分割的大任务场景下，性能会更好。 ForJoinPool 主要利用“分而治之”的算法思想 ForJoinPool 最适合计算型密集的任务 工作窃取算法 指的是某个线程从其他线程队列中获取任务来执行。这也是 Fork/Join 框架执行任务的核心机制\n当我们需要做一比较大的任务时， 我们可以把这个任务分成若干个不互相依赖的子任务，把这些子任务放在不同的队列中，并为不同的队列生成一个单独的线程去执行任务，线程和队列是一一对应的。但是有的线程干活比较快，把自己队列中的任务执行完了，不会干等着，而是去帮其他线程干活，就是取其他队列中的任务来执行。 这样的话就会有两个线程同时访问一个队列，会发生资源抢占问题，于是，把这个队列设计成双端队列， 从队列尾部偷任务执行，不和土著线程抢队头。这样就减少了资源抢占的机会。\n工作窃取算法优点是，充分利用多核 CPU 并行计算，并减少了线程间的竞争，缺点是，并没有完全避免竞争，例如队列中只有一个任务时，同时消耗了更多的系统资源。\nForkJoinPool 任务执行步骤 ForJoinPool 中的每个工作线程都维护一个队列（WorkQueue）, 这是一个双端队列 (Deque), 队列中只能存放 ForkJoinTask 子类型的任务 线程在工作中产生的新任务时（通常是调用了 fork() 方法）, 会放入队尾，并且线程在处理任务时，使用的是 LIFO 方式，也就是从队尾取出任务执行。 每个线程在处理自己的工作队列的同时，会尝试窃取一个任务（刚提交到线程池的任务，或者其他线程队列中的任务）, 窃取的任务位于其他线程队列的队头，也就是线程在窃取任务时，采取的是 FIFO 的方式。 遇到 join() 方法时，如果需要 join() 的任务尚未完成，则会先处理其他任务，并等待这个任务完成。 既没有自己的任务，也没有可窃取的任务时，进入休眠 ForkJoinPool 的使用 ForkJoin 框架要求任务必须是 ForkJoinTask 的子类，通常情况下不需要直接继承 ForkJoinTask, 而是继承它的子类，RecursiveAction 和 RecursiveTask。\nRecursiveAction 用于没有返回值的任务，必须讲数据写到磁盘，可以把数据分块，多线程去写入\nRecursiveTask 用于有返回值的任务\n使用示例 数组中的数字累加\n/** *有返回值类型的可拆分任务 */ class SumTask extends RecursiveTask\u0026lt;Integer\u0026gt; { /** * 控制最小任务的粒度 */ private final static int THRESHOLD = 20; private int[] arr; private int start; private int end; public SumTask(int[] arr, int start, int end){ this.arr = arr; // 将数组分割成开始下表为 start， 结束下标为 end 的小数组 this.start = start; this.end = end; } /** * 只计算某段的和 */ private int subTotal(){ int sum = 0; for(int i = start; i \u0026lt; end; i ++){ sum += arr[i]; } return sum; } @Override protected Integer compute() { // 达到最小粒度时，开始计算 if(end - start \u0026lt; THRESHOLD){ return subTotal(); } // 否则继续拆分成两个小任务 int middle = (start + end) /2; SumTask leftSum = new SumTask(arr, start, middle); SumTask right = new SumTask(arr, middle, end); // 提交新任务 leftSum.fork(); right.fork(); // 计算两个小任务的和 return leftSum.join() + right.join(); } } public class ForkJoinTaskDemo { public static void main(String[] args) { int size = 1000000; int[] arr = new int[size]; // 生成数组 for(int i = 0; i\u0026lt; size ; i++){ arr[i] = i + 1; } ForkJoinPool pool = new ForkJoinPool(); // 提交一个大任务给线程池 ForkJoinTask\u0026lt;Integer\u0026gt; result = pool.submit(new SumTask(arr,0, size)); // 执行 System.out.println(\u0026#34;pool 结果：\u0026#34; + result.invoke()); pool.shutdown(); } } 上面方法可以用下图说明:\n重要方法解释 构造方法 完整的构造方法如下：\nprivate ForkJoinPool(int parallelism, ForkJoinWorkerThreadFactory factory, UncaughtExceptionHandler handler, int mode, String workerNamePrefix) 参数解释：\nparallelism 使用线程的个数，默认使用等同处理器个数的线程 factory 创建线程的工厂，默认情况下使用 ForkJoinWorkerThreadFactory handler 线程异常时的处理器，默认 null mode 表示工作线程内的任务队列是采用何种方式进行调度，可以是先进先出 FIFO，也可以是后进先出 LIFO。如果为 true，则线程池中的工作线程则使用先进先出方式进行任务调度，默认情况下是 false。 workerNamePrefix 线程名字前缀。 fork 方法 fork() 做的工作只有一件事：把任务推入当前工作线程的工作队列里，源码如下：\npublic final ForkJoinTask\u0026lt;V\u0026gt; fork() { Thread t; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ((ForkJoinWorkerThread)t).workQueue.push(this); else ForkJoinPool.common.externalPush(this); return this; } join 方法 join() 的工作则复杂得多，也是 join() 可以使得线程免于被阻塞的原因——不像同名的 Thread.join()\n检查调用 join() 方法的线程是不是 ForkJoinThread 线程，如果不是，比如：main 线程，则阻塞当前线程，如果是，则不阻塞 查看任务完成状态，如果已经完成，则直接返回结果，如果没有完成，而且处于自己的工作队列内，则完成它 如果任务已经被其他的工作线程偷走，则窃取这个小偷的工作队列内的任务（以 FIFO 方式），执行，以期帮助它早日完成欲\njoin 的任务。 如果偷走任务的小偷也已经把自己的任务全部做完，正在等待需要 join 的任务时，则找到小偷的小偷，帮助它完成它的任务。 递归地执行第 4 步。 submit 方法 ForJoinPool 自认拥有工作队列，用来接收外部线程(非 ForkJoinThread)提交过来的任务,这个工作队列被称为 submitting queue, submit() 方法和 fork() 方法没有本质区别， 只不任务的目的地是 submitting queue. submitting queue 和 work queue 一样，也是被窃取的对象。因此当一个任务被成功窃取时，就意味着被提交的任务真正开始进入执行阶段。\ninvoke 方法 开始执行任务，如果必要，等待计算完成。\n参考资料 https://www.cnblogs.com/cjsblog/p/9078341.html\nhttps://note.youdao.com/ynoteshare/index.html?id=43491d79e1e5735d39b34b8f7a20c5c7\u0026amp;type=note\u0026amp;_time=1667033251690\n","date":"2022-10-29T15:24:32Z","permalink":"https://dccmmtop.github.io/posts/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8Bforkjoin%E6%A1%86%E6%9E%B6/","section":"posts","tags":["java"],"title":"并发编程之 ForkJoin 框架"},{"categories":null,"contents":"用idea工具自带的配置导入导出功能只能作用于配置文件，不能把插件也导出, 可以找到idea的配置目录，把配置目录覆盖的新的机器上，配置目录查找方法：\n把该目录打包移动到新的机器上，用同样的方法找到新机器上idea的配置路径，覆盖即可\n","date":"2022-10-24T10:01:07Z","permalink":"https://dccmmtop.github.io/posts/idea%E9%85%8D%E7%BD%AE%E5%92%8C%E6%8F%92%E4%BB%B6%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95/","section":"posts","tags":["idea"],"title":"idea配置和插件导入导出的方法"},{"categories":null,"contents":"在 线程池的应用 一文中，讲解了常见几种线程池的使用方法与差别，今天再深入学一下其中定时任务线程池的用法和原理。\n定时任务线程池 public class User { public static void main(String[] args) { // 创建一个定时任务线程池 ScheduledExecutorService executor =Executors.newScheduledThreadPool(1); // 提交任务，1 秒后开始执行 executor.schedule(new Task(2), 1000,TimeUnit.MILLISECONDS); // 关闭线程池，不再接受新任务 executor.shutdown(); } } // 任务 class Task implements Runnable { int i; public Task(int i ) { this.i = i; } @Override public void run() { System.out.println(new Date() + \u0026#34;:\u0026#34;+ i + \u0026#34;:\u0026#34; + Thread.currentThread().getName()); try { // 模拟任务要执行 4 秒钟 Thread.sleep(4000); } catch (InterruptedException e) { e.printStackTrace(); } } } 结果\n提交任务的时间是：Mon Oct 24 10:30:41 CST 2022 开始执行任务：Mon Oct 24 10:30:42 CST 2022:2:pool-1-thread-1 任务执行结束：Mon Oct 24 10:30:46 CST 2022 Process finished with exit code 0 可以看到任务一秒后开始执行\n这就是定时任务线程池的使用， 下面看一下定时 + 周期任务的使用方法\n周期任务 scheduleAtFixedRate public class User { public static void main(String[] args) { ScheduledExecutorService executor =Executors.newScheduledThreadPool(1); System.out.println(\u0026#34;提交任务的时间是：\u0026#34; + (new Date())); // 参数分别是： 任务， 多久后开始执行， 每隔多久执行一次（周期），时间单位 executor.scheduleAtFixedRate(new Task(1), 1000,2000, TimeUnit.MILLISECONDS); } } class Task implements Runnable { int i; public Task(int i ) { this.i = i; } @Override public void run() { System.out.println(\u0026#34;开始执行任务：\u0026#34; + new Date() + \u0026#34;:\u0026#34;+ i + \u0026#34;:\u0026#34; + Thread.currentThread().getName()); try { Thread.sleep(4000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;任务执行结束：\u0026#34; + new Date()); } } 设置任务的执行时间是 4 秒， 但周期是 2 秒，也就是说任务还没有执行结束，就要开始执行下一个任务。 但是线程池是不会这样做的，而是等到当前任务执行结束，如果已经超过了周期，会立刻开始执行下一个任务。\n结果\n提交任务的时间是：Mon Oct 24 10:34:01 CST 2022 开始执行任务：Mon Oct 24 10:34:02 CST 2022:1:pool-1-thread-1 // 延迟 1s 才开始执行任务 任务执行结束：Mon Oct 24 10:34:06 CST 2022 // 任务执行了 4s 开始执行任务：Mon Oct 24 10:34:06 CST 2022:1:pool-1-thread-1 // 又立刻开始下一个任务 任务执行结束：Mon Oct 24 10:34:10 CST 2022 开始执行任务：Mon Oct 24 10:34:10 CST 2022:1:pool-1-thread-1 任务执行结束：Mon Oct 24 10:34:14 CST 2022 修改一下： 任务执行时间是 1 秒， 周期是 3 秒，执行结果如下：\n提交任务的时间是：Mon Oct 24 11:21:53 CST 2022 开始执行任务：Mon Oct 24 11:21:54 CST 2022:1:pool-1-thread-1 // 延迟 1s 开始执行 任务执行结束：Mon Oct 24 11:21:55 CST 2022 // 任务执行了 1s 开始执行任务：Mon Oct 24 11:21:57 CST 2022:1:pool-1-thread-1 // 54 + 3 , 3 秒后开始执行下一个任务 任务执行结束：Mon Oct 24 11:21:58 CST 2022 开始执行任务：Mon Oct 24 11:22:00 CST 2022:1:pool-1-thread-2 任务执行结束：Mon Oct 24 11:22:01 CST 2022 scheduleWithFixedDelay 固定延迟时间，在上一个任务执行结束后，固定延迟设置的时间，再执行下一个任务\npublic class User { public static void main(String[] args) { ScheduledExecutorService executor =Executors.newScheduledThreadPool(4); System.out.println(\u0026#34;提交任务的时间是：\u0026#34; + (new Date())); executor.scheduleWithFixedDelay(new Task(2), 1000,3000, TimeUnit.MILLISECONDS); } } class Task implements Runnable { int i; public Task(int i ) { this.i = i; } @Override public void run() { System.out.println(\u0026#34;开始执行任务：\u0026#34; + new Date() + \u0026#34;:\u0026#34;+ i + \u0026#34;:\u0026#34; + Thread.currentThread().getName()); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;任务执行结束：\u0026#34; + new Date()); } } 设置每个任务执行的间隔是 3s（上个任务结束到下个任务开始的间隔），任务执行时间需要 2s，看下结果：\n提交任务的时间是：Mon Oct 24 11:32:41 CST 2022 开始执行任务：Mon Oct 24 11:32:42 CST 2022:2:pool-1-thread-1 // 延迟 1s 才开始执行任务 任务执行结束：Mon Oct 24 11:32:44 CST 2022 // 任务执行了 2s // 44 + 3 = 47, 固定延迟 3 秒开始执行下一个任务 开始执行任务：Mon Oct 24 11:32:47 CST 2022:2:pool-1-thread-1 任务执行结束：Mon Oct 24 11:32:49 CST 2022 开始执行任务：Mon Oct 24 11:32:52 CST 2022:2:pool-1-thread-2 任务执行结束：Mon Oct 24 11:32:54 CST 2022 周期性任务线程池应用场景 redis 锁续命 当我们使用 redis 作为分布式锁时，需要给这个锁设置一个超时时间，避免因为程序出错而用不释放锁，但是这个超时时间不好估量，设置的短了，任务还没结束就释放锁了，设置的过长，程序异常情况下会长时间持有锁，影响程序性能。可以在获取锁之后使用一个周期性任务每秒检查锁是否存在，如果扔存在，就给这个锁延长时间，这样只需把锁的超时时间设置一个比较短的时间，后面依赖周期性任务续命即可。\n微服务注册中心 各个微服务需要定时向注册中心上报自己的信息，确保该服务在正常运行\n与 Timer 的区别 Timer 也可以实现周期性任务，但是他是单线程的，如果任务执行中抛出异常，线程会终止，无法再次添加任务。 ScheduledThreadPool 在任务发生异常后，也会结束执行任务的线程，但紧接着会再创建一个新的线程去继续执行其他任务，但是这个发生异常的任务就从队列中移除了，无法再周期性执行。这也是特别需要注意的地方。在一点在 线程池原理 讲过 延迟任务线程池的实现原理 上述的延迟任务线程池底层采用DelayQueue存储等待的任务\nDelayQueue 内部封装了一个 PriorityQueue，它会根据 time 的先后时间排序，若 time 相同则根据 sequenceNumber 排序； DelayQueue 也是一个无界队列； 工作线程执行的过程： 工作线程会从 DelayQueue 取已经到期的任务去执行； 执行结束后重新设置任务的到期时间，再次放回 DelayQueue 下面从源码层面看一下大体流程：\n新建线程池 public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); } 可以看出底层还是调用 ThreadPoolExecutor 类，默认使用延时队列当做任务队列。\n下面看一下任务是如何添加到队列中的，以及如何执行的\n添加任务 在文章前半部分的应用示例中executor.scheduleWithFixedDelay(new Task(2), 1000,3000, TimeUnit.MILLISECONDS); 就是提交任务的入口了。进一步查看源码：\npublic ScheduledFuture\u0026lt;?\u0026gt; scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) { if (command == null || unit == null) throw new NullPointerException(); if (delay \u0026lt;= 0) throw new IllegalArgumentException(); // 将任务包装成 ScheduledFutureTask 类型 ScheduledFutureTask\u0026lt;Void\u0026gt; sft = new ScheduledFutureTask\u0026lt;Void\u0026gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(-delay)); // 再次装饰任务，可以复写 decorateTask 方法，定制化任务 RunnableScheduledFuture\u0026lt;Void\u0026gt; t = decorateTask(command, sft); sft.outerTask = t; // 放入延时队列中，ScheduledFutureTask 是接口 RunnableScheduledFuture 的一个实现类 // 所以放入队列还是 ScheduledFutureTask 类型的 delayedExecute(t); return t; } 将任务放入队列中：\n取出任务 因为把任务放在了延迟队列中，在取出任务的时候不能再和普通的阻塞队列那样：只要有任务就可以取出来。\n延迟队列中的每个任务都有一个到期时间，只有到期的任务才可以被取出，否则取出动作会被阻塞。这里不细讲延迟队列的实现方式，只需知道延迟队列中的元素要实现 Delayed 接口中的 getDelayed() 方法，这个方法返回了该任务还有多久到期。源码如下：\npublic long getDelay(TimeUnit unit) { // 下次要执行的时间 - 当前时间 return unit.convert(time - now(), NANOSECONDS); } 如果 getDelay 返回的值小于等于 0，那么就可以执行该任务了。 time 属性是任务下次执行的时间，下面会讲到。\n执行任务 通过一个线程执行任务时，多半是调用这个任务的 run() 方法，现在看一下 ScheduledFutureTask 的实现：\npublic void run() { boolean periodic = isPeriodic(); // 如果线程已经不支持执行任务，则取消任务 if (!canRunInCurrentRunState(periodic)) cancel(false); // 如果该任务不是周期性的，直接执行 run 方法，然后结束 else if (!periodic) ScheduledFutureTask.super.run(); // 如果需要周期执行，那么先执行，然后设置下次执行时间 else if (ScheduledFutureTask.super.runAndReset()) { // 计算下次执行该任务的时间 setNextRunTime(); // 再次将任务添加到队列中，重复执行 reExecutePeriodic(outerTask); } } 计算下次执行任务的时间 已经讲过，有两种执行任务的方式，一种是周期性的任务，在底层是这样区分的\n巧妙地利用正负作为一个任务类型的标识，在后面的判断中就可以利用正负号区分，计算时，把负数转成正数即可\n如下：\n// 计算下次执行时间 private void setNextRunTime() { long p = period; // 固定周期型任务 if (p \u0026gt; 0) time += p; else // 固定间隔型任务，细节不再展开 time = triggerTime(-p); } 我们已经看了任务的添加，以及任务的执行\n延时队列的特性 以上就是一个周期性任务执行的全部了。但是没有说队列中如果存在多个任务怎么办？\n任务的排序 如果队列中包含多个任务，延迟队列还会把任务以到期时间排序，对头永远是最先到期的那个任务，如果到期时间相同，则按进入队列的顺序排。\n\u0026ldquo;延迟队列还会把任务以到期时间排序\u0026rdquo;, 这其实并不是延时队列实现的功能，而是调用了队列中任务的 compareTo 方法来排序的，这就要求队列中的元素实现 Comparable 接口中的 compareTo 方法。 是的， ScheduledFutureTask 正是实现了这个方法：\npublic int compareTo(Delayed other) { if (other == this) // compare zero if same object return 0; if (other instanceof ScheduledFutureTask) { ScheduledFutureTask\u0026lt;?\u0026gt; x = (ScheduledFutureTask\u0026lt;?\u0026gt;)other; // 比较到期时间 long diff = time - x.time; if (diff \u0026lt; 0) // 当前任务小， return -1; else if (diff \u0026gt; 0) // 当前任务大 return 1; else if (sequenceNumber \u0026lt; x.sequenceNumber) // 如果相等，按 sequenceNumber 排序，小的在前 return -1; else return 1; } long diff = getDelay(NANOSECONDS) - other.getDelay(NANOSECONDS); return (diff \u0026lt; 0) ? -1 : (diff \u0026gt; 0) ? 1 : 0; } 每次只是从对头获取元素，所以只需保证对头的元素是最小的即可。底层并没有把整个队列中的元素做全排序，而是维护了一个堆的逻辑数据结构保证对头元素永远是最小的。关于堆的介绍，后续会写篇博客单独介绍\n这篇文章并没有太过详细的讲解线程池更底层的逻辑，只是介绍了几个重点方法，因为大致思想和之前讲的 AQS 差不太多，需要考虑很多并发的场景，加入很多限制\n","date":"2022-10-24T08:55:22Z","permalink":"https://dccmmtop.github.io/posts/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%91%A8%E6%9C%9F%E6%80%A7%E4%BB%BB%E5%8A%A1%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%BA%94%E7%94%A8%E5%8F%8A%E5%8E%9F%E7%90%86/","section":"posts","tags":["java","hidden"],"title":"定时任务和周期性任务线程池的应用及原理"},{"categories":null,"contents":"ThreadPoolExecute 使用示例 public class User { public static void main(String[] args) { ThreadPoolExecutor executor = new ThreadPoolExecutor(2,10,60, TimeUnit.SECONDS,new ArrayBlockingQueue\u0026lt;Runnable\u0026gt;(10)); for(int i =0 ;i \u0026lt; 20; i ++){ executor.execute(new Task(i)); } executor.shutdown(); } } class Task implements Runnable { int i; public Task(int i ) { this.i = i; } @Override public void run() { System.out.println(new Date() + \u0026#34;:\u0026#34;+ i + \u0026#34;:\u0026#34; + Thread.currentThread().getName()); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } } } 先使用构造方法生成一个线程池对象，然后使用 execute 方法将任务提交给线程池\n重要属性 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));\nprivate static final int COUNT_BITS = Integer.SIZE - 3;\nprivate static final int CAPACITY = (1 \u0026lt;\u0026lt; COUNT_BITS) - 1;\nctl 是对线程池的运行状态和线程池中有效线程的数量进行控制的一个字段，它包含两部分的信息：线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，这里可以看到，使用了 Integer 类型来保存，高 3 位保存 runState，低 29 位保存workerCount。COUNT_BITS 就是 29，CAPACITY 就是 1 左移 29 位减 1（29 个 1），这个常量表示workerCount的上限值，大约是 5 亿。\nctl 相关方法，不用过于纠结如何计算的，知道即可\n// 获取运行状态； private static int runStateOf(int c) { return c \u0026amp; ~CAPACITY; } //获取活动线程数； private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } //获取运行状态和活动线程数的值。clt 记录着 runState 和 workerCount private static int ctlOf(int rs, int wc) { return rs | wc; } 线程池状态解释 RUNNING 状态说明：线程池处在 RUNNING 状态时，能够接收新任务，以及对已添加的任务进行处理。 状态切换：线程池的初始化状态是 RUNNING。换句话说，线程池被一旦被创建，就处于 RUNNING 状态，并且线程池中的任务数为 0！ SHUTDOWN 状态说明：线程池处在 SHUTDOWN 状态时，不接收新任务，但能处理已添加的任务。 状态切换：调用线程池的 shutdown() 接口时，线程池由 RUNNING -\u0026gt; SHUTDOWN。 STOP 状态说明：线程池处在 STOP 状态时，不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。 状态切换：调用线程池的shutdownNow()接口时，线程池由 (RUNNING or SHUTDOWN ) -\u0026gt; STOP。 TIDYING 状态说明：当所有的任务已终止，ctl记录的”任务数量”为 0，线程池会变为 TIDYING 状态。当线程池变为 TIDYING 状态时，会执行钩子函数 terminated()。terminated() 在 ThreadPoolExecutor类中是空的，若用户想在线程池变为 TIDYING 时，进行相应的处理；可以通过重载 terminated() 函数来实现。\n状态切换：当线程池在 SHUTDOWN 状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN -\u0026gt; TIDYING。当线程池在 STOP 状态下，线程池中执行的任务为空时，就会由 STOP -\u0026gt; TIDYING。\nTERMINATED 状态说明：线程池彻底终止，就变成 TERMINATED 状态。\n状态切换：线程池处在 TIDYING 状态时，执行完 terminated() 之后，就会由 TIDYING -\u0026gt; TERMINATED。\n进入 TERMINATED 的条件如下：\n线程池不是 RUNNING 状态； 线程池状态不是 TIDYING 状态或 TERMINATED 状态； 如果线程池状态是 SHUTDOWN 并且workerQueue为空； workerCount 为0； 设置 TIDYING 状态成功。 execute 方法 在 线程池应用 中讲过线程的执行顺序：\n当线程数小于核心线程数时，创建线程。\n当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。\n当线程数大于等于核心线程数，且任务队列已满：\n若线程数小于最大线程数，创建线程。\n若线程数等于最大线程数，抛出异常，拒绝任务。\n优先级： 核心线程数 \u0026gt; 任务队列 \u0026gt; 最大线程数 \u0026gt; 拒绝任务\n我们现在来看一下是如何实现这个规则的\npublic void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); //获取线程池的状态和数量 if (workerCountOf(c) \u0026lt; corePoolSize) { // 如果线程数量小于核心线程数 // 第二个参数是 true，代表将创建核心线程 为 false 时创建的是非核心线程 // 以此来判断线程数是超过了核心线程数还是超过了最大线程数 if (addWorker(command, true)) // 创建一个新的线程并执行 return; // 结束 c = ctl.get(); } // 如果执行到这里，说明线程数 \u0026gt; 核心线程数 if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { // 如果入队成功 int recheck = ctl.get(); // 再次检查线程池状态，如果线程池没有在运行由于之前已经把 command 添加到 workQueue 中了， 这时需要移除该 command if (! isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); // 根据拒绝策略拒绝任务 else if (workerCountOf(recheck) == 0) // 异常情况 addWorker(null, false); // 第一个参数为 null，表示在线程池中创建一个线程，但不去启动； } // 执行到这里，入队失败，说明队列已满 else if (!addWorker(command, false))// 新增非核心线程去执行任务 reject(command); // 如果已达到最大核心线程数，拒绝任务 } 流程图表示如下：\naddWorker 方法 该方法会接收一个可运行的任务，和一个标识符 core，如果 core 为 true, 判断当前线程数是超过了核心线程数。如果 core 为 false，就判断当前线程数是否超过了最大核心数\naddWorker 方法会生成一个新的线程，并开始执行携带的任务——firstTask, 该任务执行结束后，会死循环的从阻塞任务队列中获取新任务，如果队列为空，该线程就会阻塞。源码如下：\nprivate boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 线程池状态 // Check if queue empty only if necessary. if (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; ! (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; ! workQueue.isEmpty())) // 线程池状态判断，参数校验 return false; for (;;) { int wc = workerCountOf(c); // 获取线程数 // 使用 core 标识符判断，线程数量的上限是哪个，但总共不能超过最大容量 if (wc \u0026gt;= CAPACITY || wc \u0026gt;= (core ? corePoolSize : maximumPoolSize)) return false; // cas 方式增加线程数量，如果失败， if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) // 考虑到并发场景，再读一次 continue retry; // else CAS failed due to workerCount change; retry inner loop } } boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { // 将任务包装成一个 worker, worker 是包含一个线程，一个任务的结构，下面会介绍 w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); //因为需要线程池状态做一些判断，线程池状态是共有的资源，需要加锁 try { // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs \u0026lt; SHUTDOWN || (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null)) { if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); // 记录该 worker, workers 是一个 HashSet int s = workers.size(); // largestPoolSize 记录着线程池中出现过的最大线程数量 if (s \u0026gt; largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } if (workerAdded) { // 启动 woker 中的线程，开始执行任务了 t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted; } Worker worker 是一个包含了可执行的任务，和用来执行任务的线程，它实现了Runnable 接口，本身也可以当作一个任务运行。部分源码如下：\nprivate final class Worker extends AbstractQueuedSynchronizer implements Runnable { /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; // 记录线程完成的任务的个数 /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ // 构造 worker Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); // 生成新的线程 } /** Delegates main run loop to outer runWorker */ // 真正执行任务的地方 public void run() { runWorker(this); } // ... 省略 // 获取锁 protected boolean tryAcquire(int unused) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } } Worker 继承了 AQS，使用 AQS 来实现独占锁的功能。为什么不使用ReentrantLock来实现呢？可以看到tryAcquire方法，它是不允许重入的，而 ReentrantLock 是允许重入的：\nlock 方法一旦获取了独占锁，表示当前线程正在执行任务中； 如果正在执行任务，则不应该中断线程； 如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断； 线程池在执行 shutdown 方法或 tryTerminate 方法时会调用 interruptIdleWorkers 方法来中断空闲的线程，interruptIdleWorkers 方法会使用 tryLock 方法来判断线程池中的线程是否是空闲状态； 之所以设置为不可重入，是因为我们不希望任务在调用像 setCorePoolSize 这样的线程池控制方法时重新获取锁。如果使用 ReentrantLock，它是可重入的，这样如果在任务中调用了如 setCorePoolSize 这类线程池控制的方法，会中断正在运行的线程。 所以，Worker 继承自 AQS，用于判断线程是否空闲以及是否可以被中断。\n此外，在构造方法中执行了 setState(-1);，把 state 变量设置为-1，为什么这么做呢？是因为 AQS 中默认的 state 是 0，如果刚创建了一个 Worker 对象，还没有执行任务时，这时就不应该被中断， tryAcquire 方法是根据 state 是否是 0 来判断的，所以，setState(-1); 将 state 设置为-1 是为了禁止在执行任务前对线程进行中断。\n正因为如此，在 runWorker 方法中会先调用 Worker 对象的 unlock 方法将 state 设置为 0。\n从上面源码中拿出，最终调用 runWorker 方法去执行任务， runnerWorker 源码如下：\nrunWorker final void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // 允许中断 boolean completedAbruptly = true; try { // 循环获取任务 while (task != null || (task = getTask()) != null) { w.lock(); // 如果线程池正在停止，那么要保证当前线程是中断状态； // 如果不是的话，则要保证当前线程不是中断状态； if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() \u0026amp;\u0026amp; runStateAtLeast(ctl.get(), STOP))) \u0026amp;\u0026amp; !wt.isInterrupted()) wt.interrupt(); try { // 执行任务之前做一些事情，类似于回调 beforeExecute(wt, task); Throwable thrown = null; try { // 开始运行任务 task.run(); } catch (RuntimeException x) { // 记录异常，并抛出异常跳出循环，下同 thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { // 无论是否发生异常，都要执行这里 // 任务执行结束要做的事情，类似于回调 afterExecute(task, thrown); } } finally { task = null; // 记录已经完成任务的个数 w.completedTasks++; w.unlock(); } } // 执行到这里，说明未发生异常 completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } } 关于第一个 if 语句：这里要考虑在执行该 if 语句期间可能也执行了 shutdownNow 方法，shutdownNow 方法会把状态设置为 STOP，回顾一下 STOP 状态：\n不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。在线程池处于 RUNNING 或 SHUTDOWN 状态时，调用 shutdownNow() 方法会使线程池进入到该状态。 STOP 状态要中断线程池中的所有线程，而这里使用 Thread.interrupted() 来判断是否中断是为了确保在 RUNNING 或者 SHUTDOWN 状态时线程是非中断状态的，因为 Thread.interrupted() 方法会复位中断的状态。\n当任1下面情况发生时，就退出循环\n当从队列中获取的任务为空时，退出循环 发生异常时\n退出循环后还会执行一个 processWorkerExit 方法，看一下这个方法做了什么: processWorkerExit private void processWorkerExit(Worker w, boolean completedAbruptly) { // 如果程序是发生了异常终止的，需要对线程数减一操作,为什么正常情况退出，而不需要减一操作了呢？ // 因为在 getTask 方法中针对这种正常情况已经做了处理。 if (completedAbruptly) decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // 统计完成的任务数目 completedTaskCount += w.completedTasks; // 删除线程 workers.remove(w); } finally { mainLock.unlock(); } // 判断是否需要结束线程池 tryTerminate(); /* * 当线程池是RUNNING或SHUTDOWN状态时，如果worker是异常结束，那么会直接addWorker； * 如果allowCoreThreadTimeOut=true，并且等待队列有任务，至少保留一个worker； * 如果allowCoreThreadTimeOut=false，workerCount不少于corePoolSize。 */ int c = ctl.get(); if (runStateLessThan(c, STOP)) { if (!completedAbruptly) { int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 \u0026amp;\u0026amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) \u0026gt;= min) return; // replacement not needed } addWorker(null, false); } } 所以一个线程执行结束后可能需要做的事情：\n再次添加一个线程 结束线程池 下面看一下 getTask 方法\ngetTask 方法 private Runnable getTask() { // 表示上次从从阻塞队列中取值是否超时 boolean timedOut = false; // Did the last poll() time out? for (;;) { int c = ctl.get(); int rs = runStateOf(c); /* * 如果线程池状态 rs \u0026gt;= SHUTDOWN，也就是非 RUNNING 状态，再进行以下判断： * 1. rs \u0026gt;= STOP，线程池是否正在 stop； * 2. 阻塞队列是否为空。 * 如果以上条件满足，则将 workerCount 减 1 并返回 null。 * 因为如果当前线程池状态的值是 SHUTDOWN 或以上时，不允许再向阻塞队列中添加任务。 */ if (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; (rs \u0026gt;= STOP || workQueue.isEmpty())) { // 减一 decrementWorkerCount(); return null; } int wc = workerCountOf(c); // 标记是否需要进行超时处理 // alloCoreThreadTimeOut 默认为false，也就是核心线程默认不允许超时 // wc \u0026gt; corePoolSize, 已经进入到非核心线程的状态，允许超时处理 boolean timed = allowCoreThreadTimeOut || wc \u0026gt; corePoolSize; // wc \u0026gt; maximumPoolSize的情况是因为可能在此方法执行阶段同时执行了setMaximumPoolSize方法； // (timed \u0026amp;\u0026amp; timeout) 表示需要超时处理，并且上次获取任务超时了 // (wc \u0026gt; 1 || 队列为空) 线程池中除了当前线程还有其他工作的线程 或者 任务队列已经空了 // 终上所述，1. 当获取任务超时（异常或者队列为空），并且还有其他线程可以继续工作，不会出现有任务但没线程的情况，当前线程就可以放心退出了， // 2. 当获取任务超时（异常或者队列为空），并且有任务可以做了，当前线程就可以放心的退出了， if ((wc \u0026gt; maximumPoolSize || (timed \u0026amp;\u0026amp; timedOut)) \u0026amp;\u0026amp; (wc \u0026gt; 1 || workQueue.isEmpty())) { // return 了，线程要结束生命了，把线程数量减一。如果减失败了，下次再尝试 if (compareAndDecrementWorkerCount(c)) return null; continue; } // 执行到这里，说明是第一次获取任务, 就开始准备获取任务了 // 如果获取失败，打上超时的标记，开始下次循环，走上面的逻辑，可能会结束这个线程 // 如果获取成功，直接返回该任务，在 runWork 中执行了。 try { // 下面是2中方式从阻塞队列中获取任务，根据线程数量，采用不同的方式, 详情可以搜索阻塞队列 // 1. pool(time, unit): 只等待固定的时间，超过就返回空 // 2. take(): 无限等待 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; // 打上超时的标记 timedOut = true; } catch (InterruptedException retry) { timedOut = false; } } } 所以线程池中的一个线程生命周期经过下面几个重点方法:\nexecute 是入口，决定任务是应该立即执行，还是加入队列中等待,或者拒绝任务 addWorker, 新增一个线程去执行任务 runWorker: 循环从队列获取任务，执行任务，如果未获取到任务，或者任务执行失败，跳出循环，进入线程终止方法 getTask 获取任务,无法获取到任务时，决定直接返回空，还是继续等待任务到来 processWorkerExit, 线程退出阶段，会做一些线程的统计和补偿工作，以及更改线程的状态，如果线程池运行中，且这个线程是因为异常退出的，就重新生成一个新的线程，继续工作。 流程图如下：\n","date":"2022-10-20T09:52:20Z","permalink":"https://dccmmtop.github.io/posts/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86/","section":"posts","tags":["java"],"title":"线程池原理"},{"categories":null,"contents":"java 中的线程 线程是调度 CPU 资源的最小单位，线程模型分为 KLT 模型与 ULT 模型，JVM 使用的 KLT 模\n型，Java 线程与 OS 线程保持 1:1 的映射关系，也就是说有一个 java 线程也会在操作系统里有一个对应的线程。\nJava 线程有多种生命状态 ：\nNEW, 新建 RUNNABLE, 运行 BLOCKED, 阻塞 WAITING, 等待 TIMED_WAITING, 超时等待 TERMINATED，终结 池化思想 池化技术指的是提前准备一些资源，在需要时可以重复使用这些预先准备的资源。而这种资源创建的成本比较高，例如线程，大对象，数据库连接等。\n所以池化技术的关键是：\n被池化的对象创建成本高 提前准备 重复使用 线程池 线程池”，顾名思义就是一个线程缓存，线程是稀缺资源，如果被无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，因此 Java 中提供线程池对线程进行统一分配、调优和监控\n什么时候使用线程池？ 单个任务处理时间比较短 需要处理的任务数量很大 线程池优势 重用存在的线程，减少线程创建，消亡的开销，提高性能 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 线程池的使用 有常见的 5 种创建线程的方式，说是 5 种，其实就 2 种。一种是通过 Executors 工厂类提供的方法，该类提供了 4 种不同的线程池可供使用。另一类是通过 ThreadPoolExecutor 类进行自定义创建。\nnewCachedThreadPool 会创建一个可缓冲的线程池，线程数不够时，会一直增加到最大值(Integer.MAXVALUE)，如果线程过多，用不到了，会缓存 60 秒后销毁\nprivate static void createCachedThreadPool() { ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; 10; i++) { final int index = i; executorService.execute(() -\u0026gt; { System.out.println(System.currentMillions + \u0026#34;:\u0026#34; + Thread.currentThread().getName() + \u0026#34;:\u0026#34; + index); sleep(2000); }); } } newFixedThreadPool 创建一个固定线程数量的线程池，处理不过来的任务会放到队列中，这个队列是无界队列，没有大小\nprivate static void createFixedThreadPool() { ExecutorService executorService = Executors.newFixedThreadPool(3); for (int i = 0; i \u0026lt; 10; i++) { final int index = i; executorService.execute(() -\u0026gt; { System.out.println(Thread.currentThread().getName() + \u0026#34; \u0026#34; + index); sleep(2000); }); } } newScheduledThreadPool 创建一个周期性的线程池，可以定时周期性的执行任务，底层利用的是延时队列\nprivate static void createScheduledThreadPool() { ScheduledExecutorService executorService = Executors.newScheduledThreadPool(3); System.out.println(DateUtil.now() + \u0026#34; 提交任务\u0026#34;); for (int i = 0; i \u0026lt; 10; i++) { final int index = i; executorService.schedule(() -\u0026gt; { System.out.println(DateUtil.now() + \u0026#34; \u0026#34; + Thread.currentThread().getName() + \u0026#34; \u0026#34; + index); sleep(2000); }, 3, TimeUnit.SECONDS); } } newSingleThreadExcutor 只有一个线程的线程池\nprivate static void createSingleThreadPool() { ExecutorService executorService = Executors.newSingleThreadExecutor(); for (int i = 0; i \u0026lt; 10; i++) { final int index = i; executorService.execute(() -\u0026gt; { System.out.println(DateUtil.now() + \u0026#34; \u0026#34; + Thread.currentThread().getName() + \u0026#34; \u0026#34; + index); sleep(2000); }); } } 上面四种是 java 为我们提供的几个便捷方法，来创建不同用途的线程池，虽然比较便捷，参数很少，减轻开发者的负担，但是也正是因为这样，并不能很好的使用与实际生产环境，比如 newFixedThreadPool ，使用的是无界队列，这在生产中是不允许的。阿里巴巴 java 开发规范中明确禁止使用上面四种方式创建线程池，而是使用下面自定义线程池的方式。\nThreadPoolExecutor 自定义线程池 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { } 参数解释 共 7 个参数如下：\ncorePoolSize：核心线程数，线程池中始终存活的线程数。 maximumPoolSize: 最大线程数，线程池中允许的最大线程数。 keepAliveTime: 存活时间，线程没有任务执行时最多保持多久时间会终止。 unit: 单位，参数 keepAliveTime 的时间单位，7 种可选。 TimeUnit.DAYS 天 TimeUnit.HOURS 小时 TimeUnit.MINUTES 分 TimeUnit.SECONDS 秒 TimeUnit.MILLISECONDS 毫秒 TimeUnit.MICROSECONDS 微妙 TimeUnit.NANOSECONDS 纳秒 workQueue: 一个阻塞队列，用来存储等待执行的任务，均为线程安全，7 种： ArrayBlockingQueue 一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue 一个由链表结构组成的有界阻塞队列。 SynchronousQueue 一个不存储元素的阻塞队列，即直接提交给线程不保持它们。 PriorityBlockingQueue 一个支持优先级排序的无界阻塞队列。 DelayQueue 一个使用优先级队列实现的无界阻塞队列，只有在延迟期满时才能从中提取元素。 LinkedTransferQueue 一个由链表结构组成的无界阻塞队列。与 SynchronousQueue 类似，还含有非阻塞方法。 LinkedBlockingDeque 一个由链表结构组成的双向阻塞队列 较常用的是 LinkedBlockingQueue 和 Synchronous。线程池的排队策略与 BlockingQueue 有关\nthreadFactory: 线程工厂，主要用来创建线程，默及正常优先级、非守护线程。\nhandler：拒绝策略，拒绝处理任务时的策略，4 种可选，默认为 AbortPolicoy\nAbortPolicy ` 拒绝并抛出异常。 CallerRunsPolicy 重试提交当前的任务，即再次调用运行该任务的 execute()方法。 DiscardOldestPolicy 抛弃队列头部（最旧）的一个任务，并执行当前任务。 DiscardPolicy 抛弃当前任务。 线程执行的顺序 当线程数小于核心线程数时，创建线程。 当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。 当线程数大于等于核心线程数，且任务队列已满： 若线程数小于最大线程数，创建线程。 若线程数等于最大线程数，抛出异常，拒绝任务。 优先级： 核心线程数 \u0026gt; 任务队列 \u0026gt; 最大线程数 \u0026gt; 拒绝任 j 务\n不同线程池源码的差 j 异 Executors 工程类提供的四种方法其实底层还是调用了 ThreadPollExecutor, 只不过是参数不同罢了\n我们来看一下这 4 中方法底层的代码:\nnewCachedThreadPool public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } 核心数为 0，最大线程数为整型最大值，允许最大空闲时间 60s, SynchronousQueue 是 BlockingQueue 的一种，所以 SynchronousQueue 是线程安全的。SynchronousQueue 和其他的 BlockingQueue 不同的是 SynchronousQueue 的 capacity 是 0。即 SynchronousQueue 不存储任何元素。即来一个任务创建一个线程。\nnewFixedThreadPool public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } 核心线程数和最大线程数一样，允许线程永久等待，即没有设置超时时间， 队列使用的是 LinkedBlockingQueue 无界队列。\nnewScheduledThreadPool public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); } 指定核心线程数，最大线程数是 MX_VALUE, 不设置超时时间， 任务队列是延时队列\nnewSingleThreadExecutor public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } 核心线程数和最大线程数都是1， 不设置超时时间，使用无界队列存放任务。\n通过查看以上源码发现，这4中特定的线程池都是以不同参数调用了 ThreadPoolExecutor 来实现的。\n","date":"2022-10-19T20:19:52Z","permalink":"https://dccmmtop.github.io/posts/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%BA%94%E7%94%A8/","section":"posts","tags":["java"],"title":"线程池的应用与原理"},{"categories":null,"contents":"ArrayList ArrayList 的保护机制 for(String str : list){ if(str.equals(\u0026#34;123\u0026#34;)){ list.remove(str); //抛出异常 } } 这里的 foreach 语法糖实际上调用了 ArrayList 的迭代器类。如下：\n如果在开始迭代的时候数组中有 5 个元素，但是在迭代中移除了一个元素，数组实际上还有 4 个元素，但是还是会遍历第五个元素，这就导致了下标越界错误，ArrayList 不允许这种异常发生。还有在多线程下场景下，一个线程遍历这个 ArrayList , 另一个线程移除数组中的某个元素，也会发生 ConcurrentModificationException 异常。\nfail-Fast 机制 这个机制就是 fail-Fast 机制，快速失败系统，通常设计用于停止有缺陷的过程，这是一种理念，在进行系统设计时优先考虑异常情况，一旦发生异常，直接停止并上报。\npublic int divide(int divisor, int dividend){ if (dividend == 0) { throw new RuntimeException(\u0026#34;被除数不能为 0\u0026#34;); //这里就是 fail-fast 的体现 } return divisor / dividend; } 保护机制的实现原理 在 ArrayList 中有一个成员变量： modCount, 它是从 AbstractList 继承来的， modCount 记录数组每次写操作的次数。当像数组增加或移除一个元素，其值加 1，初始值为 0，在开始遍历的时候，会记录当下数组的 modCount 值为 expectedModCount，遍历每个元素时都会比较 modCount 和 expectedModCount 两个值，如果不同，就会抛出异常，代表着在遍历的时候修改了数组。如下：\n怎么样才可以在循环中修改数组？ 如果我们每次向数组中添加或删除元素时，同步修改 exceptedModCount 就不会抛出异常了，ArrayList 没有直接提供这种方法，而是把这种方法委托给迭代器了：\n所以我们可以这样做：\nIterator \u0026lt;String\u0026gt; iterator = list.iterator(); while(iterator.hasNext()){ String str = iterator.next(); iterator.remove(); //正确做法 } CopyOnWriteArrayList CopyOnWriteArrayList 是 ArrayList 的线程安全版本，读取无锁，写时有锁。适用于 写少读多的场景，会有不一致的现象\n实现原理 见名知意—— 写时复制， 当线程在数组上移除，添加元素时，先加锁，将原数组复制一份，然后基于副本操作，最后将已经更改的副本覆盖元数组，释放锁。\n其内部有一个 ReentranLock 来控制锁的获取和释放。\n先看一下内部结构图：\nget 方法 可以看到get方法非常简单，直接获取内部数组第i个元素，没有其他加锁的操作\nadd 方法 一些其他方法都是这种套路，不再一一罗列。\n存在的问题 利用了空间换时间的思想提高性能，因为在每一步的写操作都复制了一个副本，如果数组比较大，就会导致内存占用急剧增加，引发频繁 full GC,从而影响系统性能。\n如何解决 我们可以利用 ReenTranLock 自定义一个线程安全的ArrayList, 分别定义一个 读锁和写锁，读写、写写 互斥。 读读不互斥。避免这种数组的拷贝\n","date":"2022-10-18T17:08:33Z","permalink":"https://dccmmtop.github.io/posts/arraylist%E5%92%8Ccopyonwritearraylist/","section":"posts","tags":["java"],"title":"ArrayList和CopyOnWriteArrayList"},{"categories":null,"contents":"HashTable 是什么？ 之前详细介绍过 HashMap 的原理，HashTable 与 HashMap 用法一样，都是 key-value 结构，底层的实现都差不多，最大的区别是， HashTable 是线程安全的，HashMap 不是线程安全的。\n为什么需要线程安全？ 我们知道，HashMap 的底层数据结构是数组 + 链表 + 红黑树，当两个元素都落在数组的同一个位置时，会形成链表，如果两个线程分别同时 put 这个元素，一个元素把另一个元素覆盖了，就会导致数据丢失。所以我们需要同时只有一个线程能 put 元素，也就是线程安全。\nHashTable 如何解决线程安全问题的？ HashTable 解决线程安全问题非常简单粗暴，就是在方法前加 synchronize 关键词，HasTable 不仅给写操作加锁 put remove clone 等，还给读操作加了锁 get, 如下：\npublic synchronized V get(Object key) public synchronized V put(K key, V value) public synchronized boolean remove(Object key, Object value) 虽然实现起来比较简单，但效率不高。我们一般选用 ConcurrentHashMap.\n为什么 ConcurrentHashMap 的效率高 ConcurrentHashMap 没有大量使用 synchronsize 这种重量级锁。而是在一些关键位置使用乐观锁(CAS), 线程可以无阻塞的运行。 读方法没有加锁 扩容时老数据的转移是并发执行的，这样扩容的效率更高。 Java8 中 ConcurrentHashMap 基于分段锁+CAS 保证线程安全，分段锁基于 synchronized 实现，它仅仅锁住某个数组的某个槽位，而不是整个数组\nCAS 分段锁 ConcurrentHashMap 重点成员变量 LOAD_FACTOR: 负载因子, 默认 75%, 当 table 使用率达到 75%时, 为减少 table 的 hash 碰撞, tabel 长度将扩容一倍。负载因子计算: 元素总个数%table.lengh\nTREEIFY_THRESHOLD: 默认 8, 当链表长度达到 8 时, 将结构转变为红黑树。\nUNTREEIFY_THRESHOLD: 默认 6, 红黑树转变为链表的阈值。\nMIN_TRANSFER_STRIDE: 默认 16, table 扩容时, 每个线程最少迁移 table 的槽位个数。\nMOVED: 值为-1, 当 Node.hash 为 MOVED 时, 代表着 table 正在扩容\nTREEBIN, 置为-2, 代表此元素后接红黑树。\nnextTable: table 迁移过程临时变量, 在迁移过程中将元素全部迁移到 nextTable 上。\nsizeCtl: 用来标志 table 初始化和扩容的, 不同的取值代表着不同的含义:\n0: table 还没有被初始化 -1: table 正在初始化 小于-1: 实际值为 resizeStamp(n) \u0026laquo;RESIZE_STAMP_SHIFT+2, 表明 table 正在扩容 大于 0: 初始化完成后, 代表 table 最大存放元素的个数, 默认为 0.75*n transferIndex: table 容量从 n 扩到 2n 时, 是从索引 n-\u0026gt;1 的元素开始迁移,\ntransferIndex 代表当前已经迁移的元素下标\nForwardingNode: 一个特殊的 Node 节点, 其 hashcode=MOVED, 代表着此时 table 正在做扩容操作。扩容期间, 若 table 某个元素为 null, 那么该元素设置为 ForwardingNode, 当下个线程向这个元素插入数据时, 检查 hashcode=MOVED, 就会帮着扩容\nConcurrentHashMap 重点方法解释 初始化 put 数据 cas 锁定单个槽位 锁住某个链表 协助扩容 上图只是协助扩容的时机，至于协助扩容内部执行的详细步骤比较复杂，牵涉一些位运算，不再详细探究了，大致做了一下几件事：\n定线程每轮迁移元素的个数 stride, 比如进来一个线程, 确定扩容 table 下标为 (a,b]之间元素, 下一个线程扩容(b,c]。这里对 b-a 或者 c-b 也是由最小值 16 限制的。也就是说每个线程最少扩容连续 16 个 table 的元素。而标志当前迁移的下标保存在 transferIndex 里面。 检查 nextTab 是否完成初始化, 若没有的话, 说明是第一个迁移的线程, 先初始化 nextTab, size 是之前 table 的 2 倍。 进入 while 循环查找本轮迁移的 table 下标元素区间, 保存在(bound, i]中, 注意这里是半开半闭区间。 从i -\u0026gt; bound开始遍历table中每个元素, 这里是从大到小遍历的: 若该元素为空, 则向该元素标写入ForwardingNode, 然后检查下一个元素。 当别 的线程向这个元素插入数据时, 根据这个标志符知道了table正在被别的线程迁移, 在 putVal中就会调用helpTransfer帮着迁移。 若该元素的hash=MOVED, 代表次table正在处于迁移之中, 跳过。 按道理不会跑着这里的。 否则说明该元素跟着的是一个链表或者是个红黑树结构, 若hash\u0026gt;0, 则说明是个链 表, 若f instanceof TreeBin, 则说明是个红黑树结构。 链表迁移原理如下: 遍历链表每个节点。 若节点的(f.hash\u0026amp;n == 0) 成立, 则将节 点放在i, 否则, 则将节点放在n+i上面, 这一点和之前讲解的 HashMap 没有变化 ","date":"2022-10-18T15:10:48Z","permalink":"https://dccmmtop.github.io/posts/concurrenthashmap%E4%B8%8Ehashtable/","section":"posts","tags":["java"],"title":"ConcurrentHashMap与HashTable"},{"categories":null,"contents":"在 JDK 1.8 版本之前，HashMap 底层的数据结构是数组 + 链表，如下图：\n在 1.8 及以后是数组 + 链表 + 红黑树\n重要的几个变量 DEFAULT_INITIAL_CAPACITY = 1 \u0026laquo; 4; Hash 表默认初始容量 MAXIMUM_CAPACITY = 1 \u0026laquo; 30; 最大 Hash 表容量 DEFAULT_LOAD_FACTOR = 0.75f；默认加载因子 TREEIFY_THRESHOLD = 8；链表转红黑树阈值 UNTREEIFY_THRESHOLD = 6；红黑树转链表阈值 MIN_TREEIFY_CAPACITY = 64；链表转红黑树时 hash 表最小容量阈值，达不到优先扩容 存放数据 Map \u0026lt;String,Employee\u0026gt; map = new HashMap \u0026lt;\u0026gt; (); Employee e0 = new Employee(\u0026#34;zhangshan\u0026#34;); map.put(\u0026#34;zhangshan\u0026#34;, e0); 会对 \u0026ldquo;zhangshan\u0026rdquo; 进行一次 hash 运算, 把 “zhangshan” 这个字符串映射成一个小于数组长度的整型值。就像下面这样:\nint i = hash(\u0026quot;zhangshan\u0026quot;) 假如 i 等于 1，就会把 e0 构造成一个节点，放入数组下标为 1 的位置。数组存放的是一个节点，该节点有指向下一个节点的指针 next, 如下：\nstatic class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash; final K key; V value; Node \u0026lt;K,V\u0026gt; next; } int i = hash(\u0026quot;zhangshan\u0026quot;) 把字符串映射成一个整型，不同的字符串可能映射成相同的位置，有下面这种可能：\nhash(\u0026#34;zhangshan\u0026#34;) == hash(\u0026#34;lisi\u0026#34;) 这就是 hash 碰撞，出现碰撞后，会以链表的方式追加在后面，就形成了上图中的结构。\n如何确定 key 在数组中的位置 先看 jdk 1.7 中的实现:\npublic V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); // 获取 key 对应的整型 hash值 int hash = hash(key); // 再将这个hash值转换为小于这个数组的整型值 i，然后将节点插入数组i位置 int i = indexFor(hash, table.length); for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null;} 其中 hash 方法如下：\nfinal int hash(Object k) { int h = hashSeed; if (0 != h \u0026amp;\u0026amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } 我们无需关注实现细节，只需知道这个 hash 方法会返回一个尽量分散的整型值 K. 下面一个关键步骤是如何把 k 转换为一个小于数组长度的值呢？ 我们想到最直接的方法是取余运算 %, 即： K % table.length , 是的。这样结果完全没问题，但是性能有问题，在我们常见的 + - * / % 运算中， % 效率是最低的。而 HashMap 作为一个 java 内置的数据结构，会有大量的场景使用。对性能的要求就比较高，自然这里的 indexFor 方法用的不是取余运算，而是 \u0026amp; 运算, 如下:\n/** * Returns index for hash code h. * */ static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \u0026#34;length must be a non-zero power of 2\u0026#34;; return h \u0026amp; (length-1); } 这段代码中的注释说。length 必须是 2 的 N 次方，我们来看看这是为什么\n\u0026amp; 运算的规则是，同时为 1，结果才是 1，否则是 0，即 1 \u0026amp; 1 == 1 1 \u0026amp; 0 ==0 0 \u0026amp; 0 == 0\n而 2 的 N 次方减一，的二进制一定是全为 1，比如 3 ， 7 ， 15 的二进制是 11 111 1111 。正因为是这种结构， r = h \u0026amp; ( 2 ^ n -1) 的结果 r 一定小于 n, 且 r 取决于 h 的值，由此可以代替取余运算，像这种二进制的 \u0026amp; | ! ^ 运算是最接近计算机底层的，运算速度远远高于 % 运算，我简单测试一下，大约相差 10 倍。\nHashMap 的容量 但是要保证上述运算的准确性和效率，其中数组的长度 length 必须是 2 的 N 次方。那么我们在项目中的这种代码：new HashMap\u0026lt;\u0026gt;(13) , 数组的长度是 13 吗？ 当然不是，而是以第一个大于 13 且是 2 的 N 次方的数 16, 作为数组的长度。我们先看一下 JDK 1.7 代码：\npublic V put(K key, V value) { // 如果数组为空，初始化数组，而不是在HashMap的构造方法中进行的的 if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } 初始化方法：\nprivate void inflateTable(int toSize) { // 找到第一个大于等于 toSize 的 2的 N次方的值 int capacity = roundUpToPowerOf2(toSize); threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; initHashSeedAsNeeded(capacity); } private static int roundUpToPowerOf2(int number) { // assert number \u0026gt;= 0 : \u0026#34;number must be non-negative\u0026#34;; return number \u0026gt;= MAXIMUM_CAPACITY ? MAXIMUM_CAPACITY : (number \u0026gt; 1) ? Integer.highestOneBit((number - 1) \u0026lt;\u0026lt; 1) : 1; } // 巧妙的通过或运算和位移运算得出第一个大于i的 2 的 N 的数值 public static int highestOneBit(int i) { // HD, Figure 3-1 i |= (i \u0026gt;\u0026gt; 1); i |= (i \u0026gt;\u0026gt; 2); i |= (i \u0026gt;\u0026gt; 4); i |= (i \u0026gt;\u0026gt; 8); i |= (i \u0026gt;\u0026gt; 16); return i - (i \u0026gt;\u0026gt;\u0026gt; 1); } 关于这个运算原理的讲解参考： https://segmentfault.com/a/1190000039392972\n在后续的数组扩容中，新的数组容量也要遵循这个规则，这一点， JDK 1.8 和 1.8 之前的核心实现差不多。\nHashMap 的扩容 并不是等到节点数量达到容量后才进行的扩容，而是设置了一个阈值，阈值小于等于容量。当节点数量达到阈值后就开始扩容，容量变为原来的 2 倍，在 1.8 之前，阈值 = 容量 * 加载因子。而在 1.8 中，阈值也是原来的 2 倍；如下：\n容量和阈值的增长 1.8\n1.7\n节点的移动方式 在底层数组的扩容方法上，1.8 版本和 1.8 之前的版本相差最大，其中 1.8 之前，HashMap 的扩容在多线程下会产生死循环的问题。\n我们先看一下 1.7 版本的扩容 ：\n1.7版本 节点移动步骤 /** * Transfers all entries from current table to newTable. */ void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; // 遍历旧数组 for (Entry\u0026lt;K,V\u0026gt; e : table) { // 遍历链表 while(null != e) { Entry\u0026lt;K,V\u0026gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } // 重新计算当前节点在新数组中的位置 int i = indexFor(e.hash, newCapacity); // 修改节点的指向 e.next = newTable[i]; newTable[i] = e; // 下一次循环 e = next; } } } 1.7 版本扩容的核心方法只有上面一段，理解起来也不难，主要有下几个步骤:\n外层遍历数组，假设当前元素: e0 内层遍历数组指向的链表，即 e0 为头节点的链表 扫描链表的每个节点，重新计算节点的在新数组的位置，将节点移动到新数组中对应的位置，以头插法的方式处理有 Hash 冲突的节点。 一图胜千言:\n用头插法会导致链表的顺序发生变化。其中每一步不再详解。下面看一下这种扩容方法在多线程下的问题\n并发导致的死循环问题 经过几次循环形成了环，Thread1 线程后面在进行 Put 数据时，如果某个key 落在了这个有环节点位置，就会发生死循环。如下:\npublic V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); // 因为形成了环，导致 e != null 永远成立。死循环 for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } 下面我们对比看一下 1.8 版本是如何解决这个问题的。\n1.8版本 节点移动步骤 在1.8版本中仍保留了 数组+链表的结构，只有当HashMap中的容量大于某个值时，才会把链表转换为红黑树，提高检索效率。现在我们只关注扩容部分。\n扩容的关键代码：\nfinal Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap \u0026gt; 0) { if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold } else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash \u0026amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); else { // preserve order // 定义了四个指针 Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; Node\u0026lt;K,V\u0026gt; next; // 开始扩容 do { next = e.next; // 节点的hash值与 旧数组的容量相与，oldCap 是2的N次方 // 一个数和 2的N次方相与时，结果只能是0或 oldCap if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) // 指定头指针的位置 loHead = e; else // 前一个指针的后继节点是当前节点 loTail.next = e; // 尾指针锚定当前节点 loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 低位节点的下标不变 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 高位节点下标增加 oldCap if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 这里定义了四个指针，将某个链表分为两部分，链表节点和数组长度相与的结果作为分隔，等于0的放在以loHead为头节点的链表中，等1的放在以hiHead为头节点的链表中。如下图:\n如上所示。这种移动方式没有改变节点关系的方向，所以并发之下也没有问题\n扩容因子为什么是0.75 1.8版本链表与红黑树的转换 链表长度 \u0026gt; 8 容量 \u0026gt; 64 性能提升的不是很高，在大量数据下，可能会提升5% ~10%, 数据量不大时。没有什么区别 红黑树 ","date":"2022-10-15T11:15:24Z","permalink":"https://dccmmtop.github.io/posts/hashmap%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","section":"posts","tags":["java"],"title":"HashMap底层原理"},{"categories":null,"contents":"原子操作 原子（atom）本意是“不能被进一步分割的最小粒子”，而原子操作（atomic operation）意为”不可被中断的一个或一系列操作” 。在多处理器上实现原子操作就变得有点复杂\n处理器会保证基本内存操作的原子性 处理器保证从系统内存当中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。奔腾 6 和最新的处理器能自动保证单处理器对同一个缓存行里进行 16/32/64 位的操作是原子的，但是复杂的内存操作处理器不能自动保证其原子性，比如跨总线宽度，跨多个缓存行，跨页表的访问。但是处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。\nJava 中如何实现原子操作 java 中可以通过锁和循环 CAS的方式实现原子操作\nCAS 操作就是利用上文说的处理器提供的 CMPXCHG 指令实现的，是硬件原语。自旋 CAS 就是以一直进行 CAS 操作直到 CAS 成功为止。 java 提供了 atomic 包进行一系列的原子操作。\nAtomic 在 atomic 包中一共 you 有 12 个类，4 中原子更新方式，分别是：\n原子更新基本类型 原子更新数组 原子更新引用 原子更新字段 原子更新基本类型类 AtomicBoolean ：原子更新布尔类型。 AtomicInteger ：原子更新整型。 AtomicLong ：原子更新长整型。 AtomicInteger AtomicInteger 的常用方法如下：\nint addAndGet(int delta) ：以原子方式将输入的数值与实例中的值（AtomicInteger 里的 value）相加，并返回结果 boolean compareAndSet(int expect, int update) ：如果输入的数值等于预期值，则以原子方式将该值设置为输入的值。 int getAndIncrement() ：以原子方式将当前值加 1，注意：这里返回的是自增前的值。 void lazySet(int newValue) ：最终会设置成 newValue，使用 lazySet 设置值后，可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 int getAndSet(int newValue) ：以原子方式设置为 newValue 的值，并返回旧值。 Atomic 包提供了三种基本类型的原子更新，但是 Java 的基本类型里还有 char，float 和 double 等。那么问题来了，如何原子的更新其他的基本类型呢？Atomic 包里的类基本都是使用 Unsafe 实现的，Unsafe 只提供了三种 CAS 方法，compareAndSwapObject，compareAndSwapInt 和 compareAndSwapLong，再看 AtomicBoolean 源码，发现其是先把 Boolean 转换成整型，再使用 compareAndSwapInt 进行 CAS，所以原子更新 double 也可以用类似的思路来实现。\n原子更新数组类 以原子的方式更新数组某个元素，提供一下 3 个类\nAtomicIntegerArray ：原子更新整型数组里的元素。 AtomicLongArray ：原子更新长整型数组里的元素。 AtomicReferenceArray ：原子更新引用类型数组里的元素。 AtomicIntegerArray 类主要是提供原子的方式更新数组里的整型，其常用方法如下\nint addAndGet(int i, int delta) ：以原子方式将输入值与数组中索引 i 的元素相加。 boolean compareAndSet(int i, int expect, int update) ：如果当前值等于预期值，则以原子方式将数组位置 i 的元素设置成 update 值。 原子更新字段类 如果我们只需要某个类里的某个字段，那么就需要使用原子更新字段类，Atomic 包提供了以下三个类：\nAtomicIntegerFieldUpdater ：原子更新整型的字段的更新器。 AtomicLongFieldUpdater ：原子更新长整型字段的更新器。 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于原子的更数据和数据的版本号，可以解决使用 CAS 进行原子更新时，可能出现的 ABA 问题。 原子更新字段类都是抽象类，每次使用都时候必须使用静态方法 newUpdater 创建一个更新器。原子更新类的字段的必须使用 public volatile 修饰符。\nUnsafe 正如其名，Unsafe 提供一些不安全的操作方法，如直接访问系统内存资源，自主管理系统内存资源，这些方法在提高 java 运行效率，增强 java 底层资源的操作能力发挥了很大作用。但由于 Unsafe 类使 Java 语言拥有了类似 C 语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用 Unsafe 类会使得程序出错的概率变大，使得 Java 这种安全的语言变得不再“安全”，因此对 Unsafe 的使用一定要慎重。\n如何使用 Unsafe 类 Unsafe 类为一单例实现，提供静态方法 getUnsafe 获取 Unsafe 实例，当且仅当调用 getUnsafe 方法的类为引导类加载器所加载时才合法，否则抛出 SecurityException 异常\n我们自己写的应用程序无法直接使用 Unsafe 类，可以通过反射方式使用：\npublic class UnsafeInstance { public static Unsafe reflectGetUnsafe() { try { Field field = Unsafe.class.getDeclaredField(\u0026#34;theUnsafe\u0026#34;); field.setAccessible(true); return (Unsafe) field.get(null); } catch (Exception e) { e.printStackTrace(); } return null; } } Unsafe 功能介绍 Unsafe 提供的 API 大致可分为内存操作、CAS、Class 相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类，下面进行简单介绍：\n内存操作 分配内存, 相当于 C++的 malloc 函数\npublic native long allocateMemory(long bytes);\n扩充内存\npublic native long reallocateMemory(long address, long bytes);\n释放内存\npublic native void freeMemory(long address);\n在给定的内存块中设置值\npublic native void setMemory(Object o, long offset, long bytes, byte value);\n内存拷贝\npublic native void copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);\n获取给定地址值，忽略修饰限定符的访问限制。与此类似操作还有: getInt，getDouble，getLong，getChar 等\npublic native Object getObject(Object o, long offset);\n为给定地址设置值，忽略修饰限定符的访问限制，与此类似操作还有: putInt,putDouble，putLong，putChar 等\npublic native void putObject(Object o, long offset, Object x);\npublic native byte getByte(long address);\n为给定地址设置 byte 类型的值（当且仅当该内存地址为 allocateMemory 分配时，此方法结果才是确定的）\npublic native void putByte(long address, byte x);\n为什么会用到堆外内存 通常我们使用 new 关键词构造的对象占用的都是 jvm 堆内的空间，由 jvm 统一管理。与之相对的就是堆外内存，jvm 无法管理，回收。使用 Unsafe 提供的方法可以对堆外的内存进行管理。那么我们什么场景下会使用堆外内存呢？\n改善垃圾回收性能 由于堆外内存是由操作系统管理，而不是 jvm，当我们使用堆外内存时，可以保持降低堆内内存的使用，减少垃圾回收停顿堆应用的影响，比如在上传大文件时，可以把文件对象分配到堆外内存。\n提升程序 I/O 操作的性能 通常在 IO 通信过程中，存在堆内内存到堆外内存的数据拷贝过程，- 对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。\nCAS 相关 如下源代码：\n* CAS * @param o 包含要修改field的对象 * @param offset 对象中某field的偏移量 * @param expected 期望值 * @param update 更新值 * @return true | false */ public final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5); public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6); 应用场景 atomic 包中各类原子操作，都是对 CAS 的应用\n线程调度 包括线程挂起、恢复、锁机制等方法。\n//取消阻塞线程 public native void unpark(Object thread); //阻塞线程 public native void park(boolean isAbsolute, long time); //获得对象锁（可重入锁） @Deprecated public native void monitorEnter(Object o); //释放对象锁 @Deprecated public native void monitorExit(Object o); //尝试获取对象锁 @Deprecated public native boolean tryMonitorEnter(Object o); 方法 park、unpark 即可实现线程的挂起与恢复，将一个线程进行挂起是通过 park 方法实现的，调用 park 方法后，线程将一直阻塞直到超时或者中断等条件出现；unpark 可以终止一个挂起的线程，使其恢复正常。\n应用场景 Java 锁和同步器框架的核心类 AbstractQueuedSynchronizer，就是通过调用 LockSupport.park() 和 LockSupport.unpark() 实现线程的阻塞和唤醒的，而 LockSupport 的 park、unpark 方法实际是调用 Unsafe 的 park、unpark 方式来实现。\n内存屏障 在 Java 8 中引入，用于定义内存屏障（也称内存栅栏，内存栅障，屏障指令等，是一类同步屏障指令，是 CPU 或编译器在对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作），避免代码重排序\n","date":"2022-10-11T11:03:04Z","permalink":"https://dccmmtop.github.io/posts/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8Batomic%E5%92%8Cunsafe%E9%AD%94%E6%B3%95%E7%B1%BB/","section":"posts","tags":["java"],"title":"并发编程之Atomic和Unsafe魔法类"},{"categories":null,"contents":"Semaphore Semaphore 是信号量的意思，它的作用是控制访问特定资源的线程数目，底层依赖 AQS 的状态 State，是在生产当中比较常用的一个工具类。\n可以理解为许可证，或者令牌。线程想要访问某部分资源时，必须先获取一个许可证，才能访问，否则等待，一个经典的应用场景是服务限流(Hystrix 里限流就有基于信号量方式)，\n重要方法 构造方法 // 构造方法1 // permits 许可证的数量，默认是非公平的方式抢占许可证，许可证用完之后， // 再来的线程要等待其他线程释放许可证 public Semaphore(int permits) { sync = new NonfairSync(permits); } // 构造方法2 // 同上，可以指定公平还是非公平 public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits); } 获取许可证 acquire() 此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：\n当前线程获取了 1 个可用的许可证，则会停止等待，继续执行。 当前线程被中断，则会抛出 InterruptedException 异常，并停止等待，继续执行。 acquire(int permits) 此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：\n当前线程获取了 n 个可用的许可证，则会停止等待，继续执行。 当前线程被中断，则会抛出 InterruptedException 异常，并停止等待，继续执行 acquireUninterruptibly(int permits) 此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：\n当前线程获取了 1 个可用的许可证，则会停止等待，继续执行。 与前两个的区别是，它不理会中断\nacquireUninterruptibly(int permits) 此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：\n当前线程获取了 n 个可用的许可证，则会停止等待，继续执行。\n它不理会中断 tryAcquire() 当前线程尝试去获取 1 个许可证。\n此过程是非阻塞的，它只是在方法调用时进行一次尝试。如果当前线程获取了 1 个可用的许可证，则会停止等待，继续执行，并返回 true。如果当前线程没有获得这个许可证，也会停止等待，继续执行，并返回 false。\ntryAcquire(int permits) 当前线程尝试去获取 permits 个许可证。\n此过程是非阻塞的，它只是在方法调用时进行一次尝试。如果当前线程获取了 permits 个可用的许可证，则会停止等待，继续执行，并返回 true。如果当前线程没有获得 permits 个许可证，也会停止等待，继续执行，并返回 false。\ntryAcquire(long timeout, TimeUnit unit) 当前线程在限定时间内，阻塞的尝试去获取 1 个许可证。\n此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：\n当前线程获取了可用的许可证，则会停止等待，继续执行，并返回 true。 当前线程等待时间 timeout 超时，则会停止等待，继续执行，并返回 false。 当前线程在 timeout 时间内被中断，则会抛出 InterruptedException 一次，并停止等待，继续执行。 tryAcquire(int, long, TimeUnit) 当前线程在限定时间内，阻塞的尝试去获取 permits 个许可证。\n此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：\n当前线程获取了可用的 permits 个许可证，则会停止等待，继续执行，并返回 true。 当前线程等待时间 timeout 超时，则会停止等待，继续执行，并返回 false。 当前线程在 timeout 时间内被中断，则会抛出 InterruptedException 一次，并停止等待，继续执行。 drainPermits() 当前线程获得剩余的所有可用许可证\n释放许可证 release() 当前线程释放一个许可证\nrelease(int) 当前线程释放 n 个许可证\n示例 import java.util.Date; import java.util.concurrent.Semaphore; public class SemaphoreRunner { public static void main(String[] args) { Semaphore semaphore = new Semaphore(2); for (int i = 0; i \u0026lt; 10; i++) { new Thread(new Task(semaphore, \u0026#34;任务:\u0026#34; + i)).start(); } } static class Task extends Thread { Semaphore semaphore; public Task(Semaphore semaphore, String tname) { this.semaphore = semaphore; this.setName(tname); } @Override public void run() { try { semaphore.acquire(); System.out.println(this.getName() + \u0026#34;获得许可证 at time:\u0026#34; + new Date()); Thread.sleep(3000); semaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } } } } 结果:\n任务:0获得许可证 at time:Mon Oct 10 11:42:33 CST 2022 任务:1获得许可证 at time:Mon Oct 10 11:42:33 CST 2022 任务:3获得许可证 at time:Mon Oct 10 11:42:36 CST 2022 任务:2获得许可证 at time:Mon Oct 10 11:42:36 CST 2022 任务:4获得许可证 at time:Mon Oct 10 11:42:39 CST 2022 任务:5获得许可证 at time:Mon Oct 10 11:42:39 CST 2022 任务:7获得许可证 at time:Mon Oct 10 11:42:42 CST 2022 任务:6获得许可证 at time:Mon Oct 10 11:42:42 CST 2022 任务:9获得许可证 at time:Mon Oct 10 11:42:45 CST 2022 任务:8获得许可证 at time:Mon Oct 10 11:42:45 CST 2022 可以看出当设置 2 个许可证时，同时只有两个线程执行\nCountDownLatch 与 CyclicBarrier CountDownLatch 这个类能够使一个线程等待其他线程完成各自的工作后再执行。例如，应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行, 它强调的是一个线程等待其他多个线程\nCountDownLatch 其实可以把它看作一个计数器，只不过这个计数器的操作是原子操作，同时只能有一个线程去操作这个计数器，也就是同时只能有一个线程去减这个计数器里面的值。可以向 CountDownLatch 对象设置一个初始的数字作为计数值，任何调用这个对象上的 await()方法都会阻塞，直到这个计数器的计数值被其他的线程减为 0 为止。所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。这种现象只出现一次——计数无法被重置\nCyclicBarrier 允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrier 在释放等待线程后可以重用，所以称它为循环的 barrier， CyclicBarrier 可以用来模拟并发，类似于 Jmeter, 只有多个线程都到达要并发的位置时，再统一开始执行，就像多个线程运行到一个栅栏前等待，然后把栅栏移除，多个线程同时运行。移除的时机是多个线程全部到达栅栏前\n区别 重要方法 CountDownLatch\npublic void await() throws InterruptedException { //调用await()方法的线程会被挂起，它会等待直到count值为0才继续执行 } public boolean await(long timeout, TimeUnit unit) throws InterruptedException { //和await()类似，只不过等待一定的时间后count值还没变为0的话就会继续执行 } public void countDown() { //将count值减1 } 使用示例\npublic class CountDownlatchRunner { public static void main(String[] args) throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(5); for(int i=0;i\u0026lt;5;i++){ new Thread(new ReadNum(i,countDownLatch)).start(); } // 等待所有线程结束 countDownLatch.await(); System.out.println(\u0026#34;线程执行结束。。。。\u0026#34;); } static class ReadNum implements Runnable{ private int id; private CountDownLatch latch; public ReadNum(int id,CountDownLatch latch){ this.id = id; this.latch = latch; } @Override public void run() { synchronized (this){ System.out.println(\u0026#34;id:\u0026#34;+id); latch.countDown(); System.out.println(\u0026#34;线程组任务\u0026#34;+id+\u0026#34;结束，其他任务继续\u0026#34;); } } } } CyclicBarrier\n提供了两个构造器\n// 指定了N个线程互相等待 public CyclicBarrier(int parties) { } // 指定N个线程在任务 barrierAction 处互相等待 public CyclicBarrier(int parties, Runnable barrierAction) { } 等待方法:\npublic int await() throws InterruptedException, BrokenBarrierException { //挂起当前线程，直至所有线程都到达barrier状态再同时执行后续任务； } public int await(long timeout, TimeUnit unit)throws InterruptedException,BrokenBarrierException,TimeoutException { //让这些线程等待至一定的时间，如果还有线程没有到达barrier状态 //就直接让到达barrier的线程执行后续任务 } 示例\npublic class CyclicBarrierTest { public static void main(String[] args) throws InterruptedException { CyclicBarrier cyclicBarrier = new CyclicBarrier(5, new Runnable() { @Override public void run() { System.out.println(\u0026#34;线程组执行结束\u0026#34;); } }); for (int i = 0; i \u0026lt; 5; i++) { new Thread(new ReadNum(i,cyclicBarrier)).start(); } //CyclicBarrier 可以重复利用， // 这个是CountDownLatch做不到的 // for (int i = 11; i \u0026lt; 16; i++) { // new Thread(new readNum(i,cyclicBarrier)).start(); // } } static class ReadNum implements Runnable{ private int id; private CyclicBarrier cyc; public readNum(int id,CyclicBarrier cyc){ this.id = id; this.cyc = cyc; } @Override public void run() { synchronized (this){ System.out.println(\u0026#34;id:\u0026#34;+id); try { // 线程等待，直到5各线程都运行到这里再一起执行 cyc.await(); System.out.println(\u0026#34;线程组任务\u0026#34; + id + \u0026#34;结束，其他任务继续\u0026#34;); } catch (Exception e) { e.printStackTrace(); } } } } } ","date":"2022-10-10T10:40:28Z","permalink":"https://dccmmtop.github.io/posts/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8Bsemaphore%E5%92%8Ccountdownlatch%E7%9A%84%E7%94%A8%E6%B3%95/","section":"posts","tags":["java"],"title":"并发编程之Semaphore和CountDownLatch的用法"},{"categories":null,"contents":"JDK1.5以前只有synchronized同步锁，并且效率非常低，大神Doug Lea自己写了一套并发框架，这套框架的核心就在于AbstractQueuedSynchronizer类（即AQS），性能非常高，所以被引入JDK包中，即JUC。那么AQS是怎么实现的呢？本篇就是对AQS及其相关组件进行分析，了解其原理。\nAQS 的应用 我们经常使用并发包中的阻塞队列(ArrayBlockingQueue), 可重入锁（ReentrantLock），线程栅栏（CountDownLatch）等一些工具底层都是由AQS实现的\nAQS 大致结构 ReentrantLock 实现原理 ReentrantLock 使用简单，我们就以这个类为切入口，学习一下如何利用 AQS 实现加锁释放锁的功能，以及公平和非公平锁实现的差别.\n加锁 查看 ReentrantLock 的构造方法：\n// 无参构造器默认构造一个非公平锁 public ReentrantLock() { sync = new NonfairSync(); } // 可以指定使用公平锁还是非公平锁 public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 由此可知，ReentrantLock 的公平锁和非公平锁分别是由 FairSync 和 NonfairSync实现的，\n由下面的结构图可知，FairSync 和 NonfairSync 都是继承至 Sync ,而 Sync 又是继承 AQS\n非公平锁加锁的代码:\n// 非公平锁 static final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; // 获取锁 final void lock() { // 假设t1 线程正在尝试获取锁。 // CAS算法，把 state 从0修改为1，state 表示当前被加锁的次数 // 从0变成1，表示t1第一次尝试获取锁 if (compareAndSetState(0, 1)) // 如果修改成功，就把t1设置成正在持有锁的线程 setExclusiveOwnerThread(Thread.currentThread()); else // 未获取到锁... acquire(1); } } accquire(1) 实现:\npublic final void acquire(int arg) { // tryAcquire(arg)：尝试抢锁 // addWaiter(Node.EXCLUSIVE)，将当前线程构造成一个队列节点，并入队 // acquireQueued（...） 将线程挂起，维护线程节点的状态 if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 大致意思就是，线程再抢一次锁，如果失败了，就构造一个线程节点，然后把节点放入队列，将线程挂起，等待被唤醒\n再次抢锁代码: tryAcquire(arg):\n// 非公平锁尝试获取锁 final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); // 获取已经加锁的次数 int c = getState(); // 没有线程持有锁 if (c == 0) { // 直接抢锁。没有判断队列中是否有线程排队，插队，不公平 if (compareAndSetState(0, acquires)) { // 抢锁成功 setExclusiveOwnerThread(current); return true; } } // 正在有线程持有锁，并且这个线程是自己(t1) else if (current == getExclusiveOwnerThread()) { // t1 已经获取到锁，无需再次获取锁，只需把锁的次数增加即可 int nextc = c + acquires; if (nextc \u0026lt; 0) // overflow throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); // 设置锁的次数 setState(nextc); return true; } return false; } 公平锁的实现 protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // hasQueuedPredecessors： 当线程尝试获取锁时，不是直接去抢， // 而是先判断是否存在队列，如果存在就不抢了，返回抢锁失败 if (!hasQueuedPredecessors() \u0026amp;\u0026amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc \u0026lt; 0) throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } return false; } // 是否存在队列并且(下一个待唤醒的线程不是本线程(准备重入锁)) public final boolean hasQueuedPredecessors() { // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t \u0026amp;\u0026amp; ((s = h.next) == null || s.thread != Thread.currentThread()); } 于非公平锁相比，只有tryAcquire 方法的区别，\n为什么需要再次抢锁? 因为抢锁失败有两种原因，1是当前线程确实没有获取到锁。2是当前线程之前已经获取到锁了，还想再获取一次。\n对于1这种情况，让线程再抢一次，可能会抢到锁，就不用调用系统api把线程挂起，提高性能\n对于2， 只需改变加锁的次数，就可以标记当前线程已经加锁的次数了，再释放锁时，对应的减成0就可以认为当前线程已经完全释放锁了，这就是可重入锁的实现原理\n构造队列节点及入队 下面看一下构造线程节点的实现:\naddWaiter()\nprivate Node addWaiter(Node mode) { // 以当前线程为参数，构造一个新的 node，记作当前线程节点 Node node = new Node(Thread.currentThread(), mode); // 在最开始，tail 和 pred 肯定都是null, Node pred = tail; // 最开始不会进入下面，只有队列不为空时，才会进入 if (pred != null) { node.prev = pred; // 将节点加入队尾 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 而是由 enq(node) 构造节点 enq(node); return node; } private Node enq(final Node node) { // 开始了循环 for (;;) { Node t = tail; // 最开始队列是空的。只有第一次循环会进入 if (t == null) { // Must initialize // 构造了一个空的node节点当作队列的头节点 if (compareAndSetHead(new Node())) tail = head; } else { // 第二次及后面的循环会走到这里 // 先设置当前节点的前驱节点是 队尾节点。 node.prev = t; // 用CAS算法把当前节点 设置成队尾 if (compareAndSetTail(t, node)) { // 这样上一次的队尾t就不是队尾了，t 就有了后继节点node t.next = node; return t; } } } } 经过addWaiter(Node node) 方法后，队列中至少存在两个节点，第一个就是必须的空节点，不包含线程信息，第二个才是真正待执行的线程节点，作者为什么这么做呢？\n我认为，队列中存放的不仅是待唤醒的线程节点，而是所有等待运行和正在运行的线程节点，因为已经拿到锁的正在运行的线程不需要被唤醒，所以也就不需要存储线程信息了。并且这个正在运行的线程节点是队列中的头节点\n线程挂起 下面就要看acquireQueued 方法了\nfinal boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; // 开始循环抢锁 for (;;) { // 获取当前接节点的前驱节点 final Node p = node.predecessor(); // 如果前驱节点是头节点，并且抢锁成功 if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { // 把当前节点设置成头节点，setHead会清空node中的线程信息，和初始化时设置的空头节点一样 setHead(node); // 断开前驱节点，旧的 head 会被垃圾回收 p.next = null; // help GC failed = false; return interrupted; } //走到这里说明不是头节点，或者抢锁失败 // shouldParkAfterFailedAcquire(p, node): // 检查 node 是否是可唤醒的（waitStatus == -1）,如果是，返回true // 如果node不是可唤醒的，并且node没有被取消掉，则将node设置设置为可唤醒，返回false, // 下一次循环时就会返回false // parkAndCheckInterrupt(): 挂起线程 if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) interrupted = true; } } finally { // 这个判断不会走，可以认为 failed 和 interrupted 标识这里无用。 // 程序能走到这里，说明 (p ==head \u0026amp;\u0026amp; tryQcquire(arg)) 为true，那么 failed 和 interupted 恒为false // 否则就会陷在循环中，无法到 finally 中。 if (failed) cancelAcquire(node); } } // 把node设置为队列的头节点 private void setHead(Node node) { head = node; // 清空了线程信息 node.thread = null; node.prev = null; } shouldParkAfterFailedAcquire(Node pred, Node node)\n// 接受两个参数，一个是当前节点的前驱节点，一个是当前节点 /** * 这里使用前驱节点中的waitStatus状态来判断当前节点是否可以被唤醒。 */ private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { // 前驱节点的状态 int ws = pred.waitStatus; // 如果是可唤醒的，直接返回true if (ws == Node.SIGNAL) return true; if (ws \u0026gt; 0) { // 标识前驱节点已经取消锁竞争，跳过这个前驱节点，继续向前查找 do { // 一直向前找 node.prev = pred = pred.prev; } while (pred.waitStatus \u0026gt; 0); // 到不是已取消的节点为止 // 设置有效的前驱节点 pred.next = node; } else { // 将前驱节点的 ws 设置可唤醒的 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 释放锁 释放锁的逻辑比较简单，\n减少加锁的次数(state)，如果state == 0, 代表当前线程可以释放锁，然后把持有锁的线程标记为空 唤醒队列中第一个待运行的线程也就是第二个节点，因为第一个节点是当前已获取到锁正在运行线程 public final boolean release(int arg) { // 释放锁 if (tryRelease(arg)) { Node h = head; // 头节点不为空，且头节点的waitStatus不是默认状态 if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) //传入的是头节点 unparkSuccessor(h); return true; } return false; } // 释放锁 protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } private void unparkSuccessor(Node node) { int ws = node.waitStatus; if (ws \u0026lt; 0) // 再次将ws 置为0，这里暂时不清楚为什么重置状态 compareAndSetWaitStatus(node, ws, 0); // 获取传入节点的后继节点 Node s = node.next; if (s == null || s.waitStatus \u0026gt; 0) { s = null; for (Node t = tail; t != null \u0026amp;\u0026amp; t != node; t = t.prev) if (t.waitStatus \u0026lt;= 0) s = t; } // 唤醒后继节点 if (s != null) LockSupport.unpark(s.thread); } 获取锁的流程图 队列中节点状态 ","date":"2022-10-06T15:14:59Z","permalink":"https://dccmmtop.github.io/posts/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8Baqs%E5%8E%9F%E7%90%86/","section":"posts","tags":["java"],"title":"并发编程AQS原理"},{"categories":null,"contents":"import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/spf13/viper\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) type Config struct { DesDir string SourceFiles []string } func loadConfig()(con Config){ home := os.Getenv(\u0026#34;HOME\u0026#34;) viper.SetConfigFile(filepath.Join(home,\u0026#34;config\u0026#34;,\u0026#34;syncFile.yml\u0026#34;)) viper.SetConfigType(\u0026#34;yml\u0026#34;) err := viper.ReadInConfig() checkErr(err) err = viper.Unmarshal(\u0026amp;con) checkErr(err) fmt.Printf(\u0026#34;config: %v\\n\u0026#34;, con) return con } func checkErr(err error){ if err != nil { panic(err) } } ","date":"2022-09-29T14:42:32Z","permalink":"https://dccmmtop.github.io/posts/yml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96/","section":"posts","tags":["go"],"title":"yml配置文件读取"},{"categories":null,"contents":"新建start.bat文件，输入如下命令：\nSet ws = CreateObject(\u0026#34;Wscript.Shell\u0026#34;) ws.run \u0026#34;cmd /c D:/1.exe\u0026#34;,vbhide ","date":"2022-09-22T14:32:14Z","permalink":"https://dccmmtop.github.io/posts/bat%E8%84%9A%E6%9C%AC%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8Cexe/","section":"posts","tags":["win"],"title":"bat脚本后台运行exe"},{"categories":null,"contents":"对于 java 应用我们可以通过一些配置把程序运行过程中的 gc 日志全部打印出来，然后分析 gc 日志得到关键性指标，分析 GC 原因，调优 JVM 参数：\n开启 GC 日志相关参数 java -jar ‐Xloggc:./gc‐%t.log ‐XX:+PrintGCDetails ‐XX:+PrintGCDateStamps ‐XX:+PrintGCTimeStamps ‐XX:+PrintGCCause ‐XX:+UseGCLogFileRotation ‐XX:NumberOfGCLogFiles=10 ‐XX:GCLogFileSize=100M main.jar gc-%t.log 日志文件带时间 +PrintGCDetails 打印详细信息 +PrintGCDateStamps 打印日期 +PrintGCTimeStamps 打印时间 +PrintGCCause 打印 GC 原因 +UseGCLogFileRotation 开启日志轮换 NumberOfGCLogFiles GC 日志保留个数 GCLogFileSize 每个日志文件的大小 查看jvm所有参数 java -XX:+PrintFlagsInitial 表示打印出所有参数选项的默认值 java -XX:+PrintFlagsFinal 表示打印出所有参数选项在运行程序时生效的值 GC 日志分析 我们可以看到图中第一行红框，是项目的配置参数。这里不仅配置了打印 GC 日志，还有相关的 VM 内存参数。\n第二行红框中的是在这个 GC 时间点发生 GC 之后相关 GC 情况。\n对于 2.909： 这是从 jvm 启动开始计算到这次 GC 经过的时间，前面还有具体的发生时间日期。 Full GC(Metadata GC Threshold)指这是一次 full gc，括号里是 gc 的原因， PSYoungGen 是年轻代的 GC， ParOldGen 是老年代的 GC，Metaspace 是元空间的 GC 6160K-\u0026gt;0K(141824K)，这三个数字分别对应 GC 之前占用年轻代的大小，GC 之后年轻代占用，以及整个年轻代的大小。 112K-\u0026gt;6056K(95744K)，这三个数字分别对应 GC 之前占用老年代的大小，GC 之后老年代占用，以及整个老年代的大小。 6272K-\u0026gt;6056K(237568K)，这三个数字分别对应 GC 之前占用堆内存的大小，GC 之后堆内存占用，以及整个堆内存的大小。 20516K-\u0026gt;20516K(1069056K)，这三个数字分别对应 GC 之前占用元空间内存的大小，GC 之后元空间内存占用，以及整个元空间内存的大小。 0.0209707 是该时间点 GC 总耗费时间 从日志可以发现几次fullgc都是由于元空间不够导致的，所以我们可以将元空间调大点：\n‐XX:MetaspaceSize=256M ‐XX:MaxMetaspaceSize=256M GC日日志分析工具 GC 日志太多，人工无法很好的分析出原因，可以利用一些工具：\ngceasy https://gceasy.io/\n以图形的方式展现内存变化等，还会给出一些 jvm 参数优化的建议，目前这个功能应该收费了 GC日志对性能的影响 其实GC日志就是 jvm 执行期间那些 C++ 代码打印的日志而已，和我们应用中的日志没有差别，只要系统没有非常频繁的发生GC 会导致日志太大，对应用造成的性能影响可以忽略\n","date":"2022-09-17T09:58:27Z","permalink":"https://dccmmtop.github.io/posts/gc%E6%97%A5%E5%BF%97%E8%AF%A6%E8%A7%A3/","section":"posts","tags":["java"],"title":"GC日志详解"},{"categories":null,"contents":"Class常量池 Class常量池可以理解为class文件中的资源仓库，class 文件中除了包含类信息，方法，字段，接口信息之外还有常量池信息，用于存放编译期生成的各种字面量和符号引用。\n如下:\n常量池主要包含两大类常量: 字面量和符号引用。\n字面量 字面量就是有字符串和数字等构成的常量。\n字面量只能以右值出现，int a = 1, a 是左值，1 是右值\nint a =1; int b =2; String c= \u0026#34;abc\u0026#34;； String d= \u0026#34;abc\u0026#34;； 1 2 \u0026ldquo;abc\u0026rdquo; 都是常量\n符号引用 符号引用是编译原理中概念，是相对于直接引用来说的，主要包括一三类常量:\n类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 上面的 a b就是字段名 ，是符号引用。 还有包名+类名 组成的类的全限定名，方法名，以及() 都是符号引用\n运行常量池 存在class 文件的常量都是静态信息，只有到运行时被加载到内存中，这些符号才有具体的内存地址，这些常量一旦被加载内存中，就变成运行常量池，对应的符号引用在程序运行时，会被加载到内存区域的代码直接引用，也就我们说的动态链接，例如： compute() 这个符号引用在运行时就会被转化成compute()方法具体代码在内存中的地址，\n字符串常量池 在内存中专门存放字符串字面量的区域称作字符串常量池，那么除了把对象放入堆中之外。还需要独立的区域存放字符串字面量呢？\n主要是为了性能：\n字符串的分配和其他对象分配一样，耗费高昂的时间与空间代价，作为最基础的数据类型，需要大量频繁的创建，极大影响了程序的性能 JVM 为了提高性能和减少内存开销，在实例化字符串常量的时候进行了一些优化：\n为字符串开辟一个字符串常量池，类似于缓存 创建字符串常量时，首先查询字符串常量池是否已经存在 若存在该字符串，直接返回引用示例，不存在，实例化该字符串并放入池中 什么时候会把字符串常量放入常量池 直接赋值字符串 String s = \u0026#34;hello\u0026#34;； 这种方式创建的字符串只会在常量池中，不会在堆中额外创建一个对象\n当再次创建字符串 String s1 = \u0026quot;hello\u0026quot; 时，先去常量池中通过 equals(key) 方法判断是否有相同的对象，如果有直接返回对象在常量池的引用。如果没有会在常量池新新建对象，再返回引用\n所以有如下结果:\nString s = \u0026#34;hello\u0026#34;； String s1 = \u0026#34;hello\u0026#34;； System.out.println( s == 1) // true. s 和 s1 地址一样 new String() String s = new String(\u0026#34;hello\u0026#34;); 这中方式创建字符串会保证常量池中和堆中都有这个对象，最后返回堆中的地址:\nintern 方法 String s = new String(\u0026#34;hello\u0026#34;)； String s1 = s.intern(); System.out.println(s == s1) // false String intern 方法是一个 native 方法，如果池中已经包含一个等于此 String 对象的字符串，则返回池中的字符串，否则将intern 返回的引用指向当前字符串（在 jdk 1.6 中，需要将s1字符串复制到常量池中）\n在第一种情况，String s = \u0026quot;hello\u0026quot; 会将 hello 放入常量池中， 第二种情况 String s = new String(\u0026quot;hello\u0026quot;) 也会将 hello 放入常量池中，那么什么情况下常量池会没有我们要取的字符串呢？\n常量池存放的一定是不可变的字面量, 无论是 String s = \u0026quot;hello\u0026quot; 还是 String s = new String(\u0026quot;hello\u0026quot;) 都有一个明确的字面量: hello 如果是下面这种情况:\nString s = new String(\u0026#34;hello\u0026#34;) + new String(\u0026#34;World\u0026#34;) String s1 = \u0026#34;helloWorld\u0026#34;； System.out.println(s == s1) // false System.out.println(s == s.intern()) // true 常量池中有 hello 和 World 但是没有 helloWorld， 因为在代码中没有明确的helloWorld字面量，所以 s != s1, 常量池中没有 helloWorld 字面量，所以 s.intern() 返回的是堆中的地址，故而 s == s.intern()\n再看下面一种情况:\nString s = \u0026#34;hello\u0026#34; + \u0026#34;World\u0026#34;； String s1 = \u0026#34;helloWorld\u0026#34;； System.out.println(s == s1) // true 代码中也没有明确的helloWorld 字面量， 为什么 s == s1 呢， 因为 hello 和 World 都是不可变的字面量，而不是一个引用，在 String s = \u0026quot;hello\u0026quot; + \u0026quot;World\u0026quot; 时， 编译器可以优化成 s = \u0026quot;helloWorld\u0026quot; ， 所以 s == s1 ，那为什么 String s = new String(\u0026quot;hello\u0026quot;) + new String(\u0026quot;World\u0026quot;) 不会被编译器优化呢？ 因为 new String(\u0026quot;hello\u0026quot;) 是一个对象，返回的是引用，而不是一个不会变化的字面量，后面这个引用地址可能会指向其他的对象，优化后可能会出现错误。同理：\nString s = \u0026#34;hello\u0026#34;; String s1 = \u0026#34;World\u0026#34;； String s2 = s + s1; // 不会编译器优化成 \u0026#34;helloWorld\u0026#34; String s3 = \u0026#34;helloWorld\u0026#34;; System.out.println(s3 == s2); // false 但是被 final 修饰的变量可以被优化，因为它不会发生变化了：\nfinal String s = \u0026#34;hello\u0026#34;; // s 不会再被重新赋值 final String s1 = \u0026#34;World\u0026#34;； String s2 = s + s1; // 编译器优化成 \u0026#34;helloWorld\u0026#34; String s3 = \u0026#34;helloWorld\u0026#34;; System.out.println(s3 == s2); // true 再看下面一种情况：\nfinal String s = getHello(); // s 虽然不能再被重新赋值，但getHello() 方法返回的值可能会改变 final String s1 = \u0026#34;World\u0026#34;； String s2 = s + s1; // 不会编译器优化成 \u0026#34;helloWorld\u0026#34; String s3 = \u0026#34;helloWorld\u0026#34;; System.out.println(s3 == s2); // false public String getHello(){ return \u0026#34;hello\u0026#34; } s 的值无法再编译器确定，所以无法优化成字面量\n再看最后一个例子：\nString s1 = new String(\u0026#34;hello\u0026#34;) + new String(\u0026#34;World\u0026#34;)； System.out.println(s1 == s1.intern()) // true String s = new String(\u0026#34;ja\u0026#34;) + new String(\u0026#34;va\u0026#34;)； System.out.println(s == s.intern()) // false 为什么同样的写法，结果却不一样呢？\nintern() 方法优先返回常量池中的地址, 常量池不存在时，再返回堆中的地址， 第一个 s1 != s1.intern() 是符合我们直觉的，因为常量池中没有 helloWorld , 但是第二个 s == s.intern() 为false 就说不通了，难道常量池已经有 java 这个字面量了吗？ 是的， java 这个关键词，在jvm 启动或类加载期间肯定有 java 这个字符串已经放入到常量池中了， s.inern() 返回的是常量池中的地址.\n八种基本类型的包装类和对象池 java中基本类型的包装类的大部分都实现了常量池技术(严格来说应该叫对象池，在堆上)，这些类是 Byte,Short,Integer,Long,Character,Boolean,另外两种浮点数类型的包装类则没有实现。另外 Byte,Short,Integer,Long,Character这5种整型的包装类也只是在对应值小于等于127时才可使用对象池，也即对象不负责创建和管理o大于127的这些类的对象。因为一般这种比较小的数用到的概率相对较大\npublic class Test { public static void main(String[] args) { //5种整形的包装类Byte,Short,Integer,Long,Character的对象， //在值小于127时可以使用对象池 Integer i1 = 127; //这种调用底层实际是执行的Integer.valueOf(127)，里面用到了IntegerCache对象池 Integer i2 = 127; System.out.println(i1 == i2);//输出true //值大于127时，不会从对象池中取对象 Integer i3 = 128; Integer i4 = 128; System.out.println(i3 == i4);//输出false //用new关键词新生成对象不会使用对象池 Integer i5 = new Integer(127); Integer i6 = new Integer(127); System.out.println(i5 == i6);//输出false //Boolean类也实现了对象池技术 Boolean bool1 = true; Boolean bool2 = true; System.out.println(bool1 == bool3);//输出true //浮点类型的包装类没有实现对象池技术 Double d1 = 1.0; Double d2 = 1.0; System.out.println(d1 == d2);//输出false } } ","date":"2022-09-15T10:05:51Z","permalink":"https://dccmmtop.github.io/posts/java%E5%B8%B8%E9%87%8F%E6%B1%A0/","section":"posts","tags":["java"],"title":"java常量池"},{"categories":null,"contents":"java内存模型 （JMM） 这种模型和 jvm 中的堆不同， JMM 是抽象概念，不真实存在。它是一种规范，指定了程序中的各变量的访问方式\n主内存 JMM 规定所有变量都存放在主内存，主内存是所有线程共享的，但是线程的操作在线程的工作内存中进行，先从主内存读取到线程的工作内存中，然后执行操作，再将工作内存中的值写入主内存中。线程不能直接操作主内存中的数据\n工作内存 工作内存是线程独有的，不同的线程无法访问到对方的工作内存，线程间的通信必须通过主内存传值进行\nJMM 描述的是变量在共享区域和私有区域的访问方式，变量的访问在多线程下会有 可见性，原子性，可见性三大问题\n可见性问题 因为有工作内存的划分，一个线程操作修改某变量的值，没有同步到主内存前，其他线程是无法读取到该变量最新的值，就导致了变量在另外的线程不可见。\n示例:\npublic class CodeVisibility { private static boolean initFlag = false; // private volatile static boolean initFlag = false; private static int counter = 0; public static void refresh() { System.out.println(\u0026#34;refresh data.......\u0026#34;); initFlag = true; System.out.println(\u0026#34;refresh data success.......\u0026#34;); } public static void main(String[] args) { Thread threadA = new Thread(() -\u0026gt; { while (!initFlag) { counter++; } System.out.println(\u0026#34;线程：\u0026#34; + Thread.currentThread().getName() + \u0026#34;当前线程嗅探到initFlag的状态的改变, counter: \u0026#34; + counter); }, \u0026#34;threadA\u0026#34;); threadA.start(); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } Thread threadB = new Thread(() -\u0026gt; { refresh(); }, \u0026#34;threadB\u0026#34;); threadB.start(); } } 结果:\n可见看到线程A久久不能结束，虽然线程B此时已经修改了 initFlag 的值，但是线程A无法读取到最新值，因为一直没有和主内存同步\nvolatile 这个关键词可以让变量被修改后立刻使其他线程中的副本可见。在上面的示例代码中，把第3行注释，第4行取消注释后再运行:\n可以看到线程A可以立即结束\nvolatile 原理 使用 volatile 关键字会强制将在某个线程中修改的共享变量的值立即写入主内存。 使用 volatile 关键字， 当线程 2 进行修改时， 会导致线程 1 的工作内存中变量的缓存行无效（反映到硬件层的话， 就是 CPU 的 L1或者 L2 缓存中对应的缓存行无效); 由于线程 1 的工作内存中变量的缓存行无效，所以线程1再次读取变量的值时会去主存读取。 特性 只能控制变量的可见性 不能解决原子性问题 还可以禁止CPU指令重排（见下） volatile 番外 如果不对 initFlag 添加 volatile 标识，线程A就永远无法读取到initFlag的最新值吗？\n不一定， 在判断 initFlag 的值时，CPU 先从缓存中取值，只要缓存失效，就会重新在从内存中加载。那么什么时候缓存会失效呢？ 对于CPU缓存来说，分为 L1 L2 L3 三级缓存，也就是离CPU最近的那些寄存器，他们的速度依次递减，容量依次递增。而每次CPU缓存的最小单位不是某个变量所占的空间大小，而是固定的字节 ，这样就能减少CPU和内存交互的次数，更好的利用空间局部原理和时间局部性原理。具体细节可以搜索 CPU缓存相关信息\n因为CPU一次会让一批缓存失效，有可能 initFlag 的缓存会随着其他值失效而重新从内存加载最新值。如下例子:\npublic class CodeVisibility { // initFlag 不再用 volatile 修饰 private static boolean initFlag = false; // 这里 counter 类型从 int 修改成 Integer private static Integer counter = 0; public static void refresh() { System.out.println(\u0026#34;refresh data.......\u0026#34;); initFlag = true; System.out.println(\u0026#34;refresh data success.......\u0026#34;); } public static void main(String[] args) { Thread threadA = new Thread(() -\u0026gt; { while (!initFlag) { counter++; } // 线程仍然可以很快的结束，因为 counter 会导致 cpu 缓存失效，重新从主内存加载最新数据 System.out.println(\u0026#34;线程：\u0026#34; + Thread.currentThread().getName() + \u0026#34;当前线程嗅探到initFlag的状态的改变, counter: \u0026#34; + counter); }, \u0026#34;threadA\u0026#34;); threadA.start(); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } Thread threadB = new Thread(() -\u0026gt; { refresh(); }, \u0026#34;threadB\u0026#34;); threadB.start(); } } 结果:\n那么问题又来了，为什么用 int 不会 导致 cpu 缓存失效呢？\n个人推测可能使因为 int 比 Integer 所占用的内存更小，CPU缓存放得下，一直没有触发缓存失效。\n有序性问题 先看一个例子:\npublic class VolatileReOrderSample { //定义四个静态变量 private static int x=0,y=0; private static int a=0,b=0; public static void main(String[] args) throws InterruptedException { int i=0; while (true){ i++; x=0;y=0;a=0;b=0; //开两个线程，第一个线程执行a=1;x=b;第二个线程执行b=1;y=a Thread thread1=new Thread(new Runnable() { @Override public void run() { //线程1会比线程2先执行，因此用nanoTime让线程1等待线程2 0.01毫秒 shortWait(10000); a=1; x=b; } }); Thread thread2=new Thread(new Runnable() { @Override public void run() { b=1; y=a; } }); thread1.start(); thread2.start(); thread1.join(); thread2.join(); //等两个线程都执行完毕后拼接结果 String result=\u0026#34;第\u0026#34;+i+\u0026#34;次执行x=\u0026#34;+x+\u0026#34;y=\u0026#34;+y; //如果x=0且y=0，则跳出循环 if (x==0\u0026amp;\u0026amp;y==0){ System.out.println(result); break; }else{ System.out.println(result); } } } //等待interval纳秒 private static void shortWait(long interval) { long start=System.nanoTime(); long end; do { end=System.nanoTime(); }while (start+interval\u0026gt;=end); } } 复制代码 按照正常思维，永远不会发生 x=0 y=0的场景，但事实并非如此:\n下面是线程A B 可能的正常执行情况\n线程A执行完 线程B执行\n线程B执行完 线程A执行：\n线程A B 交叉执行\n发生指令重排的情况\n指令重排 处理器为了程序的性能可以对程序的执行顺序进行重排，但是，必须满足重排后的执行结果在单线程下结果不能发生改变 这就是 as-if-serial 语义\n为了遵守 as-if-serial 语义,编译器和处理器不会对存在数据依赖的操作进行重排，因为会改变执行结果，如果两个操作不存在依赖关系，就有可能会被重排，就入上面的代码，在线程A中a=1;x=b这两各操作没有依赖关系，就有可能会重新排序成x=b;a=1 , 线程B同理。\n这个执行重排的操作在单线程下没有关系，因为没有影响到最终的执行的结果，但是如果是多线程的场景，就像上面的那个例子，就会发生错误\n如何禁止指令重排 volatile volatile 另一个作用是禁止指令重排，避免多线程下出现乱序执行的情况\n重排规则表:\n从上面的规则可以看出：\n当第二个操作是 volatile 写时，不管第一个操作是什么，都不能发生重排， 当第一个操作是 volatile 读时，不管第二个操作是什么，都不能发生重排 当第一个操作时 volatile 写，第二个操作是 volatile 读时，不能发生重排 加锁保证有序性 另外还可以使用 synchronize 和 lock 来保证有序性，因为加锁后，每时每刻只有一个线程执行代码，指令重排对单线程没有影响\n禁止指令重排的经典应用 看下懒汉模式的单例的问题:\n// 懒汉模式 + synchronized 同步锁 + double-check public final class Singleton { private static Singleton instance= null;// 不实例化 private Singleton(){}// 构造函数 public static Singleton getInstance(){// 加同步锁，通过该函数向整个系统提供实例 if(null == instance){// 第一次判断，当 instance 为 null 时，则实例化对象，否则直接返回对象 synchronized (Singleton.class){// 同步锁 if(null == instance){// 第二次判断 instance = new Singleton();// 实例化对象 } } } return instance;// 返回已存在的对象 } } 为了在多线程并发场景下单例仍然有效，加了锁以及双重检测，但是就万无一失了吗？\n在第一个判断 if(null == instance) 中，会出先变量instance有值，但是内存区域是空的（没有初始化 ），从而导致程序出现问题。造成这个问题的原因在于 instance = new Singleton()，事实上初始化对象操作不是原子性的，它包含下面两个动作:\n给对象分配内存空间， 内存空间的初始化 将内存地址赋值给变量 其中2，3没有依赖关系，经过 编译器或者cpu指令重排后，可能会导致 2,3顺序发生变化：\n给对象分配内存空间， 将内存地址赋值给变量 //此时变量已经不等于 null, 但是变量指向的内存区域还没有初始化 内存空间的初始化 假设线程A按照第二种顺序执行，在执行完步骤3时，还没有执行步骤2，线程B执行到第一个if(null == instance)判断，就会直接返回 instance。 这样对于线程B来说 getInstance() 方法返回的是一个没有经过初始化的对象，导致程序出现问题\n解决问题的方法很简单： private volatile static Singleton instance= null; 使用 volatile 关键词禁止 instance 变量被执行指令重排优化即可\n原子性问题 指的使一个操作是不可中断的，即使在多线程环境下，一旦操作开始就不会被其他线程影响\njava 中可以通过 synchronize 和 lock 保证原子性，它们能保证任意时刻只有一个线程访问代码\n","date":"2022-09-15T08:16:56Z","permalink":"https://dccmmtop.github.io/posts/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7%E5%8E%9F%E5%AD%90%E6%80%A7%E6%9C%89%E5%BA%8F%E6%80%A7%E9%97%AE%E9%A2%98/","section":"posts","tags":["java","hidden"],"title":"并发编程中的可见性,原子性，有序性问题"},{"categories":null,"contents":"jps 查看启动的 java 进程\njmap 实例个数与内存占用 看内存信息，实例个数以及占用内存大小\n[C is a char[]，[S is a short[]，[I is a int[]，[B is a byte[]，[[I is a int[][]\n堆信息 jump -heap 17680\n导出堆内存占用信息 jmap -dump:format=b,file=./j.hprof 17680\n导出是二进制，可以使用 jdk 自带的 jvisualvm.exe 导入查看\n内存溢出时导出 通常设置内存溢出自动导出 dump 文件(内存很大的时候，可能会导不出来)\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=导出路径 Jstack 查找死锁 有如下示例会产生一个死锁:\npackage io.dc; public class DeadLockTest { private static Object lock1 = new Object(); private static Object lock2 = new Object(); public static void main(String[] args) { new Thread(() -\u0026gt; { synchronized (lock1) { try { System.out.println(\u0026#34;thread1 begin\u0026#34;); Thread.sleep(5000); } catch (InterruptedException e) { } synchronized (lock2) { System.out.println(\u0026#34;thread1 end\u0026#34;); } } }).start(); new Thread(() -\u0026gt; { synchronized (lock2) { try { System.out.println(\u0026#34;thread2 begin\u0026#34;); Thread.sleep(5000); } catch (InterruptedException e) { } synchronized (lock1) { System.out.println(\u0026#34;thread2 end\u0026#34;); } } }).start(); System.out.println(\u0026#34;main thread end\u0026#34;); } } 可以使用 jstack PID 查看：\n直接可以定位到代码的大致位置\n找出占用 CPU 最高的 java 线程信息 有如下程序会导致 cpu 飙升：\npublic class Math { public static final int initData = 666; public int compute() { int a = 1; int b = 2; int c = (a + b) * 10; return c; } public static void main(String[] args) { Math math = new Math(); while (true) { // 导致CPU飙升 math.compute(); } } } 在 Linux 使用 top 命令，发现 java 进程 CPU 占用高:\n使用 top -p 105654 ，然后按 H , 查看这个进程的详细信息:\n可以看到线程 105655 占用资源比较高，将 105655 转换成 16 进制： 19cb7\n执行 jstack 105654 | grep -A 10 \u0026quot;19cb7\u0026quot; 查看此线程的相关信息:\n可以找到问题代码的大致位置\njinfo 查看正在运行的 java 程序的扩展参数\n查看 jvm 使用的参数 jinfo -flags 105654 查看 java 系统参数 jinfo -sysprops 105654 jstat 可以查看堆内存各部分使用量，以及加载类的数量\n使用方法:\njstat [-命令选项] [vmid] [间隔时间(毫秒)] [查询次数]\n垃圾回收统计 jstat -gc PID S0C：第一个幸存区的大小，单位 KB S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 OC：老年代大小 OU：老年代使用大小 MC：方法区大小(元空间) MU：方法区使用大小 CCSC: 压缩类空间大小 CCSU: 压缩类空间使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间，单位 s FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间，单位 s GCT：垃圾回收消耗总时间，单位 s 堆内存统计 jstat -gccapacity PID NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0C：第一个幸存区大小 S1C：第二个幸存区的大小 EC：伊甸园区的大小 OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC: 当前老年代大小 MCMN: 最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代 gc 次数 FGC：老年代 GC 次数 新生代垃圾回收器情况 jstat -gcnew PID S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 TT: 对象在新生代存活的次数 MTT: 对象在新生代存活的最大次数 DSS: 期望的幸存区大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 新生代内存统计 jstat -gcnewcapacity PID NGCMN：新生代最小容量 NGCMX：新生代最大容量 NGC：当前新生代容量 S0CMX：最大幸存 1 区大小 S0C：当前幸存 1 区大小 S1CMX：最大幸存 2 区大小 S1C：当前幸存 2 区大小 ECMX：最大伊甸园区大小 EC：当前伊甸园区大小 YGC：年轻代垃圾回收次数 FGC：老年代回收次数 老年代垃圾统计 jstat -gcold PID MC：方法区大小 MU：方法区使用大小 CCSC: 压缩类空间大小 CCSU: 压缩类空间使用大小 OC：老年代大小 OU：老年代使用大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 老年代堆内存统计 jstat -gcoldcapacity PID OGCMN：老年代最小容量 OGCMX：老年代最大容量 OGC：当前老年代大小 OC：老年代大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 元空间统计 jstat -gcmetacapacity PID MCMN: 最小元数据容量 MCMX：最大元数据容量 MC：当前元数据空间大小 CCSMN：最小压缩类空间大小 CCSMX：最大压缩类空间大小 CCSC：当前压缩类空间大小 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 jstat -gcutil PID JVM 运行情况预估 知道了如何统计 jvm 运行的信息，就可以根据现有的信息预估以后程序占用资源的走向，从而调整合理的 jvm 参数，比如： 堆内存大小，年轻代，老年代大小， Eden 和 Survivor 的比例，大对象的阀值，进入老年代年龄的阀值等\n年轻代对象增长速率 可以执行 jstat -gc PID 1000 20 观察 EU 区估算每秒新增多少对象，一般系统又高峰期和非高峰期，需要在不同时间段分别统计\nYoung GC 触发频率和每次耗时 知道年轻代对象的增长速率，可以预估 Young GC 多久触发一次，Young GC 的平均耗时可以 YGCT / YGC 算出，这两个结果可以知道系统大概多久系统会因为 Young GC 卡顿多久\nYoung GC 后，存活的对象和进入老年代对象的数量 每次 GC 后，Eden 区会大幅度减少，survivor 和老年代都会有增长，这些增长的对象就是 Young GC 后存活的对象，同时还可以看出每次进入老年代的对象，这就是老年代对象的增长速率\nFull GC 的触发频率和平均耗时 知道了老年代的增长速率，就可以估算出 Full GC 的触发频率了，每次耗时可以通过 FGCT / FGC 算出\n优化思路 尽量让每次 Young GC 之后的存活对象小于 Survivor 区域的 50%，尽量别让对象进入老年代，尽量减少 Full GC 的频率，避免频繁 Full GC 堆 jvm 性能的影响\n一种解决频繁 Full GC 的思路 一般是频繁创建大对象，导致老年代的占用极速增加，这部分代码可能占用 CPU 比较高。\n可以通过 jmap 找到实例数量靠前的对象，在代码中搜索新建这个对象的位置 通过上面说的 top + jstack 的方式找到占用 CPU 资源比较多的线程，再定位具体位置 内存泄漏的案例 一般电商架构可能会使用多级缓存架构，就是redis加上JVM级缓存，大多数同学可能为了图方便对于JVM级缓存就 简单使用一个hashmap，于是不断往里面放缓存数据，但是很少考虑这个map的容量问题，结果这个缓存map越来越大，一直占用着老 年代的很多空间，时间长了就会导致full gc非常频繁，这就是一种内存泄漏，对于一些老旧数据没有及时清理导致一直占用着宝贵的内存 资源，时间长了除了导致full gc，还有可能导致OOM。 这种情况完全可以考虑采用一些成熟的JVM级缓存框架来解决，比如ehcache等自带一些LRU数据淘汰算法的框架来作为JVM级的缓存\n高级工具 Arthas Arthas 是 Alibaba 在 2018 年 9 月开源的 Java 诊断工具。支持 JDK6+， 采用命令行交互模式，可以方便的定位和诊断 线上程序运行问题。Arthas 官方文档十分详细 https://arthas.aliyun.com/doc/manual-install.html?userCode=okjhlpr5\n","date":"2022-09-12T15:50:03Z","permalink":"https://dccmmtop.github.io/posts/jvm%E8%B0%83%E4%BC%98%E5%B7%A5%E5%85%B7/","section":"posts","tags":["java"],"title":"JVM调优工具"},{"categories":null,"contents":"G1 (Garbage-First) 是一款面向服务器的垃圾收集器，主要针对多颗处理器及大容量内存的机器，具备极短的 GC 停顿时间和高吞吐量的特征.\nG1 堆内存划分 G1 不像 CMS 那样，老年代和年轻代不再有明显的区分。而是将内存分为很多和区域（Region），JVM 最多可以有 2048 个区域，一般一个区域的大小等于堆大小 / 2048 ，比如堆的大小是 4096，那么一块区域的大小有 2M, 也可以通过参数指定:-XX:G1HeapRegionSize, 但是推荐默认模式\nG1 中的年轻代和老年代 G1 保留了年轻代和老年代的概念，但是不再有物理上的隔阂了，他们都是 Region 组成，可以不连续。\n年轻代占比 默认年轻代占堆的 5%，可以通过参数 -XX:G1NewSizePercent 设置年轻代的初始占比，在系统运行中会不断的增加年轻代的 Region，但是最多年轻代的占比不会超过 60%，这个最大值可以通过参数设置： -XX:G1MaxNewSizePercent\n年轻代中的 Eden 和 Survivor 的比例也是 8 :1: 1 , 一个 Region 之前是年轻代，经过垃圾回收后可能变成老年代，也就是说 Region 区域功能是动态变化的\n大对象区域 G1 收集器中对象什么时候会转移到老年代，和之前 CMS, Parallel 一样，唯一不同的是，G1 设置了专门存放大对象的区域: Humongous, 而不是让大对象直接进入老年代。\n在 G1 中，当对象超过一个 Region 的 50% ，就会判定为大对象，如果一个对象太大，会连续使用多个区域存放。\nHumongous 专门存放短期的大对象，不用直接进入老年代，节省了老年代的空间，降低了 GC 次数。\nFull GC 的时候，老年代，年轻代， Humongous 会一并清理\nG1 运行过程 初始标记 (Initial Mark， STW) 暂停用户所有线程，并标记 GC Root 所有直接引用的对象，速度很快\n并发标记 (Concurrent Mark) 同 CMS 的并发标记\n最终标记 (Remark, STW) 同 CMS 的重新标记\n筛选回收 (cleanup, STW) 不会回收所有的被标记的 Region, 先对各个 Region 回收的价值和成本进行排序，根据用户期望的 GC 停顿时间来制定回收计划。可以通过参数 -XX:MaxGCPauseMillis 可以指定 GC 停顿时间\n因为 G1 通常运行在内存比较大的机器上，如果对所有被标记的空间都进行回收，势必会花费很多时间，所以仅仅回收部分区域，满足用户对 GC 停顿时间的要求。\n回收算法主要使用复制算法，将一个 Region 中存活的对象复制到另一个 Region 中，不会像 CMS 那样回收完还有很多内存碎片进行整理，G1 采用复制算法，几乎不会有太多的碎片。\n单线程回收\nCMS 在回收阶段是可以和用户线程并发执行，但是 G1 内部实现太复杂，暂时没有实现并发回收，到了 Shenandoah 实现了并发收集，可以看做是 G1 的升级版本\n选择哪些区域回收？\nG1 内部维护了一个优先级列表，每次根据允许的收集时间，优先选择回收价值最大的 Region, 这也是它名字的由来 Garbage First , 比如一个 Region 回收需要花费 200ms, 能释放 10M 的空间，回收另一个 Region 需要花费 50ms, 能释放 20M 空间，G1 会优先回收后面的 Region\n示意图\nG1 重要的特征 并行与并发 G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 StopThe-World 停顿时间。部分其他收集器原本需要停顿 Java 线程来执行 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。\n分代收集 虽然 G1 不需要与其它收集器配合使用就能独自管理整个堆，但还是保留了分代的概念。\n空间整合 与 CMS 的 “标记-清理” 不同，G1 从整体上看，采用了标记-整理算法，局部使用了标记-复制算法\n可预测的停顿 这是 G1 与 CMS 另外一个大优势， CMS 和 G1 都特别重视降低停顿时间，但是 G1 除了追求降低停顿时间外，还可以让用户指定停顿时间，在指定时间内完成垃圾收集： -XX:MaxGCPauseMillis\n这使得 G1 在不同的场景中可以获得最佳的停顿时间和吞吐量。这个最佳值必须是合理的，不能无限低，否则每次垃圾回收的空间很小，回收的速度追赶不上垃圾生成的速度，最终也会频繁 FullGC 反而降低性能，通常把 GC 时间设置为 200~300 ms 是比较合理的\nG1 垃圾收集分类 YoungGC 不是说现有的 Eden 区域放满之后会马上触发 YoungGC , G1 会计算回收现在的 Eden 区域需要花费多少时间，如果这个回收时间远远小于 MaxGCPauseMillis 值，那么就会增加年轻代的 Region, 继续存放对象，直到下一次预估的回收时间接近 MaxGCPauseMillis , 才会触发 Young GC\nMixedGC 不是 Full GC\n不是 Full GC\n不是 Full GC\n老年代占有率达到设定值的时候会触发，回收所有的 Young 和部分 Old（根据指定的 GC 停顿时间和回收优先级进行选择），一般会先触发 Mixed GC, 在 GC 过程中，把各个 Region 存活的对象复制到别的 Region 中，如果没有足够的 Region 存放对象，就会触发 Full GC\nFull GC 这个过程会暂停用户程序，使用单线程进行标记，清理，压缩整理，以便空闲出一批 Region 供下一次 MixedGC 使用，这个过程非常耗时，在 Shenandoah 时优化为多线程收集了\nG1 垃圾收集器参数设置 -XX:+UseG1GC 使用 G1 -XX:ParallelGCthreads GC 工作的线程数量 -XX:G1HeapRegionSize 指定分区大小 (1MB ~ 32 MB), 必须是 2 的 N 次幂，默认将堆划分为 2048 个区域 -XX:MaxGCPauseMillis 指定 GC 暂停时间，默认时 200ms -XX:G1NewSizePercent 新生代初始空间，默认 5% -XX:G1MaxNewSizePercent 新生代最大空间， -XX:TargetSurvivorRatio Survivor 区填充容量，默认时 50%， Survivor 区域中的一批对象（年龄 1+年龄 2+年龄 n 的多个年龄对象) 总和超过了 Survivor 区域的 50%，此时就会把年龄 n (含) 以上的对象都放入老年代 -XX:MaxTenuringThreshold 最大年龄阀值，默认 15 -XX:InitiatingHeapOccupancyPercent 老年代占用空间达到阀值时 (默认 45%)，则执行老年代和新生代的混合收集（MixedGC）, 比如我们之前说的堆默认有 2048 个 region，如果有接近 1000 个 region 都是老年代的 region，则可能就要触发 MixedGC 了 -XX:G1MixedGCLiveThreasholdPercent region 中存活对象空间占比低于这个值（默认 85%），才会回收该 Region, 如果超过这个比例，说明这个 Region 中存活的对象过多，回收意义不大 -XX:G1MixedGCCountTarget 指定在一次回收过程中，做几次筛选回收（默认 8 次），在最后一个筛选回收阶段，可以暂停一会再回收，不至于单次停顿时间过长 -XX:G1HeapWastePercent 默认 5%， GC 过程中空出来的 Region 是否充足阀值，在混合回收的时候，是基于复制算法的，需要把要回收的 Region 中的存活对象复制到其他的 Region 中，然后把这个 Region 清空，这样在 GC 过程中就不断有新的 Region 空闲出来，一旦空闲的数量占堆总内存的 5%时，就会终止 MixedGC G1 收集器优化建议 假设参数 -XX:MaxGCPauseMills 设置的值很大，导致系统运行很久，年轻代可能都占用了堆内存的 60%了，此时才触发年轻代 gc。那么存活下来的对象可能就会很多，此时就会导致 Survivor 区域放不下那么多的对象，就会进入老年代中。\n或者是你年轻代 gc 过后，存活下来的对象过多，导致进入 Survivor 区域后触发了动态年龄判定规则，达到了 Survivor 区域的 50%，也会快速导致一些对象进入老年代中。\n所以这里核心还是在于调节 -XX:MaxGCPauseMills 这个参数的值，在保证他的年轻代 gc 别太频繁的同时，还得考虑每次 gc 过后的存活对象有多少, 避免存活对象太多快速进入老年代，频繁触发 mixed gc\n适合使用 G1 的场景 50% 以上的内存被存活对象使用 对象分配和晋升变化非常大 垃圾回收时间特别长，超过 1 秒 8G 以上的内存，经验值 期望停顿时间在 500ms 以内 每秒几十万并发的系统如何优化 JVM Kafka 类似的支撑高并发消息系统大家肯定不陌生，对于 kafka 来说，每秒处理几万甚至几十万消息时很正常的，一般来说部署 kafka 需要用大内存机器 (比如 64G)，也就是说可以给年轻代分配个三四十 G 的内存用来支撑高并发处理，这里就涉及到一个问题了，我们以前常说的对于 eden 区的 younggc 是很快的，这种情况下它的执行还会很快吗？很显然，不可能，因为内存太大，处理还是要花不少时间的，假设三四十 G 内存回收可能最快也要几秒钟，按 kafka 这个并发量放满三四十 G 的 eden 区可能也就一两分钟吧，那么意味着整个系统每运行一两分钟就会因为 younggc 卡顿几秒钟没法处理新消息，显然是不行的。那么对于这种情况如何优化了，我们可以使用 G1 收集器，设置-XX:MaxGCPauseMills 为 50ms，假设 50ms 能够回收三到四个 G 内存，然后 50ms 的卡顿其实完全能够接受，用户几乎无感知，那么整个系统就可以在卡顿几乎无感知的情况下一边处理业务一边收集垃圾。\nG1 天生就适合这种大内存机器的 JVM 运行，可以比较完美的解决大内存垃圾回收时间过长的问题\n安全点与安全区域 安全点就是指代码中一些特定的位置, 当线程运行到这些位置时它的状态是确定的, 这样 JVM 就可以安全的进行一些操作, 比如 GC 等，所以 GC 不是想什么时候做就立即触发的，是需要等待所有线程运行到安全点后才能触发。\n这些特定的安全点位置主要有以下几种:\n方法返回之前 调用某个方法之后 抛出异常的位置 循环的末尾 大体实现思想是当垃圾收集需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志位，各个线程执行过程时会不停地主动去轮询这个标志，一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的。\n安全区域又是什么？ Safe Point 是对正在执行的线程设定的。\n如果一个线程处于 Sleep 或中断状态，它就不能响应 JVM 的中断请求，再运行到 Safe Point 上。因此 JVM 引入了 Safe Region。\nSafe Region 是指在一段代码片段中，引用关系不会发生变化。在这个区域内的任意地方开始 GC 都是安全的\n","date":"2022-09-10T08:58:04Z","permalink":"https://dccmmtop.github.io/posts/g1%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","section":"posts","tags":["java"],"title":"G1垃圾收集器"},{"categories":null,"contents":"垃圾收集算法 整体可以分为以下几种算法：\n标记复制算法 标记整理算法 标记清除算法 分代收集理论 目前的虚拟机的垃圾回收器都是采用分代收集算法，一般根据对象存活周期的不同，将内存分为几块，一般将java堆分为新生代和老年代，然后根据各代的特点选择不同的垃圾回收器算法\n比如在新生代，每次收集都会有大量的对象死去（近 99% ），所以选择复制算法，只需复制少量对象就可以完成垃圾回收，成本较低。 而老年代对象存活几率是比较高的，也没有额外的空间给担保，所以要选择标记清除或标记整理算法，这两种算法比复制算法慢10倍以上\n标记复制算法 将内存分为大小相同的两个块，只把对象分配到其中一个块上，当这个块的空间不足时，就将还存活的对象复制到另外一块上去，然后把使用过那块空间清理掉。每次都是对内存空间的一半进行回收\n标记清除算法 分为两步\n标记存活的对象 清理没有被标记的对象 一般选择这种，也可以反过来，标记需要回收的对象，然后清理所有被标记的对象\n缺点:\n效率低，（要标记的对象可能很多） 内存碎片多（会产生大量的不连续下空间，无法利用）\n如图:\n标记整理算法 根据老年代的特点专门制定的清除方案，与标记清除一样，需要先标记存活对象，不同的是，后续没有直接回收垃圾，而是将存活的对象统一向一端移动，然后清理掉边界以外的内存\n垃圾回收器 上述几个算法是理论，实现垃圾回收算法的叫做垃圾收集器\n垃圾回收器没有最好的，只有最合适的\n概览 Serial 收集器 -XX:+UseSerialGC 开启年轻代使用\n-XX:+UseSerialOldGC 开启老年代使用\nSerial（串行）收集器是最基本、历史最悠久的垃圾收集器了。看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ \u0026ldquo;Stop The World\u0026rdquo; ），直到它收集结束。\n新生代采用复制算法，老年代采用标记-整理算法。\n示意图：\nSerial Old 收集器是Serial收集器的老年代版本，它同样是一个单线程收集器。\n它主要有两大用途：\n在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用 作为CMS收集器的后备方案。 实现简单，单线程的收集效率高\nParallel Scavenge 收集器 JK8 默认的垃圾回收器\n-XX:+UseParallelGC(年轻代)， 采用复制算法\n-XX:+UseParallelOldGC(老年代) 标记整理算法\nParallel 其实就是 Serial 的多线程版本，默认的收集线程数与CPU核心数相同，也可以使用参数 -XX:ParallelGCThreads 指定收集线程数，但是一般不推荐修改。\nParallel Scavenge 注重的是收集效率，后面要介绍的 CMS，G1 收集器 更注重用户体验，缩短 STW 的时长。但是他不能与 CMS 收集器配合使用\n示意图:\nParNew 收集器 ParNew 收集器和 Parallel 很类似，主要区别在于，他可以与 CMS 配合使用，新生代采用复制算法，老年代采用标记-整理算法。\n只有 Serial ,ParNew 可以与 CMS 配合使用 ， ParNew 是很多虚拟机的首要选择。\nCMS 收集器 -XX:+UseConcMarkSweepGC(old)\nCMS(ConcurrentMarkSweep) 是一种追求短 STW 为目标的收集器，注重用户体验，他是 HotSpot 虚拟机第一款真正意义上的收集器，第一次实现了让用户线程和收集线程同时工作\n从名字就可以看出来，Mark: 标记，Sweep: 清除， 它采用标记清理算法， 他的工作流程更复杂，主要分下面5个步骤:\n初始标记: 暂停用户线程，标记GC Root 直接引用 的对象，速度很快 并发标记： 从第一步标记的对象开始遍历所有引用对象，过程耗时很长，但是不需要暂停用户线程，因为有用户线程的干扰，可能会修改已经标记的对象，产生问题 重新标记：为了修正并发标记时产生的问题，产生问题的对象比例小，所以速度很快。此过程会STW 并发清理： 清理线程与用户线程同时运行，清理掉没有标记的对象，如果此时有新增的对象虽然没有被标记，统一会当作存活对象，不会误清理 并发重置： 重置本次GC被标记的数据 优点:\n并发收集 短停顿 缺点:\n和用户线程抢占CPU资源 无法处理浮动垃圾 由于时标记清除算法，会产生内存碎片，可以通过参数: -XX:+UseCMSCompactAtFullCollection 控制是否做空间整理 由于允许用户线程同时运行，会造成在GC过程中不断的有对象进入内存，当前GC没有完成，空间没有释放，可能导致没有足够的空间放新进来的对象，此时会导致 STW,然后退化成 serial 收集器进行垃圾回收 CMS的相关核心参数 -XX:+UseConcMarkSweepGC：启用cms -XX:ConcGCThreads：并发的GC线程数 -XX:+UseCMSCompactAtFullCollection：FullGC之后做压缩整理（减少碎片） -XX:CMSFullGCsBeforeCompaction：多少次FullGC之后压缩一次，默认是0，代表每次FullGC后都会压缩一次 -XX:CMSInitiatingOccupancyFraction: 当老年代使用达到该比例时会触发FullGC（默认是92，这是百分比） -XX:+UseCMSInitiatingOccupancyOnly：只使用设定的回收阈值(-XX:CMSInitiatingOccupancyFraction设定的值)，如果不指定，JVM仅在第一次使用设定值，后续则会自动调整 -XX:+CMSScavengeBeforeRemark：在CMS GC前启动一次minor gc，降低CMS GC标记阶段(也会对年轻代一起做标记，如果在minor gc就干掉了很多对垃圾对象，标记阶段就会减少一些标记时间)时的开销，一般CMS的GC耗时 80%都在标记阶段 -XX:+CMSParallellnitialMarkEnabled：表示在初始标记的时候多线程执行，缩短STW -XX:+CMSParallelRemarkEnabled：在重新标记的时候多线程执行，缩短STW; 三色标记 初始标记 并发标记 并发标记产生的问题的原因 出现问题的后果 问题的解决办法 出现漏标的两个必要条件:\n有对象的引用关系被删除 删除的对象又被重新引用 见上图\n破环任意步骤，即可避免漏标的现象。有以下两种解决方案:\n增量更新\n因为重新标记时，要再次从黑色对象出发，深度扫描所有引用，效率没有原始快照高 优点: 不会产生浮动垃圾 缺点: 效率低 原始快照\n再引用被删除时，记录关系，重新扫描时，将被引用的对象标记为非垃圾，如果后续没有再次引该对象，那么该对象就会变成浮动垃圾 缺点: 可能会产生浮动垃圾 优点: 不要重新扫描，效率高 那除此之外，还没有没有其他可能重新引用对象呢？答案只有一个， new 一个新对象。在GC期间，有新的对象进来时，统一被当成黑色处理，不会被回收\n那么上图中的 brand 会不会被重新引用呢？答案是不会！因为已经找不到引用 brand 对象的变量了，在引用程序中无法再次引用这个垃圾对象\n上图中说到，在对象被引用上，删除对象引用是，记录这个引用关系，从而方便后续的重新编辑，那么这个记录的动作是如何发生的？ 这时就要引入写屏障的技术了。\n以上无论是对引用关系记录的插入还是删除， 虚拟机的记录操作都是通过写屏障实现的。\n写屏障 类似于切面，在赋值动作的前后做一些事情。\nvoid oop_field_store(oop* field, oop new_value) { pre_write_barrier(field); // 写屏障-写前操作 *field = new_value; post_write_barrier(field, value); // 写屏障-写后操作 } 写屏障实现SATB 当对象B的成员变量的引用发生变化时，比如引用消失（a.b.d = null），我们可以利用写屏障，将B原来成员变量的引用对象D记录下来：\nvoid pre_write_barrier(oop* field) { oop old_value = *field; // 获取旧值 remark_set.add(old_value); // 记录原来的引用对象 } 写屏障实现增量更新 当对象A的成员变量的引用发生变化时，比如新增引用（a.d = d），我们可以利用写屏障，将A新的成员变量引用对象D记录下来：\nvoid post_write_barrier(oop* field, oop new_value) { remark_set.add(new_value); // 记录新引用的对象 } 读屏障 与写屏障类似\n重新标记 并发清理 并发重置 以上就是三色标记的大体流程了\n不同垃圾回收器对三色标记的选择 CMS 写屏障 + 增量更新 G1 ,Shenandoah： 写屏障 + STAB ZGC: 读屏障 为什么 G1 采用 STAB， CMS 采用 增量更新 个人理解： G1 一般用于大内存的机器，内存 8G 至百G级别, 存放的对象更多，自然要扫描的对象更多，如果采用效率低的增量更新方式，效率下降的更严重，所以采用效率较高的 STAB,虽然会产生一些浮动垃圾，但是对于大内存机器来说，影响不大。\nCMS 适用于内存较小的机器,多用于 4G 到 8G 的机器上，无论采用 STAB 还是 CMS, 效率上无法拉开明显差距，而增量更新不会产生浮动垃圾，对内存更小的机器来说，内存的使用更敏感，自然采用增量更新的方式\n记忆集与卡表 这两个结构是为了垃圾器在跨代访问时提高速度的。\n在新生代做GCRoots可达性扫描过程中可能会碰到跨代引用的对象，这种如果又去对老年代再去扫描，甚至老年代内存在很多对象的间接引用，这种效率就太低了，大大的延长了 Minor GC 的时间。\n为此，在新生代可以引入记录集（Remember Set）的数据结构（记录从非收集区到收集区的指针集合），避免把整个老年代加入GCRoots扫描范围。事实上并不只是新生代、 老年代之间也有跨代引用的问题， 所有涉及部分区域收集（Partial GC） 行为的垃圾收集器， 典型的如G1、 ZGC和Shenandoah收集器， 都会面临相同的问题。\n垃圾收集场景中，收集器只需通过记忆集判断出某一块非收集区域是否存在指向收集区域的指针即可，无需了解跨代引用指针的全部细节。\nhotspot使用一种叫做“卡表”(Cardtable)的方式实现记忆集，也是目前最常用的一种方式。关于卡表与记忆集的关系， 可以类比为Java语言中HashMap与Map的关系。\n卡表是使用一个字节数组实现：CARD_TABLE[ ]，每个元素对应着其标识的内存区域一块特定大小的内存块，称为“卡页”。\nhotSpot使用的卡页是2^9大小，即512字节\n一个卡页中可包含多个对象，只要有一个对象的字段存在跨代指针，其对应的卡表的元素标识就变成1，表示该元素变脏，否则为0.\nGC时，只要筛选本收集区的卡表中变脏的元素加入GCRoots里。\n卡表的维护\n卡表变脏上面已经说了，但是需要知道如何让卡表变脏，即发生引用字段赋值时，如何更新卡表对应的标识为1。\nHotspot使用写屏障维护卡表状态。\n","date":"2022-09-02T15:37:30Z","permalink":"https://dccmmtop.github.io/posts/cms%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E4%B8%8E%E4%B8%89%E8%89%B2%E6%A0%87%E8%AE%B0%E7%AE%97%E6%B3%95/","section":"posts","tags":["java"],"title":"CMS垃圾回收器与三色标记算法"},{"categories":null,"contents":"健康信息 命令 curl-X GET\u0026#34;localhost:9200/_cluster/health\u0026#34; 结果 ","date":"2022-08-31T16:25:06Z","permalink":"https://dccmmtop.github.io/posts/%E6%9F%A5%E7%9C%8Bes%E9%9B%86%E7%BE%A4%E4%BF%A1%E6%81%AF/","section":"posts","tags":["ES"],"title":"查看ES集群信息"},{"categories":null,"contents":"对象分配过程简略流程图 对象栈上分配 我们都知道对象分配在堆上，当对象没有被引用时就会当成垃圾回收，如果对象数量比较多，会给GC带来较大的压力，影响性能，为了减少临时对象在堆内的分配次数，JVM 通过逃逸分析，确定该对象不会被外部访问。如果不会逃逸，可以将该对象在栈上分配。该对象所占用的空间就可以随着栈帧出栈而销毁，减轻了GC的压力\n逃逸分析 public User test1() { User user = new User(); user.setId(1); user.setName(\u0026#34;zhuge\u0026#34;); //TODO 保存到数据库 return user; } public void test2() { User user = new User(); user.setId(1); user.setName(\u0026#34;zhuge\u0026#34;); //TODO 保存到数据库 } test1 方法将 user 返回了，有可能被外部对象引用，其作用域范围不确定， test2 方法没有将 user 对象返回，其作用域仅仅在方法内部，没有逃出方法范围，可以把 user 进行栈内分配。\nJVM 可以通过参数 -XX:DoEscapeAnalysis 开启逃逸分析，JDK7 之后默认开启\n标量替换 将对象进行栈内分配时也不是将整个对象全部放到栈中，JVM 不会创建对象， 而是把对象拆开，将对象中的成员变量放到栈中，这样就不会因为没有一大块连续的空间导致对象内存不够分配\n开启标量替换参数: -XX:+EliminateAllocations，JDK7之后默认开启。 如下面的例子:\npublic class EscapeAnalysis { public Person p; /** * 发生逃逸，对象被返回到方法作用域以外，被方法外部，线程外部都可以访问 */ public void escape(){ p = new Person(26, \u0026#34;TomCoding escape\u0026#34;); } /** * 不会逃逸，对象在方法内部 */ public String noEscape(){ Person person = new Person(26, \u0026#34;TomCoding noEscape\u0026#34;); return person.name; } } static class Person { public int age; public String name; ... // 省略构造方法 } 比如上述noEscape()方法中person对象只会在方法内部，通过标量替换技术得到如下伪码：\n/** * 不会逃逸，对象在方法内部 */ public String noEscape(){ int age = 26; String name = \u0026#34;TomCoding noEscape\u0026#34;; return name; } 标量和聚合量 标量即不可被进一步分解的量，而JAVA的基本数据类型就是标量（如：int，long等基本数据类型以及reference类型等），标量的对立就是可以被进一步分解的量，而这种量称之为聚合量。而在JAVA中对象就是可以被进一步分解的聚合量\n栈上分配示例 /** * 栈上分配，标量替换 * 代码调用了1亿次alloc()，如果是分配到堆上，大概需要1GB以上堆空间，如果堆空间小于该值，必然会触发GC。 * * 使用如下参数不会发生GC * -Xmx15m -Xms15m -XX:+DoEscapeAnalysis -XX:+PrintGC -XX:+EliminateAllocations * 使用如下参数都会发生大量GC * -Xmx15m -Xms15m -XX:-DoEscapeAnalysis -XX:+PrintGC -XX:+EliminateAllocations * -Xmx15m -Xms15m -XX:+DoEscapeAnalysis -XX:+PrintGC -XX:-EliminateAllocations */ public class AllotOnStack { public static void main(String[] args) { long start = System.currentTimeMillis(); for (int i = 0; i \u0026lt; 100000000; i++) { alloc(); } long end = System.currentTimeMillis(); System.out.println(end - start); } private static void alloc() { User user = new User(); user.setId(1); user.setName(\u0026#34;user1\u0026#34;); } } 可以根据打印的GC日志明显看出开启了栈内分配时，GC 次数远远小于不开启站内分配\n在EDEN区分配 虽然jvm可以通过逃逸分析来将一部分对象进行栈上分配，但是在实际代码中，不逃逸的对象还是占少量的，大部分仍对象然分配在堆上的 EDEN 区\n当Eden区没有足够的空间时将触发一次 Minor GC\n为什么 Eden 与 Survivor 的比例是 8:1:1 大量对象被分配在 Eden 区，Eden 满了之后会触发Minor GC, 可能有99% 以上的对象被当作垃圾回收，剩余的存活对象被挪到为空的 survivor 区，下一次Eden满了之后，又会触发MinorGC ,把 Eden 和 Survivor 对象回收，把剩余的对象一次性挪到另一块为空的 Survivor 区。因为新生对象大部分寿命较短，所以 JVM 默认的比例 8:1:1 是非常合适的，让 Eden 足够大。 Survivor 够用即可。\nJVM默认有这个参数-XX:+UseAdaptiveSizePolicy(默认开启)，会导致这个8:1:1比例自动变化，如果不想这个比例有变化可以设置参数-XX:-UseAdaptiveSizePolicy\n提前进入老年代 在发生MinorGC 后，Eden区的对象在向 Survivor 区转移时，如果 Survivor 区放不下这个对象。那么这个大对象直接进入老年代\n相当于这个大对象跳过了 Survivor 区，直接进入空间更大的老年代区\n直接进入老年代的场景 在 Serial 和 ParNew 垃圾回收器下，大对象会直接进分配到老年代中，不经过 Eden 和 Survivor 区。 大对象就是需要连续大内存的对象,比如字符串，数组，这样做的好处是可以避免为大对象分配内存时的复制操作降低效率\n可以通过参数调节大对象的阀值: -XX:PretenureSizeThreshold\n例子: -XX:PretenureSizeThreshold=1000000 (单位是字节) -XX:+UseSerialGC\n长期存活的对象会进入老年代 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为1。对象在 Survivor 中每熬过一次 MinorGC，年龄就增加1岁，当它的年龄增加到一定程度 （默认为15岁，CMS收集器默认6岁，不同的垃圾收集器会略微有点不同），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。\n动态判断可能为长期对象 除了上述的对象年龄稳步增加到 15 后会移到老年代之外。还有一种动态计算年龄的方法:\n当前放对象的Survivor区域里(其中一块区域，放对象的那块s区)，一批对象的总大小大于这块Survivor区域内存大小的50%(-XX:TargetSurvivorRatio可以指定)，那么此时大于等于这批对象年龄最大值的对象，就可以直接进入老年代了，例如Survivor区域里现在有一批对象，年龄1+年龄2+年龄n的多个年龄对象总和超过了Survivor区域的50%，此时就会把年龄n(含)以上的对象都放入老年代。这个规则其实是希望那些可能是长期存活的对象，尽早进入老年代。对象动态年龄判断机制一般是在minor gc之后触发的\n垃圾回收器如何工作 引用计数法(差) 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。\n这种方法实现简单，效率高，当时目前主流的虚拟机并没有选择这种算法，主要他存在循环引用的问题:\n所谓对象之间的相互引用问题，除了对象objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数算法无法通知 GC 回收器回收他们\n可达性分析算法 将 GC Roots 对象作为起点，从这些节点开始向下搜索引用的对象，找到的对象都标记为非垃圾对象，其余未标记的对象都是垃圾对象\nGC Roots 对象引用的根节点: 线程栈的本地变量、静态变量、本地方法栈的变量等等\n常见的引用类型 java的引用类型一般分为四种：强引用、软引用、弱引用、虚引用\n强引用 普通的变量引用\npublic static User user = new User(); 软引用 将对象用SoftReference软引用类型的对象包裹，正常情况不会被回收，但是GC做完后发现释放不出空间存放\n新的对象，则会把这些软引用的对象回收掉。软引用可用来实现内存敏感的高速缓存。\npublic static SoftReference\u0026lt;User\u0026gt; user = new SoftReference\u0026lt;User\u0026gt;(new User()); 软引用在实际中有重要的应用，例如浏览器的后退按钮。按后退时，这个后退时显示的网页内容是重新进行请求还是从\n缓存中取出呢？这就要看具体的实现策略了。\n（1）如果一个网页在浏览结束时就进行内容的回收，则按后退查看前面浏览过的页面时，需要重新构建\n（2）如果将浏览过的网页存储到内存中会造成内存的大量浪费，甚至会造成内存溢出\n弱引用 将对象用WeakReference软引用类型的对象包裹，弱引用跟没引用差不多，GC会直接回收掉，很少用\npublic static WeakReference\u0026lt;User\u0026gt; user = new WeakReference\u0026lt;User\u0026gt;(new User()); 虚引用 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，几乎不用\n方法区的回收 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？\n类需要同时满足下面3个条件才能算是 “无用的类” ：\n该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 ","date":"2022-08-28T22:27:11Z","permalink":"https://dccmmtop.github.io/posts/jvm%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E5%9B%9E%E6%94%B6/","section":"posts","tags":["java"],"title":"jvm对象的内存分配与回收"},{"categories":null,"contents":"\n1. 类加载检查 虚拟机遇到一条 new 指令时，首先检查这个指令的参数能否在常量池中找到一个类符号引用，并且检查这个符号引用代表的类是否已经被加载，解析，初始化过。如果没有必须先执行类的加载初始化过程。\n2. 分配内存 在类加载检查通过后，接着就可以为新生对象划分内存了，对象占用内存的大小在类加载后就可以完全确定。为对象分配内存空间就相当于把一块确定大小的内存从java堆中划分出来\n如何划分内存呢 内存是如何划分的呢？高并发的场景下如何保证同一块空间不会分给两个对象的呢？\n指针碰撞 Bump the pointer (默认) 如果虚拟机堆中内存是绝对规整的，用过和没用过的各占一块完整的内存，中间放着一个指针作为分界点的指示器，在进行内存分配时，只需把指针向空闲区域移动一段距离，以放下新对象。\n空闲列表 Free List 如果虚拟机堆中的内存不是规整的，用过的和没有用过的互相交错，就没有办法使用指针碰撞的方法；了。虚拟机必须维护一个列表，来记录队中有哪些区域是空闲的。在分配内存的时候找到一块足够大的空间分配给对象，并更新列表记录\n解决并发 CAS （compare and swap） 虚拟机采用CAS配上失败重试的方式保证更新操作的原子性来对分配内存空间的动作进行同步处理。\nCAS 通常指的是这样一种原子操作：针对一个变量，首先比较它的内存值与某个期望值是否相同，如果相同，就给它赋一个新值。CAS 指令作为一种硬件原语，有着天然的原子性，这也正是 CAS 的价值所在。\n本地线程分配缓冲（Thread Local Allocation Buffer TLAB） 每个线程预先在jvm堆中分配一块内存空间，线程声明周期内的对象分配都在这实现分配的空间中进行\n-XX: +UseTLAB 设置虚拟机使用TALAB -XX:-UseTLAB 不使用\n-XX:TLABSize 指定TALB 的大小\n3. 初始化 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值，（不包括对象头），这一步操作保证了对象的实例字段在java代码中可以不赋初始值就能直接使用，如果是TLAB 方式，这一步骤将提前到TLAB分配时进行\n4. 设置对象头 初始零值之后，要虚拟机要对对象进行一些必要的设置，比如，这个对象是哪个类的实例，如何才能找到类的一些元信息，对象的哈希码，对象的GC分代年龄信息，这些信息存放在对象头 Object-Header 中。\n在HotSpot虚拟机中，对象在内存中分布可以又3各部分祖成，对象头，实例数据，对齐填充。\n对象头 HotSpot 虚拟机的对象头包含两个部分\n4.1 自身的运行数据 哈希码 GC分代年龄 锁状态标识 线程持有锁 偏向锁ID 偏向时间戳等 实例数据 存放对象的实际数据\n对齐填充 不是必然存在，也没有特别的含义，只是起着占位符的作用，那么为什么会有这一部分的数据呢？\n这就需要了解一些计算机组成原理的知识了。\nCPU 位数 我们经常使用的CPU 有32 位和64 位\n32位架构的CPU数据总线宽度是32位，每次可以传输32位数据，可以计算4个字节。\n64位架构的CPU数据总线宽度是64位，每次可以传输64位数据，可以计算8个字节。\nCPU 寻址 CPU 在工作时，从内存中的某个地址取数据，然后进行计算。数据最小的保存单位时 位 (bit), 为了减少CPU与内存通信的次数，规定CPU一次从内存中取8位数据，也就是1字节。内存中每一位（bit） 都有一个唯一的地址。\n对于32位的CPU，在向内存取数据时，所能描述的最大地址是第 2^32 字节（CPU一次最少取1个字节，不是1位），2^32字节 约等于 4G， 内存再大，CPU 也无法访问到，所以32位的CPU 最大支持约4G的内存了。 同理，64 位的CPU 支持 2^64 字节内存，等于 17179869184 Tb\nJava对象最小占用空间 由于64 位的CPU一次能处理 64 位指令，也就是8个字节的数据，JVM 也遵循这个规则，让对象的内存占用是8字节的的整数倍，CPU 处理更高效。那么如何保证 java 对象占用的空间永远是8字节的整数倍呢？ 答案就是内存对齐，不足8字节的整数倍，末尾补充空白占用符。\nJava对象的指针压缩 上述的内存对齐，相当于把对象的占用的空间扩大了，与之对应的，JVM 还有内存压缩机制：\njdk1.6 update14开始，在64bit操作系统中，JVM支持指针压缩 启用指针压缩:\u0026ndash;XX:+UseCompressedOops(默认开启)，禁止指针压缩:\u0026ndash;XX:-UseCompressedOops 指针压缩的好处 在64位平台的HotSpot中使用32位指针，内存使用会多出1.5倍左右，使用较大指针在主内存和缓存之间移动数据，占用较大宽带，同时GC也会承受较大压力 为了减少64位平台下内存的消耗，启用指针压缩功能 在jvm中，32位地址最大支持4G内存(2的32次方)，可以通过对对象指针的压缩编码、解码方式进行优化，使得jvm只用32位地址就可以支持更大的内存配置(小于等于32G) 指针压缩的注意点 堆内存小于4G时，不需要启用指针压缩，jvm会直接去除高32位地址，即使用低虚拟地址空间 堆内存大于32G时，压缩指针会失效，会强制使用64位(即8字节)来对java对象寻址，这就会出现1的问题，所以堆内存不要大于32G为好 4.2 类型指针 对象指向他类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例\n5. 执行\u0026lt;init\u0026gt;方法 执行\u0026lt;init\u0026gt; 方法，就是按照程序员的意愿进行初始化，对应到语言层面上就是为属性赋值，和执行构造方法\n","date":"2022-08-28T09:34:49Z","permalink":"https://dccmmtop.github.io/posts/jvm%E4%B8%AD%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B/","section":"posts","tags":["java"],"title":"jvm中对象的创建过程"},{"categories":null,"contents":"\n","date":"2022-08-24T07:30:17Z","permalink":"https://dccmmtop.github.io/posts/jvm%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/","section":"posts","tags":["java"],"title":"jvm内存分配"},{"categories":null,"contents":"\n线程共享 每个线程开启的时候都会划分几块内存空间，线程栈，程序计数器，本地方法栈。这几个内存空间是依附于线程的，线程结束后，这些空间也会释放\n所有线程共享 除此之外还有堆，方法区，类加载子系统，字节码执行引擎。这些是所有线程共享的\n本地方法栈 本地方法\n简单地讲，一个Native Method就是一个Java调用非Java代码的接囗。该方法的实现由非Java语言实现，比如C。这个特征并非Java所特有，很多其它的编程语言都有这一机制，比如在C++中，你可以用extern \u0026ldquo;C\u0026rdquo; 告知C++编译器去调用一个C的函数。 在定义一个native method时，并不提供实现体（有些像定义一个Java interface），因为其实现体是由非java语言在外面实现的。\n例如java.lang.Object中的public final native Class\u0026lt;?\u0026gt; getClass()方法；又如java.lang.Thread中的private native void start0()方法\u0026hellip; \u0026hellip;\n本地接口的作用是融合不同的编程语言为Java所用，它的初衷是融合C/C++程序。\n本地方法栈\nJava虚拟机栈于管理Java方法的调用，而本地方法栈（Native Method Stack）用于管理本地方法的调用。\n本地方法栈，也是线程私有的。 程序计数器(PC) 在介绍jvm中的程序计数器(下面简称PC)之前，先看一下CPU 中的 PC:\nCPU 中的 PC\n我们用高级语言编写的复杂的程序最后都会转换成 CPU 可执行的指令，当程序运行的线程被中断的时候，需要用程序计数器记录当前执行到哪一条指令了，之后等待恢复的时候再从程序计数器中获取到被中断时执行的位置。也就是为 中断——恢复 提供一个记录 CPU中的PC是一个大小为一个字的存储设备（寄存器），在任何时候，PC中存储的都是内存地址（是不是有点像指针？），而CPU就根据PC中的内存地址，到相应的内存取出指令然后执行并且在更新PC的值。在计算机通电后这个过程会一直不断的反复进行。计算机的核心也在于此。\nJVM 中的 PC 在CPU中PC是一个物理设备，而java中PC则是一个一块比较小的内存空间，它是当前线程字节码执行的行号指示器。在java的概念模型中，字节码解释器就是通过改变这个计数器中的值来选取下一条执行的字节码指令的，它的程序控制流的指示器，分支，线程恢复等功能都依赖于这个计数器。\n我们知道多线程的实现是多个线程轮流占用CPU而实现的，而在线程切换的时候就需要保存当前线程的执行状态，这样在这个线程重新占用CPU的时候才能恢复到之前的状态，而在JVM状态的保存是依赖于PC实现的，所以PC是线程所私有的内存区域，这个区域也是java运行时数据区域唯一不会发生OOM的区域\njvm 指令概览 随便找一个class 文件， 执行下main命令可以解析 class 文件\njavap -v App.class 输出:\nClassfile /F:/code/java/io/dc/App.class Last modified 2022-8-19; size 1229 bytes MD5 checksum debb75f708cef09b6b1bf483b3e345ec Compiled from \u0026#34;App.java\u0026#34; public class io.dc.App minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPER Constant pool: #1 = Methodref #18.#31 // java/lang/Object.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #2 = Class #32 // io/dc/App$MyClassLoader #3 = String #33 // F:/code/java1 #4 = Methodref #2.#34 // io/dc/App$MyClassLoader.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:(Ljava/lang/String;)V #5 = String #35 // io.dc.User #6 = Methodref #2.#36 // io/dc/App$MyClassLoader.loadClass:(Ljava/lang/String;)Ljava/lang/Class; #7 = Methodref #37.#38 // java/lang/Class.newInstance:()Ljava/lang/Object; .... 省略 public static void main(java.lang.String[]) throws java.lang.Exception; descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=9, args_size=1 0: new #2 // class io/dc/App$MyClassLoader 3: dup 4: ldc #3 // String F:/code/java1 6: invokespecial #4 // Method io/dc/App$MyClassLoader.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:(Ljava/lang/String;)V 9: astore_1 10: aload_1 11: ldc #5 // String io.dc.User 13: invokevirtual #6 // Method io/dc/App$MyClassLoader.loadClass:(Ljava/lang/String;)Ljava/lang/Class; 16: astore_2 17: aload_2 18: invokevirtual #7 // Method java/lang/Class.newInstance:()Ljava/lang/Object; 21: astore_3 22: aload_2 23: ldc #8 // String sout 其中 #1 #2 #3 就是程序计数器保存的内容，对应指令的位置\n可以去官网找到每条指令的含义，示例如下:\n栈线程 栈具有先进后出的特性，线程栈内还有栈帧的概念，在一个线程中，每遇到一个方法都会开辟一个新的栈帧来存放方法相关的内容，栈帧内存放的还有局部变量表，操作数栈，动态链接，方法出口\n栈线程的空间可以按需调整，有时我们会看到 StackOverflow 栈溢出错误，就是栈帧过多，空间不够用了，往往发生无限制的递归调用中。\n局部变量表 顾名思义就是存放局部变量的一个表, 存放编译器生成的各种类型\n基本类型（boolean,byte,char, short, float,long, double） 对象的引用 try catch 中的异常 方法中的参数 局部变量表是以槽(shot)为单位的,其中64位长度（long,double）类型数据占用俩个变量槽，而32位的占一个变量槽。\n用一个简单的demo 看一下槽的使用\npublic class Main { public static void main(String[] args){ int a=1; int b=2; System.out.println(a+b); } } 反编译之后的jvm指令\npublic static void main(java.lang.String[]) throws java.io.IOException; descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=3, args_size=1 //local就是局部变量表的大小 0: iconst_1 1: istore_1 //栈顶元素弹出存入变量表的槽1 2: iconst_2 3: istore_2 //栈顶元素弹出存入变量表的槽2 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: iload_1 8: iload_2 9: iadd 10: invokevirtual #3 // Method java/io/PrintStream.println:(I)V 13: return LineNumberTable: line 18: 0 line 19: 2 line 20: 4 line 21: 13 LocalVariableTable: Start Length Slot Name Signature 0 14 0 args [Ljava/lang/String; 2 12 1 a I 4 10 2 b I Exceptions: throws java.io.IOException 从上面的字节码文件中我们可以看出，在java源代码被编译成class文件后每一个方法的变量表的大小就已经确定（locals的值）。而且JVM是通过索引来操作变量表的，当使用的是32位数据类型时就索引N代表使用第N个变量槽。64位则代表第N和第N+1个变量槽，因为64为占用两个变量槽\n操作数栈 Operand Stack，可以理解为存放操作数的栈。它的大小也是在编译期就已经确定好了的，就是上面反编译代码中出现的stack，栈元素可以是包括long和double在内的任意的java数据类型。\n当一个方法刚开始执行的时候，操作数栈是空的，在方法执行的过程中字节码指令会往操作数栈内写入和取出元素。\npublic static void main(java.lang.String[]) throws java.io.IOException; descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=3, args_size=1 //栈深度最大为3，3个变量槽 0: iconst_1 //常量1压入操作数栈 1: istore_1 //栈顶元素出栈存入变量槽1 2: iconst_2 //常量2压入操作数栈 3: istore_2 //栈顶元素出栈存入变量槽2 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; //调用静态方法main 7: iload_1 //将变量槽1中值压入操作数栈 8: iload_2 //将变量槽2中值压入操作数栈 9: iadd //从栈顶弹出俩个元素相加并且压入操作数栈 10: invokevirtual #3 // Method java/io/PrintStream.println:(I)V //调用虚方法 13: return //返回 动态链接 。。。 待续\n方法出口 在方法调用结束后，必须返回到该方法最初被调用时的位置，程序才能继续运行，所以在栈帧中要保存一些信息，用来帮助恢复它的上层主调方法的执行状态。方法返回地址就可以是主调方法在调用该方法的指令的下一条指令的地址\n堆 JVM中的堆是用来存放对象的内存空间，几乎所有的Java对象、数组都存储在JVM的堆内存中。比如当我们new一个对象或者创建一个数组的时候，就会在堆内存中分配出一段空间用来存放。类加载器读取了类文件后，需要把类、方法、常变量放到堆内存中，保存所有引用类型的真实信息，便于后续的执行。\n物理上可以不是连续的，逻辑上是连续的\n堆时JVM区域内存占用最大的一块，时垃圾回收的主要对象\n堆内的划分 新生代与老年代的默认比例 1:2 新生代区的默认比例是 8:1:1 在 HotSpot 中，Eden 空间和另外两个 SurvIvor 空间缺省所占的比例是 8:1:1\n垃圾回收简述 随着程序的运行，Eden 区空间不足时会触发一次 Minor GC , 查找所有对 GC Root 的引用，包含间接引用。 每个对象都会被标记非垃圾，然后将非垃圾复制到 Survivor S0 中，同时给这些非垃圾对象打上一个经历Minor GC 的次数—— 代数，每经历一次 Minor GC ，就加1, 然后清空 Eden 区域，等下次 Eden 再次空间不足时，执行一次 GC,将 Eden 区 和 S0 区中的非垃圾复制到 S1 区， 对象的代数增加1，然后清空 Eden 和 S0 区。\nS0 和 S1 这种左手换右手的方式不是无休止的，当代数增加到 15 ，就会把对象移到老年代，成为长期存在的对象。 除此之外，还有一种情况，即是当从Eden区复制内容到Survivor区时，复制内容大小超过S0或S1任一区域一半大小，也会直接被放入到老年代中，所以老年代才会需要那么大的区域\n虽然老年代空间比较大，但终究也会有满的时候，当老年代的空间也满了,比较麻烦的事情就来了，会引发一次 full GC，在 full gc 时，jvm 会先触发 STW(Stop-The-World),暂停所有线程，回收整个内存模型中的内存资源，从而造成用户用户响应超时，或者系统无响应，对于并发高的系统影响极大。\n通过gc机制，我们就可以得出一个简单有效的JVM优化办法，那就是减少full gc的次数，如何减少呢？只需要调整老年代和年轻代的内存空间分配使得在minor gc的过程中尽可能的消除大部分的垃圾对象。\n比如这种`java -Xmx3072 -Xms3072M -Xmn2048M -Xss1M\nGC Roots：在上面的gc过程中，我们还提到了JVM是如何判断垃圾对象的。简单地来说，就是从gc roots的根出发（即局部变量表中的引用对象），一路沿着引用关系找，凡是能够被找到的对象都是非垃圾对象，并且会被移动到下一个它应该去的区域中。剩下的对象，会在区域清空时，一同被清理掉而无须关心\njvm 参数简单介绍 ​ -Xmx3072M：设置JVM最大可用内存为3072M。\n​ -Xms3072M：设置JVM初始内存为3072M。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。\n​ -Xmn2048M：设置年轻代大小为2G。增大年轻代后，将会减小年老代大小。不过此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。\n​ -Xss1M：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。\n方法区 方法区的基本理解：\n方法区（Method Area) 与 Java 堆一样，是各个线程共享的内存区域。 方法区在 JVM 启动的时候创建，并且它的实际的物理内存空间和 Java 堆区一样都可以是不连续的。 方法区的大小，跟堆空间一样，可以选择固定大小或者可扩展。 方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区的溢出，虚拟机同样会抛出内存溢出错误： java.lang.OutOfMemoryError:PermGen space 或者 java.lang.OutOfMemoryError:Metaspace 关闭 JVM 就会释放这个内存区域。 方法区内存设置\n元数据大小可以使用参数 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize 指定。 默认值依赖于平台。windows 下，-XX:MetaspaceSize 是 21M， -XX:MaxMetaspaceSize 的值是 -1，即没有限制。 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存。如果元数据发生异常，虚拟机一样会抛出异常 OutOfMemoryError:Metaspace -XX:MetaspaceSize 设置初始的元空间大小。对于一个 64 位的服务器 JVM 来说，其默认的 -XX:MetaspaceSize 值为21MB 。这就是初始的高水位线，一旦触及这个水位线， Full GC 将会被触发并卸载没用的类（即这些类对应的类加载器不再存活），然后这个高水位线将会重置。新的高水位线取决于 GC 释放了多少空间。如果释放的空间不足，那么在不超过 MaxMetaspaceSize时，适当提高该值。如果释放空间过多，则适当降低该值。 如果初始化的高水位线设置过低，上述高水位线调整情况会发生很多次。通过垃圾回收器的日志可以观察到 Full GC 多次调用。为了避免频繁的GC,建议将 -XX:MetaSpaceSize 设置为一个相对较高的值。 存储内容 它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等。\n类型信息 对每个加载的类型（类 class、接口 interface、枚举enum、注解annotation），JVM 方法区中存储以下类型信息：\n这个类型的完整有效名称（全名=包名.类名） 这个类型直接父类的完整有效名（对于 interface 或是 java.lang.Object，都没有父类） 这个类型的修饰符（public,abstract ,final 的某个子集） 这个类型直接接口的一个有序列表 域（Field)信息 JVM 必须在方法区中保存类型的所有域的相关信息以及域的声明顺序。 域的相关信息包括：域名称、域类型、域修饰符（public,private,protected,static,final,volatile,transient 的某个子集) 方法信息 JVM 必须保存所有方法的以下信息，同域信息一样包括声明顺序:\n1.方法名称\n2.方法的返回类型\n3.方法参数的数量和类型（按顺序）\n4.方法的修饰符（public ,private, protected , static ,final, synchronized, native,abstract 的一个子集）\n5.方法的字节码（bytecodes)、操作数栈、局部变量表及大小（abstract 和 native方法除外）\n6.异常表 （abstract 和 native 方法除外） 每个异常处理的开始位置、结束位置、代码处理在程序计数器中的偏移地址、被捕获的异常类的常量池索引\n7.non-final 的类变量 静态变量和类关联在一起，随着类的加载而加载，他们成为类数据在逻辑上的一部分。\n8.全局常量：static final, 被声明为 final 的类变量的处理方法则不同，每个全局常量在编译的时候就会被分配了。\n运行时常量池 vs 常量池 方法区中，内部包含了运行时常量池 字节码文件，内部包含了常量池 为什么需要常量池 一个java源文件中类、接口、编译后产生一个字节码文件。而Java 中的字节码需要数据支持，通常这种数据会很大以至于不能直接存到字节码里，换另一种方式，可以存到常量池，这个字节码包含了指向常量池的引用。在动态链接的时候会用到运行时常量池，比如：如下的代码：\npublic class SimpleClass{ public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } } 虽然只有 194 字节，但是里面却使用了 String、 System、PrintStream及 Object 等结构。这里代码量其实已经很小了。如果代码多，应用到的结构会更多！这里就需要常量池了！\n小结\n常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名、方法名、参数类型、字面量等类型。\n运行时常量池 运行时常量池（Runtime Constant Pool) 是方法区的一部分。 常量池表（Constant Pool Table) 是 Class 文件的一部分，用于存放编译期生成的各种字面量与符号应用，这部分内容将在类加载后存放到方法区的运行时常量池中 在加载类和接口到虚拟机后，就会创建对应的运行时常量池。 JVM 为每个已加载到类型（类或接口）都维护了一个常量池。池中的数据项像数组项一样，是通过索引访问的。 运行时常量池中包含多种不同的常量，包括编译器就已经明确的数值字面量，也包括到运行期解析后才能够获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换为真实地址。 运行时常量池，相对于 Class 文件常量池的另一重要特征是：具备动态性。 运行时常量池类似于传统编程语言中的符号表（symbol table),但是它所包含的数据却比符号表要更加丰富一些。 当创建类或者接口的运行时常量池时，如果构造运行时常量池所需的内存空间超过了方法区所提供的最大值，则 JVM 会抛 OutOfMemoryError 异常。 方法区中的垃圾回收 方法区内常量池中主要存放的两大类常量：字面量和符号引用。字面量比较接近 Java语言层次的常量概念，如文本字符串、被声明为final的常量值等。而符号引用则属于编译原理方面的概念，包括下面三类常量：\n1.类和接口的权限定名\n2.字段的名称和描述符\n3.方法的名称和描述符\nHotSpot 虚拟机对常量池的回收策略是很明确的，之前常量池中的常量没有被任何地方引用，就可以回收。回收废弃常量与回收 java 堆中的对象非常类似。\n判定一个常量是否“废弃”还是相对简单的，而要判定一个类型是否属于不再被使用的类的条件就比较苛刻了。需要同事满足下面三个条件：\n该类的所有实例都已经被回收了，也就是 java 堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景如 OSGI、JSP 的重加载等，否则通常是很难达成的。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 ","date":"2022-08-22T07:58:28Z","permalink":"https://dccmmtop.github.io/posts/jvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","section":"posts","tags":["java"],"title":"jvm内存模型"},{"categories":null,"contents":"类加载是什么 把磁盘中的java文件加载到内存中的过程叫做类加载\n当我们用java命令运行某个类的main函数启动程序时，首先需要通过类加载器把主类加载到JVM. 有如下 User 类\npackage dc.dccmmtop; public Class User { public static void main(String[] args) { System.out.println(\u0026#34;hello\u0026#34;); } } 运行 java dc.dccmmtop.User 时， 先要找到 User.Class 文件，查找全路径就是 Class_PATH + {{package name}}，对于User类来说，就是 {$Class_APTH}/dc/dccmmtop.User.Class\n假如 User.java 在F:\\code, 并且不在Class_PATH 下，可以通过 java -Classpath \u0026quot;F:\\code\u0026quot; 临时指定。\n加载类之后还有后续的步骤:\n验证 准备 解析 初始化 使用 卸载 这篇文章主要来讲讲类加载\n类加载器 不了解类加载机制的，可能就认为，只需找到java文件所在的磁盘位置，然后进行一次读文件的操作不就完成了加载嘛，其实远非如此。\n总有一个加载类的工具，这个工具叫做类加载器，在java代码中可以通过如下方式获取当前类的类加载器是什么\npackage dccmmtop; public Class User { public static void main(String[] args) { System.out.println(\u0026#34;hello\u0026#34;); System.out.println(User.Class.getClassLoader()); } } 如图可以看到类加载器的名字叫做 AppClassLoader\n我们全局搜索一下这个类，会发现在 sun.misc.Launcher.java 文件中找到。\n那么这个AppClassLoader 本身也是一个 java 文件，它又是什么时候被加载并初始化的呢？\n我们滚动到文件顶部，看到 Launcher 类的构造方法部分：\n标记1 和标记2 实现了一个单例模式，在5 处获取到了 AppClassLoader 实例。也就是说在某一个地方通过调用 Launcher 类中的 getLauncher() 方法，会得到 AppClassLoader 实例， 那么 getLauncher() 方法又是在哪里调用的呢？追踪到这里已经无法在java代码中找到上一步了，其实这个方法是jvm (c++实现)调用的，如下图:\n以上就是类加载的主要步骤了。下面看一下双亲委派机制\n双亲委派机制 我们继续看AppClassLoader 实例化的过程：\n在5处，实例化了一个AppClassLoader的对象，同时传进去了一个参数 var1, 这个 var1 是另外一个类加载器ExtClassLoader , 我们在进入 getAppClassLoader 方法看一看是怎么实现的：\n先看一下 几个ClassLoad的继承关系：\n有上面的继承关系图可以看出来，AppClassLoader 和 ExtClassLoader 都是从 ClassLoader 继承来的。\n在 Launcher() 中可知，调用 AppClassLoader.getAppClassLoader() 方法时， 把 ExtClassLoader 的实例作为参数传递进来，最终到4这一步，作为 var2 参数，调用父类的构造方法，继续追踪父类的构造方法直到 ClassLoader :\n在 ClassLoader 构造方法中，维护了一个 parent 变量，到此我们知道了 AppClassLoader 中 parent 变量保存的是 ExtClassLoader的实例, 如下图表示\n继续看Launcher 构造方法：\nloadClass() 方法将 Class 文件加载到jvm中，我们跟踪一下这个方法，会发现最后会调到 根类ClassLoader 中：\nprotected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the Class has already been loaded Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if Class not found // from the non-null parent Class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the Class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining Class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 上面代码块中的弟6行，findLoadedClass() , 先从已加载到的类集合中查找有没有这个类，如果有的话，直接返回，没有再进行下一步, findLoadedClass 方法源码如下\n到 native finnal Class\u0026lt;?\u0026gt; findLoadedClass0(String name); 这里已经无法在向后追踪了，看到 naive ,要明白 使用native关键字说明这个方法是原生函数，也就是这个方法是用C/C++语言实现的，并且被编译成了DLL，由java去调用.\n此时 User.Class 是第一次加载，AppClassLoader 中肯定无法在已加载的集合中找到，所以继续向下走到第 10，11 行. 上面已经分析过，AppClassLoader 中的 parent 是 ExtClassLoader , 所以在11行由 ExtClassLoader 的实例执行 laodClass 方法。 ExtClassLoader 没有覆写根类ClassLoader 的loaderClass 方法，所以也会到这里，只不过 ExtClassLoader 的 parent 是 NUll， 会走到13行，调用findBootstrapClassOrNull() 方法，再看一下这个方法的实现:\n会发现这个方法也是C++实现的，虽然我们无法看到源码，但是根据注释可以知道，这个是保存了启动类加载器加载过的类。\n到此为止，我们已经见识过3中不同的类加载器了：\nAppClassLoader ExtClassLoader BootStrapClassLoader 我们先不管这个后面两个类加载器是什么， 假定他们也找不到 User.Class. 继续向下看：\n执行到第21行findClas()这里，再看源码\n在A-2 这一步，ucp 其实保存的就是当前 ClassLoader 的类加载路径，就不再展开。要记住此时的 ClassLoader 是 ExtClassLoader, 假如仍然找不到User.Class 会执行到 A-3.然后返回到 loadClass 方法中， 此时 c 是空，继续执行到33行，返回到 AppClassLoader 调用 parent.getAppClassLoader 处，在 AppClassLoader 实例的范围下继续向后执行，然后再继续调用 findClass 方法，如果在AppClassLoader的类加载路径中找到User.Class 文件，就会 执行 defindClass(name,res) 方法去加载类文件了。\n整个过程用文字描述起来比较复杂，来张图就很清楚了，为什么叫做双亲委派：\n把 loadedClassList 集合称作缓存\n先在 AppClassLoader 中缓存中找，如果找不到向 ExtClassLoader 找，如果能找到，直接返回 在 ExtClassLoader 中缓存找，如果找不到向 BootStrapClassLoader 找，如果能找到，直接返回 在 BootStrapClassLoader 找，如果找不到， 在 ExtClassLoader 类路径集合中找， 如果在 ExtClassLoader 类路径集合找不到，在 AppClassLoader 类路径集合找 如果在 AppClassLoader 类路径集合中能找到，加载该类，并放入缓存。找不到则报错 双亲指的是 ExtClassLoader 和 BootStrapClassLoader， AppClassLoader 先不加载，而是向上让其“父”加载，父加载不到时，自己再加载。这里的父不是父类，而是调用层级的关系。\n是时候介绍一下 这三个类加载器\nBootStrapClassLoader 引导类加载器\n负责加载支撑JVM运行的位于JRE的lib目录下的核心类库，比如 rt.jar、charsets.jar等\nExtClassLoader 扩展类加载器\n负责加载支撑JVM运行的位于JRE的lib目录下的ext扩展目录中的JAR 类包\nAppClassLoader 应用程序加载器\n负责加载ClassPath路径下的类包，主要就是加载你自己写的那些类\n我们可以写代码验证一下:\npackage dccmmtop; import sun.misc.Launcher; import java.net.URL; public Class User { public static void main(String[] args) { System.out.println(String.Class.getClassLoader()); // null System.out.println(com.sun.crypto.provider.DESKeyFactory.Class.getClassLoader().getClass().getName()); //sun.misc.Launcher$ExtClassLoader System.out.println(User.Class.getClassLoader().getClass().getName()); // sun.misc.Launcher$AppClassLoader System.out.println(); System.out.println(\u0026#34;bootstrapLoader加载以下文件：\u0026#34;); URL[] urls = Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i \u0026lt; urls.length; i++) { System.out.println(urls[i]); } System.out.println(); System.out.println(\u0026#34;extClassloader加载以下文件：\u0026#34;); System.out.println(System.getProperty(\u0026#34;java.ext.dirs\u0026#34;)); System.out.println(); System.out.println(\u0026#34;appClassLoader加载以下文件：\u0026#34;); System.out.println(System.getProperty(\u0026#34;java.Class.path\u0026#34;)); } } 输入如下:\nnull // 因为调用了 c++ 实现。无法获取到java对象 sun.misc.Launcher$ExtClassLoader sun.misc.Launcher$AppClassLoader the bootstrapLoader : null the extClassloader : sun.misc.Launcher$ExtClassLoader@77459877 the appClassLoader : sun.misc.Launcher$AppClassLoader@18b4aac2 bootstrapLoader加载以下文件： file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/lib/resources.jar file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/lib/rt.jar file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/lib/sunrsasign.jar file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/lib/jsse.jar file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/lib/jce.jar file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/lib/charsets.jar file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/lib/jfr.jar file:/C:/Program%20Files/Java/jdk1.8.0_261/jre/Classes extClassloader加载以下文件： C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext;C:\\WINDOWS\\Sun\\Java\\lib\\ext appClassLoader加载以下文件： C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\charsets.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\deploy.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\access-bridge-64.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\cldrdata.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\dnsns.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\jaccess.jar;...省略 为什么使用双亲委派机制 沙箱安全机制：\n自己写的java.lang.String.Class类不会被加载，这样便可以防止核心 API库被随意篡改\n避免类的重复加载：当父亲已经加载了该类时，就没有必要子ClassLoader再加载一 次，保证被加载类的唯一性\n全盘负责委托机制 “全盘负责”是指当一个ClassLoder装载一个类时，除非显示的使用另外一个ClassLoder，该类\n所依赖及引用的类也由这个ClassLoder载入\n自定义类加载器 从上述源码的描述可知，类加载器的核心方法是 findClass , 和 defineClass 。\ndefindClass 将class文件从磁盘加载文件到内存，defineClass 开始解析class文件:\n所以自定义类加载器只需继承 ClassLoader，然后从写 findClass 文件就行了：\n目录如下:\nApp.java:\nimport java.io.FileInputStream; import java.lang.reflect.Method; public class App { static class MyClassLoader extends ClassLoader { private String classPath; public MyClassLoader(String classPath) { this.classPath = classPath; } // 从磁盘加载文件 private byte[] loadByte(String name) throws Exception { name = name.replaceAll(\u0026#34;\\\\.\u0026#34;, \u0026#34;/\u0026#34;); FileInputStream fis = new FileInputStream(classPath + \u0026#34;/\u0026#34; + name + \u0026#34;.class\u0026#34;); int len = fis.available(); byte[] data = new byte[len]; fis.read(data); fis.close(); return data; } // 重写 protected Class\u0026lt;?\u0026gt; findClass(String name) throws ClassNotFoundException { try { byte[] data = loadByte(name); // defineClass将一个字节数组转为Class对象，这个字节数组是class文件读取后最终的字节 数组。 return defieClass(name, data, 0, data.length); } catch (Exception e) { e.printStackTrace(); throw new ClassNotFoundException(); } } } public static void main(String args[]) throws Exception { // 初始化自定义类加载器，会先初始化父类ClassLoader，其中会把自定义类加载器的父加载 器设置为应用程序类加载器AppClassLoader MyClassLoader classLoader = new MyClassLoader(\u0026#34;D:/dc_code/java\u0026#34;); // D盘创建 // 创建 io/dc 几级目录，将User类的复制类User.class丢入该目录 Class clazz = classLoader.loadClass(\u0026#34;io.dc.User\u0026#34;); Object obj = clazz.newInstance(); // 使用反射调用 User 类的 sout 方法 Method method = clazz.getDeclaredMethod(\u0026#34;sout\u0026#34;, null); method.invoke(obj, null); System.out.println(clazz.getClassLoader().getClass().getName()); } } 打破双亲委派机制 经过上面的源码分析发现，主要是 ClassLoader 类中的laodClass 方法来实现的双亲委派机制，自己不加载而是先让其父加载。\n所以直接复写 loadClass 方法即可，不再指定父级加载，当前类直接加载，如下:\nimport java.io.FileInputStream; import java.lang.reflect.Method; public class App { static class MyClassLoader extends ClassLoader { private String classPath; public MyClassLoader(String classPath) { this.classPath = classPath; } // 从磁盘加载文件 private byte[] loadByte(String name) throws Exception { name = name.replaceAll(\u0026#34;\\\\.\u0026#34;, \u0026#34;/\u0026#34;); FileInputStream fis = new FileInputStream(classPath + \u0026#34;/\u0026#34; + name + \u0026#34;.class\u0026#34;); int len = fis.available(); byte[] data = new byte[len]; fis.read(data); fis.close(); return data; } protected Class\u0026lt;?\u0026gt; findClass(String name) throws ClassNotFoundException { try { byte[] data = loadByte(name); // defineClass将一个字节数组转为Class对象，这个字节数组是class文件读取后最终的字节 数组。 return defineClass(name, data, 0, data.length); } catch (Exception e) { e.printStackTrace(); throw new ClassNotFoundException(); } } protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); if (name.startsWith(\u0026#34;io.dc\u0026#34;)) { // 直接查找, 限定包名 c = findClass(name); } else { // 其他包中的类还是使用双亲委派机制 // 否则会报找不到 Object 类 c = this.getParent().loadClass(name); } // this is the defining class loader; record the stats sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } if (resolve) { resolveClass(c); } return c; } } } public static void main(String args[]) throws Exception { // 初始化自定义类加载器，会先初始化父类ClassLoader，其中会把自定义类加载器的父加载 器设置为应用程序类加载器AppClassLoader MyClassLoader classLoader = new MyClassLoader(\u0026#34;D:/dc_code/java\u0026#34;); // D盘创建 // 创建 io/dc 几级目录，将User类的复制类User.class丢入该目录 Class clazz = classLoader.loadClass(\u0026#34;io.dc.User\u0026#34;); Object obj = clazz.newInstance(); // 使用反射调用 User 类的 sout 方法 Method method = clazz.getDeclaredMethod(\u0026#34;sout\u0026#34;, null); method.invoke(obj, null); System.out.println(clazz.getClassLoader().getClass().getName()); } } Tomcat 中的类加载方式 为什么Tomcat 需要打破双亲委派机制 tomcat 是一个web容器。可以同时运行多个web服务，每个服务可能会依赖同一个类的不同版本，例如 服务A 依赖 Spring 2.1.1 服务B 依赖 Spring 2.2.2 ,服务A B 所依赖的包名 + 类名肯定有一样的。但是我们知道在双亲委派机制中不能同时存在报名和类名一样的class文件。此时就需要打破 web容器也有自己依赖的类库，不能与应用程序的类库混淆。基于安全考虑，应该让容器的 类库和程序的类库隔离开来。 web容器要支持jsp的修改，我们知道，jsp 文件最终也是要编译成class文件才能在虚拟机中 运行，但程序运行后修改jsp已经是司空见惯的事情， web容器需要支持 jsp 修改后不用重启。如果使用双亲委派机制，同一个类加载过一次就不会再次加载了。很显然不能支持热加载功能。tomcat 把每一个jsp class 文件都用一个单独的加载器去加载。如果检测到 jsp 文件发生变动，直接将现有的 jsp 对应的类加载器卸载掉，重新生成一个新的加载器去加载。 模仿tomcat 实现多版本共存 用一个简单的demo来实现 tomcat 多版本共存功能:\n有两个报名和类名一样的 User 类:\njava1 -\u0026gt; User.java :\npackage io.dc; public class User{ public void sout (){ System.out.println(\u0026#34;user\u0026#34;); } public static void main (String[] args){ System.out.println(\u0026#34;user\u0026#34;); } } java2 -\u0026gt; User.java\npackage io.dc; public class User { public void sout() { System.out.println(\u0026#34;user1\u0026#34;); } public static void main(String[] args) { System.out.println(\u0026#34;user1\u0026#34;); } } App.java\npackage io.dc; import java.io.FileInputStream; import java.lang.reflect.Method; public class App { static class MyClassLoader extends ClassLoader { private String classPath; public MyClassLoader(String classPath) { this.classPath = classPath; } // 从磁盘加载文件 private byte[] loadByte(String name) throws Exception { name = name.replaceAll(\u0026#34;\\\\.\u0026#34;, \u0026#34;/\u0026#34;); FileInputStream fis = new FileInputStream(classPath + \u0026#34;/\u0026#34; + name + \u0026#34;.class\u0026#34;); int len = fis.available(); byte[] data = new byte[len]; fis.read(data); fis.close(); return data; } protected Class\u0026lt;?\u0026gt; findClass(String name) throws ClassNotFoundException { try { byte[] data = loadByte(name); // defineClass将一个字节数组转为Class对象，这个字节数组是class文件读取后最终的字节 数组。 return defineClass(name, data, 0, data.length); } catch (Exception e) { e.printStackTrace(); throw new ClassNotFoundException(); } } protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); if (name.startsWith(\u0026#34;io.dc\u0026#34;)) { // 直接查找, 限定包名 c = findClass(name); } else { // 其他包中的类还是使用双亲委派机制 // 否则会报找不到 Object 类 c = this.getParent().loadClass(name); } // this is the defining class loader; record the stats sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } if (resolve) { resolveClass(c); } return c; } } } public static void main(String args[]) throws Exception { // 初始化自定义类加载器，会先初始化父类ClassLoader，其中会把自定义类加载器的父加载 器设置为应用程序类加载器AppClassLoader MyClassLoader classLoader = new MyClassLoader(\u0026#34;F:/code/java1\u0026#34;); // D盘创建 // 创建 io/dc 几级目录，将User类的复制类User.class丢入该目录 Class clazz = classLoader.loadClass(\u0026#34;io.dc.User\u0026#34;); Object obj = clazz.newInstance(); // 使用反射调用 User 类的 sout 方法 Method method = clazz.getDeclaredMethod(\u0026#34;sout\u0026#34;, null); method.invoke(obj, null); System.out.println(clazz.getClassLoader().getClass().getName()); // 使用新的加载器加载另外一个 User.class MyClassLoader classLoader1 = new MyClassLoader(\u0026#34;F:/code/java2\u0026#34;); // D盘创建 // 创建 io/dc 几级目录，将User类的复制类User.class丢入该目录 Class clazz1 = classLoader1.loadClass(\u0026#34;io.dc.User\u0026#34;); Object obj1 = clazz1.newInstance(); // 使用反射调用 User 类的 sout 方法 Method method1 = clazz1.getDeclaredMethod(\u0026#34;sout\u0026#34;, null); method1.invoke(obj1, null); System.out.println(clazz1.getClassLoader().getClass().getName()); } } 结果：\n所以，同一个JVM内，两个相同包名和类名的类对象可以共存，因为他们的类加载器可以不一 样，所以看两个类对象是否是同一个，除了看类的包名和类名是否都相同之外，还需要他们的类 加载器也是同一个才能认为他们是同一个。\n","date":"2022-08-14T14:03:08Z","permalink":"https://dccmmtop.github.io/posts/java%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E5%92%8C%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%9C%BA%E5%88%B6/","section":"posts","tags":["java"],"title":"java中的类加载器和双亲委派机制"},{"categories":null,"contents":"程序员都了解初始化的重要性，但是会常常忘记同样重要的清理工作，java有垃圾回收器负责回收无用对象占用的内存资源，但是也有特殊情况，你的对象获得了一块特殊的内存区域，并不是通过 new 方法，由于垃圾回收器只知道释放那些经由new分配的内存， 所以它不知道如何释放这些特殊的内存。\njava 考虑到这种情况，允许在类中定义一个名为 finalize() 的方法。他的工作原理是这样的：一旦垃圾回收器准备释放对象占用的存储空间，将首先调用 finalize() 方法，并在下一次垃圾回收动作发生时，才会真正的回收对象占用的内存。就像为垃圾回收器添加一个回调方法一样。\n注意: 垃圾回收并不能保证一定能发生，也许你会发现，只要程序没有濒临存储空间用完的那一刻，对象占用的空间永远得不到释放，甚至程序运行结束 垃圾回收器也没有释放你创建的任何对象的存储空间，而是随着操作程序的退出，将资源全部还给操作系统，这个策略是恰当的，因为垃圾回收本身也有开销，要是不实用它，也就不用支付这部分开销了。\nfinalize 用户何在 到此我们应该明白，不该将 finalize () 作为通用的清理方法，请记住重要的一点，垃圾回收应该只与内存有关。所以和垃圾回收有关的任何行为尤其是finalize 方法，他们也必须同内存及其回收有关\n上面说的特殊的内存区域是怎么创建出来的呢？\n因为java中一切都是对象，任何区域中的内存都是依附于对象的，这样就将这部分区域限制到很少的场景。那就是可能在java中使用了类似与 C语言 malloc() 方法来分配内存。通过这种方式分配内存，只能使用 free() 方法来释放，否则就会发生内存泄露。\n现在我们应该明白，不要过多的使用 finalize() 方法的道理了，因为他确实不是进行普通的清理工作的合适场所。\n参考资料\n《Think in Java》 ","date":"2022-08-07T20:32:09Z","permalink":"https://dccmmtop.github.io/posts/finalize%E7%9A%84%E6%84%8F%E4%B9%89/","section":"posts","tags":["java"],"title":"finalize的意义"},{"categories":null,"contents":"介绍 BufferPool 之前，先抛出一个问题，每次更新，新增，删除的动作，都会去写磁盘吗？如果是，那样效率会不会很低，如果不是，怎么保证数据不会丢失？\n不会实时写入磁盘 如果每次对数据的操作都要实时同步到硬盘中,那么将要产生大量的IO，并且是随机IO，因为无法保证这写被写入的数据是在磁盘的相邻区域。随机IO的性能是非常差的。当然不能实时的写入磁盘，而是暂时把这些数据保存到一块内存区域中，等积累到一定大小或者一定时间后，再统一写入磁盘。这个时间是不确定的，由操作系统决定。这个内存区域就是MySQL 中的BuffrPoll，可以理解为缓存。\n数据不会丢失 既然数据是先放到内存中的，然后定期的写入磁盘，如果在数据还没有写入磁盘，服务器断电了怎么办？那样数据不就丢了吗？还怎么保证持久性呢？\nredo log 答案是redo log， 在我们执行一个事务时，redo log 会记录事务中所有的写操作，和bin_log 是一样的，只有事务中的写操作成功被记录到redo log中，该事务才能被提交。客户端才会认为这个事务正常执行了。redo log 是一个日志文件，写操作是实时被记录到该文件中的，如果在BufferPoll\n中的数据没有写入磁盘而发生了故障，等下次服务器恢复时，就会先从redo log中加载数据到Buffer，然后在合适的时机写入磁盘。\n同样都是实时写入磁盘，为什么redo log 可以接受呢？ 因为写入log时，是顺序IO，提前分配一块磁盘空间，顺序追加就行了。顺序IO比磁盘IO效率高很多\n顺序IO和随机IO 顺序IO是指读写操作的访问地址连续。在顺序IO访问中，HDD所需的磁道搜索时间显着减少，因为读/写磁头可以以最小的移动访问下一个块。数据备份和日志记录等业务是顺序IO业务。\n随机IO是指读写操作时间连续，但访问地址不连续，随机分布在磁盘的地址空间中。产生随机IO的业务有OLTP服务，SQL，即时消息服务等。\nredo log 和bin log redo log是在引擎层， bin_log 是在 server 层,和引擎无关\nredo log 是为了保证数据的持久性，不丢失，配合 BufferPoll发挥作用\nbin log 是为了恢复数据使用\nMySQL内部执行步骤细节图 ","date":"2022-08-07T17:16:46Z","permalink":"https://dccmmtop.github.io/posts/mysql%E4%B8%AD%E7%9A%84bufferpoll%E4%B8%8Eredolog%E7%9A%84%E4%BD%9C%E7%94%A8/","section":"posts","tags":["MySQL"],"title":"MySQL中的BufferPool与RedoLog的作用"},{"categories":null,"contents":"MVCC多版本并发控制机制 在上面博客中介绍了可重复的事务隔离级别下，无法读到其他事务已提交的数据，这个功能就是MVCC(Multi-Version Concurrency Control)来实现的，\n对一行数据的读和写两个操作默认是不会通过加锁互斥来保证隔离性，避免了频繁加锁互斥，而在串行化隔离级别为了保证较高的隔离性是通过将所有操作加锁互斥来实现的。\nMysql在读已提交和可重复读隔离级别下都实现了MVCC机制。\nMVCC 的原理 MVCC主要靠两个技术来实现的 undo log 和 readview\nundo log undo日志版本链是指一行数据被多个事务多次修改过后，在每个事务修改完后，Mysql会保留修改前的数据undo回滚日志，并且用两个隐藏字段trx_id和roll_pointer把这些undo日志串联起来形成一个历史记录版本链(见下图)\n事务号的生成\ntrx_id 就是事务号，每个事务开始后，遇到第一个更新，删除，新增的sql语句，会生成一个全局唯一的事务编码。这个编码是递增的。要注意的是，如果一个事务中只有查询语句，是不会生成事务号的。\nbegin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个修改操作InnoDB表的语句，事务才真正启动，才会向mysql申请事务id，mysql内部是严格按照事务的启动顺序来分配事务id的。\nroll_pointer 是指向本次行数据改动之前的版本标识。\n现在来描述一下这个版本链是如何生成的：\naccount 表。字段 Id 和 name\n事务80 新增一条记录 id = 1 ， name = \u0026rsquo;lilei\u0026rsquo;, undo log 中记录一条这样的行数据，同时维护了两个隐藏的列，trx_id 和 roll_pointer , trx_id 就是 80， 由于这一行是新增的，没有历史记录，所以 roll_pointer 是空的, 事务80提交。 事务300更新这行数据， 将name 更新为 \u0026rsquo;lilei300\u0026rsquo;, undo log 同样记录这行数据，为了便于说明，假设版本记录为2 trx_id = 300, roll_pointer 的值就是 1 版本的位置。注意此时事务 300 没有提交 事务100也会更新这条数据，将 name 更新为 \u0026rsquo;lilei1\u0026rsquo; , undo log 添加新记录，版本记录3，trx_id= 100 ， roll_pointer 指向版本2 事务100 再次更新这行记录，name = \u0026rsquo;lilei2\u0026rsquo; , undo log 添加新纪录，版本记录是4， trx_id =100, roll_pointer 指向版本3 , 注意，此时事务100仍然没有提交 事务200更新这条记录，name = \u0026rsquo;lilei3\u0026rsquo;, undo log 添加新纪录，版本记录是5， trx_id = 200， roll_pointer 指向版本4 事务200更新这条记录，name = \u0026rsquo;lilei4\u0026rsquo;, undo log 添加新纪录，版本记录是6， trx_id = 200， roll_pointer 指向版本5, 注意此时事务200 没有提交 在上图的基础上，再有一个事务300更新这条记录，name = \u0026rsquo;lilei5\u0026rsquo;, 版本记录是7 ，trx_id = 300, roll_pointer 指向版本6, 事务300 未提交 经过上述几个步骤，此行数据的版本链已经形成了, 根据最新记录，可以找到所有历史记录\nreadview 假设系统中没有其他事务在运行，在步骤7这一时刻， 正在运行的事务有 100 200 300\n现在有步骤8开始执行\n在一个新的事务A中先执行一条查询语句 select name from account where id = 1; 执行8这个查询语句时，会先找到这行数据的最新版本，然后开始遍历历史版本，那么根据什么规则来判断应该取哪一个版本的数据呢？如下：\n因为事务号在没有遇到写数据的操作时，是不会生成事务编码的。所以此时系统中正在运行的事务还是只有 100 200 300 , 以事务为单位，第一条查询开始时，会生成一个 readview 标识， 记录此时此刻所有正在运行的事务id，并生成一个有序的数组，即：100,200,300 也就是说当前系统中，最小事务是100， 最大事务是300\n对于当前事务A来说，undo log 版本中， 事务x小于100的，都是已经提交的，应该可以被当前事务A看到直接取事务X数据即可。 而事务号x大于 300 的，都是将来要运行的事务，本事务A不应该看到，继续遍历\n如果undo log 版本链中有一条数据的版本号x,处于 100 \u0026lt;= x \u0026lt;= 300。 这时还要分为两种情况,\n如果 x 在100，200， 300 中可以找到相同的，就是说明还没有提交， 事务A 看不见事务x修改的数据继续遍历。 如果找不到相同的，说明事务x已经提交，可以看到事务X修改的数据，取 x版本。\n这里可能会有一个疑问，readview 标识是在第一条查询语句开始的时候生成的。保存可所有未提交的事务，为什么会有 x 不在 其中的情况呢？因为readview 生成的时机是和事务的隔离级别有关的。\n现在增加如下操作:\n9. 事务200提交\n10. 事务A中再次执行查询语句 select name from account where id =1;\n在可重复读的的级别下 ，整个事务中，只要第一条查询语句开始的时候，就生成了一个readview 标识。以后不会再改变，即使 200 这个事务已经提交，在步骤10中， readview 仍然是 100，200，300 。根据上面的规则，200 在（100，200，300） 中，版本6 ，5 属于不可见数据。这样就实现了可重复读的机制。同样版本 4， 3 ， 2 也属于不可见数据，所以取版本1 ， name = \u0026rsquo;lilei'\n在读提交级别下，整个事务中，每条查询语句都会生成一个新的 readview 标识。在 步骤10， readview 是 (100,300) . 200 不在其中，所以版本6的数据可见， name = \u0026rsquo;lilei4'\n总结一下版本号与readview的对比规则:\n对于删除的情况可以认为是update的特殊情况，会将版本链上最新的数据复制一份，然后将trx_id修改成删除操作的trx_id，同时在该条记录的头信息（record header）里的（deleted_flag）标记位写上true，来表示当前记录已经被删除，在查询时按照上面的规则查到对应的记录如果delete_flag标记位为true，意味着记录已被删除，则不返回数据。\nMVCC机制的实现就是通过read-view机制与undo版本链比对机制，使得不同的事务会根据数据版本链对比规则读取同一条数据在版本链上的不同版本数据\n以上就MVCC的原理了。\n回滚 知道了undo log 日志是如何生成的，那么回滚的原理也就显而易见了，删除本事务的undo log，并按顺序维护 roll_pointer 的指向k。\n","date":"2022-08-07T10:48:12Z","permalink":"https://dccmmtop.github.io/posts/mvcc%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","section":"posts","tags":["MySQL"],"title":"MVCC实现原理"},{"categories":null,"contents":"在MySQL中，选择正确的数据类型，对于性能至关重要。一般应该遵循下面两步：\n（1）确定合适的大类型：数字、字符串、时间、二进制；\n（2）确定具体的类型：有无符号、取值范围、变长定长等。\n在MySQL数据类型设置方面，尽量用更小的数据类型，因为它们通常有更好的性能，花费更少的硬件资源。并且，尽量把字段定义为NOT NULL，避免使用NULL。\n数值 如果整形数据没有负数，如ID号，建议指定为UNSIGNED无符号类型，容量可以扩大一倍。 建议使用TINYINT代替ENUM、BITENUM、SET。 避免使用整数的显示宽度(参看文档最后)，也就是说，不要用INT(10)类似的方法指定字段显示宽度，直接用INT。 DECIMAL最适合保存准确度要求高，而且用于计算的数据，比如价格。但是在使用DECIMAL类型的时候，注意长度设置。 建议使用整形类型来运算和存储实数，方法是，实数乘以相应的倍数后再操作。 整数通常是最佳的数据类型，因为它速度快，并且能使用AUTO_INCREMENT。 数值的长度 INT显示宽度\n我们经常会使用命令来创建数据表，而且同时会指定一个长度，如下。但是，这里的长度并非是TINYINT类型存储的最大长度，而是显示的最大长度。\nCREATE TABLE user ( id TINYINT(2) UNSIGNED );\n这里表示user表的id字段的类型是TINYINT，可以存储的最大数值是255。所以，在存储数据时，如果存入值小于等于255，如200，虽然超过2位，但是没有超出TINYINT类型长度，所以可以正常保存；如果存入值大于255，如500，那么MySQL会自动保存为TINYINT类型的最大值255。\n在查询数据时，不管查询结果为何值，都按实际输出。这里TINYINT(2)中2的作用就是，当需要在查询结果前填充0时，命令中加上ZEROFILL就可以实现，如：\nid TINYINT(2) UNSIGNED ZEROFILL\n这样，查询结果如果是5，那输出就是05。如果指定TINYINT(5)，那输出就是00005，其实实际存储的值还是5，而且存储的数据不会超过255，只是MySQL输出数据时在前面填充了0。\n换句话说，在MySQL命令中，字段的类型长度TINYINT(2)、INT(11)不会影响数据的插入，只会在使用ZEROFILL时有用，让查询结果前填充0。\n时间 MySQL能存储的最小时间粒度为秒。 建议用DATE数据类型来保存日期。MySQL中默认的日期格式是yyyy-mm-dd。 用MySQL的内建类型DATE、TIME、DATETIME来存储时间，而不是使用字符串。 当数据格式为TIMESTAMP和DATETIME时，可以用CURRENT_TIMESTAMP作为默认（MySQL5.6以后），MySQL会自动返回记录插入的确切时间。 TIMESTAMP是UTC时间戳，与时区相关。 DATETIME的存储格式是一个YYYYMMDD HH:MM:SS的整数，与时区无关，你存了什么，读出来就是什么。 除非有特殊需求，一般的公司建议使用TIMESTAMP，它比DATETIME更节约空间，但是像阿里这样的公司一般会用DATETIME，因为不用考虑TIMESTAMP将来的时间上限问题。 有时人们把Unix的时间戳保存为整数值，但是这通常没有任何好处，这种格式处理起来不太方便，我们并不推荐它。 字符串 字符串的长度相差较大用VARCHAR；字符串短，且所有值都接近一个长度用CHAR。 CHAR和VARCHAR适用于包括人名、邮政编码、电话号码和不超过255个字符长度的任意字母数字组合。那些要用来计算的数字不要用VARCHAR类型保存，因为可能会导致一些与计算相关的问题。换句话说，可能影响到计算的准确性和完整性。 尽量少用BLOB和TEXT，如果实在要用可以考虑将BLOB和TEXT字段单独存一张表，用id关联。 BLOB系列存储二进制字符串，与字符集无关。TEXT系列存储非二进制字符串，与字符集相关。 BLOB和TEXT都不能有默认值。 ","date":"2022-08-06T11:14:09Z","permalink":"https://dccmmtop.github.io/posts/mysql%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8/","section":"posts","tags":["MySQL"],"title":"MySQL数据类型的使用"},{"categories":null,"contents":"数据库一般都会并发执行多个事务，多个事务可能会并发的对相同的一批数据进行增删改查操作，可能就会导致我们说的脏写、脏读、不可重复读、幻读这些问题。\n这些问题的本质都是数据库的多事务并发问题，为了解决多事务并发问题，数据库设计了事务隔离机制、锁机制、MVCC多版本并发控制隔离机制，用一整套机制来解决多事务并发问题\n事务及其ACID属性 事务是由一组SQL语句组成的逻辑处理单元,事务具有以下4个属性,通常简称为事务的ACID属性。\n原子性(Atomicity) ：事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行。 一致性(Consistent) ：在事务开始和完成时,数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改,以保持数据的完整性。 隔离性(Isolation) ：数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的,反之亦然。 持久性(Durable) ：事务完成之后,它对于数据的修改是永久性的,即使出现系统故障也能够保持。 并发事务处理带来的问题\n更新丢失(Lost Update)或脏写 当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题——最后的更新覆盖了由其他事务所做的更新。\n可以使用锁来解决\n脏读（Dirty Reads） 一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致的状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此作进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫做“脏读”。\n一句话：事务A读取到了事务B已经修改但尚未提交的数据，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合一致性要求。\n不可重读（Non-Repeatable Reads） 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。\n一句话：事务A内部的相同查询语句在不同时刻读出的结果不一致，不符合隔离性\n幻读（Phantom Reads） 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。\n一句话：事务A读取到了事务B提交的新增数据，不符合隔离性\n脏写是多个事务对同一行记录同时修改引起的，可以通过加锁避免，下面的脏读，不可重复读，幻读，都是事务的隔离级别引起的。需要设置合适的隔离级别\n事务隔离级别 “脏读”、“不可重复读”和“幻读”,其实都是数据库读一致性问题,必须由数据库提供一定的事务隔离机制来解决。\n数据库的事务隔离越严格,并发副作用越小,但付出的代价也就越大,因为事务隔离实质上就是使事务在一定程度上“串行化”进行,这显然与“并发”是矛盾的。\n同时,不同的应用对读一致性和事务隔离程度的要求也是不同的,比如许多应用对“不可重复读\u0026quot;和“幻读”并不敏感,可能更关心数据并发访问的能力。\n常看当前数据库的事务隔离级别: show variables like \u0026rsquo;tx_isolation';\n设置事务隔离级别：set tx_isolation='REPEATABLE-READ';\nMysql默认的事务隔离级别是可重复读，用Spring开发程序时，如果不设置隔离级别默认用Mysql设置的隔离级别，如果Spring设置了就用已经设置的隔离级别\n锁 锁是计算机协调多个进程或线程并发访问某一资源的机制。\n在数据库中，除了传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供需要用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。\n锁的分类 从性能上分为乐观锁和悲观锁 从数据库操作力度上分为表锁和行锁 从对数据库操作类型分为读锁和写锁（都属于悲观锁） 读锁 读锁（共享锁，S锁(Shared)）：针对同一份数据，多个读操作可以同时进行而不会互相影响，比如：select * from T where id=1 lock in share mode, 单独的 select 是不会加锁的。\n写锁 写锁（排它锁，X锁(eXclusive)）：当前写操作没有完成前，它会阻断其他写锁和读锁，数据修改操作都会加写锁，查询也可以通过for update加写锁，比如：`select * from T where id=1 for update`\r意向锁 意向锁（Intention Lock）：又称I锁，针对表锁，主要是为了提高加表锁的效率，是mysql数据库自己加的。当有事务给表的数据行加了共享锁或排他锁，同时会给表设置一个标识，代表已经有行锁了，其他事务要想对表加表锁时，就不必逐行判断有没有行锁可能跟表锁冲突了，直接读这个标识就可以确定自己该不该加表锁。特别是表中的记录很多时，逐行判断加表锁的方式效率很低。而这个标识就是意向锁。\n意向锁主要分为：\n意向共享锁，IS锁，对整个表加共享锁之前，需要先获取到意向共享锁。 意向排他锁，IX锁，对整个表加排他锁之前，需要先获取到意向排他锁。 表锁 每次操作锁住整张表。开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低；一般用在整表数据迁移的场景，停掉所有的应用服务器并不能保证不会再向表中写入数据了。可能会有其他DBA再操作该表。最好加一个表锁\n表锁的操作 手动增加表锁\nlock table 表名称 read(write),表名称2 read(write); 如, 加读锁 lock table employee read\n加写锁\nlock table employee write\n当前session对该表的增删改查都没有问题，其他session对该表的所有操作被阻塞\n查看表上加过的锁\nshow open tables; 删除表锁\nunlock tables; 行锁 每次操作锁住一行数据。开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度最高。\nInnoDB支持行级锁 MYISAM不支持\nInnoDB在执行查询语句SELECT时(非串行隔离级别)，不会加锁。但是update、insert、delete操作会加行锁。\n简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。\n事务的隔离级别 读未提交 set tx_isolation='read-uncommitted';\nA 事务可以读到B事务中修改的数据，尽管B事务还没有提交 , 如果B事务回滚了，那么A读的就是脏数据，如果A基于这个数据做操作了，就会发生脏写\n读已提交 set tx_isolation='read-committed';\n假如 用户U1 的 age = 30 有如下时序的操作\nA事务开启，读到U1 age 的值为30 B 事务开启， 更新U1 age 为 35 A 事务中读 age 的值为 30 B 事务提交 A 事务中读 age 的值为 35 A 事务提交 在这种隔离级别下，解决了脏写的问题，因为无法读到未提交的数据，只能读到已经提交的数据，但是问题也很明显，在 3 ，5 步骤中，尽管在同一个A事务中，读取同一行数据，不同时间得到的结果不一样，我们以哪次结果为准呢？后面这个值会不会再次发生变化呢？这就是不可重复读的问题\n可重复读 set tx_isolation='repeatable-read';\n假如 用户U1 的 age = 30 有如下时序的操作\nA事务开启，读到U1 age 的值为30 B 事务开启， 更新U1 age 为 35 A 事务中读 age 的值为 30 A事务执行查询语句 select * from employee where age = 40; B 事务新增一条数据 U2，age = 40； B 事务提交 A 事务中读 age 的值为 30 A 事务执行SQL语句 update employee set age = age + 5 where id =1; A事务执行查询语句 select * from employee where age = 40; A 事务提交 在这种级别下，在A事务读取的结果始终为 30， 无论其他事务是否修改，是否提交。\n注意 但是在执行更新的时候，是以此时此刻数据库中最新的数据为准的，A事务提交后，age 的值是 40。而不是35， 因为在步骤8中，对 age 进行修改，是以当前 age 的值为准，也就是 35. 如果是在内存中进行的算术运算，先取出 age 的值，然后用程序将 age = age +5, 然后再将age更新到数据库，此时age 就是35了。发生错误，这一点要注意。\n这种可重复读的机制是如何实现的呢？难道每个事务开启的时候，都给表进行快照一次吗？\n答案是 MVCC, MVCC 的原理请看下一篇博客\n在步骤4, 9中，4 没有结果，9 能查询到结果，就是 B 事务新增的那一条数据，发生了幻读，幻读也是不可重复读的一种，只不过不可重复读是针对数据的更新，幻读是数据的新增或者删除。\n由此可见，可重复读的级别也没有完全将事务隔离开。\n串行化 set tx_isolation='serializable';\n在这种级别下，所有的查询语句和更新语句都会被加上行锁。\n这种隔离级别并发性极低，开发中很少会用到。\n如果客户端A执行的是一个范围查询，那么该范围内的所有行包括每行记录所在的间隙区间范围(就算该行数据还未被插入也会加锁，这种是间隙锁)都会被加锁。此时如果客户端B在该范围内插入数据都会被阻塞，所以就避免了幻读。\n再谈锁 知道了事务的隔离级别后，再看下面的锁\n间隙锁 Gap-Lock 例子:\n有 employee 表：\nA 事务执行 select * from employee where age \u0026gt;= 30 and age \u0026lt;= 40; B 事务执行 update employee set age = age + 1 where id = 2;\nC 事务执行 insert into employee(id, name, age) values(5, u5, 37); 在步骤1时，会有一个间隙锁，锁的范围是 30 \u0026lt;= age \u0026lt;= 40, B 事务和C事务都会被阻塞。尽管 还没有 age = 37 这条数据。\n如果步骤1 执行的是 select * employee where age \u0026gt;= 30 ，那么锁的范围是 (30, 正无穷)\n临键锁(Next-key Locks) Next-Key Locks是行锁与间隙锁的组合。\n锁升级 无索引行锁会升级为表锁(RR级别会升级为表锁，RC级别不会升级为表锁)\n锁主要是加在索引上，如果对非索引字段更新，行锁可能会变表锁\nInnoDB的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为表锁。\n锁定某一行还可以用lock in share mode(共享锁) 和for update(排它锁)，例如：select * from test_innodb_lock where a = 2 for update; 这样其他session只能读这行数据，修改则会被阻塞，直到锁定行的session提交\n锁的一些指标 通过检查innoDB_row_lock状态变量来分析系统上的行锁的争夺情况\nshow status like 'innodb_row_lock%';\n对各个状态量的说明如下：\nInnodb_row_lock_current_waits: 当前正在等待锁定的数量 Innodb_row_lock_time: 从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg: 每次等待所花平均时间 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花时间 Innodb_row_lock_waits: 系统启动后到现在总共等待的次数 对于这5个状态变量，比较重要的主要是：\nInnodb_row_lock_time_avg （等待平均时长） Innodb_row_lock_waits （等待总次数） Innodb_row_lock_time（等待总时长） 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。\n查看INFORMATION_SCHEMA系统库锁相关数据表\n-- 查看事务 select * from INFORMATION_SCHEMA.INNODB_TRX; -- 查看锁 select * from INFORMATION_SCHEMA.INNODB_LOCKS; -- 查看锁等待 select * from INFORMATION_SCHEMA.INNODB_LOCK_WAITS; -- 释放锁，trx_mysql_thread_id可以从INNODB_TRX表里查看到 kill trx_mysql_thread_id -- 查看锁等待详细信息 show engine innodb status ; 死锁演示 set tx_isolation=\u0026#39;repeatable-read\u0026#39;; Session_1执行：select * from account where id=1 for update;\nSession_2执行：select * from account where id=2 for update;\nSession_1执行：select * from account where id=2 for update;\nSession_2执行：select * from account where id=1 for update;\n查看近期死锁日志信息：show engine innodb status;\n大多数情况mysql可以自动检测死锁并回滚产生死锁的那个事务，但是有些情况mysql没法自动检测死锁\n锁优化建议 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 合理设计索引，尽量缩小锁的范围 尽可能减少检索条件范围，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度，涉及事务加锁的sql尽量放在事务最后执行 尽可能低级别事务隔离 总结 Innodb存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一下，但是在整体并发处理能力方面要远远优于MYISAM的表级锁定的。当系统并发量高的时候，Innodb的整体性能和MYISAM相比就会有比较明显的优势了。\n但是，Innodb的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让Innodb的整体性能表现不仅不能比MYISAM高，甚至可能会更差。\n","date":"2022-08-03T23:16:55Z","permalink":"https://dccmmtop.github.io/posts/mysql%E4%BA%8B%E5%8A%A1%E5%92%8C%E9%94%81/","section":"posts","tags":["MySQL"],"title":"MySQL事务和锁详解"},{"categories":null,"contents":"CREATE TABLE `employees` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(24) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;姓名\u0026#39;, `age` int(11) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;年龄\u0026#39;, `position` varchar(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;职位\u0026#39;, `hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;入职时间\u0026#39;, PRIMARY KEY (`id`), KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT=\u0026#39;员工记录表\u0026#39;; INSERT INTO employees(name,age,position,hire_time) VALUES(\u0026#39;LiLei\u0026#39;,22,\u0026#39;manager\u0026#39;,NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES(\u0026#39;HanMeimei\u0026#39;, 23,\u0026#39;dev\u0026#39;,NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES(\u0026#39;Lucy\u0026#39;,23,\u0026#39;dev\u0026#39;,NOW()); ‐‐ 插入一些示例数据 drop procedure if exists insert_emp; delimiter ;; create procedure insert_emp() begin declare i int; set i=1; while(i\u0026lt;=100000)do insert into employees(name,age,position) values(CONCAT(\u0026#39;zhuge\u0026#39;,i),i,\u0026#39;dev\u0026#39;); set i=i+1; end while; end;; delimiter ; call insert_emp(); 如上有 employees 表，有主键索引和 (name, age, position ) 联合索引, 看下面的查询示例:\n联合索引的首字段用范围查询 EXPLAIN SELECT * FROM employees WHERE name \u0026gt; \u0026#39;LiLei\u0026#39; AND age = 22 AND position =\u0026#39;manager\u0026#39;; 结论：联合索引第一个字段就用范围查找不会走索引，mysql内部可能觉得第一个字段就用范围，结果集应该很大，回表效率不高，还不如就全表扫描\n强制走索引 EXPLAIN SELECT * FROM employees force index(idx_name_age_position) WHERE name \u0026gt; \u0026#39;LiLei\u0026#39; AND age = 22 AND position =\u0026#39;manager\u0026#39;; 结论：虽然使用了强制走索引让联合索引第一个字段范围查找也走索引，扫描的行rows看上去也少了点，但是最终查找效率不一定比全表扫描高，因为回表效率不高， 一般不会使用这个手段，除非有证据能证明强制走索引后效率大幅度提高\n覆盖索引优化 EXPLAIN SELECT name,age,position FROM employees WHERE name \u0026gt; \u0026#39;LiLei\u0026#39; AND age = 22 AND position =\u0026#39;manager\u0026#39;; 将 select * 修改为 select name, age, posiion ， 只选择索引中已经存在的列，可以不用回表，所以会利用索引\nin和or什么时候会走索引 在表数据量比较大的情况会走索引，数据量不多的情况下会选择全表扫描,示例如下:\nin 查询\nEXPLAIN SELECT * FROM employees WHERE name in (\u0026#39;LiLei\u0026#39;,\u0026#39;HanMeimei\u0026#39;,\u0026#39;Lucy\u0026#39;) AND age = 22 AND position =\u0026#39;manager\u0026#39;; 用到全部索引\nor 查询\nEXPLAIN SELECT * FROM employees WHERE (name = \u0026#39;LiLei\u0026#39; or name = \u0026#39;HanMeimei\u0026#39;) AND age = 22 AND position =\u0026#39;manager\u0026#39;; 用到全部索引\n下面新建一张 employees_copy 表，结构和 employee 一样，但数据只有三条, 再执行上面两个查询\nin 查询\nEXPLAIN SELECT * FROM employees_copy WHERE name in (\u0026#39;LiLei\u0026#39;,\u0026#39;HanMeimei\u0026#39;,\u0026#39;Lucy\u0026#39;) AND age = 22 AND position =\u0026#39;manager\u0026#39;; 全表扫描\nor查询\nEXPLAIN SELECT * FROM employees_copy WHERE (name = \u0026#39;LiLei\u0026#39; or name = \u0026#39;HanMeimei\u0026#39;) AND age = 22 AND position =\u0026#39;manager\u0026#39;; 全表扫描\nlike xx% 一般都会走索引，和数据量无关 大表\nEXPLAIN SELECT * FROM employees WHERE name like \u0026#39;LiLei%\u0026#39; AND age = 22 AND position =\u0026#39;manager\u0026#39;; 小表\nEXPLAIN SELECT * FROM employees_copy WHERE name like \u0026#39;LiLei%\u0026#39; AND age = 22 AND position =\u0026#39;manager\u0026#39;; 可以看到，无论表的数据量大小，都会利用索引，为什么呢？\n其实 like 用到了索引下推的优化\n索引下推 对于辅助联合索引，正常情况下按照最左前缀原则， SELECT * from employees where name like 'LiLei%' and age = 22 and position = 'dev' 这种情况下只会走name字段的索引，因为根据name字段过滤完，得到的索引行里的age和position是无序的，无法很好的利用索引。\n在MySQL5.6之前的版本，这个查询只能在联合索引里匹配到名字是 \u0026lsquo;LiLei\u0026rsquo; 开头的索引，然后拿这些索引对应的主键逐个回表，到主键索引上找出相应的记录，再比对age和position这两个字段的值是否符合。\nMySQL 5.6引入了索引下推优化，可以在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可以有效的减少回表次数。使用了索引下推优化后，上面那个查询在联合索引里匹配到名字是 \u0026lsquo;LiLei\u0026rsquo; 开头的索引之后，同时还会在索引里过滤age和position这两个字段，拿着过滤完剩下的索引对应的主键id再回表查整行数据。\n索引下推会减少回表次数，对于innodb引擎的表索引下推只能用于二级索引，innodb的主键索引（聚簇索引）树叶子节点上保存的是全行数据，所以这个时候索引下推并不会起到减少查询全行数据的效果。\n为什么范围查找没有用索引下推优化？ 估计应该是Mysql认为范围查找过滤的结果集过大，like KK% 在绝大多数情况来看，过滤后的结果集比较小，所以这里Mysql选择给 like KK% 用了索引下推优化，当然这也不是绝对的，有时like KK% 也不一定就会走索引下推。\n如何选择索引 先看下面的两个查询:\n同样的表，同样的字段，因为条件的不同，选择的索引也不同，MySQL 是如何选择的呢？\nTrace 工具 MySQl 提供了一个工具，可以看到选择索引的计算过程， 用法如下:\nmysql\u0026gt; set session optimizer_trace=\u0026#34;enabled=on\u0026#34;,end_markers_in_json=on; --开启trace mysql\u0026gt; select * from employees where name \u0026gt; \u0026#39;a\u0026#39; order by position; mysql\u0026gt; SELECT * FROM information_schema.OPTIMIZER_TRACE; 下面是对 trace 字段的解析\n{ \u0026#34;steps\u0026#34;: [ { \u0026#34;join_preparation\u0026#34;: { //第一阶段：SQL准备阶段，格式化sql \u0026#34;select#\u0026#34;: 1, \u0026#34;steps\u0026#34;: [ { \u0026#34;expanded_query\u0026#34;: \u0026#34;/* select#1 */ select `employees`.`id` AS `id`,`employees`.`name` AS `name`,`employees`.`age` AS `age`,`employees`.`position` AS `position`,`employees`.`hire_time` AS `hire_time` from `employees` where (`employees`.`name` \u0026gt; \u0026#39;a\u0026#39;) order by `employees`.`position`\u0026#34; } ] /* steps */ } /* join_preparation */ }, { \u0026#34;join_optimization\u0026#34;: { //第二阶段：SQL优化阶段 \u0026#34;select#\u0026#34;: 1, \u0026#34;steps\u0026#34;: [ { \u0026#34;condition_processing\u0026#34;: { //条件处理 \u0026#34;condition\u0026#34;: \u0026#34;WHERE\u0026#34;, \u0026#34;original_condition\u0026#34;: \u0026#34;(`employees`.`name` \u0026gt; \u0026#39;a\u0026#39;)\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;transformation\u0026#34;: \u0026#34;equality_propagation\u0026#34;, \u0026#34;resulting_condition\u0026#34;: \u0026#34;(`employees`.`name` \u0026gt; \u0026#39;a\u0026#39;)\u0026#34; }, { \u0026#34;transformation\u0026#34;: \u0026#34;constant_propagation\u0026#34;, \u0026#34;resulting_condition\u0026#34;: \u0026#34;(`employees`.`name` \u0026gt; \u0026#39;a\u0026#39;)\u0026#34; }, { \u0026#34;transformation\u0026#34;: \u0026#34;trivial_condition_removal\u0026#34;, \u0026#34;resulting_condition\u0026#34;: \u0026#34;(`employees`.`name` \u0026gt; \u0026#39;a\u0026#39;)\u0026#34; } ] /* steps */ } /* condition_processing */ }, { \u0026#34;substitute_generated_columns\u0026#34;: { } /* substitute_generated_columns */ }, { \u0026#34;table_dependencies\u0026#34;: [ //表依赖详情 { \u0026#34;table\u0026#34;: \u0026#34;`employees`\u0026#34;, \u0026#34;row_may_be_null\u0026#34;: false, \u0026#34;map_bit\u0026#34;: 0, \u0026#34;depends_on_map_bits\u0026#34;: [ ] /* depends_on_map_bits */ } ] /* table_dependencies */ }, { \u0026#34;ref_optimizer_key_uses\u0026#34;: [ ] /* ref_optimizer_key_uses */ }, { \u0026#34;rows_estimation\u0026#34;: [ //预估表的访问成本 { \u0026#34;table\u0026#34;: \u0026#34;`employees`\u0026#34;, \u0026#34;range_analysis\u0026#34;: { \u0026#34;table_scan\u0026#34;: { //全表扫描情况 \u0026#34;rows\u0026#34;: 10123, //扫描行数 \u0026#34;cost\u0026#34;: 2054.7 //查询成本 } /* table_scan */, \u0026#34;potential_range_indexes\u0026#34;: [ //查询可能使用的索引 { \u0026#34;index\u0026#34;: \u0026#34;PRIMARY\u0026#34;, //主键索引 \u0026#34;usable\u0026#34;: false, \u0026#34;cause\u0026#34;: \u0026#34;not_applicable\u0026#34; }, { \u0026#34;index\u0026#34;: \u0026#34;idx_name_age_position\u0026#34;, //辅助索引 \u0026#34;usable\u0026#34;: true, \u0026#34;key_parts\u0026#34;: [ \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;position\u0026#34;, \u0026#34;id\u0026#34; ] /* key_parts */ } ] /* potential_range_indexes */, \u0026#34;setup_range_conditions\u0026#34;: [ ] /* setup_range_conditions */, \u0026#34;group_index_range\u0026#34;: { \u0026#34;chosen\u0026#34;: false, \u0026#34;cause\u0026#34;: \u0026#34;not_group_by_or_distinct\u0026#34; } /* group_index_range */, \u0026#34;analyzing_range_alternatives\u0026#34;: { //分析各个索引使用成本 \u0026#34;range_scan_alternatives\u0026#34;: [ { \u0026#34;index\u0026#34;: \u0026#34;idx_name_age_position\u0026#34;, \u0026#34;ranges\u0026#34;: [ \u0026#34;a \u0026lt; name\u0026#34; //索引使用范围 ] /* ranges */, \u0026#34;index_dives_for_eq_ranges\u0026#34;: true, \u0026#34;rowid_ordered\u0026#34;: false, //使用该索引获取的记录是否按照主键排序 \u0026#34;using_mrr\u0026#34;: false, \u0026#34;index_only\u0026#34;: false, //是否使用覆盖索引 \u0026#34;rows\u0026#34;: 5061, //索引扫描行数 \u0026#34;cost\u0026#34;: 6074.2, //索引使用成本 \u0026#34;chosen\u0026#34;: false, //是否选择该索引 \u0026#34;cause\u0026#34;: \u0026#34;cost\u0026#34; } ] /* range_scan_alternatives */, \u0026#34;analyzing_roworder_intersect\u0026#34;: { \u0026#34;usable\u0026#34;: false, \u0026#34;cause\u0026#34;: \u0026#34;too_few_roworder_scans\u0026#34; } /* analyzing_roworder_intersect */ } /* analyzing_range_alternatives */ } /* range_analysis */ } ] /* rows_estimation */ }, { \u0026#34;considered_execution_plans\u0026#34;: [ { \u0026#34;plan_prefix\u0026#34;: [ ] /* plan_prefix */, \u0026#34;table\u0026#34;: \u0026#34;`employees`\u0026#34;, \u0026#34;best_access_path\u0026#34;: { //最优访问路径 \u0026#34;considered_access_paths\u0026#34;: [ //最终选择的访问路径 { \u0026#34;rows_to_scan\u0026#34;: 10123, \u0026#34;access_type\u0026#34;: \u0026#34;scan\u0026#34;, //访问类型：为scan，全表扫描 \u0026#34;resulting_rows\u0026#34;: 10123, \u0026#34;cost\u0026#34;: 2052.6, \u0026#34;chosen\u0026#34;: true, //确定选择 \u0026#34;use_tmp_table\u0026#34;: true } ] /* considered_access_paths */ } /* best_access_path */, \u0026#34;condition_filtering_pct\u0026#34;: 100, \u0026#34;rows_for_plan\u0026#34;: 10123, \u0026#34;cost_for_plan\u0026#34;: 2052.6, \u0026#34;sort_cost\u0026#34;: 10123, \u0026#34;new_cost_for_plan\u0026#34;: 12176, \u0026#34;chosen\u0026#34;: true } ] /* considered_execution_plans */ }, { \u0026#34;attaching_conditions_to_tables\u0026#34;: { \u0026#34;original_condition\u0026#34;: \u0026#34;(`employees`.`name` \u0026gt; \u0026#39;a\u0026#39;)\u0026#34;, \u0026#34;attached_conditions_computation\u0026#34;: [ ] /* attached_conditions_computation */, \u0026#34;attached_conditions_summary\u0026#34;: [ { \u0026#34;table\u0026#34;: \u0026#34;`employees`\u0026#34;, \u0026#34;attached\u0026#34;: \u0026#34;(`employees`.`name` \u0026gt; \u0026#39;a\u0026#39;)\u0026#34; } ] /* attached_conditions_summary */ } /* attaching_conditions_to_tables */ }, { \u0026#34;clause_processing\u0026#34;: { \u0026#34;clause\u0026#34;: \u0026#34;ORDER BY\u0026#34;, \u0026#34;original_clause\u0026#34;: \u0026#34;`employees`.`position`\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;item\u0026#34;: \u0026#34;`employees`.`position`\u0026#34; } ] /* items */, \u0026#34;resulting_clause_is_simple\u0026#34;: true, \u0026#34;resulting_clause\u0026#34;: \u0026#34;`employees`.`position`\u0026#34; } /* clause_processing */ }, { \u0026#34;reconsidering_access_paths_for_index_ordering\u0026#34;: { \u0026#34;clause\u0026#34;: \u0026#34;ORDER BY\u0026#34;, \u0026#34;steps\u0026#34;: [ ] /* steps */, \u0026#34;index_order_summary\u0026#34;: { \u0026#34;table\u0026#34;: \u0026#34;`employees`\u0026#34;, \u0026#34;index_provides_order\u0026#34;: false, \u0026#34;order_direction\u0026#34;: \u0026#34;undefined\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;plan_changed\u0026#34;: false } /* index_order_summary */ } /* reconsidering_access_paths_for_index_ordering */ }, { \u0026#34;refine_plan\u0026#34;: [ { \u0026#34;table\u0026#34;: \u0026#34;`employees`\u0026#34; } ] /* refine_plan */ } ] /* steps */ } /* join_optimization */ }, { \u0026#34;join_execution\u0026#34;: { //第三阶段：SQL执行阶段 \u0026#34;select#\u0026#34;: 1, \u0026#34;steps\u0026#34;: [ ] /* steps */ } /* join_execution */ } ] /* steps */ } // 结论：全表扫描的成本低于索引扫描，所以mysql最终选择全表扫描 mysql\u0026gt; select * from employees where name \u0026gt; \u0026#39;zzz\u0026#39; order by position; mysql\u0026gt; SELECT * FROM information_schema.OPTIMIZER_TRACE; # 查看trace字段可知索引扫描的成本低于全表扫描，所以mysql最终选择索引扫描 mysql\u0026gt; set session optimizer_trace=\u0026#34;enabled=off\u0026#34;; //关闭trace 深入优化 order by 和 group by order by 和 group by 也会遵循左前缀法则, 如下例子\n根据左前缀法则，用到了 name 字段的索引，同时使用 age 字段用来排序， 因为 extra 种没有 filesort\norder by 或者 group by 用到的索引不会参与到 key_len 的计算，索引 key_len 仍然只是 74， 即 name字段的长度\n再看下面一个例子:\nwhere 条件是name 排序字段是 position 跳过了age字段，所以只能用 name 索引，无法利用 position 索引进行索引排序，用到是文件排序\n再看第三个例子:\n使用name条件查询， 同时使用 age position 双字段排序，没有跳过联合索引的字段. 所以可以用索引排序\n然后颠倒一下排序顺序，先position 再 age：\n发现此时只能文件排序了\n再看下面的例子\n虽然排序字段与索引字段不一样，但仍然是索引排序， 因为查询条件中 用到是 （name， age）索引，排序中用到是 position 索引，并没有颠倒顺序。所以还是索引排序\n如果一个正序一个倒序呢？\n虽然排序字段与索引字段顺序相同， 但是 age 是正序， position 是倒叙，导致与索引的排序方式不同，无法利用索引。从而发生了文件排序， Mysql8以上版本有降序索引可以支持该种查询方式。\n先 in 查询:\n对于排序来说，多个相等条件也是范围查询, 无法利用索引排序\n先范围查询:\n这里发生了全表扫描，没有任何索引，排序自然也无法利用索引了，可以使用覆盖索引优化:\n优化总结 MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index效率高，filesort效率低。 2、order by满足两种情况会使用Using index。\norder by语句使用索引最左前列。\n使用where子句与order by子句条件列组合满足索引最左前列。\n尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最左前缀法则。\n如果order by的条件不在索引列上，就会产生Using filesort。\n能用覆盖索引尽量用覆盖索引\ngroup by与order by很类似，其实质是先排序后分组，遵照索引创建顺序的最左前缀法则。对于group by的优化如果不需要排序的可以加上order by null禁止排序。注意，where高于having，能写在where中的限定条件就不要去having限定了。\nUsing filesort文件排序原理详解 单路排序模式； 是一次性取出满足条件行的所有字段，然后在sort buffer中进行排序；用trace工具可以看到sort_mode信息里显示\u0026lt; sort_key, additional_fields \u0026gt;或者\u0026lt; sort_key, packed_additional_fields \u0026gt;\n双路排序（又叫回表排序模式） 是首先根据相应的条件取出相应的排序字段和可以直接定位行数据的行 ID，然后在 sort buffer 中进行排序，排序完后需要再次取回其它需要的字段；用trace工具可以看到sort_mode信息里显示\u0026lt; sort_key, rowid \u0026gt;\nMySQL 通过比较系统变量 max_length_for_sort_data(默认1024字节) 的大小和需要查询的字段总大小来判断使用哪种排序模式。\n如果 字段的总长度小于max_length_for_sort_data ，那么使用 单路排序模式； 如果 字段的总长度大于max_length_for_sort_data ，那么使用 双路排序模·式。 分页优化 常规的limit分页 有如下查询语句\nselect * from employees limit 10000,10; 该sql并不是只查询了10条，而是查找了10010条，然后把前10000条结果给舍弃掉, 因此要查询一个大表靠后的内容，执行效率是非常低的\n优化 根据主键排序 上面的下面的sql语句没有指定排序方式，默认使用ID排序。当使用ID排序时，我们可以使用下面的优化。\nselect * from employees where id \u0026gt; 90000 limit 5; 如果id是连续自增的，和limit 90000,5 结果没有差别，是 90001 ~ 90005 的数据。\n但是如果在90000之前删除了一条数据，结果就不一样了，id \u0026gt; 90000 limit 5 的结果是 90001 ~ 90005， 但是limit 90000, 5 的结果是 90002 ~ 90006， 很明显 90002 ~ 90006 才是符合我们直觉的。所以这个优化只能限制与排序条件是连续的。如果id不是自增的呢？会出现什么情况，假如 90000 这条数据有两个，limit 90000, 5 的结果是 90000 ~ 90004，而 id \u0026gt; 90000 limit 5 的结果仍是 90001 ~ 90005， 会把 id= 90000 的数据漏掉一条。\n所以这个优化只能用于排序的字段是连续自增的，并且不能重复\n非主键排序的优化 有如下查询语句\nEXPLAIN select * from employees ORDER BY name limit 90000,5; 发现并没有用上name的索引，因为 select * ,扫描联合索引时，无法的到全部数据，需要回表，成本比全表扫描更高，所以优化器放弃使用索引。\n可以使用索引覆盖的方法，使用分页查询仅仅找到少量的主键，然后在使用主键查找整行数据， 如下:\nselect * from employees e inner join (select id from employees order by name limit 90000,5) ed on e.id = ed.id; 看下执行计划：\n原 SQL 使用文件排序，优化后的使用索引排序\n表关联优化 先造一些数据:\nCREATE TABLE `t1` ( `id` int(11) NOT NULL AUTO_INCREMENT, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_a` (`a`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; create table t2 like t1; -- 插入一些示例数据 -- 往t1表插入1万行记录 drop procedure if exists insert_t1; delimiter ;; create procedure insert_t1() begin declare i int; set i=1; while(i\u0026lt;=10000)do insert into t1(a,b) values(i,i); set i=i+1; end while; end;; delimiter ; call insert_t1(); -- 往t2表插入100行记录 drop procedure if exists insert_t2; delimiter ;; create procedure insert_t2() begin declare i int; set i=1; while(i\u0026lt;=100)do insert into t2(a,b) values(i,i); set i=i+1; end while; end;; delimiter ; call insert_t2(); 新建 t1 t2 表，结构一样， 都在a字段上有索引，b字段没有索引，t1表有 10000 行记录，t2表只有100条记录。\n常见的表关联算法 内嵌循环连接算法 Nested-Loop Join 基于块的嵌套循环连接算法 Block Nested-Loop Join 内嵌循环连接算法 一次一行循环地从第一张表（称为驱动表）中读取行，在这行数据中取到关联字段，根据关联字段在另一张表（被驱动表）里取出满足条件的行，然后取出两张表的结果合集。\n一般关联字段有索引的时候使用这种算法, 示例:\nEXPLAIN select * from t1 inner join t2 on t1.a= t2.a; 从执行计划中可以看到这些信息：\n驱动表是 t2，被驱动表是 t1。先执行的就是驱动表；优化器一般会优先选择小表做驱动表，用where条件过滤完驱动表，然后再跟被驱动表做关联查询。所以使用 inner join 时，排在前面的表并不一定就是驱动表 当使用left join时，左表是驱动表，右表是被驱动表，当使用right join时，右表时驱动表，左表是被驱动表 使用了 NLJ算法。一般 join 语句中，如果执行计划 Extra 中未出现 Using join buffer 则表示使用的 join 算法是 NLJ。 上面sql的大致流程如下：\n从表 t2 中读取一行数据（如果t2表有查询过滤条件的，用先用条件过滤完，再从过滤结果里取出一行数据）； 从第 1 步的数据中，取出关联字段 a，到表 t1 中查找； 取出表 t1 中满足条件的行，跟 t2 中获取到的结果合并，作为结果返回给客户端； 重复上面 3 步。 整个过程会读取 t2 表的所有数据(扫描100行)，然后遍历这每行数据中字段 a 的值，根据 t2 表中 a 的值索引扫描 t1 表中的对应行(扫描100次 t1 表的索引，1次扫描可以认为最终只扫描 t1 表一行完整数据，也就是总共 t1 表也扫描了100行)。因此整个过程扫描了 200 行\n基于块的嵌套循环算法 当关联字段没有没有索引的时候会使用这种算法\n把驱动表的数据读入到 join_buffer 中，然后扫描被驱动表，把被驱动表每一行取出来跟 join_buffer 中的数据做对比。\n如下:\nEXPLAIN select * from t1 inner join t2 on t1.b= t2.b; Extra 中 的Using join buffer (Block Nested Loop)说明该关联查询使用的是 BNL 算法。\n上面sql的大致流程如下：\n把 t2 的所有数据放入到 join_buffer 中 把表 t1 中每一行取出来，跟 join_buffer 中的数据做对比 返回满足 join 条件的数据 整个过程对表 t1 和 t2 都做了一次全表扫描，因此扫描的总行数为10000(表 t1 的数据总量) + 100(表 t2 的数据总量) = 10100。并且 join_buffer 里的数据是无序的，因此对表 t1 中的每一行，都要做 100 次判断，所以内存中的判断次数是 100 * 10000= 100 万次。\n这个例子里表 t2 才 100 行，要是表 t2 是一个大表，join_buffer 放不下怎么办呢？·\njoin_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t2 的所有数据话，策略很简单，就是分段放。\n比如 t2 表有1000行记录， join_buffer 一次只能放800行数据，那么执行过程就是先往 join_buffer 里放800行记录，然后从 t1 表里取数据跟 join_buffer 中数据对比得到部分结果，然后清空 join_buffer ，再放入 t2 表剩余200行记录，再次从 t1 表里取数据跟 join_buffer 中数据对比。所以就多扫了一次 t1 表。\n为什么要使用 BNLJ 算法呢? 如果上面第二条sql使用 Nested-Loop Join，那么扫描行数为 100 * 10000 = 100万次，这个是磁盘扫描。\n很显然，用BNL磁盘扫描次数少很多，相比于磁盘扫描，BNL的内存计算会快得多。\n因此MySQL对于被驱动表的关联字段没索引的关联查询，一般都会使用 BNL 算法。如果有索引一般选择 NLJ 算法，有索引的情况下 NLJ 算法比 BNL算法性能更高\n对于关联sql的优化 关联字段加索引，让mysql做join操作时尽量选择NLJ算法，驱动表因为需要全部查询出来，所以过滤的条件也尽量要走索引，避免全表扫描，总之，能走索引的过滤条件尽量都走索引\n小表驱动大表，写多表连接sql时如果明确知道哪张表是小表可以用straight_join写法固定连接驱动方式，省去mysql优化器自己判断的时间\nstraight_join解释：straight_join功能同join类似，但能让左边的表来驱动右边的表，能改表优化器对于联表查询的执行顺序。\n比如：select * from t2 straight_join t1 on t2.a = t1.a; 代表指定mysql选着 t2 表作为驱动表。\nstraight_join只适用于inner join，并不适用于left join，right join。（因为left join，right join已经代表指定了表的执行顺序）\n尽可能让优化器去判断，因为大部分情况下mysql优化器是比人要聪明的。使用straight_join一定要慎重，因为部分情况下人为指定的执行顺序并不一定会比优化引擎要靠谱。\n小表的定义 在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。不单单是表的总数据量\nin和exsits优化 原则：小表驱动大表，即小的数据集驱动大的数据集\nin：当B表的数据集小于A表的数据集时，in优于exists select * from A where id in (select id from B) #等价于： for(select id from B){ select * from A where A.id = B.id } exists：当A表的数据集小于B表的数据集时，exists优于in 将主查询A的数据，放到子查询B中做条件验证，根据验证结果（true或false）来决定主查询的数据是否保留\nselect * from A where exists (select 1 from B where B.id = A.id) #等价于: for(select * from A){ select * from B where B.id = A.id } #A表与B表的ID字段应建立索引 关于Exists EXISTS (subquery)只返回TRUE或FALSE,因此子查询中的SELECT * 也可以用SELECT 1替换,官方说法是实际执行时会忽略SELECT清单,因此没有区别\nEXISTS子查询的实际执行过程可能经过了优化而不是我们理解上的逐条对比\nEXISTS子查询往往也可以用JOIN来代替，何种最优需要具体问题具体分析\ncount 查询优化 有下面四条查询语句\nEXPLAIN select count(1) from employees; EXPLAIN select count(id) from employees; EXPLAIN select count(name) from employees; EXPLAIN select count(*) from employees; 只有 count(字段名) 不会把该字段为null 计入总数\n其实上面四条的查询计划都一样，效率上没有太大的差别\n当字段有索引 count(*)≈count(1)\u0026gt;count(字段)\u0026gt;count(主键 id)\n字段有索引，count(字段)统计走二级索引，二级索引存储数据比主键索引少，所以count(字段)\u0026gt;count(主键 id)\n当字段没有索引 count(*)≈count(1)\u0026gt;count(主键 id)\u0026gt;count(字段)\n字段没有索引count(字段)统计走不了索引，count(主键 id)还可以走主键索引，所以count(主键 id)\u0026gt;count(字段)\ncount(1) count(1)跟count(字段)执行过程类似，不过count(1)不需要取出字段统计，就用常量1做统计，count(字段)还需要取出字段，所以理论上count(1)比count(字段)会快一点。\ncount(*) count(*) 是例外，mysql并不会把全部字段取出来，而是专门做了优化，不取值，按行累加，效率很高，所以不需要用count(列名)或count(常量)来替代 count(*)。\n为什么对于count(id)，mysql最终选择辅助索引而不是主键聚集索引？因为二级索引相对主键索引存储数据更少，检索性能应该更高，mysql内部做了点优化(应该是在5.7版本才优化)。\n常见优化方法 自己维护的总行数\nshow table status\n如果只需要知道表总行数的估计值可以用如下sql查询，性能很高\nshow table status like 'employee'\n将总数维护到Redis里\n插入或删除表数据行的时候同时维护redis里的表总行数key的计数值(用incr或decr命令)，但是这种方式可能不准，很难保证表操作和redis操作的事务一致性\n索引设计原则 索引设计原则\n1、代码先行，索引后上\n等到主体业务功能开发完毕，把涉及到该表相关sql都要拿出来分析之后再建立索引。\n2、联合索引尽量覆盖条件\n比如可以设计一个或者两三个联合索引(尽量少建单值索引)，让每一个联合索引都尽量去包含sql语句里的where、order by、group by的字段，还要确保这些联合索引的字段顺序尽量满足sql查询的最左前缀原则。\n3、不要在小基数字段上建立索引\n索引基数是指这个字段在表里总共有多少个不同的值，比如一张表总共100万行记录，其中有个性别字段，其值不是男就是女，那么该字段的基数就是2。\n如果对这种小基数字段建立索引的话，还不如全表扫描了，因为你的索引树里就包含男和女两种值，根本没法进行快速的二分查找，那用索引就没有太大的意义了。\n一般建立索引，尽量使用那些基数比较大的字段，就是值比较多的字段，那么才能发挥出B+树快速二分查找的优势来。\n4、长字符串我们可以采用前缀索引\n尽量对字段类型较小的列设计索引，比如说什么tinyint之类的，因为字段类型较小的话，占用磁盘空间也会比较小，此时你在搜索的时候性能也会比较好一点。\n当然，这个所谓的字段类型小一点的列，也不是绝对的，很多时候你就是要针对varchar(255)这种字段建立索引，哪怕多占用一些磁盘空间也是有必要的。\n对于这种varchar(255)的大字段可能会比较占用磁盘空间，可以稍微优化下，比如针对这个字段的前20个字符建立索引，就是说，对这个字段里的每个值的前20个字符放在索引树里，类似于 KEY index(name(20),age,position)\n此时你在where条件里搜索的时候，如果是根据name字段来搜索，那么此时就会先到索引树里根据name字段的前20个字符去搜索，定位到之后前20个字符的前缀匹配的部分数据之后，再回到聚簇索引提取出来完整的name字段值进行比对。\n但是假如你要是order by name，那么此时你的name因为在索引树里仅仅包含了前20个字符，所以这个排序是没法用上索引的， group by也是同理\n5、where与order by冲突时优先where\n在where和order by出现索引设计冲突时，到底是针对where去设计索引，还是针对order by设计索引？到底是让where去用上索引，还是让order by用上索引?\n一般这种时候往往都是让where条件去使用索引来快速筛选出来一部分指定的数据，接着再进行排序。\n因为大多数情况基于索引进行where筛选往往可以最快速度筛选出你要的少部分数据，然后做排序的成本可能会小很多。\n举个例子 有 employees表，name, age, sex, position 列， 有联合索引 （name, age, sex, position）,\nsex : 性别，取值0 或1\n有如下查询: select id from employees where name = 'zhangsan' and age = 18 and position = 'dev' 因为跳过了 sex 字段，position 无法利用索引\n因为 sex 只有两个取值，我们在查询语句上把 sex 的值全部枚举出来， 如下:\nselect id from employees where name = 'zhangsan' and age = 18 and sex in (0, 1) and position = 'dev'\n这样一来就可以利用全部索引了。\n再举个例子 加入我们要查询最近一周登录的用户，首先想到的是 last_login_time \u0026gt; {一周之前的时间}\n这是一个范围查询，在后面的所有字段便无法利用索引了，我们可以再设计一个字段，recent_login_flag(tinyint) 标识是否最近登录过。用定时任务定期更新该字段的值。这样就由范围查询变成了等值查询，数据可能不是太及时变化，就看业务是否允许了。\n总之就是想办法最大限度的利用索引。\n","date":"2022-08-03T17:10:57Z","permalink":"https://dccmmtop.github.io/posts/%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96/","section":"posts","tags":["MySQL","索引"],"title":"索引优化"},{"categories":null,"contents":"大体来说，MySQL可以分为 server 层和存储引擎层两部分，如下图\nServer层 主要包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数 （如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\nStore层 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。也就是说如果我们在create table时不指定表的存储引擎类型,默认会给你设置存储引擎为InnoDB。\n连接器 顾名思义，他主要与客户端连接打交道\n连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：\nmysql ‐h host[数据库地址] ‐u root[用户] ‐p root[密码] ‐P 3306 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份， 这个时候用的就是你输入的用户名和密码。\n1.如果用户名或密码不对，你就会收到一个\u0026quot;Access denied for user\u0026quot;的错误，然后客户端程序结束执行。\n2.如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。\n这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。用户的权限表在系统表空间的mysql的user表中。\n说到这里就有一个问题，为什么MySQL的权限不做成实时生效的呢？ 答案只有一个—— 为了性能\n来看一下 MySQL 系统用户表:\nselect Host,User,Password from user; 可以直接修改表中的数据来修改某个用户的权限\n长连接和短连接 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。\n短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。\n开发当中我们大多数时候用的都是长连接,把连接放在Pool内进行管理，但是长连接有些时候会导致 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。\n怎么解决这类问题呢\n定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存 参见查询缓存\n分析器 如果没有命中查询缓存，就要开始真正的执行语句了，但是 MySQL 怎么知道你要查询的是哪张表格， 哪个字段，条件是什么呢？\n这就是分析器大显身手的时候了，他会分析我们的 sql 语句，把你要查询的表，字段 和条件等都解析出来，形成特殊的结构，方便后续操作\n如果 sql 语法不对，就会得到 \u0026ldquo;You have an error in your SQL syntax\u0026rdquo; 的错误提醒, 如下 from 错误的写成 form\nmysql\u0026gt; select * fro test where id=1; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds t o your MySQL server version for the right syntax to use near \u0026#39;fro test where id=1\u0026#39; at line 1 但是分析器是如何解析sql语句的呢？ 底层是怎么工作的呢？\n词法分析器原理 词法分析器分成6个主要步骤完成对sql语句的分析\n词法分析 语法分析 语义分析 构造执行树 生成执行计划 计划的执行 SQL语句的分析分为词法分析与语法分析，mysql的词法分析由MySQLLex[MySQL自己实现的]完成，语法分析由Bison生成。关于语法树大家如果想要深入研究可以参考这篇wiki文章\n这里给出一个解析后的语法树，供参考\n优化器 经历过分析器后，MySQL 就知道自己需要做什么了， 但是程序员写出的 sql 语句可能不是最优的，这时，优化器可以对一些 sql 语句做出优化，不改变查询结果的前提下，使查询更高效\n还可以决定某条 sql 使用那个索引查询更快。 或者会决定表关联的顺序 等等\n有如下例子:\nselect * from test1 join test2 using(ID) where test1.name=\u0026#39;张珊\u0026#39; and test2.name=\u0026#39;莉丝\u0026#39;; 既可以先从表 test1 里面取出 name=\u0026lsquo;张珊\u0026rsquo; ID 值，再根据 ID 值关联到表 test2，再判断 test2 里面 name的值是否等于 \u0026lsquo;莉丝\u0026rsquo;\n也可以先从表 test2 里面取出 name=\u0026lsquo;莉丝\u0026rsquo; 的记录的 ID 值，再根据 ID 值关联到 test1，再判断 test1 里面 name 的值是否等于 \u0026lsquo;张珊\u0026rsquo;\n这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段\n执行器 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在 工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权 限)。\n要注意区分 连接器 中使用的权限， 连接器中的权限使用户级别的，而执行器中的权限使表级别的\nselect * from test where id=10; 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n比如我们这个例子中的表 test 中，ID 字段没有索引，那么执行器的执行流程是这样的：\n调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接 口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。你会在数据库的慢查询日志中看到一个rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的\nbin_log 的使用 经常听到删库跑路的消息，其实删除库之后也不用跑路，MySQL 会把我们执行的每条SQL都记录到 bin-log中， 那么什么是 bin-log 呢？\nbinlog是Server层实现的二进制日志,他会记录我们的cud操作。Binlog有以下几个特点：\nBinlog在MySQL的Server层实现（引擎共用） Binlog为逻辑日志,记录的是一条语句的原始逻辑 Binlog不限大小,追加写入,不会覆盖以前的日志 如果，我们误删了数据库,可以使用binlog进行归档!要使用binlog归档，首先我们得记录binlog，因此需要先开启MySQL的binlog功能。\n配置my.cnf log-bin=/usr/local/mysql/data/binlog/mysql-bin # 注意5.7以及更高版本需要配置本项：（自定义,保证唯一性)\r# server-id=123454\r#binlog格式，有3种statement,row,mixed binlog-format=ROW #表示每1次执行写入就与硬盘同步，会影响性能，为0时表示，事务提交时mysql不做刷盘操作，由系统决定\rsync-binlog=1 binlog命令 查看bin-log是否开启\nshow variables like \u0026#39;%log_bin%\u0026#39;; 会多一个最新的bin-log日志\nflush logs; 查看最后一个bin-log日志的相关信息\nshow master status; 清空所有的bin-log日志\nreset master; 查看binlog内容\n/usr/local/mysql/bin/mysqlbinlog --no-defaults /usr/local/mysql/data/binlog/mysql-bin.000001 数据恢复 恢复全部数据 /usr/local/mysql/bin/mysqlbinlog --no-defaults /usr/local/mysql/data/binlog/mysql-bin.000001 |mysql -uroot -p test # test 是数据库名 恢复指定时间段数据 /usr/local/mysql/bin/mysqlbinlog --no-defaults /usr/local/mysql/data/binlog/mysql-bin.000001 --stop-date= \u0026#34;2018-03-02 12:00:00\u0026#34; --start-date= \u0026#34;2019-03-02 11:55:00\u0026#34;|mysql -uroot -p test 恢复指定位置数据 /usr/local/mysql/bin/mysqlbinlog --no-defaults --start-position=\u0026#34;408\u0026#34; --stop-position=\u0026#34;731\u0026#34; /usr/local/mysql/data/binlog/mysql-bin.000001 |mysql -uroot -p test \u0026ndash;start-position = \u0026ldquo;408\u0026rdquo; \u0026ndash;start-position = \u0026lsquo;\u0026lsquo;731\u0026quot;\n怎么找到呢？\n我们需要使用工具查看bin-log信息:\n/usr/local/mysql/bin/mysqlbinlog --no-defaults /usr/local/mysql/data/binlog/mysql-bin.000001 --stop-date= \u0026#34;2018-03-02 12:00:00\u0026#34; --start-date= \u0026#34;2019-03-02 11:55:00\u0026#34;|mysql -uroot -p test 信息如下:\n由此便可以恢复指定位置或日期的数据了.\n","date":"2022-08-01T22:20:58Z","permalink":"https://dccmmtop.github.io/posts/mysql%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/","section":"posts","tags":["MySQL"],"title":"MySQL内部结构"},{"categories":null,"contents":"在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返回执行计划的信息，而不是 执行这条SQL\n注意：如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中\nexplain 两个变种 explain extended：\n会在 explain 的基础上额外提供一些查询优化的信息。紧随其后通过 show warnings 命令可\n以得到优化后的查询语句，从而看出优化器优化了什么。额外还有 filtered 列，是一个半分比的值，rows * filtered/100 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的 表）。 explain extended select * from film where id = 1; show warnings; explain partitions：\n相比 explain 多了个 partitions 字段，如果查询是基于分区表的话，会显示查询将访问的分 区。 explain中的列 现有如下表:\nDROP TABLE IF EXISTS `actor`; CREATE TABLE `actor` ( `id` int(11) NOT NULL, `name` varchar(45) DEFAULT NULL, `update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,\u0026#39;a\u0026#39;,\u0026#39;2017‐12‐22 15:27:18\u0026#39;), (2,\u0026#39;b\u0026#39;,\u0026#39;2017‐12‐22 15:27:18\u0026#39;), (3,\u0026#39;c\u0026#39;,\u0026#39;2017‐12‐22 15:27:18\u0026#39;); DROP TABLE IF EXISTS `film`; CREATE TABLE `film` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film` (`id`, `name`) VALUES (3,\u0026#39;film0\u0026#39;),(1,\u0026#39;film1\u0026#39;),(2,\u0026#39;film2\u0026#39;); DROP TABLE IF EXISTS `film_actor`; CREATE TABLE `film_actor` ( `id` int(11) NOT NULL, `film_id` int(11) NOT NULL, `actor_id` int(11) NOT NULL, `remark` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_film_actor_id` (`film_id`,`actor_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1); actor 表只有主键索引\nfilm 表有主键索引和name索引\nfilm_actor 有主键索引和 (film_id, actor_id ) 联合索引\n现有如下查询:\nexplain select * from film where id = 2; 结果：\n对结果中的每一列逐个分析:\nid列 id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。\nid列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行\nselect_type 列 select_type 表示对应行是简单还是复杂的查询。\nsimple：简单查询。查询不包含子查询和union primary：复杂查询中最外层的 select subquery：包含在 select 中的子查询（不在 from 子句中） derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含 义） union: 在 union 中的第二个和随后的 select 来看一下 primar, subquery, derived 的场景:\n-- 关闭mysql5.7新特性对衍生表的合并优化 set session optimizer_switch=\u0026#39;derived_merge=off\u0026#39;; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der; 结果:\nunion 场景:\nexplain select 1 union all select 1; 结果:\ntable 列 这一列表示 explain 的一行正在访问哪个表。\n当 from 子句中有子查询时，table列是derivenN 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查 询。\n当有 union 时，UNION RESULT 的 table 列的值为union1,2，1和2表示参与 union 的 select 行id。\n⭐type 列 这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。\n依次从最优到最差分别为：**system \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; range \u0026gt; index \u0026gt; ALL **\n**一般来说，得保证查询达到range级别，最好达到ref **\nNULL NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可 以单独查找索引来完成，不需要在执行时访问表, 如下:\nexplain select min(id) from film; 结果:\nconst, system const, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是 const的特例，表里只有一条数据匹配时为system\n例子:\nexplain extended select * from (select * from film where id = 1) tmp; 结果:\neq_ref primary key 或 unique key 索引被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。\n例子:\nexplain select * from film_actor left join film on film_actor.film_id = film.id; 结果:\nef 相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会 找到多个符合条件的行。\n简单 select 查询，name是普通索引（非唯一索引） explain select * from film where name = \u0026#39;film1\u0026#39;; 2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。\nexplain select film_id from film left join film_actor on film.id = film_actor.film_id; range 范围扫描通常出现在 in(), between ,\u0026gt; ,\u0026lt;, \u0026gt;= 等操作中。使用一个索引来检索给定范围的行。\nexplain select * from actor where id \u0026gt; 1; index 扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接 对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这 种通常比ALL快一些。\nexplain select * from film; ALL： 即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了。\nexplain select * from actor; possible_keys列 这一列显示查询可能使用哪些索引来查找。\nexplain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引 对此查询帮助不大，选择了全表查询。\n如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提 高查询性能，然后用 explain 查看效果。\nkey列 这一列显示mysql实际采用哪个索引来优化对该表的访问。\n如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。\nkey_len列 这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。\n举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通 过结果中的key_len=4可推断出查询使用了第一个列：film_id列来执行索引查找。\nexplain select * from film_actor where film_id = 2; key_len计算规则如下： 字符串\nchar(n)和varchar(n)，5.0.3以后版本中，n均代表字符数，而不是字节数，如果是utf-8，一个数字 或字母占1个字节，一个汉字占3个字节\nchar(n)：如果存汉字长度就是 3n 字节 varchar(n)：如果存汉字则长度是 3n + 2 字节，加的2字节用来存储字符串长度，因为\nvarchar是变长字符串 **数值类型 **\ntinyint：1字节 smallint：2字节 int：4字节 bigint：8字节 时间类型\ndate：3字节 timestamp：4字节 datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL\n索引是有最大长度限制的 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引。\nref列 这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id）\nrows列 这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。有时会相差很大\nExtra列 这一列展示的是额外信息。常见的重要值如下：\nUsing index：使用覆盖索引 使用了 辅助索引，并且使用了索引覆盖\n覆盖索引定义 ：mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index；覆盖索引一般针对的是辅助索引，整个查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值\n例子:\nexplain select film_id from film_actor where film_id = 1; Using where 使用 where 语句来处理结果，并且查询的列未被索引覆盖\nexplain select * from actor where name = \u0026#39;a\u0026#39;; Using index condition 查询的列不完全被索引覆盖，where条件中是一个前导列的范围；\nexplain select * from film_actor where film_id \u0026gt; 1; Using temporary mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索\n引来优化。\nactor.name没有索引，此时创建了张临时表来distinct explain select distinct name from actor; 2. film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表\nexplain select distinct name from film; Using filesort 将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一 般也是要考虑使用索引来优化的。\nactor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录 explain select * from actor order by name; 2. film.name建立了idx_name索引,此时查询时extra是using index\nexplain select * from film order by name; Select tables optimized away 使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是\nexplain select min(id) from film; 索引最佳实践 有如下示例表:\nCREATE TABLE `employees` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(24) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;姓名\u0026#39;, `age` int(11) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;年龄\u0026#39;, `position` varchar(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;职位\u0026#39;, `hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;入职时间\u0026#39;, PRIMARY KEY (`id`), KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COMMENT=\u0026#39;员工记录表\u0026#39;; INSERT INTO employees(name,age,position,hire_time) VALUES(\u0026#39;LiLei\u0026#39;,22,\u0026#39;manager\u0026#39;,NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES(\u0026#39;HanMeimei\u0026#39;, 23,\u0026#39;dev\u0026#39;,NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES(\u0026#39;Lucy\u0026#39;,23,\u0026#39;dev\u0026#39;,NOW()); employees 表有主键索引和（name, age, postiton） 联合索引\n全值匹配 EXPLAIN SELECT * FROM employees WHERE name= \u0026#39;LiLei\u0026#39;; EXPLAIN SELECT * FROM employees WHERE name= \u0026#39;LiLei\u0026#39; AND age = 22; EXPLAIN SELECT * FROM employees WHERE name= \u0026#39;LiLei\u0026#39; AND age = 22 AND position =\u0026#39;manager\u0026#39;; 由上面3个查询语句和分析结果可知，key_len 字段可以推算出用了哪些索引\n左前缀法则 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。\nEXPLAIN SELECT * FROM employees WHERE name = \u0026#39;Bill\u0026#39; and age = 31; 能用到 name 和 age 索引\nEXPLAIN SELECT * FROM employees WHERE age = 30 AND position = \u0026#39;dev\u0026#39;; 无法利用索引，因为跳过了 name 字段，可以参照索引原理\nEXPLAIN SELECT * FROM employees WHERE position = \u0026#39;manager\u0026#39;; 同样无法利用索引\n不在索引列上做任何操作 如果在索引列上计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描\nEXPLAIN SELECT * FROM employees WHERE name = \u0026#39;LiLei\u0026#39;; EXPLAIN SELECT * FROM employees WHERE left(name, 2) = \u0026#39;Li\u0026#39;; -- 名字的前两位 第二个语句在name列上使用了 left 函数。使索引失效\n存储引擎不能使用索引中范围条件右边的列 EXPLAIN SELECT * FROM employees WHERE name= \u0026#39;LiLei\u0026#39; AND age = 22 AND position =\u0026#39;manager\u0026#39;; EXPLAIN SELECT * FROM employees WHERE name= \u0026#39;LiLei\u0026#39; AND age \u0026gt; 22 AND position =\u0026#39;manager\u0026#39;; 第一条可以利用全部索引 第二条只能使用 (name age) 索引 无法利用 posotion ,因为 position 在 age 右边，且 age 是范围查询， 失效原理参见索引原理\n尽量使用覆盖索引 只访问索引包含的列，减少 select * 的查询，根据二级索引构造可知，如果访问了索引中不包含的列，需要再次扫描主键索引，也就是我们常说的回表\n使用了覆盖索引:\nEXPLAIN SELECT name,age FROM employees WHERE name= \u0026#39;LiLei\u0026#39; AND age = 23 AND position =\u0026#39;manager\u0026#39;; 未使用覆盖索引:\nEXPLAIN SELECT * FROM employees WHERE name= \u0026#39;LiLei\u0026#39; AND age = 23 AND position =\u0026#39;manager\u0026#39;; 非查询 mysql在使用不等于（！=或者\u0026lt;\u0026gt;），not in ，not exists 的时候无法使用索引会导致全表扫描\n\u0026lt; 小于、 \u0026gt; 大于、 \u0026lt;=、\u0026gt;= 这些，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引, 这个并不是铁定的规则\nEXPLAIN SELECT * FROM employees WHERE name != \u0026#39;LiLei\u0026#39;; is null,is not null 一般情况下也无法使用索引 EXPLAIN SELECT * FROM employees WHERE name is null like 左模糊查找 like以通配符开头（\u0026rsquo;$abc\u0026hellip;\u0026rsquo;）mysql索引失效会变成全表扫描操作\n如下:\nEXPLAIN SELECT * FROM employees WHERE name like \u0026#39;%Lei\u0026#39; 如果是使用了右模糊查找，可以利用索引:\nEXPLAIN SELECT * FROM employees WHERE name like \u0026#39;Lei%\u0026#39; ⭐如何解决左模糊索引不生效的问题？ 使用覆盖索引，查询字段必须是建立覆盖索引字段 EXPLAIN SELECT name,age,position FROM employees WHERE name like \u0026#39;%Lei%\u0026#39;; 如果不能使用覆盖索引则可能需要借助搜索引擎 字符串不加单引号索引失效 name 是字符串类型\nEXPLAIN SELECT * FROM employees WHERE name = 1000; 因为MySQL帮我们做了一个类型转换，相当于在 name 列上执行了一个函数。无法利用索引\n少用or或in 用or或in，用它查询时，mysql不一定使用索引，mysql内部优化器会根据检索比例、表大小等多个因素整体评 估是否使用索引，详见范围查询优化\n表数据量太少时，且无法利用覆盖索引，不会走二级索引，因为还需要回表操作，MySQL 认为还不如直接查全表来的快，反正全表也没有多少数据, 如下:\nEXPLAIN SELECT * FROM employees WHERE name = \u0026#39;LiLei\u0026#39; or name = \u0026#39;HanMeimei\u0026#39;; 范围查询优化 先给 age 字段添加一个单索引\nALTER TABLE `employees` ADD INDEX `idx_age` (`age`) USING BTREE; 再执行一下查询\nexplain select * from employees where age \u0026gt;=1 and age \u0026lt;=2000; 发现并没有走索引，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引。比如这个例子，可能是由于单次数据量查询过大导致优化器最终选择不走索引\n优化方法：可以将大的范围拆分成多个小范围, 如下\nexplain select * from employees where age \u0026gt;=1 and age \u0026lt;=1000; explain select * from employees where age \u0026gt;=1001 and age \u0026lt;=2000; 索引总结 like KK%相当于=常量，%KK和%KK% 相当于范围\n","date":"2022-08-01T10:05:55Z","permalink":"https://dccmmtop.github.io/posts/explain%E8%AF%A6%E8%A7%A3/","section":"posts","tags":["MySQL"],"title":"Explain详解"},{"categories":null,"contents":"索引是什么 索引是帮助MySQL高效获取数据的排好序的数据结构\n最重要的点是有序的，我们用索引就是为了快速的查找数据，如果一堆数据是无序的，程序只能挨个遍历每个元素，对比值，才能找到某个元素，最坏的情况要比对N次， N 是这一堆数据的长度。如果数据是有序的，我们就可以使用二分查找算法，他的时间复杂度是 O(long N)，效率比直接挨个查找快的多。\n二分查找算法关键步骤就是找到区间的中间值，然后确定要查找的值落在左区间还是右区间，一直重复这个步骤直到找到该值。于是就可以将这种查询方法映射成一种数据结构——树。我们规定一种树，有左节点，右节点，和当前节点。并且左节点 \u0026lt; 当前节点 \u0026lt; 右节点 . 如下图所示:\n由于树具有方便快速查找的特性，我们一般都会使用树结构去存储索引，并对简单的查找二叉树做了很多优化，比如 红黑树，平衡二叉树， B 树 B+树\n树的构建，删除， 查找都有一定的算法，这里不详细描述，只需知道树有一个通用的特性：树的高度越低，查找效率越高\n所以索引的构建 ， 本质上是控制树的高度\n索引数据结构 二叉树： 红黑树 Hash 表 B Tree 树形索引 表中的数据与索引结构映射关系可以理解如下图:\n加入要找到 col2 = 23 的记录，如果不使用索引，我们需要对整张表扫描，从 34 -\u0026gt; 77 -\u0026gt; 5 -\u0026gt; 91 -\u0026gt; 22 -\u0026gt; 89 -\u0026gt; 23, 需要对比7次才能找到\n使用索引时， 查找路径时是 34 -\u0026gt; 22 -\u0026gt; 23 只需对比3次就行。在表中数据量极大时，差别更明显\n树的动画 推荐一个在线工具，它以动画的形式描述了每种树的构建与查找方法\n为什么不是简单的二叉树？ 我们知道MySQL索引采用的是 B+树，那么为什么不是其他的树呢？\n因为在顺序插入下，树的高度会一直增加，等同于链表。无法控制树的高度，如下图:\n如果需要查找6，仍然需要查找6次\n为什么不是红黑树？ 红黑树（平衡二叉树）： 虽然会自动平衡节点位置，但仍然高度不可控。表比较大时会导致树的高度很高。增加查找次数\n为什么最终选择B+树 而不是B树 要解决这个疑问，我们需要知道这两种树的构造，如下图\nB Tree:\nB + Tree:\n水平方向可以存放更多的索引key B+树将数据全部放到叶子节点，留下更多的空间放 key, key 越多，宽度越宽，同样的数据量，宽度越大，高度越小。查找次数就越小。\n为什么需要 扩展树的宽度而不是树的深度呢？\n如果按照上面的说法，我们拓宽了树的宽度，减少了树的高度，但是比较次数并没有发生改变，只不过是减少了纵向的比较，增加了横向的比较\n这个疑问的前提是所有的数据都在内存中，直接在内存中进行比较大小。 但是事实并非如此，不可能把表中的所有数据都加到内存中，必须先从磁盘中加在一部分数据到内存，然后在内存中比较大小，内存中运算的速度远远大于从磁盘加载数据的速度。磁盘加载数据是机械运动，需要电机带动磁针转圈扫描磁道。内存运算则是电子运动，不可同日而语。\n数据从磁盘加载到内存中，是有最小单位的，这个单位是 页， 不是 字节或者 位， 页是固定字节数据，由操作系统决定，这样可以减少加载磁盘的次数。\n由于B Tree 的每一层都已经是有序的，我们把树中水平方向的数据放在磁盘相邻的地方，每次从磁盘加载一页数据时，便可以得到部分或全部的水平方向的结点，不用再次排序。\n在水平方向在内存中使用二分查找的效率远远大于从磁盘中加载一页数据， 所以我们希望树越宽越好,这样一次性加载的数据就越多，而不是越高越好\n对于B+ 树，我们假设要查找50这个数据，先从根节点即(15 56 77) 这些数据中找到50所处的范围，因为 (15 56 77) 已经是有序的，可以根据二分查找算法找到 50 处于 15\u0026ndash;56之间，\n然后加载 15 所指向的下一页数据 （15 20 49）,再次根据二分查找算法，找到50处于 49之后，再从磁盘加载49所指向的数据页，找到50\n数据量估算 MySQL 自己也有一个逻辑 页，一般是操作系统中 页 的整数倍，这个逻辑页的数据可以通过配置修改，但是不建议，MySQL 是经过大量的测试，为我们定义了一个合理的默认值 16Kb\n可以通过下面语句查询：\nshow global status like \u0026#39;Innodb_page_size\u0026#39; 假设上图中表示的是主键索引，类型是 bigint, 占 8 个字节。指向下一页的指针占 6 个字节， 那么这一页可以存放 16 * 1024 / (8 + 6) = 1170 个key, 同理第二页即 （15 20 49 \u0026hellip;.） 也可以放 1170 个key , 对于第三页，也就是叶子节点，包含了主键和对应整行的数据。就按照一行数据放1KB 吧(已经比较大了) 能放 16 行，那么只有一页根节点的话， 这个索引索引树能放 1170 * 1170 * 16 =21,902,400 行数据。 这棵树的高度只有3，就已经能支持上千万的数据量了。也就是只需加载3次磁盘就可以查找到数据了。并且MySQL 存放根节点的页还有优化，可能会把这个页常驻内存。\n叶子节点包含所有的索引字段 如上图所示，在主键索引中，叶子节点包含了表中的所有字段，对于一些全表扫描的查询来说，直接扫描叶子节点便可以得到数据，不用再从索引树上挨个查找\n叶子节点直接包含双向指针,范围查找效率高 对于一些范围查询比如 id \u0026gt; 20 and id \u0026lt; 50, 在索引树上定位到 20 之后直接使用右向指针定位到下一个比20大的数据，依次往下，直到 50，便可以检出该区间的数据，如果没有这个指针，（B Tree）则需要再次回到索引树中去查找 , 极大的提高了范围查找的性能\nHash 索引 hash 索引原理如下：\n更快 大多情况下 Hash 索引比B+ Tree 索引更快，Hash 计算的效率非常高，且仅需一次查找就可以定位到数据(无hash冲突的情况)\n不支持范围查询 图中有些歧义，Hash 后的值是没有顺序的，也不是整数，所以无法进行高效的范围查询查询\nhash 冲突问题 如果在某列上有很多相同的行，比如 name 字段，叫 张三的人非常多。会产生很多次hash冲突，只能退化成列表搜索了\n表引擎 我们常说的 MyISAM 引擎 或者 InnoDB 引擎是基于表的，是表的一个属性， 可不是基于数据库的， 同一个数据库中可以有不同引擎的表\nMyISAM 和 InnoDB 引擎 不同引擎的表在磁盘中产生的文件也不一样，数据库文件位置默认在安装目录/data 下\nMyISAM 引擎 frm: 表结构相关, frame（框架） 缩写` MYD: MyISAM Data 表数据 MYI: MyISAM Index 表索引 索引结构中的叶子节点的 data 存放的是 数据行的位置，及这一行在 MYD 文件的位置， 而不是直接放的真实数据\nInnoDB frm 表结构信息 ibd 表数据加索引 表数据组织形式 表结构本身就是按照 B+ Tree 结构存储， 叶子节点放的是出索引列其他列的数据\n聚集与非聚集索引 聚集索引 (InnoDB 主键索引)\n叶子节点直接包含整行数据\n非聚集索引 (MyISAM 索引, InnoDB 非主键索引)\n叶子节点不包含整行数据,包含的是对应行所在的位置，或者主键Id\n单从索引结构的来看，聚集索引的查找速度高于非聚集索引\nInnoDB 只有一个聚集索引，默认是主键索引， 非主键索引的叶子节点存放的是主键的值，如下图\n这样做的目的有两个：\n节约空间，避免将整行的数据存放多份 保证数据的一致性，否则每增加一行，对应的每个索引都要维护一份行数据。必须要等到每个索引都更新完，数据才能插入成功 ★★★ 为什么建议InnoDB 表必须有主键，并且是整型自增的？ InnoDB 整个表的数据就是用B+ 树组织的，如果存在主键，就用主键为索引，叶子节点存储行数据\n如果没有主键，InnoDB 就会找到一个每行数据都不相同的列作为索引来组织整个表的数据\n如果没有找到这种列，就会建一个隐藏的列，自动维护值，用这个隐藏的列来组织数据，所以我们要主动做这种工作减少数据库的负担\n为什么是整型 因为在查找数据的过程中，需要多次比较大小，整型的比较运算速度大于字符串， 并且占用空间小\n为什么是自增 这一点涉及到B+ 树的构建，我们知道索引一个最重要的特性就是排好序 的。如果我们不是顺序插入的，那么树就要自己额外做排序，调整树结构，浪费了性能\n避免叶子节点的分裂 避免B+ 树做平衡调整 联合索引 联合索引和单索引差不多，只不过是先按第一个字段排序，再按第二个字段排序，然后再按第三个字段排序。\n这种排序规则表明了只有在第一个字段相等的情况下，第二字段才是有序的。第二字段相等的情况下，第三个字段才是有序的。\n所以 name = 'Bill' and age = 20 and position = 'dev' 可以用到全部索引， 因为 name 确定了，age 是有序的，age 可以走索引， age 确定后 position 可以走索引。这个联合索引可以全部用到\n如果是 name = 'Bill and age \u0026gt; 30 and position = 'dev'' , 首先name 可以走索引，name 确定后 age 是有序的，age 也可以走索引，但是 age \u0026gt; 30 导致 age 查出来的数据有多个（31 32）, 31 和 32 下的 position (dev admin ) 不是有序的，便无法利用二分算法进行查找。所以无法利用 position 这个索引，这也就是左前缀法则的原理和联合索引失效的原理\n","date":"2022-07-31T23:48:52Z","permalink":"https://dccmmtop.github.io/posts/%E7%B4%A2%E5%BC%95%E7%9A%84%E6%9C%AC%E8%B4%A8/","section":"posts","tags":["索引","MySQL"],"title":"索引的本质"},{"categories":null,"contents":"需求 最近想实现一个自动部署惊天博客的功能，我有一个静态博客项目,是使用hugo进行编译和部署的，之前自己写了一个脚本将变动的博客自动编译部署到github page 上，也不是很麻烦。但是需要在本机执行一次命令，没有完全自动化，以前了解过github action的功能，可以在某个分支提交代码时触发一个任务，很适合我这个场景，今天来尝试一下。\ngithub action 其实就是设置一个触发条件，然后github提供一个运行环境去执行我们实现定义好的程序，每次执行这个任务时，所给的环境都是崭新的，不保存数据。并且任务的执行时长和一天内的任务执行次数是有限制的。不然早就被薅秃了。\n我的需求是，当我在博客项目main 分支 推送代码时，触发github action 执行 hugo --minify 将markdown 文件编译成静态的 html 文件，然后推送到我的github page 上，完成部署。锁执行的任务就只有三步——下载代码，编译和部署。\n先说编译，这一步是需要用到hugo命令的，github action 给我们提供的环境肯定时没有这个命令的，我们需要下载安装，非常棒的是，github 收录了开发者已经写好的 action ，我们可以直接拿来用就好了，这个仓库中就有 hugo 相关的action—— peaceiris/actions-hugo@v2\n同理， 部署步骤也有人提供了对应的action，我们也是直接拿来用就好了。—— peaceiris/actions-gh-pages@v3\n编译之前其实还有一步，那就是下载代码，在一个全新的环境中，如果没有代码，难道要编译空气？下载代码肯定离不开 git 工具，难道要我们自己装一个 git ? 这倒不用自己做，也有现成的 action —— actions/checkout@v2\n实现 我们的需求和步骤已经梳理完了，下面看怎么操作吧\n在项目根目录下新建 .github/workflows/pages.yml 文件， 其中 yml 文件是可以随意命名的，但路径是固定的。 编写page.yml name: dcblog_action # 名字 on: # 触发条件 push: # 有推送动作时触发 branchesjkj: - main # 这里的意思是当 main分支发生push的时候，运行下面的jobs jobs: # 要执行的任务，可以时多个 deploy: # 任务名 runs-on: ubuntu-18.04 # 在什么环境运行任务 steps: - uses: actions/checkout@v2 # 引用actions/checkout这个action，与所在的github仓库同名 with: submodules: true # Fetch Hugo themes (true OR recursive) 获取submodule主题 fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo # 步骤名自取 uses: peaceiris/actions-hugo@v2 # hugo官方提供的action，用于在任务环境中获取hugo with: hugo-version: \u0026#39;latest\u0026#39; # 获取最新版本的hugo - name: Build run: hugo --minify # 使用hugo构建静态网页 - name: Deploy uses: peaceiris/actions-gh-pages@v3 # 一个自动发布github pages的action with: external_repository: dccmmtop/dccmmtop.github.io # 发布到哪个repo personal_token: xxxx # 发布到其他repo需要提供上面生成的personal access token publish_dir: ./public # 注意这里指的是要发布哪个文件夹的内容，而不是指发布到目的仓库的什么位置，因为hugo默认生成静态网页到public文件夹，所以这里发布public文件夹里的内容 publish_branch: master # 发布到哪个branch personal_token 可以去你的github setting 中获取,记得保密.\n验证 下面在main 分支上推送一次代码，可以在github action 标签页下看到action 运行成功的标识，以及日志:\n","date":"2022-07-30T23:01:00Z","permalink":"https://dccmmtop.github.io/posts/github_action%E4%BD%BF%E7%94%A8/","section":"posts","tags":["git"],"title":"github_Action使用"},{"categories":null,"contents":"数字性循环 #!/bin/bash for((i=1;i\u0026lt;=10;i++)); do echo $(expr $i \\* 3 + 1); done #!/bin/bash for i in $(seq 1 10) do echo $(expr $i \\* 3 + 1); done #!/bin/bash for i in {1..10} do echo $(expr $i \\* 3 + 1); done #!/bin/bash awk \u0026#39;BEGIN{for(i=1; i\u0026lt;=10; i++) print i}\u0026#39; 字符性循环 #!/bin/bash for i in `ls`; do echo $i is file name\\! ; done #!/bin/bash for i in $* ; do echo $i is input chart\\! ; done #!/bin/bash for i in f1 f2 f3 ; do echo $i is appoint ; done #!/bin/bash list=\u0026#34;rootfs usr data data2\u0026#34; for i in $list; do echo $i is appoint ; done 路径查找 #!/bin/bash for file in /proc/*; do echo $file is file path \\! ; done #!/bin/bash for file in $(ls *.sh) do echo $file is file path \\! ; done 现在一般都使用for in结构，for in结构后面可以使用函数来构造范围，比如$()、``这些，里面写一些查找的语法，比如ls test*，那么遍历之后就是输出文件名了。\n","date":"2022-07-28T18:26:36Z","permalink":"https://dccmmtop.github.io/posts/linux%E4%B8%8B%E7%9A%84%E5%BE%AA%E7%8E%AF/","section":"posts","tags":["linux"],"title":"Linux下的循环"},{"categories":null,"contents":"package main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; ) func main() { filename := \u0026#34;./1.txt\u0026#34; f, err := os.Open(filename) if err != nil { fmt.Printf(\u0026#34;read %s fail, err: %v\\n\u0026#34;, filename, err) } defer f.Close() reader := bufio.NewReader(f) for { line, _, err := reader.ReadLine() if err == io.EOF { fmt.Println(\u0026#34;done!\u0026#34;) break } fmt.Println(line) // TODO } } ","date":"2022-07-28T09:03:18Z","permalink":"https://dccmmtop.github.io/posts/go%E9%80%90%E8%A1%8C%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6/","section":"posts","tags":["go"],"title":"Go逐行读取文件"},{"categories":null,"contents":"package model import ( \u0026#34;os\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; ) func InitLogger(level string) *zap.SugaredLogger { encoder := getEncoder() // 同时输出到文件和控制台 core := zapcore.NewTee( zapcore.NewCore(encoder, stdWriter(), getLogLevel(level)), zapcore.NewCore(encoder, fileWriter(), getLogLevel(level)), ) return zap.New(core, zap.AddCaller()).Sugar() } func getLogLevel(level string) zapcore.LevelEnabler { switch level { case \u0026#34;DEBUG\u0026#34;: return zapcore.DebugLevel case \u0026#34;INFO\u0026#34;: return zapcore.InfoLevel case \u0026#34;WARN\u0026#34;: return zapcore.WarnLevel case \u0026#34;ERROR\u0026#34;: return zapcore.ErrorLevel default: return zapcore.InfoLevel } } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalColorLevelEncoder return zapcore.NewConsoleEncoder(encoderConfig) } func stdWriter() zapcore.WriteSyncer { return zapcore.AddSync(os.Stdout) } func fileWriter() zapcore.WriteSyncer { file, _ := os.Create(fmt.Sprintf(\u0026#34;./%d_log_name.log\u0026#34;, time.Now().Unix())) return zapcore.AddSync(file) } ","date":"2022-07-27T09:17:52Z","permalink":"https://dccmmtop.github.io/posts/zap%E5%8C%85%E4%BD%BF%E7%94%A8/","section":"posts","tags":["zap","log","go"],"title":"zap包使用"},{"categories":null,"contents":"每个RabbitMQ 节点只有两种类型，要么是内存节点，要么是磁盘节点\n内存节点 内存节点将所有的队列、交换器、绑定、用户、权限和vhost的元数据定义都仅存储在内存中.\n显而易见，内存节点拥有更出色的性能\n磁盘节点 磁盘节点则将元数据存储在磁盘中,重启之后，队列数据仍然存在。但是速度远远落后于内存节点。\n单节点系统只允许磁盘类型的节点；否则，每次你重启RabbitMQ之后，所有关于系统的配置信息都会丢失。\n如何抉择 当在集群中声明队列、交换器或者绑定的时候，这些操作会直到所有集群节点都成功提交元数据变更后才返回。对于内存节点来说，这意味着将变更写入内存；而对于磁盘节点来说，这意味着昂贵的磁盘写入操作，直到完成之后，节点才能说：“完事儿了！”假设你有五个节点的集群，并且所有节点都是磁盘节点，则你必须得等待所有这五个节点将元数据写入磁盘后，队列声明操作才能返回。\n不过在集群中，你可以选择配置部分节点为内存节点。为什么会想要选择将元数据仅存储在内存中？因为它使得像队列和交换器声明之类的操作更加快速。\nRabbitMQ只要求在集群中至少有一个磁盘节点。所有其他节点可以是内存节点。记住，当节点加入或者离开集群时，它们必须要将该变更通知到至少一个磁盘节点。\n假如集群中只有一个磁盘节点，不巧的是它有刚好崩溃了。集群仍然可以保持运行，分发消息，但是你无法做任何更改，包括 创建队列、 创建交换器、 创建绑定、 添加用户、 更改权限、 添加或删除集群节点。\n为了应对这种情况，我们通用的做法是“双活”。 至少设置两个磁盘节点，保证在任何时候都可以保存元数据的变更。除非你的所有磁盘节点都挂了。\n当内存节点重启后，它们会连接到预先配置的磁盘节点，下载当前集群元数据拷贝。所以当添加内存节点时，确保告知其所有的磁盘节点（内存节点唯一存储到磁盘的元数据信息是集群中磁盘节点的地址）。只要内存节点可以找到至少一个磁盘节点，那么它就能在重启后重新加入集群。\n","date":"2022-07-07T23:08:21Z","permalink":"https://dccmmtop.github.io/posts/rabbitmq%E4%B8%AD%E7%9A%84%E5%86%85%E5%AD%98%E8%8A%82%E7%82%B9%E4%B8%8E%E7%A3%81%E7%9B%98%E8%8A%82%E7%82%B9/","section":"posts","tags":["rabbitMQ"],"title":"RabbitMQ中的内存节点与磁盘节点"},{"categories":null,"contents":"交换器不像队列那样有真实的进程， 它只是一张名称与队列进程PID的关系表\n当你将消息发布到交换器时，实际上是由你所连接到的信道将消息上的路由键同交换器的绑定列表进行比较，然后路由消息。正是信道（channel）按照绑定匹配的结果，将消息路由到队列。 信道才是真正的路由器\n由于交换器只是一张表，因此将交换器在整个集群中进行复制，更加简单\n举例来说，当创建一个新的交换器时，RabbitMQ所要做的是将查询表添加到集群中的所有节点上。这时，每个节点上的每条信道都可以访问到新的交换器了。因此，相对于默认情况下队列的完整信息存在于集群中的单一节点来说，集群中的每个节点拥有每个交换器的所有信息。就可用性来讲，这非常棒，因为这意味着你不用担心在节点故障时重新声明交换器。只需让故障节点上的生产者重新连接到集群上，它们立即就能开始往交换器上发布消息了。\n消息丢失 那么当消息已经发布到信道上，但在路由完成之前节点发生故障的话，这些消息会怎样呢？\nAMQP的basic.publish命令不会返回消息的状态。这意味着当信道节点崩溃时信道可能仍然在路由消息，而生产者已经继续创建下一条消息了。在这种情况下，你将承受丢失消息的风险。\n","date":"2022-07-07T22:58:26Z","permalink":"https://dccmmtop.github.io/posts/rabbitmq%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E4%BA%A4%E6%8D%A2%E5%99%A8/","section":"posts","tags":["rabbitMQ"],"title":"RabbitMQ集群中的交换器"},{"categories":null,"contents":"从直觉上来看,一说到集群，我们就联想到高可用，一个节点宕机了，不会影响整体服务，客户端会从其他节点拿数据。比如 ES, Redis, mongoDB 等的集群都是符合直觉的架构。\n但是，事情到了rabbitMQ 这里，却完全不一样了。\n在将两个节点组成集群的那一刻，事情发生了巨大的变化：不是每一个节点都有所有队列的完全拷贝。\n在单一节点设置中，所有关于队列的信息（元数据、状态和内容）都完全存储在该节点上。但是如果在集群中创建队列的话，集群只会在单个节点而不是在所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息。所有其他非所有者节点只知道队列的元数据和指向该队列存在的那个节点的指针。因此当集群节点崩溃时，该节点的队列和关联的绑定就都消失了。附加在那些队列上的消费者丢失了其订阅信息，并且任何匹配该队列绑定信息的新消息也都丢失了。\n宕机后的队列 不要担心， 节点宕机后，我们可以让消费者重新连接到集群中并重新创建Q队列， 如果队列Q再一开始就是内存级别的，也就说我们可以承受节点宕机后，队列内容丢失的风险，重新创建一个全新的队列也没有什么不可。\n但是如果队列Q一开始是被设置成了持久化的呢？我们希望队列中的数据更可靠，不能重启节点就丢失了。所以如果允许重新创建队列，不就是把之前的Q队列给覆盖了吗？ rabbit 已经考虑这种情况， 所以在节点恢复之前， 不允许在集群中重新声明同名队列了， 如果非要这么做的话，你会得到一个 404 NOT_FOUND 的错误。除非将节点恢复，这种机制保证了持久化的队列中的消息不会丢失。\n产生上面现象的 “罪魁祸首” 就是因为，默认情况下，队列只存在单一节点上而不是每个节点丢复制一份呢？\nrabbit 这样设计肯定时有道理的嘛！但是为什么呢？\n一切为了性能 （1）存储空间——如果每个集群节点都拥有所有队列的完整拷贝，那么添加新的节点不会给你带来更多存储空间。举个例子，如果一个节点可以存储1GB的消息，那么添加两个节点只会给你带来两个一模一样的1GB消息的拷贝。\n（2）性能——消息的发布需要将消息复制到每一个集群节点。对于持久化消息来说，每一条消息都会触发磁盘活动。每次新增节点，网络和磁盘负载都会增加，最终只能保持集群性能的平稳（甚至更糟）。\n通过设置集群中的唯一节点来负责任何特定队列，只有该负责节点才会因队列消息而遭受磁盘活动的影响。所有其他节点只需要将接收到的该队列的消息传递给该队列的所有者节点。因此，往Rabbit集群添加更多的节点意味着你将拥有更多的队列，这些新增节点为你带来了性能的提升。当负载增加时，RabbitMQ集群是性能扩展的最佳方案。\n","date":"2022-07-07T00:22:11Z","permalink":"https://dccmmtop.github.io/posts/%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E9%98%9F%E5%88%97/","section":"posts","tags":["rabbitMQ"],"title":"集群中的队列"},{"categories":null,"contents":"本文采用docker-compose 的方式部署 rabbitMQ集群\ndocker-compose.yml version: \u0026#34;3\u0026#34; services: rabbitmq: restart: always container_name: rabbitMQ image: rabbitmq:3.7.7-management # 注意 hostname（本地域名）, rabbitMQ集群之间的通信就是靠此寻址的。物理机部署时也可以使用固定IP hostname: rabbitmq environment: RABBITMQ_NODE_PORT: 5672 # 指定虚拟主机的名称 RABBITMQ_DEFAULT_VHOST: my_vhost RABBITMQ_DEFAULT_USER: admin RABBITMQ_DEFAULT_PASS: admin # 指定 erlang_cookie, 集群中的所有节点的值必须保持一致， erlang 使用该值作为通信的密钥 RABBITMQ_ERLANG_COOKIE: rabbitmq_erlang_cookie ports: - 15672:15672 volumes: - ./data/rabbit:/var/lib/rabbitmq - ./conf:/etc/rabbitmq rabbitmq1: restart: always container_name: rabbitMQ_1 image: rabbitmq:3.7.7-management hostname: rabbitmq1 environment: RABBITMQ_NODE_PORT: 5672 RABBITMQ_DEFAULT_VHOST: my_vhost RABBITMQ_DEFAULT_USER: admin RABBITMQ_DEFAULT_PASS: admin RABBITMQ_ERLANG_COOKIE: rabbitmq_erlang_cookie ports: - 15673:15672 volumes: - ./data/rabbit_1:/var/lib/rabbitmq - ./conf:/etc/rabbitmq rabbitmq2: restart: always container_name: rabbitMQ_2 image: rabbitmq:3.7.7-management hostname: rabbitmq2 environment: RABBITMQ_NODE_PORT: 5672 RABBITMQ_DEFAULT_VHOST: my_vhost RABBITMQ_DEFAULT_USER: admin RABBITMQ_DEFAULT_PASS: admin RABBITMQ_ERLANG_COOKIE: rabbitmq_erlang_cookie ports: - 15674:15672 volumes: - ./data/rabbit_2:/var/lib/rabbitmq - ./conf:/etc/rabbitmq 集群节点之间如何通信 如上配置所示，我们并没有像之前的服务那样，使用 link 明确的标识与其他服务的链接关系。那么这是如何让 3 个节点互相通信的呢？\n原来docker 1.0 版本之后，会内置一个域名服务器，可以进入容器内部查看 /etc/reslv.conf 文件，会发现 nameserver 127.0.0.11 配置。\n这就是默认域名解析服务器的地址。我们在配置文件中指定了 hostname 的值,在容器启动的时候，就会向域名服务器注册信息，域名服务器会记域名与该机器ip的对应关系，后面我们在容器中使用 hostname 去访问其他服务时，便会通过域名服务器找到对应的IP是什么，从而访问该服务\n目录结构 如上配置所示， 分别使用 ./data/rabbit ./data/rabbit_1 ./data/rabbit_2 作为每个节点的数据卷\n使用 ./conf 作为配置文件的卷\n所以有如下目录结构\n其中 rabbitmq.conf 配置如下\nloopback_users.guest = false\rlisteners.tcp.default = 5672\rdefault_pass = admin\rdefault_user = admin\rdefault_vhost = my_vhost\rhipe_compile = false\rmanagement.listener.port = 15672\rmanagement.listener.ssl = false\r# 下面配置针对 rabbitmq-mqtt 插件, 没有开启mqtt插件的，忽略\rmqtt.default_user = mqtt_user\rmqtt.default_pass = Aa111111\rmqtt.allow_anonymous = true\rmqtt.vhost = my_vhost enabled_plugins 配置如下\n[rabbitmq_management,rabbitmq_mqtt,rabbitmq_web_mqtt]. 开启了 web 端管理界面，mqtt 、 mqtt_web 插件\n启动容器 检查配置正确后，启动容器\ndocker-compose up -d 集群配置 可以分别进入三个容器内部检查容器日志，确定全部启动成功后，进入 rabbitMQ_1 容器中，执行\nrabbitmqctl stop_app 该命令会停止当前容器中的rabbitMQ 服务，rabbitMQ 是使用 erlang 语言编写的， rabbitMQ 自然要运行在 erlang 虚拟中，就像 java class 运行在 jvm 虚拟机中一样的道理。 所以上面的命令只是停止 rabbitMQ服务，erlang 虚拟机仍然在运行着。\n现在我们将 rabbitMQ 作为主节点， rabbitMQ_1, rabbitMQ_2 作为从节点，把 rabbtitMQ_1 加入 rabbitMQ 中:\nrabbitmqctl join_cluster rabbit@rabbitmq # rabbitmq 就是你的域名或ip地址, 前面的 rabbit@ 是固定写法 等待命令成功执行后，再执行如下命令启动 rabbit 服务\nrabbitmqctl start_app 然后可以执行 rabbitmqctl cluser_status 查看节点运行状态， rabbitMQ_2 容器中重复上述步骤即可\n集群状态如下:\n可以看到集群中已经由三个磁盘节点了。\n至于为什么叫磁盘节点，先按下不表，下篇文章再探讨， 下面我们先看一下如何让一个节点从集群中全身而退。\n退出集群 在节点上执行如下命令，即可安全的让节点退出集群\nrabbitmqctl stop_app rabbitmqctl reset rabbitmqctl start_app 这里的关键命令是 rabbitmqctl reset, 此命令清空节点状态，并将其恢复到空白状态，执行此命令时，节点会和集群中其他节点通信，告诉它们，我马上要退出集群了，这一步非常重要，不然其他节点以为当前节点是因为故障而断开的，并期望某一天一定会再回来的，同时阻止其他新的节点加入集群，直到该节点恢复。 所以当要离开集群时，务必先要重设节点状态!!!\n如下:\n从集群移除后，可以看到该节点已经称为独立节点了。\n","date":"2022-07-04T22:27:10Z","permalink":"https://dccmmtop.github.io/posts/rabbitmq%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","section":"posts","tags":["rabbitMQ"],"title":"RabbitMQ集群部署"},{"categories":null,"contents":"生产者在某个队列上等待消费者返回的消息， 这个队列被称为应答队列,RabbitMQ 天生支持应答队列的机制。\n什么时候需要应答队列 当一条消息被消费者处理后，需要告知生产者一些信息时，消费者就需要在应答队列上发布一条应答消息。当应答消息被生产者处理后，应答队列就应该被删除\n这时，生产者就像客户端的角色，而消费者就像服务器的角色。由此可知，应答队列应该与消息一对一存在，并且队列名称唯一，应答消息被处理后，队列就应该删除。很棒的是，RabbitMQ的匿名队列很好的满足了这个特性.\n上代码 客户端（生产者） 服务端(消费者) 从上代码中可以看出，利用应答队列是不是也可以实现 HTTP 服务器的功能，客户端发起请求， 服务器处理请求，返回数据，客户端处理返回数据\n我们知道，RabbitMQ中的队列要绑定到交换器上，才能通过路由将消息正确的投递到队列上，但是上面的服务端发送应答消息时，将消息头中的应答队列设置为消息的路由key，并没有绑定的动作，为什么也行得通呢？\n其实，不同于通过RabbitMQ发布的任何其他消息，这里没有交换器。这是关于通过Rabbit来实现RPC通信的唯一两处特别的地方：使用reply_to作为发布应答消息的目的地，同时发布的时候无须指定交换器。\n","date":"2022-06-27T22:57:24Z","permalink":"https://dccmmtop.github.io/posts/%E5%BA%94%E7%AD%94%E9%98%9F%E5%88%97/","section":"posts","tags":["rabbitMQ"],"title":"RabbitMQ应答队列"},{"categories":null,"contents":"安装powershell 下载地址\n安装scoop 打开powershell 执行\n修改策略\nset-executionpolicy remotesigned -s cu 安装scoop\niex (new-object net.webclient).downloadstring('https://get.scoop.sh') 自动补全 PSReadLine 在 V5 或以上版本中自带\n命令 $profile 可见看见配置文件的路径，如果没有此文件，新建即可\n打配置文件 notepad $profile,输入一下内容\nImport-Module PSReadLine # Shows navigable menu of all options when hitting Tab Set-PSReadLineKeyHandler -Key Tab -Function MenuComplete # Autocompleteion for Arrow keys Set-PSReadLineOption -HistorySearchCursorMovesToEnd Set-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackward Set-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForward Set-PSReadLineOption -ShowToolTips Set-PSReadLineOption -PredictionSource History #Set the color for Prediction (auto-suggestion) Set-PSReadLineOption -Colors @{ Command = \u0026#39;Magenta\u0026#39; Number = \u0026#39;DarkBlue\u0026#39; Member = \u0026#39;DarkBlue\u0026#39; Operator = \u0026#39;DarkBlue\u0026#39; Type = \u0026#39;DarkBlue\u0026#39; Variable = \u0026#39;DarkGreen\u0026#39; Parameter = \u0026#39;DarkGreen\u0026#39; ContinuationPrompt = \u0026#39;DarkBlue\u0026#39; Default = \u0026#39;DarkBlue\u0026#39; InlinePrediction = \u0026#39;DarkGray\u0026#39; } oh-my-posh Oh My Posh是一个定制的提示引擎，适用于任何能够使用函数或变量调整提示字符串的shell。\n安装\nscoop install oh-my-posh\n配置\n在配置文件 $profile 输入：\noh-my-posh init pwsh --config ~\\scoop\\apps\\oh-my-posh\\current\\themes\\robbyrussel.omp.json | Invoke-Expression Get-ChildItemColor 安装\nInstall-Module -AllowClobber Get-ChildItemColor -Scope CurrentUser\n配置:\n在配置文件 $profile 输入：\nImport-Module Get-ChildItemColor\n快速跳转目录 Install-Module ZLocation -Scope CurrentUser\n完整配置 oh-my-posh init pwsh --config C:\\Users\\Administrator\\scoop\\apps\\oh-my-posh\\current\\themes\\robbyrussel.omp.json | Invoke-Expression Import-Module PSReadLine # Shows navigable menu of all options when hitting Tab Set-PSReadLineKeyHandler -Key Tab -Function MenuComplete # Autocompleteion for Arrow keys Set-PSReadLineOption -HistorySearchCursorMovesToEnd Set-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackward Set-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForward Set-PSReadLineOption -ShowToolTips Set-PSReadLineOption -PredictionSource History #Set the color for Prediction (auto-suggestion) Set-PSReadLineOption -Colors @{ Command = \u0026#39;Magenta\u0026#39; Number = \u0026#39;DarkBlue\u0026#39; Member = \u0026#39;DarkBlue\u0026#39; Operator = \u0026#39;DarkBlue\u0026#39; Type = \u0026#39;DarkBlue\u0026#39; Variable = \u0026#39;DarkGreen\u0026#39; Parameter = \u0026#39;DarkGreen\u0026#39; ContinuationPrompt = \u0026#39;DarkBlue\u0026#39; Default = \u0026#39;DarkBlue\u0026#39; InlinePrediction = \u0026#39;DarkGray\u0026#39; } Import-Module Get-ChildItemColor #Set-Alias ll Get-ChildItem -option AllScope #Set-Alias ls Get-ChildItemColorFormatWide -option AllScope Function vim {F:\\Neovim\\bin\\nvim.exe $args} ","date":"2022-05-13T17:28:48Z","permalink":"https://dccmmtop.github.io/posts/powershell%E9%85%8D%E7%BD%AE/","section":"posts","tags":["powershell"],"title":"powershell配置"},{"categories":null,"contents":"Shell学习笔记：awk实现group by分组统计功能\n日常部分数据以 txt 的文件格式提供，为避免入库之后再进行统计的麻烦，故学习 shell 进行处理，减少工作量。\n1.样例数据\ntest.txt YD5Gxxx|6618151|6825449073|6476534190|36251|超级会员|0 YD5Gxxx|8968336|1445546463|6476534190|36251|超级会员|0 YD5Gxxx|2545939|6904742993|0858636804|36251|超级会员|80%以上 YD5Gxxx|3200810|6896525523|6501574903|36251|普通|0 YD5Gxxx|3378244|6926264463|6519442719|36251|超级会员|80%以上 YD5Gxxx|8075700|6854827783|0858523344|36251|普通|80%以上 YD5Gxxx|3368804|6934387193|0000487348|36251|超级会员|(0，50%] YD5Gxxx|2865288|6865082233|0859114957|36251|普通|(0，50%] YD5Gxxx|6655543|6930124273|6521876215|36251|超级会员|(0，50%] YD5Gxxx|2952781|6820973583|0858704189|36251|超级会员|0 每5行切分为一个文件 通过 split -l 对文件进行切分。\nsplit -l 5 super_user.txt\n分组统计 实现分组，count[$6]++ 实现计数。\nawk -F '|' '{count[$6]++;} END {for(i in count) {print i count[i]}}' test.txt\n普通3 超级会员7 根据第7列进行筛选之后，再按第6列进行分组统计。\nawk -F '|' '{if($7==\u0026quot;0\u0026quot;) {count[$6]++;}} END {for(i in count) {print i count[i]}}' test.txt\n普通1 超级会员3 分组求和 对所有进行求和。\nawk -F '|' '{sum += $2} END {print sum}' test.txt\n分组一般使用x[$2]=x[$3]的方式来实现，其中x[$2]中的$2为要分的组，可以多个分组，x[$3]为要处理的值。\n一次分组 awk -F '|' '{x[$6] += $2} END {for(i in x){print i, x[i]}}' test.txt\n普通 14141798 超级会员 34487798 二次分组 awk -F '|' '{x[$6\u0026quot;-\u0026quot;$7] += $2} END {for(i in x){print i, x[i]}}' test.txt\n超级会员-80%以上 5924183 超级会员-0 18539268 普通-(0，50%] 2865288 超级会员-(0，50%] 10024347 普通-0 3200810 普通-80%以上 8075700 格式化处理 分组求平均值 awk -F '|' '{sum += $2} END {print \u0026quot;Average = \u0026quot;, sum/NR}' test.txt\nAverage = 4.86296e+06 awk -F '|' '{a[$6] += $2; ca[$6]++} END {for(i in a){print(i,a[i]/ca[i])}}' test.txt\n普通 4.71393e+06 超级会员 4.92683e+06 分组求最大最小 awk -F '|' 'BEGIN {max=0} {if($2\u0026gt;max){max=$2}} END {print max}' test.txt\n最大值 awk -F '|' '{if($2\u0026gt;x[$6]){x[$6]=$2}} END {for(i in x) {print i, x[i]}}' test.txt\n普通 8075700 超级会员 8968336 分组整理字符 awk -F '|' '{x[$6]=x[$6]\u0026quot;\\n\u0026quot;$2} END {for(i in x){print i \u0026quot;:\u0026quot; x[i]}}' test.txt\n普通: 3200810 8075700 2865288 超级会员: 6618151 8968336 2545939 3378244 3368804 6655543 2952781 ","date":"2022-05-10T15:51:00Z","permalink":"https://dccmmtop.github.io/posts/awk%E5%88%86%E7%BB%84%E7%BB%9F%E8%AE%A1/","section":"posts","tags":["awk"],"title":"awk分组统计"},{"categories":null,"contents":"OCP：开闭原则 软件的5大设计原则 “SOLID” 原则,其中之一就是 OCP(开闭原则)\n该设计原则是由Bertrand Meyer在20世纪80年代大力推广的，其核心要素是：如果软件系统想要更容易被改变，那么其设计就必须允许新增代码来修改系统行为，而非只能靠修改原来的代码。\n对原有代码改动越少，系统就越稳定。致力于实现增加新的功能只通过增加一个插件就可以实现\n案例 某程序想实现对输入参数的转换，参数目前是来自命令行，以后可能来自网页，或者从文本读取。如果把处理业务的逻辑与参数转换耦合到一起，那么以后修改参数来源时，势必会需改部分业务逻辑，造成系统的不稳定。本文将模仿 \u0026ldquo;database/sql\u0026rdquo; 的方式，只需更换不同的包，实现更换转换器的效果\n目录结构如下:\nmain.go\npackage main import ( \u0026#34;ocp/arg_parse\u0026#34; //安装命令行转换器 //_ \u0026#34;ocp/commandline_arg\u0026#34; // 或者安装网页参数转换器 _ \u0026#34;ocp/web_arg\u0026#34; \u0026#34;os\u0026#34; ) func main(){ // 从原代码层面看，就是通过接口进行参数转换。在运行时，一个具体的实现就注册到 Driver 上。 // 实则是通过该实现类的实例来完成转换的功能 arg_parse.Driver.ParseArg(os.Args) } parse.go\n该包中，大部分应该是接口，表明了各具体实现工具需要实现的一种方法，业务中通过调用接口的方式，最终调用具体实现。\n就像一个领导说，我公司有一个开发职位，该职位要求会实现某种转换参数的技能， 你们这些工人必须拥有这项技能，才能来我这里上班\n这里职位是虚拟的，职位本身不会干活，但是他要求了应聘该职位工人所拥有的技能\n最后领导会协调不同工种相互配合，完成整体任务，而不会关心具体执行任务的工人。\npackage arg_parse // 定义参数转换的接口，其他各种参数转换器都要实现本接口的功能。 // 具体的实现类的实例会赋值到 Driver，(一个职位) var Driver Parse type Args struct { Name string Age int Location string } // 待实现的方法(职位要求的技能) type Parse interface { ParseArg([] string) *Args } commandline_arg.go\n具体干活的工人1\npackage commandline_arg import ( \u0026#34;fmt\u0026#34; \u0026#34;ocp/arg_parse\u0026#34; ) type CommandlineParse struct {} func init(){ // 注册转换器, 该转换器拥有 parse 接口的方法， 就可以注册 (该工人应聘成功) fmt.Println(\u0026#34;注册 CommandlineParse\u0026#34;) arg_parse.Driver = \u0026amp;CommandlineParse{} } // 具体实现的功能， 实现了 ParseArg 方法 (该工人掌握的技能) func (c CommandlineParse) ParseArg(args []string) *arg_parse.Args{ fmt.Println(\u0026#34;我是命令行的参数转换器\u0026#34;) return \u0026amp;arg_parse.Args{} } web_arg.go\n具体干活的工人2\npackage web_arg import ( \u0026#34;fmt\u0026#34; \u0026#34;ocp/arg_parse\u0026#34; ) type WebParse struct {} func init(){ // 注册转换器, （工人应聘成功） fmt.Println(\u0026#34;注册 WebParse\u0026#34;) arg_parse.Driver = \u0026amp;WebParse{} } // 工人掌握的技能 func (c WebParse) ParseArg(args []string) *arg_parse.Args{ fmt.Println(\u0026#34;我是网页的参数转换器\u0026#34;) return \u0026amp;arg_parse.Args{} } ","date":"2022-05-08T22:06:51Z","permalink":"https://dccmmtop.github.io/posts/golnag%E6%8F%92%E4%BB%B6%E5%BC%8F%E5%BC%80%E5%8F%91%E7%9A%84%E4%B8%80%E7%A7%8D%E6%A1%88%E4%BE%8B/","section":"posts","tags":["go"],"title":"golnag插件式开发的一种案例"},{"categories":null,"contents":"想把自己写的一个控制台程序添加到右键，可以通过修改注册表的方式实现，但是修改起来比较麻烦，推荐使用右键管理器进行修改\n右侧的 ”文件“ ”文件夹“ ”目录“ 等，代表在该项右键时，右键菜单显示的内容\n点击 + 新建：\n点击确定\n由于是控制台程序不能直接运行，需要作为cmd.exe 参数来启动，如下:\ncmd.exe /c F:\\bin\\mix.exe push 另外，间接启动一个程序的时候也可以传入 /k 参数。与 /c 参数不同的是：\n/c 在执行完程序之后，cmd.exe 也会终止\n/k 在执行完程序之后，cmd.exe 依然会继续运行\n所以 /c 命令会更适用于自动化的脚本，而 /k 命令则更适用于半自动化的脚本。\n创建成功后可以修改命令以图标：\n效果：\n","date":"2022-05-08T16:39:06Z","permalink":"https://dccmmtop.github.io/posts/%E5%8F%B3%E9%94%AE%E6%B3%A8%E5%86%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E7%A8%8B%E5%BA%8F/","section":"posts","tags":["tools"],"title":"右键注册自定义程序"},{"categories":null,"contents":"软件架构这项工作的实质就是规划如何将系统切分成组件，并安排好组件之间的排列关系，以及组件之间互相通信的方式。 (拆分、组合、通信)\n目标 设计良好的架构可以让系统便于理解、易于修改、方便维护，并且能轻松部署。软件架构的终极目标就是最大化程序员的生产力，同时最小化系统的总运营成本。\n生命周期 设计良好架构的手段 始终保持多的可选项 软件架构师的目标是创建一种系统形态，该形态会以策略为最基本的元素，并让细节与策略脱离关系，以允许在具体决策过程中推迟或延迟与细节相关的内容。\n不应该过早的关注使用关系型，非关系型，还是分布式数据库 不应该过早的关注使用web服务，或者以某种形式发布（网页，app 等） 不应该确定是否使用 REST模式等 不应该过早的采用依赖注入框架 如果在开发高层策略时有意地让自己摆脱具体细节的纠缠，我们就可以将与具体实现相关的细节决策推迟或延后，因为越到项目的后期，我们就拥有越多的信息来做出合理的决策。\n组件的层次 离io越远，层次越高 我们对“层次”是严格按照“输入与输出之间的距离”来定义的。也就是说，一条策略距离系统的输入/输出越远，它所属的层次就越高。而直接管理输入/输出的策略在系统中的层次是最低的。\n","date":"2022-05-05T22:29:49Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E5%85%AB%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘八——软件架构"},{"categories":null,"contents":"SOLID 原则 SRP：单一职责原则。 该设计原则是基于康威定律（Conway\u0026rsquo;s Law）[1]的一个推论——一个软件系统的最佳结构高度依赖于开发这个系统的组织的内部结构。这样，每个软件模块都有且只有一个需要被改变的理由。\nOCP：开闭原则。 该设计原则是由Bertrand Meyer在20世纪80年代大力推广的，其核心要素是：如果软件系统想要更容易被改变，那么其设计就必须允许新增代码来修改系统行为，而非只能靠修改原来的代码。\nLSP：里氏替换原则。 该设计原则是Barbara Liskov在1988年提出的一个著名的子类型定义。简单来说，这项原则的意思是如果想用可替换的组件来构建软件系统，那么这些组件就必须遵守同一个约定，以便让这些组件可以相互替换。\nISP：接口隔离原则。 这项设计原则主要告诫软件设计师应该在设计中避免不必要的依赖。\n任何层次的软件设计如果依赖了它并不需要的东西，就会带来意料之外的麻烦。\nDIP：依赖反转原则。 该设计原则指出高层策略性的代码不应该依赖实现底层细节的代码，恰恰相反，那些实现底层细节的代码应该依赖高层策略性的代码。\n如果想要设计一个灵活的系统，在源代码层次的依赖关系中就应该多引用抽象类型，而非具体实现\n稳定的抽象层 我们每次修改抽象接口的时候，一定也会去修改对应的具体实现。但反过来，当我们修改具体实现时，却很少需要去修改相应的抽象接口。所以我们可以认为接口比实现更稳定。\n的确，优秀的软件设计师和架构师会花费很大精力来设计接口，以减少未来对其进行改动。毕竟争取在不修改接口的情况下为软件增加新的功能是软件设计的基础常识。\n应在代码中多使用抽象接口，尽量避免使用那些多变的具体实现类\n应避免在代码中写入与任何具体实现相关的名字，或者是其他容易变动的事物的名字。这基本上是DIP原则的另外一个表达方式。\n","date":"2022-05-05T10:45:55Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%B8%83%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘七——设计原则"},{"categories":null,"contents":"函数式编程所依赖的原理，在很多方面其实是早于编程本身出现的。因为函数式编程这种范式强烈依赖于Alonzo Church在20世纪30年代发明的λ演算。\n什么是函数式编程 举个例子：计算0-25范围内整数的平方\nJava版：\npublic class Squint { public static void main(String args[]){ for (int i=0; i＜25; i++) System.out.println(i*i); } } 下面我们改用Clojure语言来写这个程序，Clojure是LISP语言的一种衍生体，属于函数式编程语言。其代码如下：\n(println(take 25(map(fn[x](* x x))(range)))) 看不懂面的语言没有关系，我们讨论它的主要目标是要突显出Clojure和Java这两种语言之间的巨大区别。在Java程序中，我们使用的是可变量，即变量i，该变量的值会随着程序执行的过程而改变，故被称为循环控制变量。而Clojure程序中是不存在这种变量的，变量x一旦被初始化之后，就不会再被更改了。\n这句话有点出人意料：函数式编程语言中的变量（Variable）是不可变（Vary）的。\n不可变性与软件架构 为什么不可变性是软件架构设计需要考虑的重点呢？为什么软件架构师要操心变量的可变性呢？答案显而易见：所有的竞争问题、死锁问题、并发更新问题都是由可变变量导致的。如果变量永远不会被更改，那就不可能产生竞争或者并发更新问题。如果锁状态是不可变的，那就永远不会产生死锁问题。\n换句话说，一切并发应用遇到的问题，一切由于使用多线程、多处理器而引起的问题，如果没有可变变量的话都不可能发生。\n但是，能做到没有可变变量吗？即不可变性\n如果我们能忽略存储器与处理器在速度上的限制，那么答案是肯定的。否则的话，不可变性只有在一定情况下是可行的。\n可变性隔离 一种常见方式是将应用程序，或者是应用程序的内部服务进行切分，划分为可变的和不可变的两种组件。如下图：\n由于状态的修改会导致一系列并发问题的产生，所以我们通常会采用某种事务型内存来保护可变变量，避免同步更新和竞争状态的发生。\n要点是：一个架构设计良好的应用程序应该将状态修改的部分和不需要修改状态的部分隔离成单独的组件，然后用合适的机制来保护可变量。\n事件溯源 举个简单的例子，假设某个银行应用程序需要维护客户账户余额信息，当它执行存取款事务时，就要同时负责修改余额记录。\n如果我们不保存具体账户余额，仅仅保存事务日志，那么当有人想查询账户余额时，我们就将全部交易记录取出，并且每次都得从最开始到当下进行累计。当然，这样的设计就不需要维护任何可变变量了。\n但显而易见，这种实现是有些不合理的。因为随着时间的推移，事务的数目会无限制增长，每次处理总额所需要的处理能力很快就会变得不能接受。如果想使这种设计永远可行的话，我们将需要无限容量的存储，以及无限的处理能力。\n但是可能我们并不需要这个设计永远可行，而且可能在整个程序的生命周期内，我们有足够的存储和处理能力来满足它。\n这就是事件溯源，在这种体系下，我们只存储事务记录，不存储具体状态。当需要具体状态时，我们只要从头开始计算所有的事务即可。\n更重要的是，这种数据存储模式中不存在删除和更新的情况，我们的应用程序不是CRUD，而是CR。因为更新和删除这两种操作都不存在了，自然也就不存在并发问题。\n如果我们有足够大的存储量和处理能力，应用程序就可以用完全不可变的、纯函数式的方式来编程。\n","date":"2022-05-03T16:43:03Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E5%85%AD%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘六——函数式编程"},{"categories":null,"contents":"譬如封装（encapsulation）、继承（inheritance）、多态（polymorphism）。其隐含意思就是说面向对象编程是这三项的有机组合，或者任何一种支持面向对象的编程语言必须支持这三个特性。\n封装 通过采用封装特性，我们可以把一组相关联的数据和函数圈起来，使圈外面的代码只能看见部分函数，数据则完全不可见。\n而C++作为一种面向对象编程语言，反而破坏了C的完美封装性。\nC++通过在编程语言层面引入public、private、protected这些关键词，部分维护了封装性。但所有这些都是为了解决编译器自身的技术实现问题而引入的hack——编译器由于技术实现原因必须在头文件中看到成员变量的定义。\n而Java和C#则彻底抛弃了头文件与实现文件分离的编程方式，这其实进一步削弱了封装性。因为在这些语言中，我们是无法区分一个类的声明和定义的。\n由于上述原因，我们很难说强封装是面向对象编程的必要条件。而事实上，有很多面向对象编程语言[3]对封装性并没有强制性的要求。\n面向对象编程在应用上确实会要求程序员尽量避免破坏数据的封装性。但实际情况是，那些声称自己提供面向对象编程支持的编程语言，相对于C这种完美封装的语言而言，其封装性都被削弱了，而不是加强了。\n继承 既然面向对象编程语言并没有提供更好的封装性，那么在继承性方面又如何呢？\n嗯，其实也就一般般吧。简而言之，继承的主要作用是让我们可以在某个作用域内对外部定义的某一组变量与函数进行覆盖。这事实上也是C程序员[4]早在面向对象编程语言发明之前就一直在做的事了。\n虽然面向对象编程在继承性方面并没有开创出新，但是的确在数据结构的伪装性上提供了相当程度的便利性。\n回顾一下到目前为止的分析，面向对象编程在封装性上得0分，在继承性上勉强可以得0.5分（满分为1）。\n多态 在面向编程对象语言被发明之前，我们所使用的编程语言能支持多态吗？\n答案是肯定的，请注意看下面这段用C语言编写的copy程序：\n#include ＜stdio.h＞ void copy（）{ int c； while （（c=getchar（））!= EOF） putchar（c）； } 在上述程序中，函数getchar（）主要负责从STDIN中读取数据。但是STDIN究竟指代的是哪个设备呢？同样的道理，putchar（）主要负责将数据写入STDOUT，而STDOUT又指代的是哪个设备呢？很显然，这类函数其实就具有多态性，因为它们的行为依赖于STDIN和STDOUT的具体类型。\n这里的STDIN和STDOUT与Java中的接口类似，各种设备都有各自的实现。当然，这个C程序中是没有接口这个概念的，那么getchar（）这个调用的动作是如何真正传递到设备驱动程序中，从而读取到具体内容的呢？\n其实很简单，UNIX操作系统强制要求每个IO设备都要提供open、close、read、write和seek这5个标准函数。[6]也就是说，每个IO设备驱动程序对这5种函数的实现在函数调用上必须保持一致\ngetchar（）只是调用了STDIN所指向的FILE数据结构体中的read函数指针指向的函数。\n换句话说，getchar（）只是调用了STDIN所指向的FILE数据结构体中的read函数指针指向的函数。\n当然了，面向对象编程语言虽然在多态上并没有理论创新，但它们也确实让多态变得更安全、更便于使用了。\n用函数指针显式实现多态的问题就在于函数指针的危险性。毕竟，函数指针的调用依赖于一系列需要人为遵守的约定。程序员必须严格按照固定约定来初始化函数指针，并同样严格地按照约定来调用这些指针。只要有一个程序员没有遵守这些约定，整个程序就会产生极其难以跟踪和消除的Bug。\n依赖反转 在安全和便利的多态支持出现之前，软件是什么样子的。下面有一个典型的调用树的例子，main函数调用了一些高层函数，这些高层函数又调用了一些中层函数，这些中层函数又继续调用了一些底层函数。在这里，源代码层面的依赖不可避免地要跟随程序的控制流。如下图\n如你所见，main函数为了调用高层函数，它就必须能够看到这个函数所在的模块。在C中，我们会通过#include来实现，在Java中则通过import来实现，而在C#中则用的是using语句。总之，每个函数的调用方都必须要引用被调用方所在的模块。\n显然，这样做就导致了我们在软件架构上别无选择。在这里，系统行为决定了控制流，而控制流则决定了源代码依赖关系。\n但一旦我们使用了多态，情况就不一样了，如下图\n模块HL1调用了ML1模块中的F（）函数，这里的调用是通过源代码级别的接口来实现的。当然在程序实际运行时，接口这个概念是不存在的，HL1会调用ML1中的F（）函数[。\n请注意模块ML1和接口I在源代码上的依赖关系（或者叫继承关系），该关系的方向和控制流正好是相反的，我们称之为依赖反转\n当某个组件的源代码需要修改时，仅仅需要重新部署该组件，不需要更改其他组件，这就是独立部署能力。\n如果系统中的所有组件都可以独立部署，那它们就可以由不同的团队并行开发，这就是所谓的独立开发能力。\n总结 面向对象编程到底是什么？业界在这个问题上存在着很多不同的说法和意见。然而对一个软件架构师来说，其含义应该是非常明确的：面向对象编程就是以多态为手段来对源代码中的依赖关系进行控制的能力，这种能力让软件架构师可以构建出某种插件式架构，让高层策略性组件与底层实现性组件相分离，底层组件可以被编译成插件，实现独立于高层组件的开发和部署。\n","date":"2022-05-02T23:29:24Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%BA%94%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘五——面向对象"},{"categories":null,"contents":"科学方法论不需要证明某条结论是正确的，只需要想办法证明它是错误的。如果某个结论经过一定的努力无法证伪，我们则认为它在当下是足够正确的。\n我们的程序也是如此,我们不能证明我们的代码是绝对正确的，而是做各种case覆盖证明它可能存在错误。如果没有，就认为相对稳定的\nDijkstra曾经说过“测试只能展示Bug的存在，并不能证明不存在Bug”，换句话说，一段程序可以由一个测试来证明其错误性，但是却不能被证明是正确的。测试的作用是让我们得出某段程序已经足够实现当前目标这一结论。\n这种证伪过程只能应用于可证明的程序上。某段程序如果是不可证明的，例如，其中采用了不加限制的goto语句，那么无论我们为它写多少测试，也不能够证明其正确性。\n结构化编程范式促使我们先将一段程序递归降解为一系列可证明的小函数，然后再编写相关的测试来试图证明这些函数是错误的。如果这些测试无法证伪这些函数，那么我们就可以认为这些函数是足够正确的，进而推导整个程序是正确的。\n结构化编程范式中最有价值的地方就是，它赋予了我们创造可证伪程序单元的能力。这就是为什么现代编程语言一般不支持无限制的goto语句。更重要的是，这也是为什么在架构设计领域，功能性降解拆分仍然是最佳实践之一。\n","date":"2022-05-02T23:13:43Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E5%9B%9B%E7%BB%93%E6%9E%84%E5%8C%96%E7%BC%96%E7%A8%8B/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘四——结构化编程"},{"categories":null,"contents":"编程范式指的是程序的编写模式，与具体的编程语言关系相对较小。这些范式会告诉你应该在什么时候采用什么样的代码结构。\n目前也只有三个编程范式：\n它们分别是结构化编程（structured programming）、面向对象编程（object-oriented programming）以及函数式编程（functional programming）。\n结构化编程 结构化编程是第一个普遍被采用的编程范式（但是却不是第一个被提出的），由Edsger Wybe Dijkstra于1968年最先提出。与此同时，Dijkstra还论证了使用goto这样的无限制跳转语句将会损害程序的整体结构。接下来的章节我们还会说到，也是这位 Dijkstra 最先主张用我们现在熟知的 if/then/else 语句和do/while/until语句来代替跳转语句的。\n我们可以将结构化编程范式归结为一句话： 结构化编程对程序控制权的直接转移进行了限制和规范。\n面向对象编程 就是面向对象编程了。事实上，这个编程范式的提出比结构化编程还早了两年，是在1966年由Ole Johan Dahl和Kriste Nygaard在论文中总结归纳出来的。这两个程序员注意到在ALGOL语言中，函数调用堆栈（call stack frame）可以被挪到堆内存区域里，这样函数定义的本地变量就可以在函数返回之后继续存在。这个函数就成为了一个类（class）的构造函数，而它所定义的本地变量就是类的成员变量，构造函数定义的嵌套函数就成为了成员方法（method）。这样一来，我们就可以利用多态（polymorphism）来限制用户对函数指针的使用。\n在这里，我们也可以用一句话来总结面向对象编程： 面向对象编程对程序控制权的间接转移进行了限制和规范。\n函数式编程 尽管第三个编程范式是近些年才刚刚开始被采用的，但它其实是三个范式中最先被发明的。事实上，函数式编程概念是基于与阿兰·图灵同时代的数学家Alonzo Church在1936年发明的λ演算的直接衍生物。1958年John Mccarthy利用其作为基础发明了LISP语言。众所周知，λ演算法的一个核心思想是不可变性——某个符号所对应的值是永远不变的，所以从理论上来说，函数式编程语言中应该是没有赋值语句的。大部分函数式编程语言只允许在非常严格的限制条件下，才可以更改某个变量的值。\n因此，我们在这里可以将函数式编程范式总结为下面这句话： 函数式编程对程序中的赋值进行了限制和规范。\n它们都从某一方面限制和规范了程序员的能力。没有一个范式是增加新能力的。也就是说，每个编程范式的目的都是设置限制。这些范式主要是为了告诉我们不能做什么，而不是可以做什么\n","date":"2022-05-02T22:52:25Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%B8%89%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘三——编程范式"},{"categories":null,"contents":"对于每个软件系统，我们都可以通过行为和架构两个维度来体现它的实际价值。软件研发人员应该确保自己的系统在这两个维度上的实际价值都能长时间维持在很高的状态\n大部分程序员认为这就是他们的全部工作。他们的工作是且仅是：按照需求文档编写代码，并且修复任何Bug。这真是大错特错。\n软件系统必须保持灵活。软件发明的目的，就是让我们可以以一种灵活的方式来改变机器的工作行为。对机器上那些很难改变的工作行为，我们通常称之为硬件（hardware）。\n变更实施的难度应该和变更的范畴（scope）成等比关系，而与变更的具体形状（shape）无关。\n如果系统的架构设计偏向某种特定的“形状”，那么新的变更就会越来越难以实施。所以，好的系统架构设计应该尽可能做到与“形状”无关。\n如果你问业务部门，是否想要能够变更需求，他们的回答一般是肯定的，而且他们会增加一句：完成现在的功能比实现未来的灵活度更重要。但讽刺的是，如果事后业务部门提出了一项需求，而你的预估工作量大大超出他们的预期，这帮家伙通常会对你放任系统混乱到无法变更的状态而勃然大怒。\n请记住：如果忽视软件架构的价值，系统将会变得越来越难以维护，终会有一天，系统将会变得再也无法修改。如果系统变成了这个样子，那么说明软件开发团队没有和需求方做足够的抗争，没有完成自己应尽的职责。\n","date":"2022-05-02T22:52:09Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%BA%8C/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘二"},{"categories":null,"contents":"架构是什么 按照Bob大叔的说法，所谓架构就是“用最小的人力成本来满足构建和维护系统需求”的设计行为。\n所谓软件架构，就是你希望在项目一开始就能做对，但是却不一定能够做得对的决策的集合。\n从人力成本的角度来定义架构，所以，架构是否合理，完全取决于是否节省人力，便于维护、扩展。\n第一性原理：技术是为商业价值和社会价值服务的。从这个原则出发，架构就是确保业务 短期快速发展、中期可延展、长期稳定，且低成本。在业务视角，对架构提出的要求就是“既要又要还要…” 架构，绝对不是“炫技”，现在大公司晋升汇报，都会需要讲技术能力，为了试点新技术，而强加给业务迭代，且往往业务和产品很难发现和知晓，这是非常本末倒置的行为。\n架构的衡量 一个软件架构的优劣，可以用它满足用户需求所需要的成本来衡量。如果该成本很低，并且在系统的整个生命周期内一直都能维持这样的低成本，那么这个系统的设计就是优良的。如果该系统的每次发布都会提升下一次变更的成本，那么这个设计就是不好的。就这么简单。\n现代的goto 大家对结构化编程的一般理解是，由if-else、switch-case之类的语句组织程序代码的编程方式，它杜绝了goto导致的混乱。但是从更深的层次上看，它也是一种设计范式，避免随意使用goto，使用if-else、switch-case之类控制语句和函数、子函数组织起来的程序代码，可以保证程序的结构是清楚的，自顶向下层层细化，消灭了杂错，杜绝了混淆。\n联系如今的分布式系统，我们在设计的时候，真的能够做到自顶向下层层细化吗？有多少次，我看到的系统设计图里，根本没有“层次”的概念，各个模块没有一致的层次划分，与子系统交互的不是子系统，而是一盘散沙式的接口，甚至接口之间随意互调、关系乱成一团麻的情况也时常出现，带来的就是维护和调试的噩梦。吹散历史的迷雾，不正是古老的goto陷阱的再现吗？\n接口的设计 有多少次，我看到接口的设计非常随意，接口不是基于行为而是基于特定场景的实现，没有做适当的抽象，也没有为未来预留空间，直接导致契约僵硬死板。每新增一种终端呈现形式，整个内容生产流程就要大动干戈，这样的例子并不罕见。抹去历史的尘埃，这不正是“多态”出现之前的困境吗？\n接口定义需要基于行为不可以基于场景设计，否则接口会变得杂乱无章\n接口设计是站在自身角度，明确领域 应用定位，明确接口方法提供的能力，而不是基于需求方的需求（场景）\n软件架构是系统设计过程中的重要设计决定的集合，可以通过变更成本来衡量每个设计决定的重要程度。\n一个好的架构，不仅要在某一特定时刻满足软件用户、开发者和所有者的需求，更要在一段时间内持续满足他们的后续需求。如果你觉得好架构的成本太高，那你可以试试选择差的架构加上返工重来的成本。\n某个系统因为其组件错综复杂，相互耦合紧密，而导致不管多么小的改动都需要数周的恶战才能完成。又或是某个系统中到处充满了腐朽的设计和连篇累牍的恶心代码，处处都是障碍。再或者，你有没有见过哪个系统的设计如此之差，让整个团队的士气低落，用户天天痛苦，项目经理们手足无措？你有没有见过某个软件系统因其架构腐朽不堪，而导致团队流失，部门解散，甚至公司倒闭？作为一名程序员，你在编程时体会过那种生不如死的感觉吗？\n哈哈，深有体会，经历过这种痛苦之后，才会想着改变，想寻求更好的方法\n相信每个写代码的人都知道这些编码的准则，但是大部分人包括我自己在写代码的时候可能早就把这些代码准则抛之脑后了，可能因为各种原因，工期紧张或者就是自己懒惰的思想作怪，于是代码就越写越烂了，有一天连自己也懒得看这玩意了，当需求改动的时候就会变成所有人都痛苦的时候，所以在写代码的时候花一点时间是琢磨一下准备是非常有必要的，这些都是前人总结出来经验。\n","date":"2022-05-02T22:51:57Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%B8%80/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘一"},{"categories":null,"contents":"当查询命中缓存时，立即返回结果。跳过了解析 优化和执行阶段\n鸡肋 查询缓存在大部分时候都很鸡肋， 在 5.8 版本已经将查询缓存去掉了\n下面几个特性是它鸡肋的证据:\n什么时候不会被缓存 查询涉及的相关表数据发生变化时 查询缓存系统会跟踪查询中涉及的每个表，如果这些表发生变化，那么和这个表相关的所有的缓存数据都将失效。这种机制效率看起来比较低，因为数据表变化时很有可能对应的查询结果并没有变更，但是这种简单实现代价很小，而这点对于一个非常繁忙的系统来说非常重要。\n查询语句任何细微变化时 MySql将查询结果存放在引用表中，通过一个哈希值引用，这个哈希值包含了如下因素，查询本身、要查询得数据库、客户端协议的版本等其他可能会影响返回结果的信息，\n判断缓存是否命中时，MySql不会解析、参数化、任何规整查询sql的操作，直接使用客户端发来的原始sql语句。任何字符上的不同，如空格、注释都会导致缓存不被命中。\n查询语句中有不确定数据时 包含 NOW() 、 CURRENT_DATE() 、CURRENT_USER() 、CONNECTION_ID() 等变化的信息 包含任何用户自定义函数，存储函数，用户变量，临时表，MySQL系统表 等 包含 子查询，存储过程（子查询的sql不是完整的，而是运行时被计算出来的） 查询结果太大 查询缓存内存用完 如果查询语句中包含任何的不确定函数，那么在查询缓存中是不可 能找到缓存结果的。因为即使之前刚刚执行了这样的查询，结果也不会放在查询缓存中。 MySQL在任何时候只要发现不能被缓存的部分，就会禁止这个查询被缓存。\n查询缓存的缺点 对读和写操作带来额外的消耗 读查询在开始之前必须先检查是否命中缓存 MySql执行完SQL时，如果该SQL可以缓存，但是此时还没被缓存，会将数据写入缓存中 影响写操作，执行写入操作时，将此表对应的所有缓存都设置失效。对查询缓存失效的操作是靠全局锁保护的。防止此时又被缓存了旧数据。所有与该表相关的查询都要等待该锁。无论此查询是否命中缓存，以及检测缓存是否失效。 如果缓存大，或者碎片很多，那么就会有很大的系统消耗。（设置了很大的查询缓存的时候） 事务对查询缓存的影响 对InnoDB用户来说，事务的一些特性会限制查询缓存的使用。当一个语句在事务中修改了某个表，MySQL会将这个表的对应的查询缓存都设置失效，而事实上，InnoDB的多版本特性会暂时将这个修改对其他事务屏蔽。在这个事务提交之前，这个表的相关查询是无法被缓存的，所以所有在这个表上面的查询一内部或外部的事务——都只能在该事务提交后才被缓存。因此，长时间运行的事务，会大大降低查询缓存的命中率。\n缓存对系统的影响 只有当缓存带来的资源节约大于其本身的资源消耗时才会给系统带来性能提升\n适合做缓存的查询 汇总查询，如 count, max 等 复杂的查询，但结果少。如多表关联后需要分组，排序在分页的查询。同时涉及的表更新操作少于查询操作，防止缓存频发失效 命中率的计算 一个判断查询缓存是否有效的直接数据是命中率，就是使用查询缓存返回结果占总查询的比率。当MySQL接收到一个SELECT查询的时候，要么增加Qcache hits的值，要么增加Com select的值。所以查询缓存命中率可以由如下公式计算：Qcache hits / (Qcache hits+Com select)\n命中率低不代表性能提升少 不过，查询缓存命中率是一个很难判断的数值。命中率多大才是好的命中率？具体情况要具体分析。只要查询缓存带来的效率提升大于查询缓存带来的额外消耗，即使30%命中率对系统性能提升也有很大好处。另外，缓存了哪些查询也很重要，例如，被缓存的查询本身消耗非常巨大，那么即使缓存命中率非常低，也仍然会对系统性能提升有好处。所以，没有一个简单的规则可以判断查询缓存是否对系统有好处。\n命中和写入的比率 即Qcache hits和Qcache inserts的比值。根据经验来看，当这个比值大于3：1时通常查询缓存是有效的，不过这个比率最好能够达到10：1。如果你的应用没有达到这个比率，那么就可以考虑禁用查询缓存了，除非你能够通过精确的计算得知：命中带来的性能提升大于缓存失效的消耗，并且查询缓存并没有成为系统的瓶颈。\n缓存失效的一些指标检查 更新导致\n可以通过参数Com*来查看数据修改的情况（包括Com update,Com delete,等等) 缓存空间不足\n通过Qcache lowmem prunes来查看有多少次失效是由于内存不足导致的. 缓存的数据没有被查询\n查看Com select和Qcache inserts的相对值。\n如果每次查询操作都是缓存未命中，然后需要将查询结果放到缓存中，那么Qcache inserts的大小应该和Com select相当。所以在缓存完成预热后，我们总希 望看到Qcache inserts远远小于Com select。不过由于缓存和服务器内部的复杂和多样性，仍然很难说，这个比率是多少才是一个合适的值。 缓存空间的设置和使用 并非越大越好 每一个应用程序都会有一个“最大缓存空间”，甚至对一些纯读的应用来说也一样。最大缓存空间是能够缓存所有可能查询结果的缓存空间总和。理论上，对多数应用来说， 这个数值都会非常大。而实际上，由于缓存失效的原因，大多数应用最后使用的缓存空间都比预想的要小。即使你配置了足够大的缓存空间，由于不断地失效，导致缓存空间 一直都不会接近“最大缓存空间”\n设置一个合理值 通常可以通过观察查询缓存内存的实际使用情况，来确定是否需要缩小或者扩大查询缓存。如果查询缓存空间长时间都有剩余，那么建议缩小；如果经常由于空间不足而导致查询缓存失效，那么则需要增大查询缓存。不过需要注意，如果查询缓存达到了几十兆 这样的数量级，是有潜在危险的。（这和硬件以及系统压力大小有关)。\n查询缓存的一些配置参数 由于查询缓存这种鸡肋的特性，MySQL 提供一中按照需求决定是否使用 查询缓存，将决定权交给了用户:\n配置文件 my.cnf\nquery_cache_type有3个值 0代表关闭查询缓存OFF，1代表开启ON，2（DEMAND）代表当sql语句中有SQL_CACHE 关键词时才缓存\nquery_cache_type=2 查看当前mysql实例是否开启缓存机制 show global variables like \u0026#34;%query_cache_type%\u0026#34;; 监控查询缓存的命中率: show status like\u0026#39;%Qcache%\u0026#39;; //查看运行的缓存信息 Qcache_free_blocks:表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片过多了，可能在一定的时间进行整理。 Qcache_free_memory:查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，是多了，还是不够用，DBA可以根据实际情况做出调整。 Qcache_hits:表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越理想。 Qcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行查询处理后把结果insert到查询缓存中。这样的情况的次数，次数越多，表示查询缓存应用到的比较少，效果也就不理 想。当然系统刚启动后，查询缓存是空的，这很正常。 Qcache_lowmem_prunes:该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的调整缓存大小。 Qcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。 Qcache_queries_in_cache:当前缓存中缓存的查询数量。 Qcache_total_blocks:当前缓存的block数量 参考资料 《高性能MySql》 ","date":"2022-04-24T22:56:45Z","permalink":"https://dccmmtop.github.io/posts/mysql%E4%B8%AD%E7%9A%84%E6%9F%A5%E8%AF%A2%E7%BC%93%E5%AD%98/","section":"posts","tags":["MySQL"],"title":"mysql中的查询缓存"},{"categories":null,"contents":"基准测试 http_load 可以通过一个输入文件提供多个 URL，Hpttp_load 在这些 URL 中随机选择进行测试。\n也可以定制 axtp_1oad，使其按照时间比率进行测试，而不仅仅是测试最大请求处理\n下面通过一个简单的例子来演示如何使用 http_1oad。首先创建一个 urls.txt文件，输入\n如下的 URL :\nhttp://ww.mysqlperformanceblog.com/\rhttp: //www.mysqlperformanceblog.com/page/2/\rhttp: //www.mysqlperformanceblog .com/mysql-patches/\rhttp: //www.mysqlperformanceblog . com/mysql-performance-presentations/\rhttp: //www.mysqlperformanceblog . com/2006/09/06/slow-query-log-analyzes-tools/ hitp_load 最简单的用法，就是循环请求给定的 URL 列表。测试程序将以最快的速度请\n求这些 URL :\n$ http_load -parallel 1 -seconds 10 urls.txt\r19 fetches, 1 max parallel, 837929 bytes, in 10.0003 seconds\r44101.5 mean bytes/connection\r1.89995 fetches/sec, 83790.7 bytes/sec\rmsecs/connect: 41.6647 mean, 56.156 max, 38.21 min\rmsecs/first-response: 320.207 mean, 508.958 max, 179.308 min\rHTTP response codes:\rcode 200 - 19 测试的结果很容易理解，只是简单地输出了请求的统计信息。下面是另外一个稍微复杂\n的测试，还是尽可能快地循环请求给定的 URL 列表，不过模拟同时有五个并发用户在\n进行请求 ;\n$ http_load -parallel 5 -seconds 10 urls.txt\r94 fetches, 5 max parallel, 4.75565e+06 bytes, in 10.0005 seconds\r50592 mean bytes/connection\r9.39953 fetches/sec, 475541 bytes/sec\rmsecs/connect: 65.1983 mean, 169.991 max, 38.189 min\rmsecs/first-response: 245.014 mean, 993.059 max, 99.646 min\rHTTP response codes:\rcode 200 - 94 另外，除了测试最快的速度，也可以根据预估的访问请求率 〈比如每秒 5 次) 来做压力\n模拟测试。\n$ http_load -rate 5 -seconds 10 urls.txt\r48 fetches, 4 max parallel, 2.50104e+06 bytes, in 10 seconds\r52105 mean bytes/connection\r4.8 fetches/sec, 250104 bytes/sec\rmsecs/connect: 42.5931 mean, 60.462 max, 38.117 min\rmsecs/first-response: 246.811 mean, 546.203 max, 108.363 min\rHTTP response codes:\rcode 200 - 48 最后，还可以模拟更大的负载，可以将访问请求率提高到每秒 20 次请求。请注意，连\n接和请求响应时间都会随着负载的提高而增加。\n$ http_load -rate 20 -seconds 10 urls.txt\r111 fetches, 89 max parallel, 5.91142e+06 bytes, in 10.0001 seconds\r53256.1 mean bytes/connection\r11.0998 fetches/sec, 591134 bytes/sec\rmsecs/connect: 100.384 mean, 211.885 max, 38.214 min\rmsecs/first-response: 2163.51 mean, 7862.77 max, 933.708 min\rHTTP response codes:\rcode 200 -- 111 sysbench sysbench 的 CPU 基准测试\n最典型的子系统测试就是 CPU 基准测试。该测试使用 64 位整数，测试计算素数直到某\n个最大值所需要的时间。 下面的例子将比较两台不同的GNU/Linux 服务器上的测试结果。\n第一台机器的 CPU 配置如下 :\n[servert ~]$ cat /proc/cpuinfo\rmodel nane + AMD Opteron(tm) Processor 246\rstepping a\rcpu Miz + 192.857\rcache size: 1024 KB 这台服务器上运行如下的测试 :\n[serverl ~]$ sysbench -testccpu -cpu-max-prime=20000 run\rsysbench v0.4.8: multithreaded system evaluation benchnark\rTest execution summary: total tine: 121.74048 第二台服务器配置了不同的 CPU ，\n[server2 ~]$ cat /proc/cpuinfo\rmodel name: Intel(R) Xeon(R) CPU 5130 @ 2.00GH2\rstepping\rcpu Miz 测试结果如下 :\n[serverl ~]$ sysbench --test=cpu --cpu-max-prime=20000 run\rsysbench v0.4.8: multithreaded system evaluation benchnark\rTest execution summary: total time: 6.85965 测试的结果简单打印出了计算出素数的时间，很容易进行比较。在上面的测试中，第二\n人台服务器的测试结果显示比第一台快两倍。\n","date":"2022-04-14T00:00:02Z","permalink":"https://dccmmtop.github.io/posts/%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/","section":"posts","tags":["MySQL"],"title":"基准测试"},{"categories":null,"contents":"什么是字符集，什么是排序规则 字符集是只从二进制编码到某类字符符号的映射，该字符集的排序规则简称校对，\n如 ASCII码，一个字节就可以表示一个英文字母，参照ASCII码表\n为什么需要字符集 因为人类无法直接理解二进制所表示的含义，需要转换\n多字节字符集 一个字符占用超过一个字节的字符集叫做多字节字符集\n例如 UTF-8\n常见字符集 1，ASCII码：一个英文字母（不分大小写）占一个字节的空间，一个中文汉字占两个字节的空间。\n2，UTF-8编码：一个英文字符等于一个字节，一个中文（含繁体）等于三个字节。中文标点占三个字节，英文标点占一个字节\n3，Unicode编码：一个英文等于两个字节，一个中文（含繁体）等于两个字节。中文标点占两个字节，英文标点占两个字节\nMySQL 为什么提供那么多的字符集，为什么不统一使用UTF-8 为了性能\n在某些情况下可以节省空间，提高效率，例如某个字段只有阿拉伯语，直接使用 cp1256 字符集，该字符集只需一个字节就可以存下所有阿拉伯字符。\n如果使用UTF-8字符集，会消耗更多的空间\nMySQL 是如何设置字符集的 创建对象时（建库、建表、建列） 库 \u0026gt; 表 \u0026gt; 列，每个范围上都有自己的字符集设置，采用的时继承关系。如果没有明确指定字符集，则继承上一级的。\n客户端和服务器通信时 服务器端总是假设客户端是按照character set client设置的字符来传输数据和SQL语句的。 当服务器收到客户端的SQL语句时，它先将其转换成字符集character_set_connection。它还使用这个设置来决定如何将数据转换成字符串。 当服务器端返回数据或者错误信息给客户端时，它会将其转换成character_set_result。\n如图：\nMySQL 如何比较字符串大小 先将字符串转换成相同的字符集，如果字符集不兼容，会抛出错误，在MySQL 5.0 及以后，这个转换动作是自动进行的。\n字符集对查询的影响 可能无法使用索引排序，退化到文件排序 只有排序查询要求的字符集与保存到服务器上数据的字符集相同的时候，才会使用索引排序。\n例子：film 表有字段 title, 使用的是 utf8_general_ci 排序规则。现有查询要求 title 字段按照 utf8_bin 排序\n这中情况下无法使用索引排序，只能用 文件排序\n可能无法使用索引进行关联表 为了适应各种字符集，MySQL在查询的时候可能会进行字符集转换，例如，在使用不同字符集的列去关联两张表的时候，mysql会尝试转换其中一个列的字符集，这和在列外面封装一个函数一样，会让mysql无法使用该列上的索引\n对字符长度的处理 UTF-8 是一种多字节编码，一个字符所占的空间可能是 1、2、3个字节(不是固定的)。但是在MySQL内部，通常使用固定的空间来存储，这样做的目的是希望总是保证缓存中有足够的空间来存储字符串，例如一个 UTF-8的char(10)需要30个字节。\nLENGTH() \u0026gt;= CHAR_LENGTH(), LENGTH() 计算字节长度， CHAR_LENGTH() 返回字符的个数\n","date":"2022-04-13T00:22:41Z","permalink":"https://dccmmtop.github.io/posts/mysql%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E9%9B%86/","section":"posts","tags":["MySQL"],"title":"mysql中的字符集"},{"categories":null,"contents":"第一种 设置一个滑动窗口，左下标记 l， 右下标记r\nr 向右移动，记录每个字符的最后一次出现的位置 m\n如果当前字符在 m 中存在,并且重复字符出现的位置在l右侧,让l移动到重复字符的下一个位置,跳过重复的字符\nr每次移动时，计算r与l的距离,记录最大值\nfunc lengthOfLongestSubstring(s string) int { c := []rune(s) size := len(c) if size \u0026lt;= 1 { return size } l, r, maxLen, k := 0, 0, 0, 0 ok := false m := make(map[rune]int, size) for ; r \u0026lt; size; r++ { if k, ok = m[c[r]]; ok \u0026amp;\u0026amp; k \u0026gt;= l { l = k + 1 } if r-l+1 \u0026gt; maxLen { maxLen = r - l + 1 } m[c[r]] = r } return maxLen } 第二种 速度与第一种差不多，但内存占用少\nfunc lengthOfLongestSubstring(s string) int { c := []rune(s) size := len(c) if size \u0026lt;= 1 { return size } l, r, maxLen := 0, 0, 0 // 标记字符最后一次出现的位置,不可以使用默认0值，因为下标是从0开始的 m := [128]int{} for i := 0; i \u0026lt; 128; i++ { m[i] = -1 } for ; r \u0026lt; size; r++ { // 当出现重复字符时，左标记跳到该字符的下一个位置 l = max(l, m[c[r]]+1) // 记录窗口最大长度 maxLen = max(maxLen, r-l+1) // 记录字符出现的位置 m[c[r]] = r } return maxLen } func max(a int, b int) int { if a \u0026gt; b { return a } return b } ","date":"2022-04-08T00:04:33Z","permalink":"https://dccmmtop.github.io/posts/%E6%9C%80%E9%95%BF%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/","section":"posts","tags":["go","算法"],"title":"最长无重复子串"},{"categories":null,"contents":"InnoDB 是MySQL中唯一支持外键约束的内置引擎\n缺点 多一次查询 在每次修改数据时，都要在另外一张表执行一次查询操作，如果外键列的选择性很低，会导致存在一个很大但是选择性很低的索引。\n比如在一个很大的users表中，有一个 status 列，该列只有2个值，虽然列本身很小，但是如果主键很大，那么这个索引就会很大，而这个索引除了做外键限制，也没有其他作用了\n外键维护效率低 在外键相关数据的更新和删除时，外键的维护时逐行进行的，比批量删除和批量更新效率低\n额外的锁等待 外键约束使查询需要访问一些额外的表，就意味着需要额外的锁：比如 学生表有一列存储班级的主键，当新增一条学生记录时，需要在对应班级的行加锁，防止在事务结束前，该班级被删除。这会导致额外的锁等待，甚至死锁。由于没有直接访问该表，往往难以排查该问题\n优点 保持数据一致性时，比在应用中做效率高 总结 有时可以使用触发器做外键约束，对于相关数据的同时更新，外键更合适，但是如果只对列做数值约束，使用触发器或者限制取值会更好，\n如果只是做外键约束，在应用程序内做会更好，外键会带来很大的额外消耗，往往会成为性能瓶颈\n","date":"2022-04-06T23:09:21Z","permalink":"https://dccmmtop.github.io/posts/mysql%E5%A4%96%E9%94%AE%E7%BA%A6%E6%9D%9F/","section":"posts","tags":["MySQL"],"title":"MySQL外键约束"},{"categories":null,"contents":"1.介绍 Integer类型，即整数类型，MySQL支持的整数类型有TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT。\n1.1 空间和范围 每种整数类型所需的存储空间和范围如下：\n2. INT(11) 2.1 数字是否限制长度？ id INT(11) NOT NULL AUTO_INCREMENT\n在一些建表语句会出现上面 int(11) 的类型，那么其代表什么意思呢？\n对于Integer类型括号中的数字称为字段的显示宽度。这与其他类型字段的含义不同。对于DECIMAL类型，表示数字的总数。对于字符字段，这是可以存储的最大字符数，例如VARCHAR（20）可以存储20个字符。\n显示宽度并不影响可以存储在该列中的最大值。 INT(5) 和 INT(11)可以存储相同的最大值。哪怕设置成 INT(20) 并不意味着将能够存储20位数字(BIGINT)，该列还是只能存储INT的最大值。\n示例\n创建一个临时表：\nCREATE TEMPORARY TABLE demo_a ( id INT(11) NOT NULL AUTO_INCREMENT, a INT(1) NOT NULL, b INT(5) NOT NULL, PRIMARY KEY (`id`) ) 插入超过\u0026quot;长度\u0026quot;的数字：\nINSERT INTO demo_a(a,b) VALUES(255, 88888888);\n查看结果：发现数字并不是设置长度\nmysql\u0026gt; SELECT * FROM demo_a; +----+-----+----------+ | id | a | b | +----+-----+----------+ | 1 | 255 | 88888888 | +----+-----+----------+ 1 row in set (0.03 sec) 2.2 数字表达什么意思？ 当列设置为UNSIGNED ZEROFILL时，INT(11)才有意义，其表示的意思为如果要存储的数字少于11个字符，则这些数字将在左侧补零。\n注意：ZEROFILL默认的列为无符号，因此不能存储负数。\n示例\n创建一个临时表：b列设置为UNSIGNED ZEROFILL\nCREATE TEMPORARY TABLE demo_a ( id INT(11) NOT NULL AUTO_INCREMENT, a INT(11) NOT NULL, b INT(11) UNSIGNED ZEROFILL NOT NULL, PRIMARY KEY (`id`) ); 插入数值：\nINSERT INTO demo_a(a,b) VALUES(1, 1);\n结果：b列的左侧使用了0填充长度\nmysql\u0026gt; SELECT * FROM demo_a; +----+---+-------------+ | id | a | b | +----+---+-------------+ | 1 | 1 | 00000000001 | +----+---+-------------+ 1 row in set (0.18 sec) ","date":"2022-04-01T23:28:49Z","permalink":"https://dccmmtop.github.io/posts/mysql_integer%E7%B1%BB%E5%9E%8B%E9%95%BF%E5%BA%A6/","section":"posts","tags":["MySQL"],"title":"MySQL Integer类型与INT(11)"},{"categories":null,"contents":"并发编程下，如何将goroutine中发生的错误传递给其他程序，从而进行优雅的处理呢，\n一种解决方案是,将异步任务中产生的错误写入通道中，在另一个程序中读取该通道，从而实现通信，二次处理错误信息\n例子\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; ) // 错误信息的封装 type Result struct { Error error Response *http.Response } func main() { // 又不可达的链接，会触发错误 urls := []string{\u0026#34;http://10.102.49.2/web_client/#/main\u0026#34;, \u0026#34;http://10.102.204.36/console-acp/workspace/eop~region-k1~eop-uat8/deployment/detail/eop-dpl-cir-u6\u0026#34;, \u0026#34;http://123.com\u0026#34;} for result := range checkStatus(urls) { if result.Error != nil { // 在此进行错误处理 fmt.Printf(\u0026#34;Error: %v\\n\u0026#34;, result.Error) continue } fmt.Printf(\u0026#34;Response: %v\\n\u0026#34;, result.Response) } } // 将任务的处理结果放入通道中，并返回 func checkStatus(urls []string) \u0026lt;-chan Result { results := make(chan Result) go func() { defer close(results) var wg sync.WaitGroup for _, url := range urls { log.Println(\u0026#34;visist: \u0026#34;, url) wg.Add(1) go func(url string) { defer wg.Done() resp, err := http.Get(url) // 将结果写入通道 results \u0026lt;- Result{Error: err, Response: resp} }(url) } wg.Wait() }() return results } ","date":"2022-02-21T19:43:14Z","permalink":"https://dccmmtop.github.io/posts/%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E9%94%99%E8%AF%AF%E4%BC%A0%E9%80%92/","section":"posts","tags":["go"],"title":"异步任务中的错误传递"},{"categories":null,"contents":"一个通道的输出，作为下一个通道的输入，连绵不绝\n下面实现了加法 乘法的流水线\n// 流水线通道 package main import \u0026#34;fmt\u0026#34; func main() { done := make(chan interface{}) defer close(done) // 数据源 numStream := generate(done, 1, 2, 3, 4, 5) // 乘法 加法 乘法 pipeline := multi(done, add(done, multi(done, numStream, 2), 1), 2) for num := range pipeline { fmt.Println(num) } } // 接收一个中止信号，防止泄露 // 返回只读通道 func generate(done \u0026lt;-chan interface{}, num ...int) \u0026lt;-chan int { numStream := make(chan int) go func() { defer close(numStream) for _, i := range num { select { case \u0026lt;-done: return // 不断的向通道中写入数据 case numStream \u0026lt;- i: } } }() // 注意：隐式将通道转换成只读通道 return numStream } // 乘法器，从一个通道中接收数据，然后 乘以factor,将结果写入另一个通道中 //仍然要接收一个终止信号 func multi(done \u0026lt;-chan interface{}, numStream \u0026lt;-chan int, factor int) \u0026lt;-chan int { multiStream := make(chan int) go func() { defer close(multiStream) for i := range numStream { select { case \u0026lt;-done: return case multiStream \u0026lt;- (factor * i): } } }() return multiStream } // 加法器， 同上 func add(done \u0026lt;-chan interface{}, numStream \u0026lt;-chan int, factor int) \u0026lt;-chan int { addStream := make(chan int) go func() { defer close(addStream) for i := range numStream { select { case \u0026lt;-done: return case addStream \u0026lt;- (factor + i): } } }() return addStream } ","date":"2022-02-21T19:32:54Z","permalink":"https://dccmmtop.github.io/posts/%E9%80%9A%E9%81%93%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F/","section":"posts","tags":["go"],"title":"通道流水线工作模式"},{"categories":null,"contents":"goroutine 泄露 当 goroutine 被永远阻塞，或者只有主 goroutine 终止时，子 goroutine 才会终止，\n即子goroutine 没有自行终止的时机\ngoroutine 便无法释放其所占的内存空间\n一般解决方案: 由父goroutine告知子goroutine终止时机\n准则: 父 goroutine 创建子 goroutine,那么父要确保子能够停止\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; ) func main() { done := make(chan interface{}) randStream := newRandStream(done) fmt.Println(\u0026#34;3 random ints:\u0026#34;) for i := 1; i \u0026lt;= 3; i++ { fmt.Printf(\u0026#34;%d: %d\\n\u0026#34;, i, \u0026lt;-randStream) } // 通知子goroutine停止 close(done) // 模拟正在运行的工作 time.Sleep(1 * time.Second) } // 生产者 // 只在生产者作用域内声明 chan, 并在内部进行写入逻辑，然后返回只读的通道, 防止在生产者外部向该通道中写入数据 // 维护了该通道的纯净 func newRandStream(done \u0026lt;-chan interface{}) \u0026lt;-chan int { randStream := make(chan int) go func() { // 当此goroutine结束时，打印。如果是main groutine 终止时，导致该goroutine终止，则不会打印 defer fmt.Println(\u0026#34;newRandStream closure existed\u0026#34;) defer close(randStream) for { select { case randStream \u0026lt;- rand.Int(): case \u0026lt;-done: // 接收到关闭信号,避免通道泄露 return } } }() // 有一个隐式转换，将可读可写的 randStream 转换成只读的 return randStream } ","date":"2022-02-21T19:23:11Z","permalink":"https://dccmmtop.github.io/posts/%E9%98%B2%E6%AD%A2goroutine%E6%B3%84%E9%9C%B2%E7%9A%84%E4%B8%80%E8%88%AC%E6%9C%BA%E5%88%B6/","section":"posts","tags":["go"],"title":"防止goroutine泄露的一般机制"},{"categories":null,"contents":"Go中的select和channel配合使用，通过select可以监听多个channel的I/O读写事件，当 IO操作发生时，触发相应的动作。\n基本使用 // 常规示例 func example() { done := make(chan interface{}) // 一段时间后发送关闭信号 go func() { time.Sleep(5 * time.Second) close(done) }() workCounter := 0 breakLoop := false for { select { case \u0026lt;-done: breakLoop = true default: } if breakLoop { break } workCounter++ time.Sleep(1 * time.Second) } fmt.Printf(\u0026#34;收到结束信号，任务执行了 %d 次\u0026#34;, workCounter) } 超时机制 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { ch := make(chan int) quit := make(chan bool) //新开一个协程 go func() { for { select { case num := \u0026lt;-ch: fmt.Println(\u0026#34;num = \u0026#34;, num) case \u0026lt;-time.After(3 * time.Second): fmt.Println(\u0026#34;超时\u0026#34;) quit \u0026lt;- true } } }() for i := 0; i \u0026lt; 5; i++ { ch \u0026lt;- i time.Sleep(time.Second) } // 收到超时信号，停止阻塞 \u0026lt;-quit fmt.Println(\u0026#34;程序结束\u0026#34;) } 最快返回 多个 goroutine 做同一件工作，取最快的返回结果\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/kirinlabs/HttpRequest\u0026#34; ) func main() { ch1 := make(chan int) ch2 := make(chan int) ch3 := make(chan int) go Getdata(\u0026#34;https://www.baidu.com\u0026#34;,ch1) go Getdata(\u0026#34;https://www.baidu.com\u0026#34;,ch2) go Getdata(\u0026#34;https://www.baidu.com\u0026#34;,ch3) select{ case v:=\u0026lt;- ch1: fmt.Println(v) case v:=\u0026lt;- ch2: fmt.Println(v) case v:=\u0026lt;- ch3: fmt.Println(v) } } func Getdata(url string,ch chan int){ req,err := HttpRequest.Get(url) if err != nil{ }else{ ch \u0026lt;- req.StatusCode() } } 死锁与默认情况 package main func main() { ch := make(chan string) select { case \u0026lt;-ch: } } 在第 4 行创建了一个信道 ch。我们在 select 内部（第 6 行），试图读取信道 ch。由于没有 Go 协程向该信道写入数据，因此 select 语句会一直阻塞，导致死锁。该程序会触发运行时 panic\n空select package main func main() { select {} } 除非有 case 执行，select 语句就会一直阻塞着。在这里，select 语句没有任何 case，因此它会一直阻塞，导致死锁。该程序会触发 panic\n","date":"2022-02-21T19:06:18Z","permalink":"https://dccmmtop.github.io/posts/select%E7%94%A8%E6%B3%95%E7%A4%BA%E4%BE%8B/","section":"posts","tags":["go"],"title":"select用法示例"},{"categories":null,"contents":"安装 samba yum install samba 修改配置文件 修改配置文件 vim /etc/samba/smb.conf 添加要共享的目录\n[opt] # 被共享目录的别名 path = /home/dccmmtop/opt # 要共享的目录 browseable = yes writable = yes # 可以写入 valid users = dccmmtop # 用户，该用户要有目录的权限 create mode=0777 directory mode=0777 force create mode=0777 force directory mode=0777 添加用户 将用户 dccmmtop 设置为可以登录 samba\nsmbpasswd -a dccmmtop // 设置 dccmmtop 的samba服务密码，可与用户密码相同 smbpasswd -e dccmmtop // 启用samba用户，显示Enable则成功 关闭网络防火墙或添加端口 sudo systemctl stop firewalld.service 或者开放端口\nsudo firewall-cmd --zone=public --add-port=139/tcp --permanent sudo firewall-cmd --zone=public --add-port=445/tcp --permanent sudo firewall-cmd --zone=public --add-port=137/udp --permanent sudo firewall-cmd --zone=public --add-port=138/udp --permanent sudo firewall-cmd --reload sudo systemctl restart firewalld.service 关闭SELinux 安全增强型Linux（SELinux）是一个Linux内核的功能，它提供支持访问控制的安全政策保护机制\n打开 /etc/selinux/config ,将 SELINUX 设置为 disabled\n重启生效\n临时修改 执行命令 setenforce 0 临时关闭SELinux\n运行命令getenforce，验证SELinux状态为disabled，表明SELinux已关闭。\n重启服务 systemctl restart smb.service // 不报错说明没问题 windows 挂载 开启windows SMB 服务，如下\n映射网络位置\n之后输入上面设置的用户名密码就可以了\n","date":"2022-01-19T11:27:26Z","permalink":"https://dccmmtop.github.io/posts/linux%E6%90%AD%E5%BB%BAsamba%E6%9C%8D%E5%8A%A1/","section":"posts","tags":["linux"],"title":"Linux搭建samba服务"},{"categories":null,"contents":"初始化项目 新建 myapp 目录，在下面添加 Dockerfile 文件，如下:\nDockerfile FROM ruby:2.5\rRUN apt-get update -qq \u0026amp;\u0026amp; apt-get install -y nodejs default-mysql-client\rADD . /myapp\rWORKDIR /myapp\rRUN bundle install\rEXPOSE 3000\rCMD [\u0026#34;bash\u0026#34;] Gemfile 再新建 Gemfile 文件\nsource \u0026#39;https://gems.ruby-china.com\u0026#39; # 安装 Rails gem \u0026#39;rails\u0026#39;, \u0026#39;~\u0026gt; 5.1.3\u0026#39; docker-compose.yml version: \u0026#39;3.3\u0026#39; # 使用已经存在的外部网络 networks: default: external: name: dev_network services: web: build: . command: bash -c \u0026#34;rm -f tmp/pids/server.pid \u0026amp;\u0026amp; bundle exec rails s -p 3000 -b \u0026#39;0.0.0.0\u0026#39;\u0026#34; volumes: - .:/myapp ports: - \u0026#34;3000:3000\u0026#34; # 链接已经存在的mysql external_links: - mysql:mysql 加入网络 加入已经存在的 dev_net 网络，以便访问 mysql 服务, 如果没有 dev_net,可以通过下面命令创建:\ndocker network create dev_net 在 mysql 的 docker-compose.yml 中同样添加下面内容，可以加入网络 dev_net\n# 使用已经存在的外部网络 networks: default: external: name: dev_network 生成项目骨架 docker-compose run web rails new . --force --database=mysql --skip-bundle Compose 会先使用 Dockerfile 为 web 服务创建一个镜像，接着使用这个镜像在容器里运行 rails new 和它之后的命令。一旦这个命令运行完后，应该就可以看一个崭新的应用已经生成了\n再次构建 新生成的 Gemfile 覆盖了原来gem源的配置, 修改 Gemfile 的源为 source 'https://gems.ruby-china.com', 另外可以根据需要添加 gem\n由于 修改 Gemfile , 需要再次构建， 将 gem 包安装到镜像中，以后每次修改 Gemfile,都要重新构建。 或者可以将 gem 的安装目录通过添加卷的方式映射到本地。\ndocker-compose build 修改数据库配置 修改 config/database.yml\n# MySQL. Versions 5.1.10 and up are supported. # # Install the MySQL driver # gem install mysql2 # # Ensure the MySQL gem is defined in your Gemfile # gem \u0026#39;mysql2\u0026#39; # # And be sure to use new-style password hashing: # http://dev.mysql.com/doc/refman/5.7/en/old-client.html # default: \u0026amp;default adapter: mysql2 encoding: utf8 pool: \u0026lt;%= ENV.fetch(\u0026#34;RAILS_MAX_THREADS\u0026#34;) { 5 } %\u0026gt; username: root password: 123456 host: mysql development: \u0026lt;\u0026lt;: *default database: videobot # Warning: The database defined as \u0026#34;test\u0026#34; will be erased and # re-generated from your development database when you run \u0026#34;rake\u0026#34;. # Do not set this db to the same as development or production. test: \u0026lt;\u0026lt;: *default database: myapp_test # As with config/secrets.yml, you never want to store sensitive information, # like your database password, in your source code. If your source code is # ever seen by anyone, they now have access to your database. # # Instead, provide the password as a unix environment variable when you boot # the app. Read http://guides.rubyonrails.org/configuring.html#configuring-a-database # for a full rundown on how to provide these environment variables in a # production deployment. # # On Heroku and other platform providers, you may have a full connection URL # available as an environment variable. For example: # # DATABASE_URL=\u0026#34;mysql2://myuser:mypass@localhost/somedatabase\u0026#34; # # You can use this database configuration with: # # production: # url: \u0026lt;%= ENV[\u0026#39;DATABASE_URL\u0026#39;] %\u0026gt; # production: \u0026lt;\u0026lt;: *default database: myapp_production username: myapp password: \u0026lt;%= ENV[\u0026#39;MYAPP_DATABASE_PASSWORD\u0026#39;] %\u0026gt; 启动 docker-compose up ","date":"2022-01-13T09:05:07Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%84%E5%BB%BAdocker%E7%8E%AF%E5%A2%83%E5%BC%80%E5%8F%91rails/","section":"posts","tags":["docker","rails"],"title":"构建docker环境开发Rails"},{"categories":null,"contents":"为什么需要池 用来约束创建和复用昂贵的场景，比如数据库连接\nGo是怎么实现的池 通过 sync.Pool 包实现，并发安全\n怎么使用 Get 方法，首先检查池中是否有可用的实例返回给调用者,如果没有，调用New 方法创建新的实例,并返回 使用完该实例后，调用 Put 方法，将实例归还到池中 示例 package main import( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main(){ myPool := sync.Pool{ New: func() interface{} { fmt.Println(\u0026#34;创建一个新的实例\u0026#34;) return struct{}{} }, } // 第一次取，池中没有实例,要新建 myPool.Get() // 第二次取，池中也没有实例，因为没有把第一次取得实例放入到池中, 要新建 instance := myPool.Get() // 将取到得实例放入池中 myPool.Put(instance) // 池中已经有实例，不用新建 myPool.Get() // 所以会输出 2 次结果 } 结果:\n第二个例子 package main import( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;os\u0026#34; ) func main(){ numCalcsCreated := 0 calcPool := \u0026amp;sync.Pool{ New: func() interface{} { numCalcsCreated ++ // 使用了 1Kb 的内存 mem := make([]byte,1024) return \u0026amp;mem }, } calcPool.Put(calcPool.New()) calcPool.Put(calcPool.New()) calcPool.Put(calcPool.New()) calcPool.Put(calcPool.New()) const numWorks = 1024 var wg sync.WaitGroup wg.Add(numWorks) sleepTime, err := strconv.Atoi(os.Args[1]) if err != nil { fmt.Println(err) return } for i := 0; i \u0026lt; numWorks ; i ++ { go func(){ defer wg.Done() // 使用池化技术，每次用完之后，立即放入池中，永远只会使用少量的实例 // 也有可能归还太慢，导致用池中暂时没有实例可用，从而继续生成新的实例 mem := calcPool.Get() time.Sleep(time.Duration(sleepTime) * time.Nanosecond) defer calcPool.Put(mem) }() } wg.Wait() fmt.Println(\u0026#34;创建了\u0026#34;,numCalcsCreated, \u0026#34;次\u0026#34;) } 调整休眠时间，运行结果如下\n","date":"2021-12-15T19:56:44Z","permalink":"https://dccmmtop.github.io/posts/go%E6%B1%A0/","section":"posts","tags":["go","池化"],"title":"Go池"},{"categories":null,"contents":"概念 测试是编程工作中非常重要的一环，但很多人却忽视了这一点，又或者只是把测试看作是一种可有可无的补充手段。Go语言提供了一些基本的测试功能，这些功能初看上去可能会显得非常原始，但正如将要介绍的那样，这些工具实际上已经能够满足程序员对自动测试的需要了。\n除了Go语言内置的testing包之外，还会介绍check和Ginkgo这两个流行的Go测试包，它们提供的功能比testing包更为丰富。\ntesting testing包需要与go test命令以及源代码中所有以-test.go后缀结尾的测试文件一同使用。尽管Go并没有强制要求，但一般来说，测试文件的名字都会与被测试源码文件的名字相对应。\n举个例子，对于源码文件server.go，我们可以创建出一个名为server-test.go的测试文件，这个测试文件包含我们想对server.go进行的所有测试。另外需要注意的一点是，被测试的源码文件和测试文件必须位于同一个包之内。\n为了测试源代码，用户需要在测试文件中创建具有以下格式的测试函数，其中xxx可以是任意英文字母以及数字的组合，但是首字符必须是大写的英文字母：\nfunc TestXxx（*testing.T）{..}\n在测试函数的内部，用户可以使用Error，Fail等一系列方法表示测试失败。当用户在终端里面执行go test命令的时候，所有符合上述格式的测试函数就会被执行。如果一个测试在执行的时候没有任何失败，就认为通过了测试\n一般来说，一个单元通常会与程序中的一个函数或者一个方法相对应，但这并不是必须的。程序中的一个部分能否独立地进行测试，是评判这个部分能否被归纳为“单元”的一个重要指标。\n检测错误 func TestDecode(t *testing.T){ post := Decode(\u0026#34;./tsconfig.json\u0026#34;) if post.Id != 1{ t.Error(\u0026#34;错误的postId, 期望得到1，实际是: \u0026#34;, post.Id) } if post.Content != \u0026#34;Hello world!\u0026#34; { t.Error(\u0026#34;错误的post content, 期望得到\u0026#39;Hello world\u0026#39;，实际是: \u0026#34;, post.Content) } } testing.T 有几个常用的方法:\nLog 将给定的文本记录到错误日志里， 与 fmt.Println 类似 Logf 根据给定的格式,将文本记录到错误日志，与 fmt.Printf 类似 Fail 将测试函数标记已失败，但允许测试函数继续执行 FailNow 将测试函数标记已失败，并停止执行测试函数 除此之外。还提供了其他便利的方法\nError 先执行 Log 函数，再执行 Fail 函数, 其他3个类似\n执行 go test 这条命令会执行当前目录中名字以_test.go为后缀的所有文件。结果如下\ngo test 输出很精简, 可以加 -v 参数使输出内容更详细, -cover 获知测试用例对代码的覆盖率\n跳过测试用例 Skip 在进行测试驱动开发（test-driven development，TDD）的时候，通常会让测试用例持续地失败，直到函数被真正地实现出来为止；\n但是，为了避免测试用例在函数尚未实现之前一直打印烦人的失败信息，用户也可以使用testing.T 结构提供的skip函数，暂时跳过指定的测试用例。\n此外，如果某个测试用例的执行时间非常长，我们也可以在实施完整性检查（sanity check）的时候，使用Skip函数跳过该测试用例。\n示例代码:\nfunc TestEncode(t *testing.T){ // Skip 暂时跳过对某个方法的测试 t.Skip(\u0026#34;暂时跳过对Encode方法的测试\u0026#34;) } 再次执行测试：\nShort 除了可以直接跳过整个测试用例，用户还可以通过向go test命令传入短暂标志 -short，并在测试用例中使用某些条件逻辑来跳过测试中的指定部分。\n注意，这种做法跟在go test命令中通过选项来选择性地执行指定的测试不一样：选择性执行只会执行指定的测试，并跳过其他所有测试，而-short标志则会根据用户编写测试代码的方式，跳过测试中的指定部分或者跳过整个测试用例。\n示例：\nfunc TestLongTime(t *testing.T) { // 如果带有 -short 参数 if testing.Short() { t.Skip(\u0026#34;跳过长时间的测试\u0026#34;) } time.Sleep(10 * time.Second) } 并行测试 正如之前所说，单元测试的目的是独立地进行测试。尽管有些时候，测试套件会因为内部存在依赖关系而无法独立地进行单元测试，但是只要单元测试可以独立地进行，用户就可以通过并行地运行测试用例来提升测试的速度了\ntesting 通过 Parallel 方法实现并行\n示例：\nfunc TestSleep5(t *testing.T) { t.Parallel() time.Sleep(5 * time.Second) } func TestSleep3(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } func TestSleep1(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } 通过 -parallel n 控制可以同时运行几个单元测试\n基准测试 Go 的 testing 包支持两种测试，一种是上面的功能测试，还一种就是检验程序的性能的基准测试\n基准测试用例也许要放到以 _test.go 结尾的文件中 基准测试函数需要符合 func BenchmarkXXXX(b *testing.B) {\u0026hellip;} 格式 测试用例的迭代次数是Go自行决定的 使用 go test -bench . 执行本目录下的所有基准测试 示例：\n// 基准测试 func BenchmarkDecode(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Decode(\u0026#34;./tsconfig.json\u0026#34;) } 结果:\n执行了 21745 次。花费了 55310 纳秒，即 55 毫秒\n完整代码 json文件\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;content\u0026#34;: \u0026#34;Hello world!\u0026#34;, \u0026#34;author\u0026#34;:{ \u0026#34;id\u0026#34;:2, \u0026#34;name\u0026#34;: \u0026#34;Sau Sheong\u0026#34; }, \u0026#34;comments\u0026#34;: [ { \u0026#34;id\u0026#34;: 3, \u0026#34;content\u0026#34;: \u0026#34;Have a great day!\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Adam\u0026#34; }, { \u0026#34;id\u0026#34;: 4, \u0026#34;content\u0026#34;: \u0026#34;How are you today?\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Betty\u0026#34; } ] } package main import ( \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; ) func TestDecode(t *testing.T){ post := Decode(\u0026#34;./tsconfig.json\u0026#34;) if post.Id != 1{ t.Error(\u0026#34;错误的postId, 期望得到1，实际是: \u0026#34;, post.Id) } if post.Content != \u0026#34;Hello world!\u0026#34; { t.Error(\u0026#34;错误的post content, 期望得到\u0026#39;Hello world\u0026#39;，实际是: \u0026#34;, post.Content) } } func TestSleep5(t *testing.T) { t.Parallel() time.Sleep(5 * time.Second) } func TestSleep3(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } func TestSleep1(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } func TestEncode(t *testing.T){ // Skip 暂时跳过对某个方法的测试 t.Skip(\u0026#34;暂时跳过对Encode方法的测试\u0026#34;) } func TestLongTime(t *testing.T) { // 如果带有 -short 参数 if testing.Short() { t.Skip(\u0026#34;跳过长时间的测试\u0026#34;) } time.Sleep(10 * time.Second) } // 基准测试 func BenchmarkDecode(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Decode(\u0026#34;./tsconfig.json\u0026#34;) } } ","date":"2021-11-23T23:21:17Z","permalink":"https://dccmmtop.github.io/posts/go%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","section":"posts","tags":["go","单元测试"],"title":"Go单元测试"},{"categories":null,"contents":"这个封装程序使用的结构和之前分析JSON时使用的结构是相同的。\n程序首先会创建一些结构，然后通过调用MarshalIndent函数将结构封装为由字节切片组成的JSON数据 最后，程序会将封装所得的JSON数据存储到指定的文件中。 也可以通过编码器手动将Go结构编码为json数据\n流程如下:\n示例 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) type Post struct { // 处理对象属性与json字段的映射关系 // 如果对象属性与json字段名称相同。可以省略 Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author Author `json:\u0026#34;author\u0026#34;` Comments []Comment `json:\u0026#34;comments\u0026#34;` } type Author struct { Id int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type Comment struct { Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author string `json:\u0026#34;author\u0026#34;` } func main(){ post := Post{ Id: 1, Content: \u0026#34;Hello World\u0026#34;, Author: Author{ Id: 2, Name: \u0026#34;dc1\u0026#34;, }, Comments: []Comment{ { Id: 3, Content: \u0026#34;Have a great day\u0026#34;, Author: \u0026#34;Adam\u0026#34;, }, { Id: 4, Content: \u0026#34;How are you today\u0026#34;, Author: \u0026#34;Betty\u0026#34;, }, }, } fmt.Printf(\u0026#34;post: %v\\n\u0026#34;, post) // 将 json 结构封装为由字节切片组成的Json数据 output, err := json.MarshalIndent(\u0026amp;post,\u0026#34;\u0026#34;,\u0026#34;\\t\\t\u0026#34;) if err != nil { panic(err) } // 将数据写入文件中 err = ioutil.WriteFile(\u0026#34;post.json\u0026#34;,output,0666) if err != nil { panic(err) } // 使用Encoder将结构编码到文件中 // 创建用于储存json的文件 jsonFile, err := os.Create(\u0026#34;./post1.json\u0026#34;) if err != nil { panic(err) } // 创建解码器 encoder := json.NewEncoder(jsonFile) // 把结构编码到json文件中 err = encoder.Encode(\u0026amp;post) if err != nil { panic(err) } } ","date":"2021-11-18T23:08:29Z","permalink":"https://dccmmtop.github.io/posts/go%E5%86%99%E5%85%A5json/","section":"posts","tags":["go"],"title":"Go写入json"},{"categories":null,"contents":"可以使用Unmarshal函数来解封JSON，还可以使用Decoder手动地将JSON数据解码到结构里面，以此来处理流式的JSON数据，\n流程如下\n要解析的json文件 { \u0026#34;id\u0026#34;: 1, \u0026#34;content\u0026#34;: \u0026#34;Hello world!\u0026#34;, \u0026#34;author\u0026#34;:{ \u0026#34;id\u0026#34;:2, \u0026#34;name\u0026#34;: \u0026#34;Sau Sheong\u0026#34; }, \u0026#34;comments\u0026#34;: [ { \u0026#34;id\u0026#34;: 3, \u0026#34;content\u0026#34;: \u0026#34;Have a great day!\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Adam\u0026#34; }, { \u0026#34;id\u0026#34;: 4, \u0026#34;content\u0026#34;: \u0026#34;How are you today?\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Betty\u0026#34; } ] } 示例 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) type Post struct { // 处理对象属性与json字段的映射关系 // 如果对象属性与json字段名称相同。可以省略 Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author Author `json:\u0026#34;author\u0026#34;` Comments []Comment `json:\u0026#34;comments\u0026#34;` } type Author struct { Id int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type Comment struct { Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author string `json:\u0026#34;author\u0026#34;` } func main(){ jsonFile, err := os.Open(\u0026#34;./tsconfig.json\u0026#34;) defer jsonFile.Close() if err != nil { panic(err) } var post Post jsonContent, err := ioutil.ReadAll(jsonFile) if err != nil { panic(err) } // 将json数据解封至结构 err = json.Unmarshal(jsonContent, \u0026amp;post) if err != nil { panic(err) } fmt.Printf(\u0026#34;post: %v\\n\u0026#34;, post) // 另一种方式 使用解码器 jsonFile1, err := os.Open(\u0026#34;./tsconfig.json\u0026#34;) defer jsonFile1.Close() if err != nil { panic(err) } decoder := json.NewDecoder(jsonFile1) var post1 Post err = decoder.Decode(\u0026amp;post1) if err != nil \u0026amp;\u0026amp; err != io.EOF { panic(err) } fmt.Printf(\u0026#34;post1: %v\\n\u0026#34;, post1) } 选择 最后，在面对JSON数据时，我们可以根据输入决定使用Decoder还是Unmarshal：\n如果JSON数据来源于io.Reader流， 如http.Request的Body，那么使用Decoder更好；\n如果JSON数据来源于字符串或者内存的某个地方，那么使用Unmarshal更好。\n","date":"2021-11-18T22:40:07Z","permalink":"https://dccmmtop.github.io/posts/go%E8%A7%A3%E6%9E%90json%E6%96%87%E4%BB%B6/","section":"posts","tags":["go"],"title":"Go解析json文件"},{"categories":null,"contents":"自动迁移 因为Gorm可以通过自动数据迁移特性来创建所需的数据库表，并在用户修改相应的结构时自动对数据库表进行更新， 当我们运行这个程序时，程序所需的数据库表就会自动生成\n负责执行数据迁移操作的AutoMigrate方法是一个变长参数方法，这种类型的方法和函数能够接受一个或多个参数作为输入。\n在下面展示的代码中，AutoMigrate方法接受的是Post结构和Comment结构。得益于自动数据迁移特性的存在，当用户向结构里面添加新字段的时候，Gorm就会自动在数据库表里面添加相应的新列。\n自动设置创建时间 Comment结构里面出现了一个类型为time.Time的CreatedAt字段，包含这样一个字段意味着Gorm每次在数据库里创建一条新记录的时候，都会自动对这个字段进行设置。\n此外，Comment结构的其中一些字段还用到了结构标签，以此来指示Gorm应该如何创建和映射相应的字段。比如，Comment结构的Author字段就使用了结构标签、sql：\u0026ldquo;not null\u0026rdquo;，以此来告知Gorm，该字段对应列的值不能为null\n处理映射关系 在Comment结构里设置了一个PostId字段。Gorm会自动把这种格式的字段看作是外键，并创建所需的关系。\n示例代码 package main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; \u0026#34;github.com/jinzhu/gorm\u0026#34; \u0026#34;time\u0026#34; ) type Post struct { // 会被自动设置成主键 Id int Content string // 数据库层面不可以为空约束 Author string `sql:\u0026#34;not null\u0026#34;` Comments []Comment // 约定。 创建时会被自动赋值 CreatedAt time.Time } type Comment struct { Id int Content string Author string `sql:\u0026#34;not null\u0026#34;` // 以Id结尾的字段被视为外键,在关联对象时起作用. index 会在此字段上创建索引 PostId int `sql:\u0026#34;index\u0026#34;` CreatedAt time.Time } var Db *gorm.DB func init(){ var err error Db, err = gorm.Open(\u0026#34;mysql\u0026#34;,\u0026#34;esns:dccmmtop@tcp(192.168.32.128:3306)/chitchat?parseTime=true\u0026#34;) if err != nil { panic(err) } // 开启详细日志，会把执行的sql打印出来 Db.LogMode(true) // 执行数据库迁移。包括新增表，新增字段，修改字段 Db.AutoMigrate(\u0026amp;Post{},\u0026amp;Comment{}) } func main() { post := Post{ Content: \u0026#34;乱花渐欲迷人眼\u0026#34;, Author: \u0026#34;李商隐\u0026#34;, } // 创建post,并映射 Db.Create(\u0026amp;post) comment := Comment{ Content: \u0026#34;太棒了\u0026#34;, Author: \u0026#34;李白\u0026#34;, } // 创建关联的对象。这里通过外键Id 自动查找映射关系 Db.Model(\u0026amp;post).Association(\u0026#34;Comments\u0026#34;).Append(comment) var readPost Post // 查询 Db.Where(\u0026#34;id = ?\u0026#34;, post.Id).First(\u0026amp;readPost) var comments []Comment // 关联查询 Db.Model(\u0026amp;readPost).Related(\u0026amp;comments) fmt.Printf(\u0026#34;post: %v\\n\u0026#34;,readPost) fmt.Printf(\u0026#34;comments: %v\\n\u0026#34;,comments) } gorm 官方文档有详细的教程 https://gorm.io/zh_CN/\n","date":"2021-11-16T23:14:23Z","permalink":"https://dccmmtop.github.io/posts/gorm%E5%8C%85%E4%BD%BF%E7%94%A8/","section":"posts","tags":["go"],"title":"gorm包使用"},{"categories":null,"contents":"sqlx是一个第三方库，它为database/sql包提供了一系列非常有用的扩展功能。\n因为sqlx和database/sql包使用的是相同的接口，所以sqlx能够很好地兼容使用database/sql包的程序， 除此之外，sqlx还提供了以下这些额外的功能：\n通过结构标签（struct tag）将数据库记录（即行）封装为结构、映射或者切片； 为预处理语句提供具名参数支持。 表结构 create table blog ( id int auto_increment primary key, title varchar(255) not null, content text not null, creator_id int not null ); 示例 package main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; ) type Blog struct { Id int Title string Content string // 当属性与字段名不一致时，使用 struct_tag 进行映射 Creator int `db:\u0026#34;creator_id\u0026#34;` } var db *sqlx.DB func init(){ var err error db, err = sqlx.Open(\u0026#34;mysql\u0026#34;,\u0026#34;esns:dccmmtop@tcp(192.168.32.128:3306)/chitchat?parseTime=true\u0026#34;) if err != nil { panic(err) } } func (blog *Blog)save(){ result, err := db.Exec(\u0026#34;insert into blog(title, content, creator_id) values (?,?,?)\u0026#34;,blog.Title,blog.Content, blog.Creator) if err != nil { panic(err) } // mysql 不支持返回插入后的自增Id,需要额外处理 id, err := result.LastInsertId() if err != nil { panic(err) } blog.Id = int(id) return } func findById(id int)(blog Blog){ blog = Blog{} // StructScan 会自动赋值属性 err := db.QueryRowx(\u0026#34;select id, title, content, creator_id from blog where id = ?\u0026#34;, id).StructScan(\u0026amp;blog) if err != nil { panic(err) } return } func main(){ blog := Blog{ Title: \u0026#34;go语言学习\u0026#34;, Content: \u0026#34;开启go编程之旅吧\u0026#34;, Creator: 1, } blog.save() blog = findById(blog.Id) fmt.Printf(\u0026#34;blog: %v\\n\u0026#34;, blog) } ","date":"2021-11-16T22:07:23Z","permalink":"https://dccmmtop.github.io/posts/sqlx%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/","section":"posts","tags":["go"],"title":"sqlx包的使用"},{"categories":null,"contents":"连接数据库 sq1.DB结构是一个数据库句柄（handle），它代表的是一个包含了零个或任意多个数据库连接的连接池（pool），这个连接池由sql包管理。程序可以通过调用Open函数，并将相应的数据库驱动名字（driver name）以及数据源名字（data source name）传递给该函数来建立与数据库的连接。\n比如，在下面展示的例子中，程序使用的是mysql驱动。数据源名字是一个特定于数据库驱动的字符串，它会告诉驱动应该如何与数据库进行连接。\nOpen函数在执行之后会返回一个指向sq1.DB结构的指针作为结果。Open函数在执行时，不会真正的与数据库连接，甚至不会检查参数.\nOpen函数真正的作用是设置好连接数据库所需要的结构，并以惰性的方式，等真正需要的时候才建立与数据库连接\nvar Db *sql.DB func init() { var err error Db, err = sql.Open(\u0026#34;mysql\u0026#34;, \u0026#34;esns:dccmmtop@tcp(192.168.32.128:3306)/chitchat\u0026#34;) if err != nil { log.Fatal(err) } return } 创建用户 type User struct { Id int64 Uuid string Name string Email string Password string CreatedAt time.Time } func (u *User) Create() (err error) { statement := \u0026#34;insert into users(uuid,name,email,password, created_at) value(?,?,?,?,?)\u0026#34; // 预编译 stmt, err := Db.Prepare(statement) if err != nil { return err } defer stmt.Close() // 加密密码 u.Password = Encrypt(u.Password) // 生成UUID u.Uuid = CreateUUID() u.CreatedAt = time.Now() // 执行 result, err := stmt.Exec(u.Uuid,u.Name,u.Email,u.Password,u.CreatedAt) if err != nil { return err } // 返回插入后的自增ID u.Id, err = result.LastInsertId() if err != nil { util.Danger.Println(\u0026#34;创建用户返回Id错误: \u0026#34;,err) return err } util.Info.Println(\u0026#34;新增用户: \u0026#34;, fmt.Sprintf(\u0026#34;%v\u0026#34;,*u)) userJson, err := json.Marshal(*u) if err != nil { return err } util.Info.Println(\u0026#34;新增用户: \u0026#34;, string(userJson)) return } 查询用户 // 根据ID查询用户 func FindUserById(id int64)(u User, err error) { sql := \u0026#34;select id, uuid, `name`, email, password, created_at from users where id = ?\u0026#34; u = User{} // scan 将查询出来的每一列赋值给对应的属性 err = Db.QueryRow(sql, id).Scan(\u0026amp;u.Id, \u0026amp;u.Uuid, \u0026amp;u.Name, \u0026amp;u.Email, \u0026amp;u.Password, \u0026amp;u.CreatedAt) if err != nil { util.Danger.Println(\u0026#34;查询用户错误: \u0026#34;, err) return } return } 获取多个对象 type Thread struct { Id int64 Uuid string UserId int64 Topic string CreatedAt time.Time } // 获取用户发布多个帖子 func ThreadsList(userId int64)(threads []Thread){ sql := \u0026#34;select id, uuid, user_id, topic,created_at from threads where user_id = ? order by created_at desc\u0026#34; rows, err := Db.Query(sql,userId) if err != nil { util.Danger.Println(\u0026#34;查询 threads 错误, 返回空数据,err:\u0026#34;, err) return } defer rows.Close() for rows.Next() { thread := Thread{} err := rows.Scan(\u0026amp;thread.Id,\u0026amp;thread.Uuid,\u0026amp;thread.UserId,\u0026amp;thread.Topic,\u0026amp;thread.CreatedAt) if err != nil { util.Danger.Println(err) continue } threads = append(threads,thread) } return } ","date":"2021-11-13T22:21:13Z","permalink":"https://dccmmtop.github.io/posts/go%E4%B8%8Esql/","section":"posts","tags":["go"],"title":"Go与SQL"},{"categories":null,"contents":"\n","date":"2021-11-12T08:57:15Z","permalink":"https://dccmmtop.github.io/posts/sql%E4%BC%98%E5%8C%96/","section":"posts","tags":["MySQL"],"title":"SQL优化"},{"categories":null,"contents":"什么是数据的一致性 “数据一致”一般指的是：缓存中有数据，缓存的数据值 = 数据库中的值。\n但根据缓存中是有数据为依据，则”一致“可以包含两种情况：\n缓存中有数据，缓存的数据值 = 数据库中的值（需均为最新值，本文将“旧值的一致”归类为“不一致状态”） 缓存中本没有数据，数据库中的值 = 最新值（有请求查询数据库时，会将数据写入缓存，则变为上面的“一致”状态） ”数据不一致“：缓存的数据值 ≠ 数据库中的值；缓存或者数据库中存在旧值，导致其他线程读到旧数据\n数据不一致情况及应对策略 根据是否主动向缓存中写值，可以把缓存分成读写缓存和只读缓存。\n只读缓存：只在缓存进行数据查找，即使用 更新数据库+删除缓存策略； 读写缓存：需要在缓存中对数据进行增删改查，即使用 更新数据库+更新缓存策略。 只读缓存（更新数据库+删除缓存） 新增数据时 ，写入数据库；访问数据时，缓存缺失，查数据库，更新缓存（始终是处于”数据一致“的状态，不会发生数据不一致性问题) 更新（修改/删除）数据时 ，会有个时序问题：更新数据库与删除缓存的顺序（这个过程会发生数据不一致性问题）, 如下图 在更新数据的过程中，可能会有如下问题：\n无并发请求下，其中一个操作失败的情况\n并发请求下，其他线程可能会读到旧值\n因此，要想达到数据一致性，需要保证两点：\n无并发请求下，保证 A 和 B 步骤都能成功执行\n并发请求下，在 A 和 B 步骤的间隔中，避免或消除其他线程的影响\n接下来，我们针对有/无并发场景，进行分析并使用不同的策略。\nA. 无并发情况 无并发请求下，在更新数据库和删除缓存值的过程中，因为操作被拆分成两步，那么就很有可能存在“步骤 1 成功，步骤 2 失败” 的情况发生（由于单线程中步骤 1 和步骤 2 是串行执行的，不太可能会发生 “步骤 2 成功，步骤 1 失败” 的情况）。\n先删除缓存，再更新数据库,如图\n先更新数据库，再删除缓存\n两种方案执行情况对比:\n执行时序 潜在问题 结果 是否存在一致性问题 先删除缓存，在更新数据库 删除缓存成功，更新数据库失败 请求无法命中缓存，读取数据库旧值 是 先更新数据库,后删除缓存 更新数据库成功，删除缓存失败 请求命中缓存，读取到旧值 是 解决策略：\na.消息队列+异步重试\n无论使用哪一种执行时序，可以在执行步骤 1 时，将步骤 2 的请求写入消息队列，当步骤 2 失败时，就可以使用重试策略，对失败操作进行 “补偿”。\n如图:\n具体步骤如下：\n把要删除缓存值或者是要更新数据库值操作生成消息，暂存到消息队列中（例如使用 Kafka 消息队列）； 当删除缓存值或者是更新数据库值操作成功时，把这些消息从消息队列中去除（丢弃），以免重复操作； 当删除缓存值或者是更新数据库值操作失败时，执行失败策略，重试服务从消息队列中重新读取（消费）这些消息，然后再次进行删除或更新； 删除或者更新失败时，需要再次进行重试，重试超过的一定次数，向业务层发送报错信息 b.订阅 Binlog 变更日志\n创建更新缓存服务，接收数据变更的 MQ 消息，然后消费消息，更新/删除 Redis 中的缓存数据； 使用 Binlog 实时更新/删除 Redis 缓存。利用 Canal，即将负责更新缓存的服务伪装成一个 MySQL 的从节点，从 MySQL 接收 Binlog，解析 Binlog 之后，得到实时的数据变更信息，然后根据变更信息去更新/删除 Redis 缓存； MQ+Canal 策略，将 Canal Server 接收到的 Binlog 数据直接投递到 MQ 进行解耦，使用 MQ 异步消费 Binlog 日志，以此进行数据同步； 不管用 MQ/Canal 或者 MQ+Canal 的策略来异步更新缓存，对整个更新服务的数据可靠性和实时性要求都比较高，如果产生数据丢失或者更新延时情况，会造成 MySQL 和 Redis 中的数据不一致。因此，使用这种策略时，需要考虑出现不同步问题时的降级或补偿方案。 B. 高并发情况 使用以上策略后，可以保证在单线程/无并发场景下的数据一致性。但是，在高并发场景下，由于数据库层面的读写并发，会引发的数据库与缓存数据不一致的问题（本质是后发生的读请求先返回了）\n(1) 先删除缓存，再更新数据库\n假设线程 A 删除缓存值后，由于网络延迟等原因导致未及更新数据库，而此时，线程 B 开始读取数据时会发现缓存缺失，进而去查询数据库。而当线程 B 从数据库读取完数据、更新了缓存后，线程 A 才开始更新数据库，此时，会导致缓存中的数据是旧值，而数据库中的是最新值，产生“数据不一致”。其本质就是，本应后发生的“B 线程-读请求” 先于 “A 线程-写请求” 执行并返回了。\n时序如下：\n时间 线程A 线程B 问题 T1 删除数据X的缓存值 T2 1. 读取缓存值X，缓存缺失，从数据库读取X值 线程B读到旧值 T3 2. 将数据X值写入缓存 导致其他线程读到旧值 T4 更新数据库中X的值 缓存时旧值，数据库是新值。数据不一致 解决方案\na.设置缓存过期时间 + 延时双删\n通过设置缓存过期时间，若发生上述淘汰缓存失败的情况，则在缓存过期后，读请求仍然可以从 DB 中读取最新数据并更新缓存，可减小数据不一致的影响范围。虽然在一定时间范围内数据有差异，但可以保证数据的最终一致性。\n此外，还可以通过延时双删进行保障：在线程 A 更新完数据库值以后，让它先 sleep 一小段时间，确保线程 B 能够先从数据库读取数据，再把缺失的数据写入缓存（此时有可能写入的是旧值），然后，线程 A 再进行删除，确保缓存中最终会将是最新的值。后续，其它线程读取数据时，发现缓存缺失，会从数据库中读取最新值\n延时删除只是确保最终缓存中的值与数据库保持一致。不能防止中间的不一致\nredis.delKey(X) db.update(X) Thread.sleep(N) redis.delKey(X) sleep 时间：在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算(较难)\n注意：如果难以接受 sleep 这种写法，可以使用延时队列进行替代。\n先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力，也就是缓存穿透的问题。针对缓存穿透问题，可以用缓存空结果、布隆过滤器进行解决。\n(2) 先更新数据库，再删除缓存\n如果线程 A 更新了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。其本质也是，本应后发生的“B 线程-读请求” 先于 “A 线程-删除缓存” 执行并返回了。\n如下:\n时间 线程A 线程B 问题 T1 更新数据库中的数据X T2 读取数据X,命中缓存。读取旧值 线程A尚未删除缓存。导致线程B读到旧值 T3 更新数据库中X的值 导致其他线程读到旧值 或者，在”先更新数据库，再删除缓存”方案下，“读写分离 + 主从库延迟”也会导致不一致：\n时间 线程A 线程B MySQL集群 问题 T1 更新主库X=2(原值 X=1) T2 删除缓存 T3 查询缓存，没有命中。查询从库，得到旧值 X=1 T4 将X=1写入缓存 T5 从库同步完成 X=2 缓存中是旧值X=1,数据库(主+从)是新值 X=2。数据不一致 解决方案：\na.延迟消息 凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率\nb.订阅 binlog，异步删除 通过数据库的 binlog 来异步淘汰 key，利用工具(canal)将 binlog 日志采集发送到 MQ 中，然后通过 ACK 机制确认处理删除缓存。\nc.删除消息写入数据库 通过比对数据库中的数据，进行删除确认 先更新数据库再删除缓存，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力，也就是缓存穿透的问题。针对缓存穿透问题，可以用缓存空结果、布隆过滤器进行解决。\nd.加锁 更新数据时，加写锁；查询数据时，加读锁 保证两步操作的“原子性”，使得操作可以串行执行。“原子性”的本质是什么？不可分割只是外在表现，其本质是多个资源间有一致性的要求，操作的中间状态对外不可见。\n建议： 优先使用“先更新数据库再删除缓存”的执行时序，原因主要有两个：\n先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力； 业务应用中读取数据库和写缓存的时间有时不好估算，进而导致延迟双删中的 sleep 时间不好设置。 读写缓存（更新数据库+更新缓存） 读写缓存：增删改在缓存中进行，并采取相应的回写策略，同步数据到数据库中\n同步直写：使用事务，保证缓存和数据更新的原子性，并进行失败重试（如果 Redis 本身出现故障，会降低服务的性能和可用性）\n异步回写：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库（没写回数据库前，缓存发生故障，会造成数据丢失） 该策略在秒杀场中有见到过，业务层直接对缓存中的秒杀商品库存信息进行操作，一段时间后再回写数据库。\n一致性：同步直写 \u0026gt; 异步回写 因此，对于读写缓存，要保持数据强一致性的主要思路是：利用同步直写 同步直写也存在两个操作的时序问题：更新数据库和更新缓存\n无并发情况 执行顺序 潜在问题 结果 是否存在一致性问题 解决策略 先更新缓存，后更新数据库 缓存更新成功，数据库更新失败 数据库为旧值 是 消息队列+重试机制 先更新数据库，后更新缓存 数据库更新成功，缓存更新失败 请求命中缓存，读取缓存中的旧值 是 消息队列+重试机制；订阅Binlog日志 高并发情况 有一下四种情况\n时序 并发类型 潜在问题 影响程度 先更新数据库，后更新缓存 写+读 并发 1. 线程A先更新数据库\n2. 线程B读取数据，命中缓存，读取到旧值\n3. 线程A更新缓存成功，后续请求会命中缓存，得到新值 线程A未更新完缓存之前，这期间的读请求会读到短暂旧值。对业务影响短暂 先更新缓存，后更新数据库 写+读 并发 1. 线程A先更新缓存成功\n2. 线程B读取数据，此时线程B命中缓存，读取到最新值\n3.线程A更新数据库成功 虽然线程A还未更新完数据库，数据库与缓存会存在短暂的不一致。但在这之前进来的读请求都能命中缓存，获取到最新值，对业务影响较小 先更新数据库，后更新缓存 写+写 并发 1.线程A,B同时更新同一条数据\n2. 更新缓数据库的顺序是先A后B\n3. 更新缓存的顺序是先B后A 会导致数据库与缓存不一致。对业务影响较大 先更新缓存，后更新数据库 写+写 并发 1.线程A,B同时更新同一条数据\n2. 更新缓存的顺序是先A后B\n3. 更新数据库的顺序是先B后A 会导致数据库与缓存不一致。对业务影响较大 针对场景 1 和 2 的解决方案是：保存请求对缓存的读取记录，延时消息比较，发现不一致后，做业务补偿\n针对场景 3 和 4 的解决方案是：\n对于写请求，需要配合分布式锁使用。写请求进来时，针对同一个资源的修改操作，先加分布式锁，保证同一时间只有一个线程去更新数据库和缓存；没有拿到锁的线程把操作放入到队列中，延时处理。用这种方式保证多个线程操作同一资源的顺序性，以此保证一致性。\n如图：\n其中，分布式锁的实现可以使用以下策略：\n乐观锁\n使用版本号、updatetime；缓存中，只允许高版本覆盖低版本 Watach实现Redis乐观锁\nwatch监控rediskey的状态值，创建redis事务，key+1，执行事务，key被修改过则回滚 setnx\n获取锁：set/setnx；释放锁：del命令/Lua脚本 Redisson分布式锁\n利用Redis的Hash结构作为储存单元，将业务指定的名称作为key，将随机UUID和线程ID作为field，最后将加锁的次数作为value来储存；线程安全 强一致性策略 上述策略只能保证数据的最终一致性。要想做到强一致，最常见的方案是 2PC、3PC、Paxos、Raft 这类一致性协议，但它们的性能往往比较差，而且这些方案也比较复杂，还要考虑各种容错问题。如果业务层要求必须读取数据的强一致性，可以采取以下策略：\n（1）暂存并发读请求\n在更新数据库时，先在 Redis 缓存客户端暂存并发读请求，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。\n（2）串行化\n读写请求入队列，工作线程从队列中取任务来依次执行\n修改服务 Service 连接池，id 取模选取服务连接，能够保证同一个数据的读写都落在同一个后端服务上\n修改数据库 DB 连接池，id 取模选取 DB 连接，能够保证同一个数据的读写在数据库层面是串行的\n（3）使用 Redis 分布式读写锁\n将淘汰缓存与更新库表放入同一把写锁中，与其它读请求互斥，防止其间产生旧数据。读写互斥、写写互斥、读读共享，可满足读多写少的场景数据一致，也保证了并发性。并根据逻辑平均运行时间、响应超时时间来确定过期时间。\n总结 ![](/images/20211111222206819_26974.png =968x)\n针对读写缓存时：同步直写，更新数据库+更新缓存： 针对只读缓存时：更新数据库+删除缓存： ![](/images/20211111222315560_29131.png =968x)\n较为通用的一致性策略拟定： 在并发场景下，使用 “更新数据库 + 更新缓存” 需要用分布式锁保证缓存和数据一致性，且可能存在”缓存资源浪费“和”机器性能浪费“的情况；一般推荐使用 “更新数据库 + 删除缓存” 的方案。如果根据需要，热点数据较多，可以使用 “更新数据库 + 更新缓存” 策略。\n在 “更新数据库 + 删除缓存” 的方案中，推荐使用推荐用 “先更新数据库，再删除缓存” 策略，因为先删除缓存可能会导致大量请求落到数据库，而且延迟双删的时间很难评估。在 “先更新数据库，再删除缓存” 策略中，可以使用“消息队列+重试机制” 的方案保证缓存的删除。并通过 “订阅 binlog” 进行缓存比对，加上一层保障。\n此外，需要通过初始化缓存预热、多数据源触发、延迟消息比对等策略进行辅助和补偿。【多种数据更新触发源：定时任务扫描，业务系统 MQ、binlog 变更 MQ，相互之间作为互补来保证数据不会漏更新】\n数据一致性中需要注意的其他问题有哪些？ k-v 大小的合理设置\nRedis key 大小设计：由于网络的一次传输 MTU 最大为 1500 字节，所以为了保证高效的性能，建议单个 k-v 大小不超过 1KB，一次网络传输就能完成，避免多次网络交互；k-v 是越小性能越好Redis 热 key：（1） 当业务遇到单个读热 key，通过增加副本来提高读能力或是用 hashtag 把 key 存多份在多个分片中；（2）当业务遇到单个写热 key，需业务拆分这个 key 的功能，属于设计不合理- 当业务遇到热分片，即多个热 key 在同一个分片上导致单分片 cpu 高，可通过 hashtag 方式打散——[引自腾讯云技术分享] 避免其他问题导致缓存服务器崩溃，进而简直导致数据一致性策略失效\n缓存穿透、缓存击穿、缓存雪崩、机器故障等问题：\n总结 对于读多写少的服务，加入缓存可以提高性能，如果写多读少，又不能容忍缓存数据的不一致，那就没必要加缓存了，直接操作数据库。\n当然，如果数据库扛不住压力，还可以把缓存作为哦数据读写的主存储，然后异步的将数据同步到数据库，此时数据库只做为数据的备份\n放入缓存的数据应该是对实时性、一致性要求不是很高的数据。切记不要为了用缓存，同时又要保证绝对的一致性做大量的过度设计和控制，增加系统复杂性！\n参考资料 https://mp.weixin.qq.com/s/GU3cbUkI84IMwttDz16P3w ","date":"2021-11-11T20:29:30Z","permalink":"https://dccmmtop.github.io/posts/mysql%E5%92%8Credis%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/","section":"posts","tags":["架构","redis"],"title":"MySQL 和 Redis 的数据一致性问题"},{"categories":null,"contents":"#ruby的分布式锁实现，基于redis class Redlock DefaultRetryCount=3 DefaultRetryDelay=200 ClockDriftFactor = 0.01 UnlockScript=\u0026#39; if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end\u0026#39; def initialize(*server_urls) @servers = [] server_urls.each{|url| @servers \u0026lt;\u0026lt; Redis.new(:url =\u0026gt; url) } @quorum = server_urls.length / 2 + 1 @retry_count = DefaultRetryCount @retry_delay = DefaultRetryDelay @urandom = File.new(\u0026#34;/dev/urandom\u0026#34;) end def set_retry(count,delay) @retry_count = count @retry_delay = delay end def lock_instance(redis,resource,val,ttl) begin return redis.client.call([:set,resource,val,:nx,:px,ttl]) rescue return false end end def unlock_instance(redis,resource,val) begin redis.client.call([:eval,UnlockScript,1,resource,val]) rescue # Nothing to do, unlocking is just a best-effort attempt. end end def get_unique_lock_id val = \u0026#34;\u0026#34; bytes = @urandom.read(20) bytes.each_byte{|b| val \u0026lt;\u0026lt; b.to_s(32) } val end def lock(resource,ttl) val = get_unique_lock_id @retry_count.times { n = 0 start_time = (Time.now.to_f*1000).to_i @servers.each{|s| n += 1 if lock_instance(s,resource,val,ttl) } # Add 2 milliseconds to the drift to account for Redis expires # precision, which is 1 milliescond, plus 1 millisecond min drift # for small TTLs. drift = (ttl*ClockDriftFactor).to_i + 2 validity_time = ttl-((Time.now.to_f*1000).to_i - start_time)-drift if n \u0026gt;= @quorum \u0026amp;\u0026amp; validity_time \u0026gt; 0 return { :validity =\u0026gt; validity_time, :resource =\u0026gt; resource, :val =\u0026gt; val } else @servers.each{|s| unlock_instance(s,resource,val) } end # Wait a random delay before to retry sleep(rand(@retry_delay).to_f/1000) } return false end def unlock(lock) @servers.each{|s| unlock_instance(s,lock[:resource],lock[:val]) } rescue =\u0026gt;e puts \u0026#34;RedLock err:\u0026#34; + e.to_s end end #初始化分布式锁（一般在初始化程序中 config/initializers/xxx.rb）\n$distributed_locks = Redlock.new(\u0026ldquo;redis://#{REDIS_HOST}:6379\u0026rdquo;)\n使用示例\ndef self.apply_join(user_id, tag_info_id) # 设置重试次数和每次重试的间隔时间 $distributed_locks.set_retry(1, 100) # 持有锁的时间 tag_user_lock = $distributed_locks.lock(\u0026#34;#{user_id}_#{tag_info_id}\u0026#34;, 60 * 1000) result = false begin if tag_user_lock unless TagUserTag.where(user_id: user_id, tag_id: tag_info_id).first # 并发导致创建多条相同记录 TagUserTag.where(user_id: user_id, tag_id: tag_info_id, status: 1).delete_all TagUserTag.create(user_id: user_id, tag_id: tag_info_id, status: 1) update_user_tag_cache_status(user_id, tag_info_id, 1) result = true else Rails.logger.info(\u0026#34;该用户已经在本系统标签下，或 已提出申请\u0026#34;) end end # 释放锁 $distributed_locks.unlock(tag_user_lock) rescue =\u0026gt; e # 释放锁 $distributed_locks.unlock(tag_user_lock) end result end ","date":"2021-11-11T20:24:42Z","permalink":"https://dccmmtop.github.io/posts/ruby%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","section":"posts","tags":["ruby"],"title":"ruby分布式锁"},{"categories":null,"contents":"对Go语言来说，CSV文件可以通过encoding/csv包进行操作，下面通过这个包来读写CSV文件。\n由于程序在接下来的代码中立即就要对写入的posts.csv文件进行读取，而刚刚写入的数据有可能还滞留在缓冲区中，所以程序必须调用写入器的Flush方法来保证缓冲区中的所有数据都已经被正确地写入文件里面了。\n读取CSV文件的方法和写人文件的方法类似。首先，程序会打开文件，并通过将文件传递给NewReader函数来创建出一个读取器（reader），接着，程序会将读取器的FieldsPer Record字段的值设置为负数，这样的话，即使读取器在读取时发现记录（record）里面缺少了某些字段，读取进程也不会被中断。\n反之，如果FieldsPerRecord字段的值为正数，那么这个值就是用户要求从每条记录里面读取出的字段数量，当读取器从CsV文件里面读取出的字段数量少于这个值时，Go就会抛出一个错误。\n最后，如果FieldsPerRecord字段的值为0，那么读取器就会将读取到的第一条记录的字段数量用作FieldsPerRecord的值。\n在设置好FieldsPerRecord字段之后，程序会调用读取器的ReadAl1方法，一次性地读取文件中包含的所有记录；但如果文件的体积较大，用户也可以通过读取器提供的其他方法，以每次一条记录的方式读取文件。\npackage main import ( \u0026#34;encoding/csv\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; ) type Blog struct { Id int Content string } func main(){ csvFile, err := os.Create(\u0026#34;testCsv.csv\u0026#34;) if err != nil { panic(err) } defer csvFile.Close() csvWriter := csv.NewWriter(csvFile) allPost := []Blog{ {Id: 1, Content: \u0026#34;昨夜西风凋敝树\u0026#34;}, {Id: 2, Content: \u0026#34;忽如一夜春风来\u0026#34;}, {Id: 3, Content: \u0026#34;千树万树梨花开\u0026#34;}, {Id: 4, Content: \u0026#34;卷我屋上三重茅\u0026#34;}, } for _, blog := range allPost { csvWriter.Write([]string{strconv.Itoa(blog.Id), blog.Content}) } csvWriter.Flush() // 读取csv file,err := os.Open(\u0026#34;./testCsv.csv\u0026#34;) if err != nil { panic(nil) } defer file.Close() csvReader := csv.NewReader(file) // 设置每行至少的列数,遇到少于此数的行数会报错。-1 代表不检查列数 csvReader.FieldsPerRecord = -1 record, err := csvReader.ReadAll() if err != nil { panic(err) } var posts []Blog for _, item := range record { id ,_ := strconv.ParseInt(item[0],0,0) post := Blog{Id: int(id), Content: item[1]} posts = append(posts,post) } fmt.Println(posts[0]) } ","date":"2021-11-09T22:47:15Z","permalink":"https://dccmmtop.github.io/posts/go%E8%AF%BB%E5%86%99csv/","section":"posts","tags":["go"],"title":"Go读写CSV"},{"categories":null,"contents":"概念 幂等这个概念，是一个数学上的概念，即：f……(f(f(x))) = f(x)。用在计算机领域，指的是系统里的接口或方法对外的一种承诺，使用相同参数对同一资源重复调用某个接口或方法的结果与调用一次的结果相同。\n业务场景 从业务场景上来说，如：现在互联网电商的下单服务，同一个用户在短时间内调用某一个下单服务，只能下单成功一次；银行账户之间的转账，A账户给B账户转账，无论系统出现什么问题或故障，也只能转账成功一次；前端页面对相同表单的内容多次向后端发起提交请求，后端只能给出一个相同的结果等都属于幂等的范畴。\n试想一下，如果提供的这些服务不是幂等的，客户在下单时由于网络不稳定或是连续点了几次下单按钮，实际客户只下了一单，结果系统里给客户生成了多单，那平台/商家将是无法承受的，如果被“羊毛党”盯上，损失是无可估量的；银行之间的转账，A账户本来实际给B账户只转了一百万，结果B账户收到了几百万，这在业务上是不可接受的。分析这些业务场景，开发者发现，无论是下单服务、转账服务还是表单提交都是一个个业务请求，提供这些业务服务的接口或方法都应该保证无论服务是超时、重试或有故障等异常情况，都要满足业务上的处理结果是正确的。业务上的一次或多次请求，最终的处理结果是一致的，即：在一定时间内，服务的幂等其实就是请求的幂等。\n架构分析 从系统架构上进行分析，幂等该在哪一层去做，怎么做？\n上图为一个最常见的经典系统框架图，Web端发起一个请求到后端，幂等该在哪一层来处理呢？不妨一层一层的分析。\nNginx是否需要做幂等，Nginx的主要功能是做Web服务器、反向代理、负载均衡等，把请求转发到后端的服务器上，本身不参与具体的业务，所以Nginx是不需要做幂等处理的；Gateway是负责权限校验、安全防御、认证鉴权、流量控制、协议转换、日志审计、监控等，本身也不含对任何业务的处理，所以其也不需要做幂等处理；Service层通常是对业务逻辑进行处理、编排，可能会改变数据，但对于数据的改变结果，最终也还是需要通过数据访问层，写入到数据库，所以Service层也不需要做数据幂等；DAO层主要是和数据库交互，把Service层的结果写入数据库，对Service层提供读取、写入数据库的功能。\n在写入数据库的时候，针对每一次的写入，可能返回不同的结果，此时就需要按场景进行具体的分析对待；DataBase层，主要提供数据的存储，并不参与具体的业务逻辑计算。所以，通过对该架构的每一层的功能分析，得出对于请求的幂等处理，需要在DAO层做处理，以便保证多次请求和一次请求的结果是一致的。\n数据库操作分析 通过上面的分析，得出幂等需要在DAO层来处理，再进一步分析，得出DAO层的操作主要就是CRUD。下面逐一对每一种操作分析是否需要做幂等，以及怎么做。\nR（read）：对应的操作SQL语句为select。只要查询条件不变，在一定的时间内，执行一次和执行多次返回的结果肯定是相同的，所以其本身是幂等的，不需要再做处理。\nselect * from user where id = 1; 查询一次或多次结果是一致的，所以是幂等的。\nC（create）：对应的操作SQL语句为insert。此时，需要分情况，如果用到的数据库主键为数据库自增，不考虑业务主键防重的情况下，每一次写入数据库就不是幂等的，所以为了保证幂等，需要在数据insert前做业务防重或是在数据库表上对业务主键加唯一索引。如果数据库主键不是自增，是由业务系统写入的，需要在业务系统里把数据库主键和业务主键做一对一映射，或是由独立服务提供数据库主键和业务主键的映射关系，保证多次请求获取到的数据库主键和业务主键是一致的，确保写入数据库操作是幂等的。综合来说，就是相同的数据多次写入数据库后，能否保证只有一条数据。\ninsert into user (id,age,sex,ts) values(1,10,‘male’,2021-07-20 10:22:23); U（update）：对应的操作SQL语句为update。更新操作时，一定是要用绝对值进行更新操作，而不要用相对值进行更新，相对值更新可能导致更新操作不幂等。\n幂等：\nupdate user set age = 10 where id = 1; 非幂等：\nupdate user set age++ where id = 1; D（delete）：对应的操作SQL语句为delete。删除操作时，如果删除的是一个范围，生产上最好是禁止该类操作；比较推荐的做法是把按范围操作删除转换为先按范围查询，再按查询的主键进行删除。而且按范围删除的操作不是幂等的。\n幂等：\ndelete from user where id = 1; 非幂等：该类操作要禁止。\ndelete from user where id in （select id from user order by id desc limit 10); 常见业务场景 保证幂等的实现方式有多种，此处例举几类常见的业务场景，在实际应用中，根据业务场景进行选用。\n页面token机制 进入页面时，从服务器获取token，在服务器端把token进行存储，提交时把token带到服务器端进行验证,这里的token相当于业务ID\n常见的处理流程如下\n乐观锁机制 使用数据库的版本号实现乐观锁，数据库更新时，判断版本号是否与查询时保持一致，一致更新成功，否则更新失败；\nselect+insert 数据写入前，先查询数据是否存在，存在直接返回，不存在则写入数据，保证写入数据库的数据正确性；常用于并发不高的一些后台系统或是防止任务的重复执行；\n悲观锁机制 一般id为主键或唯一索引，仅锁定当前记录；\nselect * from table where id = \u0026#39;1234\u0026#39; for update; 去重表 每一次写入或更新业务表时，先查询去重表是否已经存在记录，再操作业务表。\n数据库唯一索引 为业务表建立唯一索引，避免业务数据多次写入；\n状态机 务状态在变更之前是有条件的，必须按设定的状态条件进行更新；\n在实际开发中，保证提供的接口或服务的幂等（性），是一个最基本的技术要求，希望通过该分析，能对还未理解幂等（性）的研发人员有所帮助。\n参考资料 https://mp.weixin.qq.com/s/_Nn5F98PvoWk_xjVrIpFNQ ","date":"2021-11-09T21:52:29Z","permalink":"https://dccmmtop.github.io/posts/%E5%B9%82%E7%AD%89%E8%AE%BE%E8%AE%A1/","section":"posts","tags":["架构"],"title":"幂等设计"},{"categories":null,"contents":"logrotate 在很多 Linux 发行版上都是默认安装的。系统会定时运行 logrotate，一般是每天一次。系统是这么实现按天执行的。crontab 会每天定时执行 /etc/cron.daily 目录下的脚本，而这个目录下有个文件叫 logrotate。在 centos 上脚本内容是这样的：\n系统自带 cron task：/etc/cron.daily/logrotate，每天运行一次。\n[root@gop-sg-192-168-56-103 logrotate.d]# cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \u0026#34;ALERT exited abnormally with [$EXITVALUE]\u0026#34; fi exit 0 可以看到这个脚本主要做的事就是以 /etc/logrotate.conf 为配置文件执行了 logrotate。就是这样实现了每天执行一次 logrotate。\n很多程序的会用到 logrotate 滚动日志，比如 nginx。它们安装后，会在 /etc/logrotate.d 这个目录下增加自己的 logrotate 的配置文件。logrotate 什么时候执行 /etc/logrotate.d 下的配置呢？看到 /etc/logrotate.conf 里这行，一切就不言而喻了。\ninclude /etc/logrotate.d logrotate 原理 ogrotate 是怎么做到滚动日志时不影响程序正常的日志输出呢？logrotate 提供了两种解决方案。\ncreate copytruncate Linux 文件操作机制\n介绍一下相关的 Linux 下的文件操作机制。\nLinux 文件系统里文件和文件名的关系如下图\n目录也是文件，文件里存着文件名和对应的 inode 编号。通过这个 inode 编号可以查到文件的元数据和文件内容。文件的元数据有引用计数、操作权限、拥有者 ID、创建时间、最后修改时间等等。文件件名并不在元数据里而是在目录文件中。因此文件改名、移动，都不会修改文件，而是修改目录文件。\ncreate 这也就是默认的方案，可以通过 create 命令配置文件的权限和属组设置；这个方案的思路是重命名原日志文件，创建新的日志文件。详细步骤如下：\n重命名正在输出日志文件，因为重命名只修改目录以及文件的名称，而进程操作文件使用的是 inode，所以并不影响原程序继续输出日志。 创建新的日志文件，文件名和原日志文件一样，注意，此时只是文件名称一样，而 inode 编号不同，原程序输出的日志还是往原日志文件输出。 最后通过某些方式通知程序，重新打开日志文件；由于重新打开日志文件会用到文件路径而非 inode 编号，所以打开的是新的日志文件。 如上也就是 logrotate 的默认操作方式，也就是 mv+create 执行完之后，通知应用重新在新文件写入即可。mv+create 成本都比较低，几乎是原子操作，如果应用支持重新打开日志文件，如 syslog, nginx, mysql 等，那么这是最好的方式。比如通过 kill 命令向程序发送一个 HUP 信号，使之重新加载\n不过，有些程序并不支持这种方式，压根没有提供重新打开日志的接口；而如果重启应用程序，必然会降低可用性，为此引入了如下方式。\ncopytruncate 该方案是把正在输出的日志拷 (copy) 一份出来，再清空 (trucate) 原来的日志；详细步骤如下：\n将当前正在输出的日志文件复制为目标文件，此时程序仍然将日志输出到原来文件中，此时，原文件名也没有变。 清空日志文件，原程序仍然还是输出到预案日志文件中，因为清空文件只把文件的内容删除了，而 inode 并没改变，后续日志的输出仍然写入该文件中。 如上所述，对于 copytruncate 也就是先复制一份文件，然后清空原有文件。\n通常来说，清空操作比较快，但是如果日志文件太大，那么复制就会比较耗时，从而可能导致部分日志丢失。不过这种方式不需要应用程序的支持即可。\n配置 logrotate 执行文件： /usr/sbin/logrotate\n主配置文件: /etc/logrotate.conf\n自定义配置文件: /etc/logrotate.d/*.conf\n修改配置文件后，并不需要重启服务。\n由于 logrotate 实际上只是一个可执行文件，不是以 daemon 运行。\n运行 logrotate logrotate [OPTION...] \u0026lt;configfile\u0026gt; -d, --debug ：debug 模式，测试配置文件是否有错误。 -f, --force ：强制转储文件。 -m, --mail=command ：压缩日志后，发送日志到指定邮箱。 -s, --state=statefile ：使用指定的状态文件。 -v, --verbose ：显示转储过程 crontab 定时 通常惯用的做法是配合 crontab 来定时调用。默认是一天执行一次，可以自己添加 crontab 规则\ncrontab -e */30 * * * * /usr/sbin/logrotate /etc/logrotate.d/rsyslog \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 手动运行 debug 模式：指定 [-d|\u0026ndash;debug]\nlogrotate -d 并不会真正进行 rotate 或者 compress 操作，但是会打印出整个执行的流程，和调用的脚本等详细信息。\nverbose 模式： 指定 [-v|\u0026ndash;verbose]\nlogrotate -v 会真正执行操作，打印出详细信息（debug 模式，默认是开启 verbose）\nlogrotate 参数 详细介绍请自行 man logrotate\n主要介绍下完成常用需求会用到的一些参数。\n一个典型的配置文件如下：\n[root@localhost ~]# vim /etc/logrotate.d/log_file /var/log/log_file { monthly rotate 5 compress delaycompress missingok notifempty create 644 root root postrotate /usr/bin/killall -HUP rsyslogd endscript } monthly: 日志文件将按月轮循。其它可用值为 daily，weekly 或者 yearly。 rotate 5: 一次将存储 5 个归档日志。对于第六个归档，时间最久的归档将被删除。 compress: 在轮循任务完成后，已轮循的归档将使用 gzip 进行压缩。 delaycompress: 总是与 compress 选项一起用，delaycompress 选项指示 logrotate 不要将最近的归档压缩，压缩 将在下一次轮循周期进行。这在你或任何软件仍然需要读取最新归档时很有用。 missingok: 在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。 notifempty: 如果日志文件为空，轮循不会进行。 create 644 root root: 以指定的权限创建全新的日志文件，同时 logrotate 也会重命名原始日志文件。 postrotate/endscript: 在所有其它指令完成后，postrotate 和 endscript 里面指定的命令将被执行。在这种情况下，rsyslogd 进程将立即再次读取其配置并继续运行。 上面的模板是通用的，而配置参数则根据你的需求进行调整，不是所有的参数都是必要的。\n/var/log/log_file {\rsize=50M\rrotate 5\rdateext\rcreate 644 root root\rpostrotate\r/usr/bin/killall -HUP rsyslogd\rendscript\r} 在上面的配置文件中，我们只想要轮询一个日志文件，size=50M 指定日志文件大小可以增长到 50MB,不满50MB不会被分割\ndateext 指示让旧日志文件以创建日期命名。\n常见配置参数\ndaily: 指定转储周期为每天 weekly: 指定转储周期为每周 monthly: 指定转储周期为每月 rotate count: 指定日志文件删除之前转储的次数，0 指没有备份，5 指保留 5 个备份 tabooext [+] list：让 logrotate 不转储指定扩展名的文件，缺省的扩展名是：.rpm-orig, .rpmsave, v, 和～ missingok：在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。 size size：当日志文件到达指定的大小时才转储，bytes (缺省) 及 KB (sizek) 或 MB (sizem) compress： 通过 gzip 压缩转储以后的日志 nocompress： 不压缩 copytruncate：用于还在打开中的日志文件，把当前日志备份并截断 nocopytruncate： 备份日志文件但是不截断 create mode owner group: 转储文件，使用指定的文件模式创建新的日志文件 nocreate: 不建立新的日志文件 delaycompress： 和 compress 一起使用时，转储的日志文件到下一次转储时才压缩 nodelaycompress： 覆盖 delaycompress 选项，转储同时压缩。 errors address: 专储时的错误信息发送到指定的 Email 地址 ifempty: 即使是空文件也转储，这个是 logrotate 的缺省选项。 notifempty: 如果是空文件的话，不转储 mail address: 把转储的日志文件发送到指定的 E-mail 地址 nomail: 转储时不发送日志文件 olddir directory：储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统 noolddir： 转储后的日志文件和当前日志文件放在同一个目录下 prerotate/endscript： 在转储以前需要执行的命令可以放入这个对，这两个关键字必须单独成行 nginx 日志轮换示例 /var/log/nginx/*.log /var/log/nginx/*/*.log{\rdaily\rmissingok\rrotate 14\rcompress\rdelaycompress\rnotifempty\rcreate 640 root adm\rsharedscripts\rpostrotate\r[ ! -f /var/run/nginx.pid ] || kill -USR1 `cat /var/run/nginx.pid`\rendscript\r} 关于 USR1 信号解释 USR1 亦通常被用来告知应用程序重载配置文件；例如，向 Apache HTTP 服务器发送一个 USR1 信号将导致以下步骤的发生：停止接受新的连接，等待当前连接停止，重新载入配置文件，重新打开日志文件，重启服务器，从而实现相对平滑的不关机的更改。\n对于 USR1 和 2 都可以用户自定义的，在 POSIX 兼容的平台上，SIGUSR1 和 SIGUSR2 是发送给一个进程的信号，它表示了用户定义的情况。它们的符号常量在头文件 signal.h 中定义。在不同的平台上，信号的编号可能发生变化，因此需要使用符号名称。\nkill -HUP pid\nkillall -HUP pName\n其中 pid 是进程标识，pName 是进程的名称。\n如果想要更改配置而不需停止并重新启动服务，可以使用上面两个命令。在对配置文件作必要的更改后，发出该命令以动态更新服务配置。根据约定，当你发送一个挂起信号 (信号 1 或 HUP) 时，大多数服务器进程 (所有常用的进程) 都会进行复位操作并重新加载它们的配置文件。\n参考资料 Linux 日志切割神器 logrotate 原理介绍和配置详解\n","date":"2021-11-08T22:25:00Z","permalink":"https://dccmmtop.github.io/posts/%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2logrotate%E5%8E%9F%E7%90%86%E5%92%8C%E9%85%8D%E7%BD%AE/","section":"posts","tags":null,"title":"日志切割logrotate原理和配置"},{"categories":null,"contents":"上下文感知 Go语言的模板引擎可以根据内容所处的上下文改变其显示.\n上下文感知的一个显而易见的用途就是对被显示的内容实施正确的转义（escape）：这意味着，如果模板显示的是HTML格式的内容，那么模板将对其实施HTML转义；如果模板显示的是JavaScript格式的内容，那么模板将对其实施JavaScript转义；诸如此类。除此之外，Go模板引擎还可以识别出内容中的URL或者css样式。\n示例 package main import ( \u0026#34;html/template\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/testContextAware\u0026#34;, testContextAware) server.ListenAndServe() } func testContextAware(w http.ResponseWriter, r *http.Request) { t, err := template.ParseFiles(\u0026#34;./testContextAware.tmpl\u0026#34;) if err != nil { panic(err) } content := `我问: \u0026lt;i\u0026gt; \u0026#34;发生了什么\u0026#34; \u0026lt;/i\u0026gt;` err = t.Execute(w,content) if err != nil { panic(err) } } 上下文感知模板 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt;{{ . }}\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/{{ . }}\u0026#34;\u0026gt;Path\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/?q={{ . }}\u0026#34;\u0026gt;Query\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a onclick=\u0026#34;f (\u0026#39;{{ .}}\u0026#39;) \u0026#34;\u0026gt;Onclick\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `` ## 结果 ```txt HTTP/1.1 200 OK Date: Mon, 08 Nov 2021 13:52:59 GMT Content-Length: 569 Content-Type: text/html; charset=utf-8 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt;我问: \u0026amp;lt;i\u0026amp;gt; \u0026amp;#34;发生了什么\u0026amp;#34; \u0026amp;lt;/i\u0026amp;gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/%e6%88%91%e9%97%ae:%20%3ci%3e%20%22%e5%8f%91%e7%94%9f%e4%ba%86%e4%bb%80%e4%b9%88%22%20%3c/i%3e\u0026#34;\u0026gt;Path\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/?q=%e6%88%91%e9%97%ae%3a%20%3ci%3e%20%22%e5%8f%91%e7%94%9f%e4%ba%86%e4%bb%80%e4%b9%88%22%20%3c%2fi%3e\u0026#34;\u0026gt;Query\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a onclick=\u0026#34;f (\u0026#39;我问: \\u003ci\\u003e \\u0022发生了什么\\u0022 \\u003c\\/i\\u003e\u0026#39;) \u0026#34;\u0026gt;Onclick\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 原本有可能会被浏览器执行的js已经被转义了，原样展示\n应用场景 由上可见，上下文感知特性可以很方便的避免XSS攻击\n上下文感知功能不仅能够自动对HTML进行转义，它还能够防止基于JavaScript，Css甚至URL的XSS攻击。那么这是否意味着我们只要使用Go的模板引擎就可以无忧无虑地进行开发了呢？并非如此，上下文感知虽然很方便，但它并非灵丹妙药，而且有不少方法可以绕开上下文感知。\n实际上，如果需要，用可以完全不使用上下文感知特性的。\n不使用上下文感知 可以使用类型转换，把内容转换成html\nfunc testContextAware(w http.ResponseWriter, r *http.Request) { t, err := template.ParseFiles(\u0026#34;./testContextAware.tmpl\u0026#34;) if err != nil { panic(err) } content := `我问: \u0026lt;i\u0026gt; \u0026#34;发生了什么\u0026#34; \u0026lt;/i\u0026gt;` err = t.Execute(w,template.HTML(content)) if err != nil { panic(err) } } ","date":"2021-11-08T21:32:35Z","permalink":"https://dccmmtop.github.io/posts/go%E6%A8%A1%E6%9D%BF%E4%B9%8B%E4%B8%8A%E4%B8%8B%E6%84%9F%E7%9F%A5/","section":"posts","tags":["go"],"title":"Go模板之上下文感知"},{"categories":null,"contents":"Go的模板动作就是嵌入模板的命令\n条件动作 {{ if arg }} some content {{ else }} other content {{ end }} 迭代动作 迭代动作可以对数组，切片，映射，或者通道进行迭代, 在迭代循环内部， 点(.) 会被设置正在当前迭代内容\n设置动作 设置动作允许为指定的范围的点(.) 设置指定的值。减少重复代码\n包含动作 包含动作（include action）允许用户在一个模板里面包含另一个模板，从而构建出嵌套的模板。\n包含动作的格式为{{ template \u0026ldquo;name\u0026rdquo; arg }}，其中name参数为被包含模板的名字。没有特别指定名称时，就是文件名， arg 是参数名\n参数、变量和管道 参数\n一个参数（argument）就是模板中的一个值。它可以是布尔值、整数、字符串等字面量，也可以是结构、结构中的一个字段或者数组中的一个键。除此之外，参数还可以是一个变量、一个方法（这个方法必须只返回一个值，或者只返回一个值和一个错误）或者一个函数。最后，参数也可以是一个点（.），用于表示处理器向模板引擎传递的数据。\n变量\n除了参数之外，用户还可以在动作中设置变量。变量以美元符号（$）开头，就像这样：\n在这个例子中，点（.）是一个映射，而动作range在迭代这个映射的时候，会将变量$key和$value分别初始化为当前被迭代映射元素的键和值。\n管道\n模板中的管道（pipeline）是多个有序地串联起来的参数、函数和方法，它的工作方式和语法跟Unix的管道也非常相似：\n{{p1 | p2 | p3}} 这里的p1,p2和p3可以是参数或者函数。\n管道允许用户将一个参数的输出传递给下一个参数，而各个参数之间则使用 | 分隔。\n函数 Go的模板引擎函数都是受限制的：尽管这些函数可以接受任意多个参数作为输入，但它们只能返回一个值，或者返回一个值和一个错误。\n为了创建一个自定义模板函数，需要：\n创建一个名为FuncMap的映射，并将映射的键设置为函数的名字，而映射的值则设置为实际定义的函数 将FuncMap 与模板绑定 示例：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;html/template\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/testFuncMap\u0026#34;, testFuncMap) server.ListenAndServe() } func testFuncMap(w http.ResponseWriter, re *http.Request) { funcMap := template.FuncMap{ \u0026#34;fdate\u0026#34;: formatDate, } t := template.New(\u0026#34;testFuncMap.tmpl\u0026#34;).Funcs(funcMap) // 前后模板的名字必须相同 t, err := t.ParseFiles(\u0026#34;./testFuncMap.tmpl\u0026#34;) if err != nil { fmt.Println(err) return } err = t.Execute(w,time.Now()) if err != nil { fmt.Println(err) return } } func formatDate(t time.Time) string{ layout := \u0026#34;2006-01-02\u0026#34; return t.Format(layout) } 模板文件 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; the date/time is {{. | fdate}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 结果 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; the date/time is 2021-11-08 \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","date":"2021-11-02T22:44:33Z","permalink":"https://dccmmtop.github.io/posts/go%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%8A%A8%E4%BD%9C/","section":"posts","tags":["go"],"title":"Go模板之动作"},{"categories":null,"contents":"Go的模板都是文本文档（其中Web应用的模板通常都是HTML），它们都嵌入了一些称为动作（action）的指令。从模板引擎的角度来说，模板就是嵌入了动作的文本（这些文本通常包含在模板文件里面），而模板引擎则通过分析并执行这些文本来生成出另外一些文本。Go语言拥有通用模板引擎库 text/template，它可以处理任意格式的文本，除此之外，Go语言还拥有专门为HTML格式而设的模板引擎库 html/template 模板中的动作默认使用两个大括号 {{}}）包围，如果用户有需要，也可以通过模板引擎提供的方法自行指定其他定界符（delimiter）。\n使用步骤 使用 Go 的模板引擎需要两个步骤：\n对文本格式的模板源进行语法分析，创建一个经过语法分析的模板结构，其中模板源既可以是一个字符串，也可以是模板文件包含的内容， 执行经过语法分析的模板，将 ResponseWriter 和模板所需要的动态数据传递给模板引擎，被调用的模板引擎会把分析后的模板结构和数据结合起来，生成最终的HTML,并将HTML写入 ResponseWriter.\n示例:\ntmp.html: \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{.}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; package main import ( \u0026#34;html/template\u0026#34; \u0026#34;net/http\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/process\u0026#34;,process) server.ListenAndServe() } func process(w http.ResponseWriter, request *http.Request) { t, _ := template.ParseFiles(\u0026#34;./tmpl.html\u0026#34;) t.Execute(w, \u0026#34;你好哇！李银河\u0026#34;) // 第二种方式 /** t = template.New(\u0026#34;tmpl.html\u0026#34;) t.ParseFiles(\u0026#34;./tmpl.html\u0026#34;) t.Execute(w,\u0026#34;hello\u0026#34;) */ } 对模板进行语法分析 ParseFiles是一个独立的（standalone）函数，它可以对模板文件进行语法分析，并创建出一个经过语法分析的模板结构以供Execute方法执行。实际上，ParseFiles函数只是为了方便地调用Template结构的ParseFiles方法而设置的一个函数-当用户调用Parseriles函数的时候，Go会创建一个新的模板，并将用户给定的模板文件的名字用作这个新模板的名字：\nt, _ := template.ParseFiles(\u0026#34;tmpl.html\u0026#34;) 这相当于创建一个新模板，然后调用它的ParseFiles方法\nt := template.New(\u0026#34;tmpl.html\u0026#34;) t, _ := t.ParseFiles(\u0026#34;tmpl.html\u0026#34;) 上面两种方式都可以接受多个模板参数，但只返回第一个参数对应的模板结构\n当用户向ParseFiles函数或Parseriles方法传入多个文件时，ParseFiles只会返回用户传入的第一个文件的已分析模板，并且这个模板也会根据用户传入的第一个文件的名字进行命名；至于其他传入文件的已分析模板则会被放置到一个映射里面，这个映射可以在之后执行模板时使用。\n换句话说，我们可以这样认为：在向Parseriles传入单个文件时，Parseriles返回的是一个模板；而在向ParseFiles传入多个文件时，ParseFiles返回的则是一个模板集合，理解这一点能够帮助我们更好地学习嵌套模板技术。\n对字符串分析 在绝大多数情况下，程序都是对模板文件进行语法分析，但是在需要时，程序也可以直接对字符串形式的模板进行语法分析。实际上，所有对模板进行语法分析的手段最终都需要调用Parse方法来执行实际的语法分析操作。比如说，在模板内容相同的情况下，语句\nt, _ := template.ParseFiles(\u0026#34;./tmpl.html\u0026#34;) 和代码\ntmpl := `\u0026lt;! DOCTYPE html\u0026gt; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34;content=\u0026#34;text/html; charset=utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Go Web programming\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {.}} \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;` t := template. New (\u0026#34;tmpl. html\u0026#34;) t, = t. Parse (tmpl). t. Execute (w, \u0026#34;Hello world!\u0026#34;) 将产生相同的效果\n对错误的处理 一直都没有处理分析模板时可能会产生的错误。虽然Go语言的一般做法是手动地处理错误，但Go也提供了另外一种机制，专门用于处理分析模板时出现的错误：\nt := template.Must(template.ParseFiles(\u0026quot;tmpl.html\u0026quot;))\nMust函数可以包裹起一个函数，被包裹的函数会返回一个指向模板的指针和一个错误，如果这个错误不是nil，那么Must函数将产生一个panic。\n在Go里面，panic会导致正常的执行流程被终止：如果panic是在函数内部产生的，那么函数会将这个panic返回给它的调用者。panic会一直向调用栈的上方传递，直至main函数为止，并且程序也会因此而崩溃。\n执行模板 执行模板最常用的方法就是调用模板的Execute方法，并向它传递Responsewriter以及模板所需的数据。在只有一个模板的情况下，上面提到的这种方法总是可行的，但如果模板不止一个，那么当对模板集合调用Execute方法的时候，Execute方法只会执行模板集合中的第一个模板。如果想要执行的不是模板集合中的第一个模板而是其他模板，就需要使用ExecuteTemplate 方法\nt,_ := template.ParseFiles(\u0026#34;t1. html\u0026#34;, \u0026#34;t2. html\u0026#34;) 变量t就是一个包含了两个模板的模板集合，其中第一个模板名为t1.html，而第二个模板则名为t2.html\n（正如前面所说，除非显式地对模板名进行修改，否则模板的名字和后缀名将由传入的模板文件决定），如果对这个模板集合调用Execute方法：\nt.Execute(w,\u0026#34;你好哇！\u0026#34;) 就只有模板t1.html会被执行。如果想要执行的是模板t2.html而不是t1.html，则需要执行以下语句：\nt.ExecuteTemplate(w,\u0026#34;t2.html\u0026#34;,\u0026#34;你好哇!\u0026#34;) `` ","date":"2021-11-01T22:51:35Z","permalink":"https://dccmmtop.github.io/posts/go%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E/","section":"posts","tags":["go"],"title":"Go的模板引擎"},{"categories":null,"contents":"为了向用户报告某个动作的执行情况，应用程序有时候会向用户展示一条简短的通知消息，\n比如说，如果一个用户尝试在论坛上发表一篇帖子，但是这篇帖子因为某种原因而发表失败了，那么论坛应该向这个用户展示一条帖子发布失败的消息。\n这种通知消息应该出现在用户当前所在的页面，但是在通常情况下，用户在访问这个页面时却不应该看到这样的消息。因此程序实际上要做的是在某个条件被满足时，才在页面上显示一条临时出现的消息，这样用户在刷新页面之后就不会再看见相同的消息了-我们把这种临时出现的消息称为闪现消息（flash message）\n实现闪现消息的方法有很多种，但最常用的方法是把这些消息存储在页面刷新时就会被移除的会话cookie里面\n添加闪现消息到cookie setMessage处理器函数的定义跟之前展示过的setCookie处理器函数的定义非常相似，主要的区别在于setMessage对消息使用了Base64URL编码，以此来满足响应首部对cookie值的URL编码要求。在设置cookie时，如果cookie的值没有包含诸如空格或者百分号这样的特殊字符，那么不对它进行编码也是可以的；但是因为在发送闪现消息时，消息本身通常会包含诸如空格这样的字符，所以对cookie的值进行编码就成了一件必不可少的事情了。\nfunc setMessage(w http.ResponseWriter, r *http.Request) { msg := []byte(\u0026#34;创建失败，缺少必填字段!\u0026#34;) cookie := \u0026amp;http.Cookie{ Name: \u0026#34;flash\u0026#34;, // 必须对 cookie 进行url编码 Value: base64.URLEncoding.EncodeToString(msg), } http.SetCookie(w, cookie) } 展示闪现消息 // 展示闪现消息 func showMessage(w http.ResponseWriter, r *http.Request) { msg, err := r.Cookie(\u0026#34;flash\u0026#34;) if err != nil { if err == http.ErrNoCookie { fmt.Fprintln(w, \u0026#34;not found message\u0026#34;) return } } // 使cookie过期，让浏览器删除cookie cookie := http.Cookie{ Name: \u0026#34;flash\u0026#34;, MaxAge: -1, Expires: time.Unix(1,0), } http.SetCookie(w, \u0026amp;cookie) fmt.Fprintln(w, msg) } 这个函数首先会尝试获取指定的cookie，如果没有找到该cookie，它就会把变量err设置成一个http.ErrNoCookie值，并向浏览器返回一条\u0026quot;No message found\u0026quot;消息。如果找到了这个cookie，那么它必须完成以下两个操作\n创建一个同名的cookie，将它的MaxAge值设置为负数，并且将Expires值也设置成一个已经过去的时间； 使用SetCookie方法将刚刚创建的同名cookie发送至客户端。 初看上去，这两个操作的目的似乎是要替换已经存在的cookie，但实际上，因为新cookie的MaxAge值为负数，并且Expires值也是一个已经过去的时间，所以这样做实际上就是要完全地移除这个cookie。在设置完新cookie之后，程序会对存储在旧cookie中的消息进行解码，并通过响应返回这条消息。\n完整代码 package main import ( \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) /** * 利用cookie实现闪现消息 */ func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;setMessage\u0026#34;,setMessage) http.HandleFunc(\u0026#34;showMessage\u0026#34;,showMessage) server.ListenAndServe() } // 展示闪现消息 func showMessage(w http.ResponseWriter, r *http.Request) { msg, err := r.Cookie(\u0026#34;flash\u0026#34;) if err != nil { if err == http.ErrNoCookie { fmt.Fprintln(w, \u0026#34;not found message\u0026#34;) return } } // 使cookie过期，让浏览器删除cookie cookie := http.Cookie{ Name: \u0026#34;flash\u0026#34;, MaxAge: -1, Expires: time.Unix(1,0), } http.SetCookie(w, \u0026amp;cookie) fmt.Fprintln(w, msg) } func setMessage(w http.ResponseWriter, r *http.Request) { msg := []byte(\u0026#34;创建失败，缺少必填字段!\u0026#34;) cookie := \u0026amp;http.Cookie{ Name: \u0026#34;flash\u0026#34;, // 必须对 cookie 进行url编码 Value: base64.URLEncoding.EncodeToString(msg), } http.SetCookie(w, cookie) } ","date":"2021-10-28T23:20:50Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%A9%E7%94%A8cookie%E5%AE%9E%E7%8E%B0%E9%97%AA%E7%8E%B0%E6%B6%88%E6%81%AF/","section":"posts","tags":["go"],"title":"利用cookie实现闪现消息"},{"categories":null,"contents":"解析请求头 // 解析请求头 func headers(w http.ResponseWriter, request *http.Request) { // 获取所欲请求头，Header 是个 map, key 是字符串，value 是字符串切片 headers := request.Header fmt.Printf(\u0026#34;所有请求头headers: %v\\n\u0026#34;, headers) // 获取单个请求头,返回的是字符串切片 // [gzip, deflate] coding := headers[\u0026#34;Accept-Encoding\u0026#34;] fmt.Printf(\u0026#34;Accept-Encoding: %v\\n\u0026#34;, coding) // 获取单个请求头,如果请求头的值是多个，用逗号拼接 // gzip, deflate coding1 := headers.Get(\u0026#34;Accept-Encoding\u0026#34;) fmt.Printf(\u0026#34;Accept-Encoding1: %v\\n\u0026#34;, coding1) fmt.Fprintln(w, headers) } 从流中解析请求体 直接读取 body 字节流，再转换成自己想要的格式，比较麻烦，但可以处理任何请求类型的参数\n// 接请求体-原始 // 可以处理 application/json 类型的参数 func body(w http.ResponseWriter, request *http.Request) { // 获取请求体的长度 len := request.ContentLength body := make([]byte, len) //　将数据读取到字节数组中 request.Body.Read(body) fmt.Fprintln(w,string(body)) } 解析表单 调用ParseForm方法或者ParseMultipartForm方法，对请求进行语法分析。 根据步骤1调用的方法，访问相应的Form字段、PostForm字段或MultipartForm字段 // 解析表单-手动解析语法 func form(w http.ResponseWriter, request *http.Request) { // 手动先进行语法分析 request.ParseForm() // 再访问Form 或PostForm 字段 // Form 会同时返回 URL 上的参数值 和 form 表单中的参数值，如果两处有相同的参数，值是字符串切片 fmt.Printf(\u0026#34;Form: %v\\n\u0026#34;,request.Form) // PostForm 只会包含表单中的参数 fmt.Printf(\u0026#34;PostForm: %v\\n\u0026#34;,request.PostForm) // multipart/form-data 类型参数 // 取出1024字节数据 request.ParseMultipartForm(1024) fmt.Printf(\u0026#34;multipart: %v\\n\u0026#34;,request.MultipartForm) } 直接获取表单值 因为Formvalue方法即使在给定键拥有多个值的情况下，也只会从Form结构中取出给定键的第一个值，所以如果想要获取给定键包含的所有值，那么就需要直接访问Form结构\n// 解析表单-自动解析语法 func formValue(w http.ResponseWriter, request *http.Request) { // FormValue 方法会自动调用 ParseForm 或 ParseMultipartForm 方法 // 如果参数有多个值，只会取第一个, 如果需要获取全部值，用Form字段 request.FormValue(\u0026#34;userId\u0026#34;) // PostFormValue 同上，但只包含表单中的参数，包含URl中参数 request.PostFormValue(\u0026#34;name\u0026#34;) } 获取文件 func uploadFile(w http.ResponseWriter, request *http.Request) { // 第一种方法，解析文件流 request.ParseMultipartForm(1024) // 取出文件头 fileHeader := request.MultipartForm.File[\u0026#34;uploaded\u0026#34;][0] // 打开文件 file, err := fileHeader.Open() if err != nil { data ,err := ioutil.ReadAll(file) if err != nil { // 将文件写入响应体 fmt.Println(\u0026#34;file: \u0026#34;, string(data)) } } // 第二种，FormFile 方法 file1,_, err := request.FormFile(\u0026#34;uploaded1\u0026#34;) if err != nil { data, err := ioutil.ReadAll(file1) if err != nil { fmt.Println(\u0026#34;file1: \u0026#34;, string(data)) } } } 完整代码 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/header\u0026#34;,headers) http.HandleFunc(\u0026#34;/body\u0026#34;,body) http.HandleFunc(\u0026#34;/form\u0026#34;,form) http.HandleFunc(\u0026#34;/formValue\u0026#34;,formValue) http.HandleFunc(\u0026#34;/uploadFile\u0026#34;,uploadFile) server.ListenAndServe() } func uploadFile(w http.ResponseWriter, request *http.Request) { // 第一种方法，解析文件流 request.ParseMultipartForm(1024) // 取出文件头 fileHeader := request.MultipartForm.File[\u0026#34;uploaded\u0026#34;][0] // 打开文件 file, err := fileHeader.Open() if err != nil { data ,err := ioutil.ReadAll(file) if err != nil { // 将文件写入响应体 fmt.Println(\u0026#34;file: \u0026#34;, string(data)) } } // 第二种，FormFile 方法 file1,_, err := request.FormFile(\u0026#34;uploaded1\u0026#34;) if err != nil { data, err := ioutil.ReadAll(file1) if err != nil { fmt.Println(\u0026#34;file1: \u0026#34;, string(data)) } } } // 解析表单-自动解析语法 func formValue(w http.ResponseWriter, request *http.Request) { // FormValue 方法会自动调用 ParseForm 或 ParseMultipartForm 方法 // 如果参数有多个值，只会取第一个, 如果需要获取全部值，用Form字段 request.FormValue(\u0026#34;userId\u0026#34;) // PostFormValue 同上，但只包含表单中的参数，包含URl中参数 request.PostFormValue(\u0026#34;name\u0026#34;) } // 解析表单-手动解析语法 func form(w http.ResponseWriter, request *http.Request) { // 手动先进行语法分析 request.ParseForm() // 再访问Form 或PostForm 字段 // Form 会同时返回 URL 上的参数值 和 form 表单中的参数值，如果两处有相同的参数，值是字符串切片 fmt.Printf(\u0026#34;Form: %v\\n\u0026#34;,request.Form) // PostForm 只会包含表单中的参数 fmt.Printf(\u0026#34;PostForm: %v\\n\u0026#34;,request.PostForm) // multipart/form-data 类型参数 // 取出1024字节数据 request.ParseMultipartForm(1024) fmt.Printf(\u0026#34;multipart: %v\\n\u0026#34;,request.MultipartForm) } // 接请求体-原始 // 可以处理 application/json 类型的参数 func body(w http.ResponseWriter, request *http.Request) { // 获取请求体的长度 len := request.ContentLength body := make([]byte, len) //　将数据读取到字节数组中 request.Body.Read(body) fmt.Fprintln(w,string(body)) } // 解析请求头 func headers(w http.ResponseWriter, request *http.Request) { // 获取所欲请求头，Header 是个 map, key 是字符串，value 是字符串切片 headers := request.Header fmt.Printf(\u0026#34;所有请求头headers: %v\\n\u0026#34;, headers) // 获取单个请求头,返回的是字符串切片 // [gzip, deflate] coding := headers[\u0026#34;Accept-Encoding\u0026#34;] fmt.Printf(\u0026#34;Accept-Encoding: %v\\n\u0026#34;, coding) // 获取单个请求头,如果请求头的值是多个，用逗号拼接 // gzip, deflate coding1 := headers.Get(\u0026#34;Accept-Encoding\u0026#34;) fmt.Printf(\u0026#34;Accept-Encoding1: %v\\n\u0026#34;, coding1) fmt.Fprintln(w, headers) } ","date":"2021-10-28T00:19:44Z","permalink":"https://dccmmtop.github.io/posts/%E8%A7%A3%E6%9E%90%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E8%AF%B7%E6%B1%82%E4%BD%93/","section":"posts","tags":["go"],"title":"解析请求头和请求体"},{"categories":null,"contents":"在HTTP协议中，首部和请求体是分开传输的，将一些认证信息参数放在请求头中，服务端先解析请求头，如果认证不通过，可以直接返回认证失败，不用再传输请求体，从而提高服务器的性能。\n下面做实验验证,实验思路：\n编写一个带有身份验证的上传文件接口，此接口先解析请求头中的 token参数，如果token正确，继续解析请求体中的附件，如果token错误，直接返回401，\n上传一个超大的文件，比较两种情况的接口耗时。\n接口 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/test\u0026#34;,test) server.ListenAndServe() } func test(w http.ResponseWriter, request *http.Request) { token := request.Header.Get(\u0026#34;Authorization\u0026#34;) if token != \u0026#34;Bearer 12345\u0026#34; { w.WriteHeader(401) fmt.Fprintln(w,\u0026#34;认证失败\u0026#34;) return } fmt.Println(\u0026#34;解析文件。。。\u0026#34;) data, _ ,err := request.FormFile(\u0026#34;file\u0026#34;) if err != nil { file, _ := ioutil.ReadAll(data) fmt.Fprintln(w, string(file)) } } 测试 通过 curl 调用该接口, 为使效果明显，上传了一个较大的镜像文件。\ntoken 错误的情况 curl -i --location --request GET \u0026#39;127.0.0.1:8080/test\u0026#39; --header \u0026#39;Authorization:Bearer 123456\u0026#39; --form \u0026#39;file=@\u0026#34;/home/dc/windows10.iso\u0026#34;\u0026#39; 几毫秒内就返回了失败的结果:\nHTTP/1.1 401 Unauthorized Date: Wed, 27 Oct 2021 14:45:54 GMT Content-Length: 13 Content-Type: text/plain; charset=utf-8 Connection: close 认证失败 token 正确的情况\ncurl -i --location --request GET \u0026#39;127.0.0.1:8080/test\u0026#39; --header \u0026#39;Authorization:Bearer 12345\u0026#39; --form \u0026#39;file=@\u0026#34;/home/dc/windows10.iso\u0026#34;\u0026#39; 通过后台日志可以看到正在读取文件。由于测试的附件有3G多，需要漫长的等待。\n总结 由此证明，请求头和请求体时分开传输的, 我们往往把一些身份认证信息等放在首部，便于服务快速的响应。此外还需注意一点，在一些web框架中提供的通用身份校验中间件，或者自己编写的请求过滤器，需要先解析请求头，再解析请求体。才能利用上此特性。\n","date":"2021-10-27T22:12:48Z","permalink":"https://dccmmtop.github.io/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8A%8A%E6%9F%90%E4%BA%9B%E5%8F%82%E6%95%B0%E6%94%BE%E5%9C%A8%E8%AF%B7%E6%B1%82%E5%A4%B4%E4%B8%AD/","section":"posts","tags":["HTTTP","go"],"title":"为什么把某些参数放在请求头中"},{"categories":null,"contents":"将 cookie 发送给至客户端 Cookie结构的string方法可以返回一个经过序列化处理的cookie，其中Set-Cookie响应首部的值就是由这些序列化之后的cookie组成的。\npackage main import \u0026#34;net/http\u0026#34; func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/setCookie\u0026#34;,setCookie) server.ListenAndServe() } func setCookie(w http.ResponseWriter, request *http.Request) { c1 := http.Cookie{ Name: \u0026#34;first_cookie\u0026#34;, Value: \u0026#34;吃饭了吗\u0026#34;, HttpOnly: true, } c2 := http.Cookie{ Name: \u0026#34;second_cookie\u0026#34;, Value: \u0026#34;吃啥呢\u0026#34;, HttpOnly: true, } // String() 方法返回序列化后得 cookie w.Header().Set(\u0026#34;Cookie\u0026#34;,c1.String()) w.Header().Add(\u0026#34;Cookie\u0026#34;,c2.String()) // 第二种设置 cookie 的方法 c3 := http.Cookie{ Name: \u0026#34;cookie3\u0026#34;, Value: \u0026#34;天气怎么样\u0026#34;, HttpOnly: true, } http.SetCookie(w, \u0026amp;c3) } ","date":"2021-10-26T23:32:10Z","permalink":"https://dccmmtop.github.io/posts/cookie%E6%93%8D%E4%BD%9C/","section":"posts","tags":["go"],"title":"Cookie操作"},{"categories":null,"contents":"会话 cookie 与持久 cookie 没有设置Expires字段的cookie通常称为会话cookie或者临时cookie，这种cookie在浏览器关闭的时候就会自动被移除。相对而言，设置了Expires字段的cookie通常称为持久cookie，这种cookie会一直存在，直到指定的过期时间来临或者被手动删除为止。\ncookie 过期时间 Expires字段和MaxAge字段都可以用于设置cookie的过期时间，其中Expires字段用于明确地指定cookie应该在什么时候过期，而MaxAge字段则指明了cookie在被浏览器创建出来之后能够存活多少秒。之所以会出现这两种截然不同的过期时间设置方式，是因为不同浏览器使用了各不相同的cookie实现机制，跟Go语言本身的设计无关。虽然HTTP 1.1中废弃了Expires，推荐使用MaxAge来代替Expires，但几乎所有浏览器都仍然支持Expires；而且，微软的IE6，IE7和IE8都不支持MaxAge。为了让cookie在所有浏览器上都能够正常地运作，一个实际的方法是只使用Expires，或者同时使用Expires和 МаxAge.\n","date":"2021-10-26T23:28:06Z","permalink":"https://dccmmtop.github.io/posts/cookie%E6%A6%82%E8%A7%88/","section":"posts","tags":["HTTP"],"title":"Cookie概览"},{"categories":null,"contents":"返回体 func writeExample(w http.ResponseWriter, request *http.Request) { // 没有手动设置响应类型，会通过检测响应的前 512 个字节自动判断响应类型 // 这里是 Content-Type: text/html; charset=utf-8 str:= `\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Go Web Programming\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;hl\u0026gt;Hello World\u0026lt;/hl\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` w.Write([]byte(str)) } 设置状态码 func writeHeaderExample(w http.ResponseWriter, request *http.Request) { // 设置HTTP 状态码, WriteHeader 方法名有误导，只能设置状态码，而不是其他响应首部， // 默认是200 // 调用 WriteHeader 之后不能在对响应首部做任何操作。但是可以继续写入响应体 w.WriteHeader(500) // 在 WriteHeader 之后对首部的设置无效 //w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;json\u0026#34;) fmt.Fprintln(w,\u0026#34;服务异常\u0026#34;) } 设置重定向 func headerExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Location\u0026#34;,\u0026#34;http://www.baidu.com\u0026#34;) w.WriteHeader(302) } 返回JSON func jsonExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;application/json\u0026#34;) post := \u0026amp;Post{ User: \u0026#34;He Dong\u0026#34;, Thread: []string{\u0026#34;First\u0026#34;,\u0026#34;Second\u0026#34;,\u0026#34;Three\u0026#34;}, } json, _ := json.Marshal(post) w.Write(json) } 完整代码 package main import ( json \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) type Post struct { User string Thread []string } // 通过编写响应首部重定向 func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/write\u0026#34;,writeExample) http.HandleFunc(\u0026#34;/writeHeader\u0026#34;,writeHeaderExample) http.HandleFunc(\u0026#34;/redirect\u0026#34;,headerExample) http.HandleFunc(\u0026#34;/jsonExample\u0026#34;,jsonExample) server.ListenAndServe() } func writeExample(w http.ResponseWriter, request *http.Request) { // 没有手动设置响应类型，会通过检测响应的前 512 个字节自动判断响应类型 // 这里是 Content-Type: text/html; charset=utf-8 str:= `\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Go Web Programming\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;hl\u0026gt;Hello World\u0026lt;/hl\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` w.Write([]byte(str)) } func writeHeaderExample(w http.ResponseWriter, request *http.Request) { // 设置HTTP 状态码, WriteHeader 方法名有误导，只能设置状态码，而不是其他响应首部， // 默认是200 // 调用 WriteHeader 之后不能在对响应首部做任何操作。但是可以继续写入响应体 w.WriteHeader(500) // 在 WriteHeader 之后对首部的设置无效 //w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;json\u0026#34;) fmt.Fprintln(w,\u0026#34;服务异常\u0026#34;) } func headerExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Location\u0026#34;,\u0026#34;http://www.baidu.com\u0026#34;) w.WriteHeader(302) } // 返回json func jsonExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;application/json\u0026#34;) post := \u0026amp;Post{ User: \u0026#34;He Dong\u0026#34;, Thread: []string{\u0026#34;First\u0026#34;,\u0026#34;Second\u0026#34;,\u0026#34;Three\u0026#34;}, } json, _ := json.Marshal(post) w.Write(json) } ","date":"2021-10-26T22:54:48Z","permalink":"https://dccmmtop.github.io/posts/%E8%AE%BE%E7%BD%AE%E5%93%8D%E5%BA%94%E9%A6%96%E9%83%A8/","section":"posts","tags":["go"],"title":"设置响应首部及响应体示例"},{"categories":null,"contents":"package main import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;crypto/rsa\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;crypto/x509/pkix\u0026#34; \u0026#34;encoding/pem\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main(){ max := new(big.Int).Lsh(big.NewInt(1), 128) serialNumber, _ := rand.Int(rand.Reader,max) subject := pkix.Name{ Organization: []string {\u0026#34;YX\u0026#34;}, OrganizationalUnit: []string {\u0026#34;YX\u0026#34;}, CommonName: \u0026#34;DC\u0026#34;, } template := x509.Certificate{ SerialNumber: serialNumber, Subject: subject, NotBefore: time.Now(), NotAfter: time.Now().Add(365 * 24 * time.Hour), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth}, IPAddresses: []net.IP{net.ParseIP(\u0026#34;127.0.0.1\u0026#34;)}, } pk, _ := rsa.GenerateKey(rand.Reader,2048) derBytes , _ := x509.CreateCertificate(rand.Reader, \u0026amp;template, \u0026amp;template, \u0026amp;pk.PublicKey, pk) cerOut, _ := os.Create(\u0026#34;cert.pem\u0026#34;) pem.Encode(cerOut, \u0026amp;pem.Block{Type: \u0026#34;CERTIFICATE\u0026#34;, Bytes: derBytes}) cerOut.Close() keyOut , _ := os.Create(\u0026#34;key.pem\u0026#34;) pem.Encode(keyOut, \u0026amp;pem.Block{Type: \u0026#34;RAS PRIVATE KEY\u0026#34;, Bytes: x509.MarshalPKCS1PrivateKey(pk)}) keyOut.Close() } ","date":"2021-10-25T22:02:11Z","permalink":"https://dccmmtop.github.io/posts/%E7%94%9F%E6%88%90%E8%AF%81%E4%B9%A6%E5%8F%8A%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%A7%81%E9%92%A5/","section":"posts","tags":null,"title":"生成证书及服务端私钥"},{"categories":null,"contents":"什么是无状态 因为HTTP是一种无连接协议（connection-less protocol），通过这种协议发送给服务器的请求对服务器之前处理过的请求一无所知，所以应用程序才会以cookie的方式在客户端实现数据持久化，并以会话的方式在服务器上实现数据持久化，而不了解这一点的人是很难理解为什么要在不同连接之间使用cookie和会话实现信息持久化\n","date":"2021-10-24T22:17:14Z","permalink":"https://dccmmtop.github.io/posts/http%E6%A6%82%E8%A7%88/","section":"posts","tags":["HTTP"],"title":"HTTP概览"},{"categories":null,"contents":"nobody在linux中是一个不能登陆的帐号，一些服务进程如apache，aquid等都采用一些特殊的帐号来运行，比如nobody,news,games等等，这是就可以防止程序本身有安全问题的时候，不会被黑客获得root权限\n1、Windows系统在安装后会自动建立一些用户帐户，在Linux系统中同样有一些用户帐户是在系统安装后就有的，就像Windows系统中的内置帐户一样。\n2、它们是用来完成特定任务的，比如nobody和ftp等，我们访问 www.111cn.net的网页程序时，官网的服务器就是让客户以 nobody 身份登录的(相当于Windows系统中的匿名帐户);我们匿名访问ftp时，会用到用户ftp或nobody。\n3、首先，nobody是一个普通用户，非特权用户。 使用nobody用户名的\u0026rsquo;目的\u0026rsquo;是，使任何人都可以登录系统，但是其 UID 和 GID 不提供任何特权，即该uid和gid只能访问人人皆可读写的文件。\n4、其次，许多系统中都按惯例地默认创建一个nobody，尽量\u0026rsquo;限制它的权限至最小\u0026rsquo;，当服务器向外服务时，可能会让client以nobody的身份登录。\n5、nobody就是一个普通账户，因为默认登录shell是 \u0026lsquo;/sbin/nologin\u0026rsquo;，所以这个用户是无法直接登录系统的，也就是黑客很难通过漏洞连接到你的服务器来做破坏。此外这个用户的权限也给配置的很低。因此有比较高的安全性。一切都只给最低权限。这就是nobody存在的意义。\n","date":"2021-10-21T17:30:36Z","permalink":"https://dccmmtop.github.io/posts/linux_nobody%E7%94%A8%E6%88%B7/","section":"posts","tags":null,"title":"Linux_nobody用户"},{"categories":null,"contents":"概览 为了理解Elasticsearch中数据是如何组织的，从以下两个角度来观察\n逻辑设计 搜索应用所要注意的。用于索引和搜索的基本单位是文档，可以将其认为 文档以类型来分组，类型包含若干文档， 类似表格包含若干\n行。 是关系数据库里的一行。 最终一个或多个类型存在于同一索引中，索引是更大的容器， 类似于SQL中的数据库\nES SQL 文档 行 类型 表 索引 数据库 物理设计 在后台Elasticsearch是如何处理数据的。\nElasticsearch将每个索引划分为分片，每份分片可以在集群中的不同服务器间迁移。通常，应用程序无须关心这些，因为无论Elasticsearch是单台还是多台服务器，应用和Elasticsearch的交互基本保持不变。但是，开始管理集群的时候，就需要留心了。 原因是，物理设计的配置方式决定了集群的性能可扩展性和可用性\n逻辑设计 这个索引一类型-ID的组合唯一确定了Elasticsearch中的某篇文档。当进行搜索的时候，可以查找特定索引、特定类型中的文档，也可以跨多个类型甚至是多个索引进行搜索。\n文档 无模式\nElasticsearch中的文档是无模式的，也就是说并非所有的文档都需要拥有相同的字段，它们不是受限于同一个模式。例如，在所有信息完备之前就要使用组织者数据时，你可以彻底忽略位置数据。\n映射包含某个类型中当前索引的所有文档的所有字段。但是不是所有的文档必须要有所有的字段 动态添加字段\n如果一篇新近文档拥有一个映射中尚不存在的字段, Elasticsearch会自动地将新字段加入映射 自动推导类型\n如果值是7, Elasticsearch会假设字段是长整型。\n这种新字段的自动检测也有缺点,因为 Elasticsearch可能猜得不对。例如,在索引了值7之后,你可能想再索引he11owor1a,这时由于它是 string而不是1ong,索引就会失败。对于线上环境、最安全的方式是在索引数据之前,就定义好所需的映射。 类型 类型是文档的逻辑容器,类似于表格是行的容器。在不同的类型中,最好放入不同结构(模式)的文档\n没有物理实体与类型系相对应 ，从物理角度来看,同一索引中的文档都是写入磁盘,而不考虑它们所属的映射类型。 索引 准实时 每个索引有一个称为 refresh interva1的设置,定义了新近索引的文档对于搜索可见的时间间隔。从性能的角度来看,刷新操作的代价是非常昂贵的,这也是为什么更新只是偶尔进行。默认是每秒更新一次,而不是每来一篇新的文档就更新一次。如果看到 Elasticsearch被称为准实时的,就是指的这种刷新过程。\n物理设计 分片 默认情况下，每个索引由5个主分片组成，每个主分片有一个副本。一共10个分片。（至少一个主分片，0个或多个副分片）\n一份分片是一个包含倒排索引的目录中 也是 Lucene的索引\n分片也是 Elasticsearch将数据从一个节点迁移到另一个节点的最小单位\n副分片可以在运行的时候添加或者移除，而主分片不可以，创建索引之前必须确定主分片的数量\n过少的分片将限制可扩展性,但是过多的分片会影响性能。默认设置的5份是一个不错开始\n存储文档 默认情况下,当存储一篇文档的时候,系统首先根据文档ID的散列值选择一个主分片,并将文档发送到该主分片\n然后文档被发送到该主分片的所有副本分片进行存储。这使得副本分片和主分片之间保持数据的同步。\n数据同步使得副本分片可以服务于搜索请求,并在原有主分片无法访问时自动升级为主分片。\n默认地,文档在分片中均匀分布:对于每篇文档,分片是通过其ID字符串的散列决定的。每份分片拥有相同的散列范围,接收新文档的机会均等\n搜索文档 在搜索的时候,接受请求的节点将请求转发到一组包含所有数据的分片。 Elasticsearch使用 round-robin的轮询机制选择可用的分片(主分片或副本分片),并将搜索请求转发过去。 Elasticsearch然后从这些分片收集结果,将其聚集到单一的回复,然后将回复返回给客户端应用程序。\n倒排索引 倒排索引的结构使得 Elasticsearch在不扫描所有文档的情况下,就能告诉你哪些文档包含特定的词条(单词)。\n词条字典\n词条字典将每个词条和包含该词条的文档映射起来，搜索时没有必要为了某个词条扫描所有的文档,而是根据这个字典快速地识别匹配的文档。 词频\n词频使得 Elasticsearch可以快速地获取某篇文档中某个词条岀现的次数。这对于计算结果的相关性得分非常重要。例如,如果搜索“ denver”,包含多个“ denver\u0026quot;”的文档通常更为相关。 水平扩展 增加更多的节点，工作负载被分摊，抵御高并发。无法提高单个搜搜索速度\n垂直扩展 给机器增加更多的内存或cpu核心，更快的磁盘\n工具 有几个图形化界面\nElasticsearch Head\nElasticsearch Head可以通过 Elasticsearch插件的形式来安装这个工具,一个单\n机的HTTP服务器,或是可以从文件系统打开的网页。可以从那里发送HTTP请求,\n但是Head作为监控工具是最有用的,向你展示集群中分片是如何分布的。 参考资料 《ElasticSearch实战》\n","date":"2021-10-18T23:23:43Z","permalink":"https://dccmmtop.github.io/posts/elasticsearch%E8%AE%A4%E7%9F%A5/","section":"posts","tags":null,"title":"ElasticSearch认知"},{"categories":null,"contents":"","date":"2021-10-17T09:59:49Z","permalink":"https://dccmmtop.github.io/posts/https%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6/","section":"posts","tags":null,"title":"HTTPS自签证书"},{"categories":null,"contents":"插入并保存文档 db.foo.insert({\u0026#34;bar\u0026#34;: \u0026#34;baz\u0026#34;}) 这个操作会给文档自动增加一个\u0026quot;_id\u0026quot;键（要是原来没有的话），然后将其保存到MongoDB中。\n批量插入 如果要向集合中插入多个文档，使用批量插入会快一些。使用批量插入，可以将一组文档传递给数据库。\n在shell中，可以使用batchInsert函数实现批量插入，它与insert函数非常像，只是它接受的是一个文档数组作为参数：\ndb.foo.batchInsert([{\u0026#34;_id\u0026#34;: 1},{\u0026#34;_id\u0026#34;: 2}, {\u0026#34;_id\u0026#34;: 3}]) 要是只导入原始数据（例如，从数据feed或者MySQL中导入），可以使用命令行工具，如mongoimport，而不是批量插入\n当前版本的MongoDB能接受的最大消息长度是48 MB，所以在一次批量插入中能插入的文档是有限制的。如果试图插入48 MB以上的数据，多数驱动程序会将这个批量插入请求拆分为多个48 MB的批量插入请求。具体可以查看所使用的驱动程序的相关文档。\n如果在执行批量插入的过程中有一个文档插入失败，那么在这个文档之前的所有文档都会成功插入到集合中，而这个文档以及之后的所有文档全部插入失败。\n在批量插入中遇到错误时，如果希望batchInsert忽略错误并且继续执行后续插入，可以使用continueOnError选项\n插入校验 检查文档的基本结构，如果没有\u0026quot;_id\u0026quot;字段，就自动增加一个。 所有文档都必须小于16 MB（这个值是MongoDB设计者人为定的，未来有可能会增加）。作这样的限制主要是为了防止不良的模式设计，并且保证性能一致 ","date":"2021-09-05T17:46:38Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%9B%E5%BB%BA%E6%96%87%E6%A1%A3/","section":"posts","tags":["mongoDB"],"title":"Mongo创建文档"},{"categories":null,"contents":"运行shell MongoDB自带JavaScript shell，可在shell中使用命令行与MongoDB实例交互。\nshell是一个功能完备的JavaScript解释器，可运行任意JavaScript程序\n另外，可充分利用JavaScript的标准库:\n再者，可定义和调用JavaScript函数：[插图]\n需要注意，可使用多行命令。shell会检测输入的JavaScript语句是否完整，如没写完可在下一行接着写。在某行连续三次按下回车键可取消未输入完成的命令，并退回到＞-提示符。\n启动mongo shell时不连接到任何mongod有时很方便。通过\u0026ndash;nodb参数启动shell，启动时就不会连接任何数据库,启动mongo shell时不连接到任何mongod有时很方便。通过\u0026ndash;nodb参数启动shell，启动时就不会连接任何数据库\n可以通过db.help()查看数据库级别的帮助，使用db.foo.help()查看集合级别的帮助。\n如果想知道一个函数是做什么用的，可以直接在shell输入函数名（函数名后不要输入小括号），这样就可以看到相应函数的JavaScript实现代码。例如，如果想知道update函数的工作机制，或者是记不清参数的顺序，就可以像下面这样做：\n使用shell执行脚本 如果希望使用指定的主机/端口上的mongod运行脚本，需要先指定地址，然后再跟上脚本文件的名称\n也可以使用load()函数，从交互式shell中运行脚本：\n在脚本中可以访问db变量，以及其他全局变量。然而，shell辅助函数（比如\u0026quot;usedb\u0026quot;和\u0026quot;show collections\u0026quot;）不可以在文件中使用。这些辅助函数都有对应的JavaScript函数\n编辑复合变量 shell的多行支持是非常有限的：不可以编辑之前的行。如果编辑到第15行时发现第1行有个错误，那会让人非常懊恼。因此，对于大块的代码或者是对象，你可能更愿意在编辑器中编辑。为了方便地调用编辑器，可以在shell中设置EDITOR变量（也可以在环境变量中设置）：\nEDITOR = \u0026#34;/usr/bin/vim\u0026#34; 现在，如果想要编辑一个变量，可以使用\u0026quot;edit变量名\u0026quot;这个命令，比如：\n用户主目录下创建一个名为.mongorc.js的文件,在.mongorc.js文件中添加一行内容，EDITOR=\u0026ldquo;编辑器路径\u0026rdquo;;，以后就不必单独设置EDITOR变量了。\n","date":"2021-09-05T17:36:12Z","permalink":"https://dccmmtop.github.io/posts/mongo_shell%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/","section":"posts","tags":null,"title":"Mongo_Shell基础操作"},{"categories":null,"contents":"文档是MongoDB中数据的基本单元，非常类似于关系型数据库管理系统中的行，但更具表现力。\n类似地，集合（collection）可以看作是一个拥有动态模式（dynamic schema）的表。\nMongoDB的一个实例可以拥有多个相互独立的数据库（database），每一个数据库都拥有自己的集合。\n每一个文档都有一个特殊的键\u0026quot;_id\u0026quot;，这个键在文档所属的集合中是唯一的。\n文档 MongoDB而且区分大小写，\n下面两个文档是不同的\n{ \u0026#34;foo\u0026#34;: 3 } { \u0026#34;Foo\u0026#34;: 3 } 文档中的键/值对是有序的：{\u0026ldquo;x\u0026rdquo; : 1, \u0026ldquo;y\u0026rdquo;:2}与{\u0026ldquo;y\u0026rdquo;: 2, \u0026ldquo;x\u0026rdquo;: 1}是不同的。\n通常，字段顺序并不重要，无须让数据库模式依赖特定的字段顺序（MongoDB会对字段重新排序）。在某些特殊情况下，字段顺序变得非常重要，\n集合 文档中的键/值对是有序的：{\u0026ldquo;x\u0026rdquo; : 1, \u0026ldquo;y\u0026rdquo;:2}与{\u0026ldquo;y\u0026rdquo;: 2, \u0026ldquo;x\u0026rdquo;: 1}是不同的。通常，字段顺序并不重要，无须让数据库模式依赖特定的字段顺序（MongoDB会对字段重新排序）。在某些特殊情况下，字段顺序变得非常重要，\n集合是动态模式的。这意味着一个集合里面的文档可以是各式各样的\n命名 可以使用db.collectionName获取一个集合的内容，但是，如果集合名称中包含保留字或者无效的JavaScript属性名称，db.collectionName就不能正常工作了。\n假设要访问version集合，不能直接使用db.version，因为db.version是db的一个方法（会返回当前MongoDB服务器的版本）：\n在JavaScript中，x.y等同于x[\u0026lsquo;y\u0026rsquo;]。也就是说，除了名称的字面量之外，还可以使用变量访问子集合。因此，如果需要对blog的每一个子集合进行操作，可以使用如下方式进行迭代\n相同数据结构归为一个文档 如果把各种各样的文档不加区分地放在同一个集合里，无论对开发者还是对管理员来说都将是噩梦。\n在一个集合里查询特定类型的文档在速度上也很不划算，分开查询多个集合要快得多。\n把同种类型的文档放在一个集合里，数据会更加集中\n创建索引时，需要使用文档的附加结构（特别是创建唯一索引时）。索引是按照集合来定义的。在一个集合中只放入一种类型的文档，可以更有效地对集合进行索引。\n命名 集合名不能以“system.”开头，这是为系统集合保留的前缀。例如，system.users这个集合保存着数据库的用户信息，而system.namespaces集合保存着所有数据库集合的信息。 用户创建的集合不能在集合名中包含保留字符’$\u0026rsquo;。因为某些系统生成的集合中包含$，很多驱动程序确实支持在集合名里包含该字符。除非你要访问这种系统创建的集合，否则不应该在集合名中包含$。 子集合 组织集合的一种惯例是使用“.”分隔不同命名空间的子集合。例如，一个具有博客功能的应用可能包含两个集合，分别是blog.posts和blog.authors\n数据库 据库最终会变成文件系统里的文件，而数据库名就是相应的文件名，这是数据库名有如此多限制的原因。\n另外，有一些数据库名是保留的，可以直接访问这些有特殊语义的数据库。这些数据库如下所示。 · admin从身份验证的角度来讲，这是“root”数据库。如果将一个用户添加到admin数据库，这个用户将自动获得所有数据库的权限。再者，一些特定的服务器端命令也只能从admin数据库运行，如列出所有数据库或关闭服务器。 local这个数据库永远都不可以复制，且一台服务器上的所有本地集合都可以存储在这个数据库中 config MongoDB用于分片设置时，分片信息会存储在config数据库中。 启动MongoDB 通常，MongoDB作为网络服务器来运行，客户端可连接到该服务器并执行操作。下载MongoDB（http://www.mongodb.org/downloads）并解压，运行mongod命令，启动数据库服务器\nmongod在没有参数的情况下会使用默认数据目录/data/db（Windows系统中为C:\\data\\db） 默认情况下，MongoDB监听27017端口 mongod还会启动一个非常基本的HTTP服务器，监听数字比主端口号高1000的端口，也就是28017端口。这意味着，通过浏览器访问http://localhost:28017，能获取数据库的管理信息 shell中的基本操作 要查看db当前指向哪个数据库，可以使用db命令\n选择数据库 use\n读取 find和findOne方法可以用于查询集合里的文档。若只想查看一个文档，可用findOne：\nfind和findOne可以接受一个查询文档作为限定条件。这样就可以查询符合一定条件的文档。使用find时，shell会自动显示最多20个匹配的文档，也可获取更多文档\n更新 使用update修改博客文章。update接受（至少）两个参数：第一个是限定条件（用于匹配待更新的文档），第二个是新的文档\n例子:\n修改变量post，增加\u0026quot;comments\u0026quot;键：\npost.comment = [] db.blog.update({title: \u0026#34;my blog\u0026#34;},post) 删除 使用remove方法可将文档从数据库中永久删除。如果没有使用任何参数，它会将集合内的所有文档全部删除。它可以接受一个作为限定条件的文档作为参数\n","date":"2021-09-05T16:47:25Z","permalink":"https://dccmmtop.github.io/posts/mongodb%E5%9F%BA%E7%A1%80/","section":"posts","tags":["mongoDB"],"title":"MongoDB基础"},{"categories":null,"contents":"Go 的包管理方式是逐渐演进的， 最初是 monorepo 模式，所有的包都放在 GOPATH 里面，使用类似命名空间的包路径区分包，不过这种包管理显然是有问题，由于包依赖可能会引入破坏性更新，生产环境和测试环境会出现运行不一致的问题。\n从 v1.5 开始开始引入 vendor 包模式，如果项目目录下有 vendor 目录，那么 go 工具链会优先使用 vendor 内的包进行编译、测试等，这之后第三方的包管理思路都是通过这种方式来实现，比如说由社区维护准官方包管理工具 dep。\n不过官方并不认同这种方式，在 v1.11 中加入了 Go Module 作为官方包管理形式，就这样 dep 无奈的结束了使命。最初的 Go Module 提案的名称叫做 vgo，下面为了介绍简称为 gomod。不过在 v1.11 和 v1.12 的 Go 版本中 gomod 是不能直接使用的。可以通过 go env 命令返回值的 GOMOD 字段是否为空来判断是否已经开启了 gomod，如果没有开启，可以通过设置环境变量 export GO111MODULE=on 开启。\n目前 gomod 在 Go v1.12 功能基本稳定，到下一个版本 v1.13 将默认开启，是时候开始在项目中使用 gomod 了。\nHello,World\nGo 维护者 Russ Cox 写一个简单的库，用于说明 gomod 的使用，下文我将使用这个库开始介绍。\n首先在个人包命名空间目录新建一个文件夹，然后直接使用 go mod init 即可。\nmkdir $GOPATH/github.com/islishude/gomodtest cd $GOPATH/github.com/islishude/gomodtest go mod init 更新：现在不允许在 GOPATH 下使用 gomod，需要更改成以下命令：\nmkdir -p ~/gopher/gomodtest cd ~/gopher/gomodtest go mod init github.com/islishude/gomodtest 这时可看到目录内多了 go.mod 文件，内容很简单只有两行：\nmodule github.com/islishude/gomodtest\ngo 1.12\n首行为当前的模块名称，接下来是 go 的使用版本。这两行和 npm package.json 的 name 和 engine 字段的功能很类似。\n然后新建一个 main.go 写入以下内容，这里我们引用了 rsc.io/quote 包，注意我们现在还没有下载这个包。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;rsc.io/quote\u0026#34; ) func main() { fmt.Println(quote.Hello()) } 如果是默认情况下，使用 go run main.go 肯定会提示找不到这个包的错误，但是当前 gomod 模式，如果没有此依赖回先下载这个依赖。\n$ go run main.go go: finding rsc.io/quote v1.5.2 go: finding rsc.io/sampler v1.3.0 go: finding golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: downloading rsc.io/quote v1.5.2 go: extracting rsc.io/quote v1.5.2 go: downloading rsc.io/sampler v1.3.0 go: extracting rsc.io/sampler v1.3.0 go: downloading golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: extracting golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c Hello, world. 因为包含 http://golang.org 下的包，记得设置代理。这个时候当前包目录除了 go.mod 还有一个 go.sum 的文件，这个类似 npm package-lock.json。\ngomod 不会在 $GOPATH/src 目录下保存 rsc.io 包的源码，而是包源码和链接库保存在 $GOPATH/pkg/mod 目录下。\n$ ls $GOPATH/pkg/mod cache golang.org rsc.io 除了 go run 命令以外，go build、go test 等命令也能自动下载相关依赖包。\n包管理命令\n当然我们平常都不会直接先写代码，写上引入的依赖名称和路径，然后在 build 的时候在下载。\n安装依赖 如果要想先下载依赖，那么可以直接像以前那样 go get 即可，不过 gomod 下可以跟语义化版本号，比如 go get foo@v1.2.3，也可以跟 git 的分支或 tag，比如go get foo@master，当然也可以跟 git 提交哈希，比如 go get foo@e3702bed2。需要特别注意的是，gomod 除了遵循语义化版本原则外，还遵循最小版本选择原则，也就是说如果当前版本是 v1.1.0，只会下载不超过这个最大版本号。如果使用 go get foo@master，下次在下载只会和第一次的一样，无论 master 分支是否更新了代码，如下所示，使用包含当前最新提交哈希的虚拟版本号替代直接的 master 版本号。\n$ go get golang.org/x/crypto/sha3@master go: finding golang.org/x/crypto/sha3 latest go: finding golang.org/x/crypto latest $ cat go.mod module github.com/adesight/test go 1.12 require ( golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a // indirect rsc.io/quote v1.5.2 ) 如果下载所有依赖可以使用 go mod download 命令。\n升级依赖 查看所有以升级依赖版本：\n$ go list -u -m all go: finding golang.org/x/sys latest go: finding golang.org/x/crypto latest github.com/adesight/test golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a [v0.0.0-20190316082340-a2f829d7f35f] golang.org/x/text v0.3.0 rsc.io/quote v1.5.2 rsc.io/sampler v1.99.99 ###升级次级或补丁版本号：\ngo get -u rsc.io/quote 仅升级补丁版本号： go get -u=patch rscio/quote 升降级版本号，可以使用比较运算符控制：\ngo get foo@\u0026#39;\u0026lt;v1.6.2\u0026#39; 移除依赖 当前代码中不需要了某些包，删除相关代码片段后并没有在 go.mod 文件中自动移出。\n运行下面命令可以移出所有代码中不需要的包：\ngo mod tidy 如果仅仅修改 go.mod 配置文件的内容，那么可以运行 go mod edit --droprequire=path，比如要移出 golang.org/x/crypto 包\ngo mod edit --droprequire=golang.org/x/crypto\n查看依赖包 可以直接查看 go.mod 文件，或者使用命令行：\n$ go list -m all github.com/adesight/test golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a golang.org/x/text v0.3.0 rsc.io/quote v1.5.2 rsc.io/sampler v1.99.99 $ go list -m -json all # json 格式输出 { \u0026#34;Path\u0026#34;: \u0026#34;golang.org/x/text\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;v0.3.0\u0026#34;, \u0026#34;Time\u0026#34;: \u0026#34;2017-12-14T13:08:43Z\u0026#34;, \u0026#34;Indirect\u0026#34;: true, \u0026#34;Dir\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/golang.org/x/text@v0.3.0\u0026#34;, \u0026#34;GoMod\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/cache/download/golang.org/x/text/@v/v0.3.0.mod\u0026#34; } { \u0026#34;Path\u0026#34;: \u0026#34;rsc.io/quote\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;v1.5.2\u0026#34;, \u0026#34;Time\u0026#34;: \u0026#34;2018-02-14T15:44:20Z\u0026#34;, \u0026#34;Dir\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/rsc.io/quote@v1.5.2\u0026#34;, \u0026#34;GoMod\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/cache/download/rsc.io/quote/@v/v1.5.2.mod\u0026#34; } 模块配置文本格式化 由于可手动修改 go.mod 文件，所以可能此文件并没有被格式化，使用下面命令进行文本格式化。\ngo mod edit -fmt 发布版本 发布包新版本和其它包管理工具基本一致，可以直接打标签，不过打标签之前需要在 go.mod 中写入相应的版本号：\n$ go mod edit --module=github.com/islishude/gomodtest/v2 $ cat go.mod module github.com/islishude/gomodtest/v2 go 1.12 require ( golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a // indirect rsc.io/quote v1.5.2 ) 官方推荐将上述过程在一个新分支来避免混淆，那么类如上述例子可以创建一个 v2 分支，但这个不是强制要求的。\n还有一种方式发布新版本，那就是在主线版本种加入 v2 文件夹，相应的也需要内置 go.mod 这个文件。\n从老项目迁移 从很多第三方的包管理工具迁移到 gomod 特别简单，直接运行 go mod init 即可。\n如果没有使用任何第三方包管理工具，除了运行 go mod init 初始化以外，还要使用 go get ./\u0026hellip; 下载安装所有依赖包，并更新 go.mod 和 go.sum 文件。\n默认情况下，go get 命令使用 @latest 版本控制符对所有依赖进行下载，如果想要更改某一个包的版本，可以使用 go mod edit \u0026ndash;require 命令，比如要更新 rsc.io/quote 到 v3.1.0 版本。\ngo mod edit --require=rsc.io/quote@v3.1.0 ","date":"2021-08-18T16:23:14Z","permalink":"https://dccmmtop.github.io/posts/go_mod%E7%9A%84%E4%BD%BF%E7%94%A8/","section":"posts","tags":["go"],"title":"go_mod的使用"},{"categories":null,"contents":"count(1) and count(*) **当表的数据量大些时，对表作分析之后，使用count(1)还要比使用count(*)用时多了！ **\n从执行计划来看，count(1)和count()的效果是一样的。 但是在表做过分析之后，count(1)会比count()的用时少些（1w以内数据量），不过差不了多少。\n如果count(1)是聚索引,id,那肯定是count(1)快。但是差的很小的。\n因为count(),自动会优化指定到那一个字段。所以没必要去count(1)，用count()，sql会帮你完成优化的 因此： count(1)和count(*)基本没有差别！\ncount(1) and count(字段) 两者的主要区别是\n（1） count(1) 会统计表中的所有的记录数， 包含字段为null 的记录。\n（2） count(字段) 会统计该字段在表中出现的次数，忽略字段为null 的情况。即 不统计字段为null 的记录。\ncount(*) 和 count(1)和count(列名)区别\n执行效果上\ncount(*)包括了所有的列，相当于行数，在统计结果的时候， 不会忽略列值为NULL\ncount(1)包括了忽略所有列，用1代表代码行，在统计结果的时候， 不会忽略列值为NULL\ncount(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数， 即某个字段值为NULL时，不统计。\n执行效率上\n列名为主键，count(列名)会比count(1)快\n列名不为主键，count(1)会比count(列名)快\n如果表多个列并且没有主键，则 count（1） 的执行效率优于 count（）\n如果有主键，则 select count（主键）的执行效率是最优的\n如果表只有一个字段，则 select count（）最优。\n","date":"2021-08-15T10:41:01Z","permalink":"https://dccmmtop.github.io/posts/count1%E4%B8%8Ecountx%E7%9A%84%E5%8C%BA%E5%88%AB/","section":"posts","tags":["MySQL"],"title":"count(1)与count(x)的区别"},{"categories":null,"contents":"唯一索引 唯一索引是索引具有的一种属性，让索引具备唯一性，确保这张表中，该条索引数据不会重复出现。在每一次insert和update操作时，都会进行索引的唯一性校验，保证该索引的字段组合在表中唯一。\ndb.containers.createIndex({name: 1},{unique:true, background: true}) db.packages.createIndex({ appId: 1, version: 1 },{unique:true, background: true}) 知识点一：\n创建索引时,1表示按升序存储,-1表示按降序存储。\n知识点二:\nMongo提供两种建索引的方式foreground和background。\n前台操作，它会阻塞用户对数据的读写操作直到index构建完毕；\n后台模式，不阻塞数据读写操作，独立的后台线程异步构建索引，此时仍然允许对数据的读写操作。\n创建索引时一定要写{background: true}\n创建索引时一定要写{background: true}\n创建索引时一定要写{background: true}\n复合索引 概念：指的是将多个键组合到一起创建索引，终极目的是加速匹配多个键的查询。\ndb.flights.createIndex({ flight: 1, price: 1 },{background: true}) 内嵌索引 可以在嵌套的文档上建立索引，方式与建立正常索引完全一致。\n个人信息表结构如下,包含了省市区三级的地址信息，如果想要给城市（city）添加索引，其实就和正常添索引一样\ndb.personInfos.createIndex({“address.city”:1}) 数组索引 MongoDB支持对数组建立索引，这样就可以高效的搜索数组中的特定元素。\n知识点四：\n但是！对数组建立索引的代价是非常高的，他实际上是会对数组中的每一项都单独建立索引，就相当于假设数组中有十项，那么就会在原基础上，多出十倍的索引大小。如果有一百个一千个呢？\n所以在mongo中是禁止对两个数组添加复合索引的，对两个数组添加索引那么索引大小将是爆炸增长，所以谨记在心。\n过期索引（TTL）\n可以针对某个时间字段，指定文档的过期时间（经过指定时间后过期 或 在某个时间点过期）\n哈希索引（Hashed Index） 是指按照某个字段的hash值来建立索引，hash索引只能满足字段完全匹配的查询，不能满足范围查询等\n地理位置索引（Geospatial Index） 能很好的解决一些场景，比如『查找附近的美食』、『查找附近的加油站』等\n文本索引（Text Index） 能解决快速文本查找的需求，比如，日志平台，相对日志关键词查找，如果通过正则来查找的话效率极低，这时就可以通过文本索引的形式来进行查找\nExplain查询计划 提到查的慢，二话不说直接看查询计划好么？具体每一个字段的含义我就不做赘述了很容易查到，我截取winningPlan的部分和大家一起看一下。WinningPlan就是在查询计划中胜出的方案，那肯定就有被淘汰的方案，是在rejectPlan里。\n// 查询计划中的winningPlan部分 \u0026#34;winningPlan\u0026#34;: { \u0026#34;stage\u0026#34;: \u0026#34;FETCH\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;createdAt\u0026#34;: { \u0026#34;$gte\u0026#34;: ISODate(\u0026#34;2019-07-22T12:00:44.000Z\u0026#34;) } }, \u0026#34;inputStage\u0026#34;: { \u0026#34;stage\u0026#34;: \u0026#34;IXSCAN\u0026#34;, \u0026#34;keyPattern\u0026#34;: { \u0026#34;load\u0026#34;: 1 }, \u0026#34;indexName\u0026#34;: \u0026#34;load_1\u0026#34;, \u0026#34;isMultiKey\u0026#34;: false, \u0026#34;multiKeyPaths\u0026#34;: { \u0026#34;load\u0026#34;: [] }, \u0026#34;isUnique\u0026#34;: false, \u0026#34;isSparse\u0026#34;: false, \u0026#34;isPartial\u0026#34;: false, \u0026#34;indexVersion\u0026#34;: 2, \u0026#34;direction\u0026#34;: \u0026#34;backward\u0026#34;, \u0026#34;indexBounds\u0026#34;: { \u0026#34;load\u0026#34;: [ \u0026#34;[MaxKey, MinKey]\u0026#34; ] } } }, 知识点六： explain 结果将查询计划以阶段树的形式呈现。\n每个阶段将其结果（文档或索引键）传递给父节点。\n中间节点操纵由子节点产生的文档或索引键。\n根节点是MongoDB从中派生结果集的最后阶段。\n对于新人一定要特别注意：在看查询结果的阶段树的时候一定一定是从最里层一层一层往外看的，不是直接顺着读下来的。\n知识点七： 在查询计划中出现了很多stage，下面列举的经常出现的stage以及他的含义：\nCOLLSCAN：全表扫描\nIXSCAN：索引扫描\nFETCH：根据前面扫描到的位置抓取完整文档\nSORT：进行内存排序，最终返回结果\nSORT_KEY_GENERATOR：获取每一个文档排序所用的键值\nLIMIT：使用limit限制返回数\nSKIP：使用skip进行跳过\nIDHACK：针对_id进行查询\nCOUNTSCAN：count不使用用Index进行count时的stage返回\nCOUNT_SCAN：count使用了Index进行count时的stage返回\nTEXT：使用全文索引进行查询时候的stage返回\n最期望看到的查询组合 Fetch+IDHACK\nFetch+ixscan\nLimit+（Fetch+ixscan）\nPROJECTION+ixscan\n最不期望看到的查询组合\nCOLLSCAN（全表扫）\nSORT（使用sort但是无index）\nCOUNTSCAN（不使用索引进行count）\n最左前缀原则 假定索引(a，b，c) 它可能满足的查询如下：\n1. a 2. a，b 3. a，b，c 4. a，c [该组合只能用a部分] 5. a, c, b [cb在查询时会被优化换位置] 显然，最左前缀的核心是查询条件字段必须含有索引第一个字段\n最左值尽可能用最精确过滤性最好的值，不要用那种可能会用于范围模糊查询，用于排序的字段\n效率极低的操作符 $where和$exists：这两个操作符，完全不能使用索引。\n$ne和$not:通常来说取反和不等于,可以使用索引，但是效率极低，不是很有效，往往也会退化成扫描全表。\n$nin:不包含，这个操作符也总是会全表扫描\n对于管道中的索引，也很容易出现意外，只有在管道最开始时的match sort可以使用到索引，一旦发生过project投射，group分组，lookup表关联，unwind打散等操作后，就完全无法使用索引。\n索引设计和优化原则\n最后祭出李丹老师的索引设计和优化原则\n1.主键的设置\n业务无关、显示指定、递增属性\n2.数据区分度\n原则上区分度高的字段优先做索引字段，如果是组合索引优先放前面\n3.字段更新频率 频繁更新的字段是否做索引字段需要综合考虑对业务的影响及查询的代价\n4.前缀索引问题 需要注意的是因前缀索引只包含部分值因此无法通过前缀索引优化排序\n5.适当冗余设计 对于存储较长字符串字段可额外增加字段存储原字段计算(如hash)后的值\n创建索引时只需要对额外字段创建索引即可\n6.避免无效索引 通常类似表已经含有主键ID就无需再创建额外唯一性的ID索引\n7.查询覆盖率 设计一个索引我们需要考虑尽量覆盖更多的查询场景\n8.控制字段数 如果你设计的索引例如含有7、8个字段通常需要考虑设计是否合理\n优化原则 1.减少网络带宽 按需返回所需字段、尽量避免返回大字段\n2.减少内存计算 减少无必要中间结果存储、减少内存计算\n3.减少磁盘IO 添加合适的索引、关注SQL改写\n","date":"2021-08-12T23:13:39Z","permalink":"https://dccmmtop.github.io/posts/%E7%B4%A2%E5%BC%95/","section":"posts","tags":["mongo"],"title":"索引"},{"categories":null,"contents":"开启查询日志 方法一：执行MongoDB命令 这个命令只能设置单个组件的日志等级，如果想要一次性设置多个组件的日志等级，可以使用下面的方法：\n新建 start_log.js， 内容如下\ndb.adminCommand( { setParameter: 1, logComponentVerbosity: { verbosity: 1, query: { verbosity: 2 }, storage: { verbosity: 2, journal: { verbosity: 1 } } } } ); 执行命令：\n如果开启了认证,还需要加上用户和密码信息\n./mongo 127.0.0.1:27019/eop_task ./start_log.js\n上面例子中的方法，\n将全局的日志等级设置成1；\n将query的日志等级设置成2；\n将storage的日志等级设置成2；\n将storage.journal的日志等级设置成1；\n恢复原级别：\n新建 close_log.js,内容如下：\ndb.auth(\u0026#34;eop\u0026#34;,\u0026#34;password\u0026#34;); db.adminCommand( { setParameter:0, logComponentVerbosity: { verbosity: 0, query: { verbosity: -1 } } }); 执行命令 ./mongo 127.0.0.1:27019/eop_task ./close_log.js\n方法二：写入配置文件 执行这个命令，等同于在配置文件中写入：\nsystemLog: verbosity: 1 component: query: verbosity: 2 storage: verbosity: 2 journal: verbosity: 1 日志轮换 有时候，长时间没有清理日志，日志的数据量会变的很大，这个时候我们可以通过两种方法来对日志进行滚动：\n利用日志轮滚的方法，直接在MongoDB的命令行里面输入：\nuse admin //切换到admin数据库 db.runCommand({logRotate:1}) 这种方法采用了命令来切换日志文件，不需要关闭mongodb服务，是一个比较推荐的做法。\n当然，如果需要人手工的定期执行这个命令，好像也不够优雅，所以可以配合crontab去做这个事情，每天定时执行一次，达到日志文件轮滚的目的。\n查询日志分析 调整日志等级后，在日志文件中会发现如下类似日志\n{ aggregate: \u0026#34;zyb_work_task\u0026#34;, pipeline: [ { $match: { level: 0 } }, { $match: { $or: [ { \u0026#34;creator.id\u0026#34;: 811 }, { assigneeType: 0, assigneeId: 811 }, { assigneeType: 1, assigneeId: { $in: [ 5778 ] } }, { assigneeType: 2, assigneeId: { $in: [ 29295, 28087, 28118 ] } }, { \u0026#34;ccUser.id\u0026#34;: 811 }, { subAssigneeList: 811 }, { subCcList: 811 }, { subCreatorList: 811 } ] } }, { $addFields: { currentEmpFocus: { $ifNull: [ \u0026#34;$empFocus.811\u0026#34;, 0 ] } } }, { $match: { finishFlag: false } }, { $sort: { currentEmpFocus: -1, createTime: -1 } }, { $limit: 20 } ], cursor: {} } 然后将其复制到js文件中：\nfind_task_list.js\n//认证 db.auth(\u0026#34;eop\u0026#34;,\u0026#34;pass\u0026#34;); // 格式化输出 print(JSON.stringify( // 执行命令 db.runCommand( // 直接复制日志中的查询命令 { aggregate: \u0026#34;zyb_work_task\u0026#34;, pipeline: [ { $match: { level: 0 } }, { $match: { $or: [ { \u0026#34;creator.id\u0026#34;: 811 }, { assigneeType: 0, assigneeId: 811 }, { assigneeType: 1, assigneeId: { $in: [ 5778 ] } }, { assigneeType: 2, assigneeId: { $in: [ 29295, 28087, 28118 ] } }, { \u0026#34;ccUser.id\u0026#34;: 811 }, { subAssigneeList: 811 }, { subCcList: 811 }, { subCreatorList: 811 } ] } }, { $addFields: { currentEmpFocus: { $ifNull: [ \u0026#34;$empFocus.811\u0026#34;, 0 ] } } }, { $match: { finishFlag: false } }, { $sort: { currentEmpFocus: -1, createTime: -1 } }, { $limit: 20 } ], cursor: {} } ) ) ); 执行: ./mongo 127.0.0.1:27019/eop_task ./find_task_list.js\n此时就可以将应用的查询语句对应的结果显示出来，进行下一步的调试\n","date":"2021-08-12T23:13:15Z","permalink":"https://dccmmtop.github.io/posts/%E5%BC%80%E5%90%AF%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97/","section":"posts","tags":["mongoDB"],"title":"开启查询日志"},{"categories":null,"contents":"require \u0026#34;socket\u0026#34; local_ip = UDPSocket.open {|s| s.connect(\u0026#34;1.1.1.1\u0026#34;, 1);s.addr.last} ","date":"2021-08-12T23:12:27Z","permalink":"https://dccmmtop.github.io/posts/%E8%8E%B7%E5%8F%96%E6%9C%AC%E6%9C%BAip%E5%9C%B0%E5%9D%80/","section":"posts","tags":["ruby"],"title":"获取本机ip地址"},{"categories":null,"contents":"sidekiq清空队列里任务的方式主要有两种，一是使用sidekiq的api，二是直接操作redis\n一、使用sidekiq的api清空队列的任务 sidekiq里有提供操作队列的api，首先引入\nrequire\u0026rsquo;sidekiq/api'\n获取所有队列：Sidekiq::Queue.all\n获取默认队列：Sidekiq::Queue.new# the \u0026ldquo;default\u0026rdquo; queue\n按名称获取队列：Sidekiq::Queue.new(\u0026ldquo;mailer\u0026rdquo;)\n清空队列的所有任务：Sidekiq::Queue.new.clear\n按条件来删除队列的任务：\nqueue=Sidekiq::Queue.new(\u0026#34;mailer\u0026#34;) queue.eachdo |job| job.klass# =\u0026gt; \u0026#39;MyWorker\u0026#39; job.args# =\u0026gt; [1, 2, 3] job.delete if job.jid==\u0026#39;abcdef1234567890\u0026#39; end 二、直接操作redis来删除队列里的任务 首先获取配置文件config，再连接redis，这里使用了redis的Gem包\nredis= Redis.new(:host =\u0026gt; config[\u0026lsquo;host\u0026rsquo;], :port =\u0026gt; config[\u0026lsquo;port\u0026rsquo;], :db=\u0026gt; config[\u0026lsquo;db\u0026rsquo;], :password =\u0026gt; config[\u0026lsquo;password\u0026rsquo;])\n由于queues用的是set类型的数据，所以要用srem来删除相应的数据\nredis.srem(‘queues’, ‘队列的名称’) # 这种情况会直接删除该名称的队列 ","date":"2021-08-12T23:12:03Z","permalink":"https://dccmmtop.github.io/posts/%E6%B8%85%E9%99%A4sidekiq%E4%BB%BB%E5%8A%A1/","section":"posts","tags":["rails"],"title":"清除sidekiq任务"},{"categories":null,"contents":"例子如下：\n用sort_by\ndef order_weight_sort_by(string) string.split(\u0026#34; \u0026#34;).sort_by do |a| sum_a = a.split(\u0026#34;\u0026#34;).inject(0) { |mem, var| mem += var.to_i } first_number = a[0].to_i [a.size, sum_a, first_number] end.join(\u0026#34; \u0026#34;) end string = \u0026ldquo;56 65 74 100 99 68 86 980 90\u0026rdquo;\np order_weight_sort_by(string )\n结果是\u0026quot;90 56 65 74 68 86 99 100 980\u0026quot;\n上面的方法是先将字符串变成一个由数字字符串组成的数组。然后先按照字符串的长度进行排序，再按照字符串各数字之和进行排序，最后按照字符串的第一个数字大小进行排序。\n关键代码[a.size, sum_a, first_number]\n当最后是一个条件数组时，sort_by会按照该条件数组的顺序依次排序。\n用sort\ndef order_weight_sort(string) string.split(\u0026#34; \u0026#34;).sort do |a, b| sum_a = a.split(\u0026#34;\u0026#34;).inject(0) { |mem, var| mem += var.to_i } first_number_a = a[0].to_i size_a = a.size sum_b = b.split(\u0026#34;\u0026#34;).inject(0) { |mem, var| mem += var.to_i } first_number_b = b[0].to_i size_b = b.size [size_a, sum_a, first_number_a] \u0026lt;=\u0026gt; [size_b, sum_b, first_number_b] end.join(\u0026#34; \u0026#34;) end string = \u0026ldquo;56 65 74 100 99 68 86 980 90\u0026rdquo;\np order_weight_sort(string )\n结果与上面是一样的。\n关键代码[size_a, sum_a, first_number_a] \u0026lt;=\u0026gt; [size_b, sum_b, first_number_b]\n这样看起来sort_by比sort简洁很多。\n确实sort_by只需要一个参数，而sort需要两个参数。但他们实现多层排序是一样的，最后都是用一个条件数组来表示。\n区别\n但sort要比sort_by灵活。因为最后的排序条件还可以这样写：\n[size_b, sum_a, first_number_b] \u0026lt;=\u0026gt; [size_a, sum_b, first_number_a]\n这样就相当于先按长度倒序排列，然后再按照字符串各数字之和进行排序，最后再按照首个字符的大小倒序排列。\n作者：kamionayuki\n链接：http://www.jianshu.com/p/319d0174f246\n來源：简书\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n","date":"2021-08-12T23:10:45Z","permalink":"https://dccmmtop.github.io/posts/ruby%E5%A4%9A%E5%B1%82%E6%95%B0%E7%BB%84%E6%8E%92%E5%BA%8F/","section":"posts","tags":["ruby"],"title":"ruby多层数组排序"},{"categories":null,"contents":"rails验证码 1.安装包\ngem \u0026#39;rucaptcha\u0026#39; gem \u0026#39;dalli\u0026#39; 2.配置路由 （最新版本的不用配置路由）\nmount RuCaptcha::Engine =\u0026gt; \u0026#34;/rucaptcha\u0026#34; 3.controller部分\ndef create @user = User.new(user_params) if verify_rucaptcha?(@user)\u0026amp;\u0026amp;@user.save ...... 4.view部分\n\u0026lt;div class=\u0026#34;form-group \u0026#34;\u0026gt; \u0026lt;%= rucaptcha_input_tag( class:\u0026#39;form-control rucaptcha-text\u0026#39;) %\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#39;rucaptcha-image-box\u0026#39;\u0026gt;\u0026lt;%= rucaptcha_image_tag(class:\u0026#39;rucaptcha-image\u0026#39;, alt: \u0026#39;Captcha\u0026#39;) %\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; 5.实现点击图片刷新验证码\n#点击验证码刷新 $(\u0026#34;.rucaptcha-image\u0026#34;).click -\u0026gt; img = $(this) currentSrc = img.attr(\u0026#39;src\u0026#39;); img.attr(\u0026#39;src\u0026#39;, currentSrc.split(\u0026#39;?\u0026#39;)[0] + \u0026#39;?\u0026#39; + (new Date()).getTime()); return false ","date":"2021-08-12T23:08:20Z","permalink":"https://dccmmtop.github.io/posts/rails%E9%AA%8C%E8%AF%81%E7%A0%81/","section":"posts","tags":["rails"],"title":"rails验证码"},{"categories":null,"contents":"有时候我需要写一个页面能向show edit 那样可以接受参数的路由,弄了好久不知道怎样解决,今天恍然大悟\n我们执行 rake routes 就会看到如下\nwechat_nodes GET /wechat/nodes(.:format) wechat/nodes#index POST /wechat/nodes(.:format) wechat/nodes#create new_wechat_node GET /wechat/nodes/new(.:format) wechat/nodes#new edit_wechat_node GET /wechat/nodes/:id/edit(.:format) wechat/nodes#edit wechat_node GET /wechat/nodes/:id(.:format) wechat/nodes#show PATCH /wechat/nodes/:id(.:format) wechat/nodes#update PUT /wechat/nodes/:id(.:format) wechat/nodes#update DELETE /wechat/nodes/:id(.:format) wechat/nodes#destroy 这是使用resources 生成一些路由,我们可以模仿写出自己的路由\n我们看最后一列的内容, 对于show 来说 他的格式为/wechat/nodes/:id 再看edit 他的格式是/wechat/nodes/:id/edit\n加入我们要想写一个/wechat/node/edit/34/topic/23 类似的路由,我们可以直接这样写\nresources nodes do collection do get \u0026#39;edit/:id/topic/:node_id\u0026#39;, to: \u0026#39;nodes#topic\u0026#39; , as: \u0026#39;topic_edit\u0026#39; end end 这里为什么会有一个as呢,因为没有as和其后面的名称的话,这样是没有前面的路由信息的.\n筛选路由-路由冲突的解决方案 Rails项目有一个Article模型,对应ArticlesController控制器,其路由设置如下:\nresources :articles do end 这样它的CRUD路径就都自动创建出来了 ;)\n现在我想再添加一个对Article模型搜索的页面,那么首先要在控制器中添加对应的search方法:\ndef search render text:\u0026#34;hello search!!!\u0026#34; end 然后在Article默认路由集合后面添加一行新路由:\nget \u0026quot;articles/search\u0026quot;,to:\u0026quot;articles#search\u0026quot;1\n现在我们访问一下articles/search页面,咦?怎么出错了:\n仔细看出错信息,原来Article之前的show路由恰恰可以匹配新的search路由,只不过原来的:id变成了search这个字符串哦.这就是为什么报Couldn’t find Article with id=search的原因了!\n下面给出解决,我们只需要先禁用默认的show路由:\nresources :articles,except:[:show] do resources :comments end 然后再生成一条筛选路由即可,所谓筛选路由就是对该路由内容进行细粒度匹配的方法:\nget \u0026#34;articles/:id\u0026#34;,to:\u0026#34;articles#show\u0026#34;,constraints:{id:/\\d+/} 路由都是从上之下依次匹配的,如果上面一条被匹配则路由匹配结束!这里只匹配id为数字的articles/xxx路径,所以search就会默认被忽略从而被后面search正确的路由所匹配!\n","date":"2021-08-12T22:53:06Z","permalink":"https://dccmmtop.github.io/posts/%E5%8F%82%E6%95%B0%E8%B7%AF%E7%94%B1%E4%B8%8E%E8%B7%AF%E7%94%B1%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3%E4%B8%8E%E7%AD%9B%E9%80%89%E8%B7%AF%E7%94%B1/","section":"posts","tags":["rails"],"title":"参数路由与路由冲突解决与筛选路由"},{"categories":null,"contents":"存储过程添加列 create procedure add_col( in model_name text, in col_name text, in col_info text, out result text ) begin if not exists( select * from information_schema.COLUMNS where TABLE_NAME = model_name and COLUMN_NAME = col_name) then set @ddl=CONCAT(\u0026#39;alter table \u0026#39;, model_name, \u0026#39; add column \u0026#39;, col_name, col_info); prepare stmt from @ddl; execute stmt; set result = \u0026#39;success\u0026#39;; else set result = \u0026#39;exists\u0026#39;; end if; end; set @result = \u0026#39;\u0026#39;; call add_col(\u0026#39;dc_employee\u0026#39;,\u0026#39;senior_1 \u0026#39;,\u0026#39;tinyint(1) NULL DEFAULT 0 COMMENT \u0026#34;是否是高级用户，0 不是 1 是\u0026#34;\u0026#39;,@result); select @result; ","date":"2021-08-10T18:19:22Z","permalink":"https://dccmmtop.github.io/posts/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E6%B7%BB%E5%8A%A0%E5%88%97/","section":"posts","tags":["mysql"],"title":"存储过程添加列"},{"categories":null,"contents":"MySQL隔离级别 隔离级别是针对数据库 ACID 中的I(隔离性)来说的\n原子性 一致性 隔离性 持久性 隔离性 通常来说，一个事务所做的修改，在最终提交以前，对其他事务是不可见的，为什么是 “通常来说”，因为这种特性和隔离级别有关\n未提交读 READ UNCOMMITTED 事务中的修改，即使没有提交，其他事务也可以读到。被称为脏读。\n从性能上来说，并没有比其他级别好太多，却有很多问题，一般很少使用。\n读提交： 事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。造成了不可重复读（虚读）。\n可重复读： 事务A读取与搜索条件相匹配的若干行。事务B以插入或删除行等方式来修改事务A的结果集，然后再提交。事务A再读取时，却发现数据发生了变化。造成了幻读。\n不可重复读真正含义应该包含虚读和幻读。 所谓的虚读，也就是大家经常说的不可重复读，是指在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。比如事务T1读取某一数据，事务T2读取并修改了该数据，T1为了对读取值进行检验而再次读取该数据，便得到了不同的结果。\n一种更易理解的说法是：在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。\n所谓幻读，是指事务A读取与搜索条件相匹配的若干行。事务B以插入或删除行等方式来修改事务A的结果集，然后再提交。\n幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样.一般解决幻读的方法是增加范围锁RangeS，锁定检锁范围为只读，这样就避免了幻读。\n简单来说，幻读是由插入或者删除引起的。\n","date":"2021-08-10T18:17:48Z","permalink":"https://dccmmtop.github.io/posts/mysql%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","section":"posts","tags":["MySQL"],"title":"MySQL隔离级别"},{"categories":null,"contents":"开启慢日志 在 MySQL 中，慢查询日志默认为OFF状态，通过如下命令进行查看：\nshow variables like \u0026quot;slow_query_log\u0026quot;;\n通过如下命令进行设置为 ON 状态：\nset global slow_query_log = \u0026quot;ON\u0026quot;;\n日志存储位置 其中slow_query_log_file属性，表示慢查询日志存储位置，其日志默认名称为 host 名称。\n如下所示：\nshow variables like \u0026quot;slow_query_log_file\u0026quot;;\n+---------------------+----------------------------------------------+ | Variable_name | Value | +---------------------+----------------------------------------------+ | | slow_query_log_file | /usr/local/mysql/data/hostname.log | +---------------------+----------------------------------------------+ 2 rows in set (0.01 sec) 也可使用 以下命令进行修改：\nset global slow_query_log_file = ${path}/${filename}.log;\n设置阀值 慢查询 查询时间，当SQL执行时间超过该值时，则会记录在slow_query_log_file 文件中，其默认为 10 ，最小值为 0，(单位：\n秒)。\nmysql\u0026gt; show variables like \u0026#34;long_query_time\u0026#34;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 1 row in set (0.00 sec) 可通过以下命令进行修改：\nmysql\u0026gt; set global long_query_time = 5; 记录为走索引的sql 在 MySQL 中，还可以设置将未走索引的SQL语句记录在慢日志查询文件中(默认为关闭状态)。通过下述属性即可进行设置：\nmysql\u0026gt; set global log_queries_not_using_indexes = \u0026#34;ON\u0026#34;; Query OK, 0 rows affected (0.00 sec) 注意事项 设置该属性后，只要SQL未走索引，即使查询时间小于long_query_time值，也会记录在慢SQL日志文件中。 该设置会导致慢日志快速增长，开启前建议检查慢查询日志文件所在磁盘空间是否充足。 在生产环境中，不建议开启该参数。 解析日志文件 慢查询日志以#作为起始符。 User@Host：表示用户 和 慢查询查询的ip地址。 如上所述，表示 root用户 localhost地址。 Query_time: 表示SQL查询持续时间， 单位 (秒)。 Lock_time: 表示获取锁的时间， 单位(秒)。 Rows_sent: 表示发送给客户端的行数。 Rows_examined: 表示：服务器层检查的行数。 set timestamp ：表示 慢SQL 记录时的时间戳。 慢sql日志分析工具pt-query-digest 下载安装 yum install percona-toolkit-3.0.3-1.el7.x86_64.rpm 下载地址：https://www.percona.com/downloads/percona-toolkit/3.0.3/\n推荐用法 查询保存到query_history表查看慢sql，数据结构清晰，方便分析，方便与其他系统集成。\npt-query-digest --user=root --password=epPfPHxY --history h=10.8.8.66,D=testDb,t=query_review--create-history-table mysql_slow.log --since \u0026#39;2020-10-01 09:30:00\u0026#39; --until \u0026#39;2020-10-21 18:30:00\u0026#39; 常见用法 直接分析慢查询文件\npt-query-digest slow.log \u0026gt; slow_report.log 分析某个用户的慢sql\npt-query-digest \u0026ndash;filter \u0026lsquo;($event-\u0026gt;{user} || \u0026ldquo;\u0026rdquo;) =~ m/^root/i\u0026rsquo; slow.log\n分析某个数据库的慢sql\npt-query-digest \u0026ndash;filter \u0026lsquo;($event-\u0026gt;{db} || \u0026ldquo;\u0026rdquo;) =~ m/^sonar/i\u0026rsquo; slow.log\n分析某段时间内的慢sql pt-query-digest mysql_slow.log --since \u0026#39;2020-09-21 09:30:00\u0026#39; --until \u0026#39;2020-09-21 18:30:00\u0026#39; 输出结果说明\n第一部分：总体统计结果 Overall：总共有多少条查询 Time range：查询执行的时间范围 unique：唯一查询数量，即对查询条件进行参数化以后，总共有多少个不同的查询 total：总计 min：最小 max：最大 avg：平均 95%：把所有值从小到大排列，位置位于95%的那个数，这个数一般最具有参考价值 median：中位数，把所有值从小到大排列，位置位于中间那个数 \u0026hellip;\u0026hellip;\n#语句执行时间\n#锁占用时间\n#发送到客户端的行数\n#select语句扫描行数\n#查询的字符数\n第二部分：查询分组统计结果 Rank：所有语句的排名，默认按查询时间降序排列，通过--order-by指定 Query ID：语句的ID，（去掉多余空格和文本字符，计算hash值） Response：总的响应时间 time：该查询在本次分析中总的时间占比 calls：执行次数，即本次分析总共有多少条这种类型的查询语句 R/Call：平均每次执行的响应时间 V/M：响应时间Variance-to-mean的比率 Item：查询对象 第三部分：每一种查询的详细统计结果 由下面查询的详细统计结果，最上面的表格列出了执行次数、最大、最小、平均、95%等各项目的统计。 ID：查询的ID号，和上图的Query ID对应 Databases：数据库名 Users：各个用户执行的次数（占比） Query_time distribution ：查询时间分布, 长短体现区间占比，本例中1s-10s之间查询数量是10s以上的两倍。 Tables：查询中涉及到的表 Explain：SQL语句 ","date":"2021-08-10T18:16:04Z","permalink":"https://dccmmtop.github.io/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","section":"posts","tags":["mysql"],"title":"mysql慢日志"},{"categories":null,"contents":"安装 sudo apt-get install mysql-server mysql-client 配置远程可连接 你想myuser使用mypassword（密码）从任何主机连接到mysql服务器的话。\nmysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO \u0026#39;用户名\u0026#39;@\u0026#39;%\u0026#39;IDENTIFIED BY \u0026#39;你的密码\u0026#39; WITH GRANT OPTION; 如果你想允许用户myuser从ip为192.168.1.6的主机连接到mysql服务器，并使用mypassword作为密码\nmysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO \u0026#39;用户名\u0026#39;@\u0026#39;192.168.1.3\u0026#39;IDENTIFIED BY \u0026#39;你的密码\u0026#39; WITH GRANT OPTION; 最后\nmysql\u0026gt;FLUSH PRIVILEGES; 使修改生效，就可以了\n在远程主机上开放防火墙端口 sudo ufw allow 3306 或者关闭防火墙（不推荐）sudo ufw disable\n修改mysql配置文件 [mysqld] character-set-server = utf8 bind-address = 0.0.0.0 //修改ip地址 port = 3306 配置文件在/etc/mysql/mysql.conf.d/mysqld.cnf\n重启mysql服务：service mysql restart\n查看处于监听的服务状态：sudo netstat -aptn\n阿里云主机 如果你要连接的远程主机是阿里云服务器还需要配置安全组规则！！！\n开放入口，端口为3306/3306 优先级1 远程访问地址：0.0.0.0/0 点击保存\n","date":"2021-08-10T18:12:14Z","permalink":"https://dccmmtop.github.io/posts/ubuntu%E5%AE%89%E8%A3%85mysql%E5%8F%8A%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE/","section":"posts","tags":["MySQL"],"title":"Ubuntu 安装 mysql 及配置远程访问"},{"categories":null,"contents":"GoAccess的多种展示方式\ngoaccess有多种数据可视化的方式,分别为:\n命令行输出格式化数据\n利用access.log生成静态的可视化数据\n生成实时可视化数据\n注意，如果是编译安装且选择了 –enable-geoip=mmdb的话需要编辑配置文件，并在使用命令的时候带上参数 –config-file=/usr/local/etc/goaccess/goaccess.conf，如果是用包管理器安装的话则不需要\n命令行输出GoAccess\ngoaccess /var/log/nginx/access.log -c，会先询问你数据的格式，我这里的日志使用的是第一种。\n解析accesslog生成静态html\nGoAccess还可以解析access.log生成静态html，以更加直观的方式来展示数据。\ngoaccess /var/log/nginx/access.log -o report.html \u0026ndash;log-format=COMBINED，之后再使用浏览器访问report.html即可查看报告，各种数据一应俱全。\n实时解析访问日志\nGoAccess除了可以生成静态的html文件，还可以生成实时网站访问数据！\ngoaccess /var/log/nginx/access.log -o /var/www/html/report.html \u0026ndash;log-format=COMBINED \u0026ndash;real-time-html \u0026ndash;config-file=/usr/local/etc/goaccess/goaccess.conf\n添加中文支持\nGoaccess 1.3之后的版本提供了多语言支持，先在命令行中执行 apt install language-pack-zh-hans 安装中文包，再使用export LANG=zh_CN.UTF-8修改环境变量，再次使用 goaccess /var/log/nginx/access.log -o /var/www/html/report.html \u0026ndash;log-format=COMBINED \u0026ndash;real-time-html \u0026ndash;config-file=/usr/local/etc/goaccess/goaccess.conf启动GoAccess可以发现已经是中文界面了。\n关于实时模式，可以查看官网的demo https://rt.goaccess.io/?20200209201008\n异常退出\n如果实时模式没有正常退出，可能无法再次正常启动，GoAccess默认使用7890 websocket端口，所以使用lsof -i:7890查看占用该端口的进程号并kill即可。\nssl支持\n如果需要在加密连接上输出实时数据，则需要使用 \u0026ndash;ssl-cert= 和 \u0026ndash;ssl-key=,我在设置之后访问report.html发现数据依旧是静态的，突然想起我用了cloudflare cdn，而7890端口并不在cloudflare的支持端口列表里面，所以我使用参数 \u0026ndash;ws-url=wss://服务器域名(我们的浏览器会尝试与该域名的8443端口见了ws连接):8443 \u0026ndash;port=8443 把端口改成了8443。令人没想到的是，此时的report.html使用代理链接的时候是可以连接的，并可以查看实时信息，而直接连接的时候依旧是静态数据，tcping一看。\n去cloudflare的官网可以发现如下内容\n只有端口 80 和 443 可兼容以下服务：\n对于启用了中国网络的域名的中国境内数据中心 HTTP/HTTPS 流量，\n也就是说，国内是没办法通过cloudflare连接非80/443端口的…\n反向代理\n但是也不是没有办法连接，最后我想到了反向代理的方案。\n将启动参数改为\u0026ndash;ws-url=wss://你的域名.com/goaccess \u0026ndash;port=7890\n修改nginx站点配置文件 /etc/nginx/site-available/default,添加下面内容\nlocation /goaccess {\rproxy_redirect off;\rproxy_pass https://127.0.0.1:7890;\rproxy_http_version 1.1;\rproxy_set_header Upgrade $http_upgrade;\rproxy_set_header Connection \u0026quot;upgrade\u0026quot;;\rproxy_set_header Host $http_host;\r}\r注意，如果你的站点配置文件里面开启了url重写，为了避免 /goaccess 受到影响，我们需要把该路径排除重写。\n把重写规则都放到location / 里面去 location / { if (-f $request_filename/index.html){ rewrite (.*) $1/index.html break; } if (-f $request_filename/index.php){ rewrite (.*) $1/index.php; } if (!-f $request_filename){ rewrite (.*) /index.php; } } #下面什么都不需要做\nlocation /goaccess/ { } 之后重启nginx，再访问report.html，发现左边齿轮处终于显示connect了。\n如果你只是自己看或者不在意ip暴露，其实直接使用ip直接连接不走cdn就没那么麻烦了。\n","date":"2021-08-10T18:10:24Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%86%E6%9E%90nginx%E6%97%A5%E5%BF%97/","section":"posts","tags":["nginx"],"title":"分析nginx日志"},{"categories":null,"contents":"现在有需求，想要开机就运行一些东西，方法有两个，一个是桌面级的启动，一个是系统级的启动\n桌面级别，就是进入桌面后，自动打开一些软件\n系统级别就是再没有进入桌面就运行一些东西。\n先谈桌面级的，比如 gnome，启动 gnome-tweak 工具就可以看到开机启动项目，添加进去即可。\n系统级别的，我觉得最简单的方式就是创建 systemctl 的 service 脚本。这个脚本放什么位置呢？\n我们运行一个命令就看到了：\n$ systemctl enable sshd Created symlink /etc/systemd/system/multi-user.target.wants/sshd.service → /usr/lib/systemd/system/sshd.service. 我们开起来 sshd 服务，显示出来 service 的位置，我们模仿这个做一个开机启动出来。\n还记得很早的 linux 中有一个 rc.local 么？有什么需要开机启动的脚本直接丢进去就行了。\n升级到了systemd 之后，这个玩意就消失了，我们尝试恢复他。\n先建立一个 rc-local.service， sudo vim /usr/lib/systemd/system/rc-local.service 然后，我们模仿其他的 service ，来写一下：\n[Unit]\rDescription=\u0026#34;/etc/rc.local Compatibility\u0026#34; #Wants=sshdgenkeys.service\r#After=sshdgenkeys.service\r#After=network.target\r[Service]\rType=forking\rExecStart=/etc/rc.local start\rTimeoutSec=0\rStandardInput=tty\rRemainAfterExit=yes\rSysVStartPriority=99\r[Install]\rWantedBy=multi-user.target 然后，我们创建 /etc/rc.local 文件： sudo touch /etc/rc.local sudo chmod +x /etc/rc.local 设置 rc.local 开机自启 sudo systemctl enable rc-local.service ","date":"2021-08-10T17:55:56Z","permalink":"https://dccmmtop.github.io/posts/arch%E6%B7%BB%E5%8A%A0rc.local%E5%AE%9E%E7%8E%B0%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/","section":"posts","tags":["linux"],"title":"arch添加rc.local 实现开机自启"},{"categories":null,"contents":"笔记本可以同时连接无线和有线，如果可以指定哪些ip使用无线，哪些ip地址可以使用有线，\n假设无线连接的是内网， 有线连接的是外网，可以设置静态路由，使访问不同的地址使用不同的网络\n首先确定 本机 无线和有线 的网关： sudo ip route show 如上图 wlp3s0 是我的无线设备，enp0s20f0u1 是有线设备， 可以把网线拔了来确定哪个是什么设备\n无线设备使用 192.168.1.1 作为网关\n有线设备使用 192.168.42.129 作为网关\n设置路由 假设 某公司的内网都是 10 开头的，\nip route add 10.0.0.0/8 via 192.168.1.1 dev wlp3s0 10.0.0.0/8 是DICP表示法，不懂可以使用 站长工具 计算\n此时在使用 sudo ip route show 查看路由表\n永久生效 上述方法只能临时生效，可以写成脚本开机自启arch 添加rc.local 实现开机自启\n本方法已在 arch 中验证 其他系统 大同小异，寻找 代替 ip route 的命令即可\nwin 下设置方法 使用管理员打开 powershell\nipconfig 查看优先和无线的网关\nroute -p add 40.0.0.0 mask 255.0.0.0 192.168.2.1 添加 40 开头的ip 走内网\nroute -p add 10.0.0.0 mask 255.0.0.0 192.168.2.1 添加 10 开头的ip 走内网\n","date":"2021-08-10T17:50:41Z","permalink":"https://dccmmtop.github.io/posts/%E5%90%8C%E6%97%B6%E8%BF%9E%E6%8E%A5%E5%86%85%E7%BD%91%E5%92%8C%E5%A4%96%E7%BD%91/","section":"posts","tags":["linux"],"title":"同时连接内网和外网"},{"categories":null,"contents":"Git配置多个SSH-Key 当有多个git账号时，比如：\na. 一个gitee，用于公司内部的工作开发；\nb. 一个github，用于自己进行一些开发活动；\n解决方法\n生成一个公司用的SSH-Key\n$ ssh-keygen -t rsa -C \u0026#39;xxxxx@company.com\u0026#39; -f ~/.ssh/gitee_id_rsa 生成一个github用的SSH-Key\n$ ssh-keygen -t rsa -C \u0026#39;xxxxx@qq.com\u0026#39; -f ~/.ssh/github_id_rsa 在 ~/.ssh 目录下新建一个config文件，添加如下内容（其中Host和HostName填写git服务器的域名，IdentityFile指定私钥的路径）\n# gitee Host gitee.com HostName gitee.com PreferredAuthentications publickey IdentityFile ~/.ssh/gitee_id_rsa # github Host github.com HostName github.com PreferredAuthentications publickey IdentityFile ~/.ssh/github_id_rsa 用ssh命令分别测试\n$ ssh -T git@gitee.com $ ssh -T git@github.com ","date":"2021-08-10T17:50:06Z","permalink":"https://dccmmtop.github.io/posts/git%E7%94%9F%E6%88%90%E5%85%AC%E9%92%A5/","section":"posts","tags":["git"],"title":"git生成公钥"},{"categories":null,"contents":"docSet 文档可用于 zeal dash 软件中。 zeal 在win 下 和 Linux 均有可用版本 dash 则只在 Mac 可用\n制作 dcocSet 文档主要分 3 步\n镜像文档网站 做镜像网站就是把整个网站爬下来，并且把 css 和 js 图片等静态资源文件转换成本地的路径， 主要使用工具是 wget\n以 vue 中文文档 为例:\nwget -r -p -np -k https://cn.vuejs.org/ 制作索引文件 zeal 可以快速的搜索文档主要利用了 sqlite 数据库，在数据库中有一张 serchIndex 表， 这张表常用的字段有三个，分别是 name , type, path\nname\n关键词\ntype\n关键词的类型，代表该关键词是函数，还是类，等等可选的字段有 Sections, Fun, classes\npath\n点击关键词要跳转的路径\n所以，关键是制作一个这种合理的索引表\n下面是使用 ruby 实现制作索引表的功能，以及一些目录的生成\nrequire \u0026#39;nokogiri\u0026#39; require \u0026#34;sqlite3\u0026#34; require \u0026#34;fileutils\u0026#34; class HtmlToDoc def initialize(html_dir, docset_name) @html_path = html_dir @docset_name = \u0026#34;#{docset_name}.docset\u0026#34; @name = docset_name mkdir_file create_plist @con = SQLite3::Database.new(@dsidx) @con.execute(\u0026#34;CREATE TABLE IF NOT EXISTS searchIndex(id INTEGER PRIMARY KEY, name TEXT, type TEXT, path TEXT)\u0026#34;); end # 插入数据 def update_db(name, path, type = \u0026#39;Classes\u0026#39;) @con.execute(\u0026#39;INSERT OR IGNORE INTO searchIndex(name, type, path) VALUES (?,?,?)\u0026#39;,[name,type,path]) puts name,path end # 提取url，根据你的需求更改提取规则 def add_urls(html_path) doc = Nokogiri::HTML(File.open(html_path).read) doc.css(\u0026#34;h3\u0026gt;a\u0026#34;).each do |tag| name = tag.parent.text.strip if name.size \u0026gt; 0 \u0026amp;\u0026amp; tag[:href] path = tag[:href].strip.split(\u0026#34;#\u0026#34;).last update_db(name,html_path + \u0026#34;#\u0026#34; + path) end end end # 生成目录 def mkdir_file FileUtils.rm_r(@docset_name) if File.exists?(@docset_name) @doc_dir = \u0026#34;#{@docset_name}/Contents/Resources/Documents\u0026#34; FileUtils.mkdir_p(@doc_dir) @dsidx = \u0026#34;#{@docset_name}/Contents/Resources/docSet.dsidx\u0026#34; FileUtils.touch(@dsidx) @plist = \u0026#34;#{@docset_name}/Contents/info.plist\u0026#34; FileUtils.touch(@plist) puts \u0026#34;目录创建成功\u0026#34; end # 制作plist 文件 # 各种key 的意思请参考 dash 官方文档 def create_plist plist = \u0026lt;\u0026lt;-EOF \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;CFBundleIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;CFBundleName\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;DashDocSetFamily\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;DocSetPlatformFamily\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;requests\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;isDashDocset\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;isJavaScriptEnabled\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;dashIndexFilePath\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; EOF File.open(@plist,\u0026#34;w\u0026#34;).write(plist) end # 移动文档 def copy_files FileUtils.cp_r(@html_path.split(\u0026#34;/\u0026#34;).first, @doc_dir) # 将docSet 文档移动到 zeal 目录下 # local_doc_dir = \u0026#34;/home/dccmmtop/.local/share/Zeal/Zeal/docsets\u0026#34; # FileUtils.cp_r(@docset_name, local_doc_dir) end def start Dir.open(@html_path).each do |file| next unless file =~ /.html$/ add_urls(File.join(@html_path, file)) end copy_files end end if ARGV[0] == \u0026#34;-h\u0026#34; puts \u0026#39;ruby ./convert.rb \u0026#34;要生成文档的html地址(要包含整个网站的根目录)\u0026#34; \u0026#34;生成文档的名字\u0026#34;\u0026#39; puts \u0026#34;例子： ruby convert.rb cn.vuejs.org/v2/guide vue\u0026#34; else HtmlToDoc.new(ARGV[0],ARGV[1]).start end 移动docSet目录 最后将 制作好的 docSet 文件夹移动到 zeal 的文档目录下, 也可以将上面脚本中 copy_files 方法最后两行去掉\n","date":"2021-08-10T17:33:16Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%B6%E4%BD%9Cdocset%E6%96%87%E6%A1%A3/","section":"posts","tags":["linux"],"title":"制作docSet文档"},{"categories":null,"contents":"git 库中已经有文件被跟踪，如何忽略本地改动后的跟踪\n放入到.gitinore 并没有什么用，还是会显示改动，是否要提交，看着很烦\n忽略本地文件，且不会对线上库里的文件造成影响，执行此命令:\ngit update-index --assume-unchanged filename\n如果想撤销忽略，提交此文件的改动，执行此命令：\ngit update-index --no-assume-unchanged filename\n如果忽略的文件多了，可以使用以下命令查看忽略列表\ngit uls-files -v | grep '^h\\ '\n提取文件路径，方法如下\ngit ls-files -v | grep '^h\\ ' | awk '{print $2}'\n所有被忽略的文件，取消忽略的方法，如下\ngit ls-files -v | grep '^h' | awk '{print $2}' |xargs git update-index --no-assume-unchanged ","date":"2021-08-10T17:10:57Z","permalink":"https://dccmmtop.github.io/posts/%E5%BF%BD%E7%95%A5%E6%94%B9%E5%8A%A8/","section":"posts","tags":["git"],"title":"忽略改动"},{"categories":null,"contents":"为什么要使用tmux? 对我个人而言，tmux 对我最大的吸引力就是 多窗口，以及会话的保持与恢复，我可以很方便的切换窗口，以及快速恢复工作环境\n安装 我使用的是源码安装，\n下载源码 下载链接 解压 进入到解压目录，执行 ./configure --prefix=/usr/local/tmux3.1c \u0026amp;\u0026amp; make 然后执行 sudo make install\n我个人喜欢指定目录，方便日后卸载，或切换版本,中间可能会有缺少依赖包的错误，根据错误提示信息取网上搜索对应的包安装即可，其实linux发展到现在，很多错误提示信息都非常详细，英语不好的可以翻译下，就知道失败的原因了，有时反而比直接复制错误信息取网上搜索快的多。 此时已经安装完毕，可执行文件在 /usr/lcoal/tmux3.1c/bin/tmux, 可以在 /usr/bin/ 下创建 tmux 的软链，或者直接在 ~/.bash_profile 文件中编写 alias tmux='/usr/local/tmux3.1c/tmux' 然后在终端执行\nsource ~/.bash_profile 就可以让配置文件生效了。 配置及安装插件 下面是我的配置文件,以及说明\n##-- bindkeys --# ### 修改前缀键为 ctrl a set -g prefix ^a unbind ^b bind a send-prefix # 窗口切换 我设置的键位和 vim 相通 # ctrl + j 上一个窗口 bind -n C-j previous-window # ctrl + k 下一个窗口 bind -n C-k next-window # 设置松开鼠标不会自动跳到屏幕底部，这个特性需要稍微高一点的版本才支持 set -g mouse on unbind -T copy-mode-vi MouseDragEnd1Pane # 快速加载配置文件，不用重启 tmux bind-key r source-file ~/.tmux.conf \\; display-message \u0026#34;tmux.conf reloaded\u0026#34; # 下面是底部状态栏的配置，个人不喜欢花里胡哨，简单够用即可 set -g message-style \u0026#34;bg=#00346e, fg=#ffffd7\u0026#34; # tomorrow night blue, base3 set -g status-style \u0026#34;bg=#00346e, fg=#ffffd7\u0026#34; # tomorrow night blue, base3 set -g status-left \u0026#34;#[bg=#0087ff] ❐ Hi!\u0026#34; # blue set -g status-left-length 400 set -g status-right \u0026#34;\u0026#34; #set -g status-right \u0026#34;#[bg=red] %Y-%m-%d %H:%M \u0026#34; #set -g status-right-length 600 set -wg window-status-format \u0026#34; #I #W \u0026#34; set -wg window-status-current-format \u0026#34; #I #W \u0026#34; set -wg window-status-separator \u0026#34;|\u0026#34; set -wg window-status-current-style \u0026#34;bg=red\u0026#34; # red set -wg window-status-last-style \u0026#34;fg=red\u0026#34; # split window unbind % # 水平分隔窗口 快捷键是 prefix \\ (我的前缀是 ctrl + A) bind \\\\ split-window -h unbind \u0026#39;\u0026#34;\u0026#39; # 垂直分隔窗口 快捷键是 prefix - bind - split-window -v # select pane 面板之间跳转 bind k selectp -U # above (prefix k) bind j selectp -D # below (prefix j) bind h selectp -L # left (prefix h) bind l selectp -R # right (prefix l) # 下面是插件部分 # 用来保存会话 和 回复会话的 # 保存会话快捷键是 prefix + ctrl + s (对应我的就是 ctrl + A ctrl + s 中间不要松开 ctrl 键) # 恢复会话： prefix + ctrl + r set -g @plugin \u0026#39;tmux-plugins/tmux-resurrect\u0026#39; set -g @continuum-restore \u0026#39;on\u0026#39; set -g @resurrect-save-bash-history \u0026#39;on\u0026#39; set -g @resurrect-capture-pane-contents \u0026#39;on\u0026#39; # 设置保存 vim 的工作状态 set -g @resurrect-strategy-vim \u0026#39;session\u0026#39; # 每隔60秒自动保存一次 set -g @continuum-save-interval \u0026#39;60\u0026#39; # 鼠标选中即复制的插件 set -g @plugin \u0026#39;tmux-plugins/tmux-yank\u0026#39; set -g @yank_action \u0026#39;copy-pipe\u0026#39; set -g @yank_with_mouse on # 安装插件的插件 # ctrl + I 即自动下载插件安装。 run \u0026#39;~/.tmux/plugins/tpm/tpm\u0026#39; 结束！ 有任何问题，欢迎下面留言，或者加我好友讨论哟！或者请我吃个糖果 (^_^)\n","date":"2021-08-10T11:21:12Z","permalink":"https://dccmmtop.github.io/posts/%E4%BD%BF%E7%94%A8tmux/","section":"posts","tags":null,"title":"Linux tmux 安装及配置"},{"categories":null,"contents":"Linux修改时区的正确方法\nCentOS和Ubuntu的时区文件是/etc/localtime，但是在CentOS7以后localtime以及变成了一个链接文件\n[root@centos7 ~]# ll /etc/localtime\nlrwxrwxrwx 1 root root 33 Oct 12 11:01 /etc/localtime -\u0026gt; /usr/share/zoneinfo/Asia/Shanghai\n如果采用直接cp的方法修改系统时区，那么就会把它所链接的文件修改掉，例如把美国的时区文件内容修改成了上海的时区内容，有可能会导致有些编程语言或程序在读取系统时区的时候发生错误，因此正确的修改方法是：\nCentOS6、Ubuntu16\ncp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime CentOS7、RHEL7、Scientific Linux 7、Oracle Linux 7\n最好的方法是使用timedatectl命令\ntimedatectl list-timezones |grep Shanghai #查找中国时区的完整名称 Asia/Shanghai\ntimedatectl set-timezone Asia/Shanghai #其他时区以此类推\n或者直接手动创建软链接(验证过) ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ","date":"2021-08-10T11:18:51Z","permalink":"https://dccmmtop.github.io/posts/%E4%BF%AE%E6%94%B9%E6%97%B6%E5%8C%BA/","section":"posts","tags":null,"title":"修改时区"},{"categories":null,"contents":"在目标机器中修改/etc/ssh/sshd_conf文件\n将UseDNS 的缺省值由yes修改为no，并重启ssh\n","date":"2021-08-10T10:48:44Z","permalink":"https://dccmmtop.github.io/posts/ssh%E6%85%A2/","section":"posts","tags":null,"title":"ssh慢"},{"categories":null,"contents":"通过rename命令批量重命名文件 基本语法\n示例\n改变文件扩展名 大写改成小写 更改文件名模式 通过rename命令批量重命名文件\n基本语法\nrename [-n -v -f] \u0026lt;pcre\u0026gt; \u0026lt;files\u0026gt; \u0026lsquo;pcre’是Perl兼容正则表达式，它表示的是要重命名的文件和该怎么做。正则表达式的形式是‘s/old-name/new-name/’。\n‘-v’选项会显示文件名改变的细节（比如：XXX重命名成YYY）。\n‘-n’选项告诉rename命令在不实际改变名称的情况下显示文件将会重命名的情况。这个选项在你想要在不改变文件名的情况下模拟改变文件名的情况下很有用。\n‘-f’选项强制覆盖存在的文件。\n示例\n改变文件扩展名\n假设你有许多.jpeg的图片文件，你想要把它们的名字改成.jpg。下面的命令就会将.jpeg 文件改成 *.jpg。\nrename 's/\\.jpeg/\\.jpg/' *.jpeg 大写改成小写\n有时你想要改变文件名的大小写，你可以使用下面的命令。\n把所有的文件改成小写\nrename 'y/A-Z/a-z/'\n把所有的文件改成大写\nrename 'y/a-z/A-Z/' * 更改文件名模式\n现在让我们考虑更复杂的包含子模式的正则表达式。在PCRE中，子模式包含在圆括号中，符后接上数字（比如1，$2）。 下面的命令会将‘imgNNNN.jpeg’变成‘danNNNN.jpg’。\nroot@root:~$ rename -v \u0026#39;s/img_(\\d{4})\\.jpeg/dan_$1.jpg/\u0026#39; *.jpeg img_5417.jpeg renamed as dan_5417.jpg img_5418.jpeg renamed as dan_5418.jpg img_5419.jpeg renamed as dan_5419.jpg img_5420.jpeg renamed as dan_5420.jpg img_5421.jpeg renamed as dan_5421.jpg img_5422.jpeg renamed as dan_5422.jpg 下面的命令会将‘img_000NNNN.jpeg’变成‘dan_NNNN.jpg’。\nroot@root:~$ rename -v \u0026#39;s/img_\\d{3}(\\d{4})\\.jpeg/dan_$1.jpg/\u0026#39; *.jpeg img_0005417.jpeg renamed as dan_5417.jpg img_0005418.jpeg renamed as dan_5418.jpg img_0005419.jpeg renamed as dan_5419.jpg img_0005420.jpeg renamed as dan_5420.jpg img_0005421.jpeg renamed as dan_5421.jpg img_0005422.jpeg renamed as dan_5422.jpg 上面的例子中，子模式‘\\d{4}’会捕捉4个连续的数字，捕捉的四个数字匹配模式对应$1, 将会用于新的文件名。\n","date":"2021-08-10T10:12:57Z","permalink":"https://dccmmtop.github.io/posts/linux%E6%89%B9%E9%87%8F%E9%87%8D%E5%91%BD%E5%90%8D/","section":"posts","tags":null,"title":"linux批量重命名"},{"categories":null,"contents":"设置ss git config --global http.proxy \u0026#39;socks5://127.0.0.1:1080\u0026#39; git config --global https.proxy \u0026#39;socks5://127.0.0.1:1080\u0026#39; 设置代理 git config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 取消代理 git config --global --unset http.proxy git config --global --unset https.proxy ","date":"2021-08-08T17:18:35Z","permalink":"https://dccmmtop.github.io/posts/git%E8%AE%BE%E7%BD%AE%E5%92%8C%E5%8F%96%E6%B6%88%E4%BB%A3%E7%90%86/","section":"posts","tags":["git"],"title":"git 设置和取消代理"},{"categories":null,"contents":"\n","date":"2021-08-03T23:02:05Z","permalink":"https://dccmmtop.github.io/posts/%E5%8D%95%E5%90%91%E9%80%9A%E9%81%93/","section":"posts","tags":null,"title":"单向通道"},{"categories":null,"contents":"var 方式 var name type = expiression\nvar name string = \u0026#34;zhangsan\u0026#34; var name = \u0026#34;zhangsan\u0026#34; var name string // 默认值是 \u0026#34;\u0026#34; 变量列表声明 var 方式通常用户和初始化类型不一致的局部变量，或则初始化值不重要的情况\n短变量声明 多变量声明 i,j := 0,1\n重点 := 代表声明\n= 标识赋值\n交换值\ni,j = j,i\n第二次声明等同赋值\n第二行 err 等同于赋值\n至少声明一个变量\n","date":"2021-08-01T11:36:29Z","permalink":"https://dccmmtop.github.io/posts/%E5%8F%98%E9%87%8F/","section":"posts","tags":["go"],"title":"变量"},{"categories":null,"contents":"数组 初始化 指定长度 a := [2]int{1,2} // 指定长度和字面量 不指定长度 a := [...]int{1,2} //不指定长度，有后面的列表来确定其长度 指定总长度，通过索引初始化, 没有初始化的位置使用默认值 a := [3]int{1:1, 2:3} 不指定总长度，通过索引初始化, 最后一个索引为总长度,没有初始化的位置使用默认值 a := [...]int{1:1, 2:3, 5:9} 特点 长度固定,不可以追加元素 是值类型，赋值或作为函数参数，都是值拷贝 数组长度是类型的一部分 [10]int 和 [20]int 是不同类型 可以根据数组创建切片 切片 创建切片 由数组创建\narray[b:e]\narray 表示数组名，b表示开始索引，可以不指定，默认是0，\ne表示结束索引，可以不指定，默认是数组长度len(array)\narray[b:e] 表示创建 e-b 个元素的切片，第一个元素是 array[b],最后一个元素是 array[e-1]\n由 make 函数创建\n// len = 10 cap = 10 a := make([]int, 10) //len = 10 cap = 15 b := make([]int, 10, 15) 切片操作 len() 返回长度 cap() 返回底层数组容量 append() 追加元素 copy() 复制切片 字符串与切片的转换 str := \u0026#34;hello,世界\u0026#34; // 转换成字节切片 a := []byte(str) // 转换成 rune 切片 b := []rune(str) ","date":"2021-07-29T23:11:14Z","permalink":"https://dccmmtop.github.io/posts/%E6%95%B0%E7%BB%84%E4%B8%8E%E5%88%87%E7%89%87/","section":"posts","tags":null,"title":"数组与切片"},{"categories":null,"contents":"用在常量声明中，初始值为0，一组常量同时声明时，其值逐行增加\n类似枚举 const ( c0 = iota //c0 == 0 c1 = iota //c1 == 1 c2 = iota //c2 == 2 ) 简写模式\nconst ( c0 = iota // c0 == 0 c1 // c1 == 1 c2 // c2 == 2 ) 分开的const 分开的const语句， iota 的值每次都是从0开始\nconst c0 = iota // c0 == 0\nconst c1 = iota // c1 == 0\n","date":"2021-07-29T22:49:53Z","permalink":"https://dccmmtop.github.io/posts/iota-%E7%94%A8%E6%B3%95/","section":"posts","tags":null,"title":"iota 用法"},{"categories":null,"contents":"1.Mac下编译Linux, Windows Linux CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build filename.go Windows CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build filename.go 如: CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o helloworld-windows helloworld.go\n2.Linux下编译Mac, Windows Mac CGO_ENABLED=0 GOOS=darwin GOARCH=amd64 go build filename.go Windows CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build filename.go 3.Windows下编译Mac, Linux Mac SET CGO_ENABLED=0 SET GOOS=darwin SET GOARCH=amd64 go build filename.go Linux SET CGO_ENABLED=0 SET GOOS=linux SET GOARCH=amd64 go build filename.go\n","date":"2021-07-25T10:24:03Z","permalink":"https://dccmmtop.github.io/posts/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/","section":"posts","tags":null,"title":"交叉编译"},{"categories":null,"contents":" ps -ef | grep nginx | awk -F ' ' '{print $2}'| xargs\n","date":"2021-07-10T16:58:13Z","permalink":"https://dccmmtop.github.io/posts/%E5%A4%9A%E8%A1%8C%E5%8F%98%E5%8D%95%E8%A1%8C/","section":"posts","tags":null,"title":"多行变单行"},{"categories":null,"contents":"有时需要根据自己的工作场合去扩展 git 命令,比如\n推送到仓库后自动打开浏览器跳转到发起合并求页面 分支命名比较长其相似度比较大时，自动补全不那么有效率，给每个分支编号，输入指定编号即可切换对应的分支 我最推荐的一种方式是利用 shell 脚本的特性，将脚本命名为 git-xxxx 方式，在终端就可以通过 git xxx 的方式运行该命令\n下面是两个例子：\n给每个分支编码 在 /usr/lcoal/bin 先新建 git-brr 文件,输入下面脚本：\n#!/bin/bash git branch --no-color | cat -n | sed \u0026#39;s/*/ /\u0026#39; | awk \u0026#39;{print $2 \u0026#34; (\u0026#34;$1\u0026#34;)\u0026#34;}\u0026#39; sudo chomd +x ./git-brr 赋予可执行权限\n然后到一个项目下执行 git brr\n指定编号切换分支 在 /usr/lcoal/bin 先新建 git-coo 文件,输入下面脚本\n#!/bin/bash git checkout $( git brr | egrep \u0026#34;\\($1)$\u0026#34; | egrep -o \u0026#39;.+ \u0026#39;) 然后 sudo chomd +x ./git-coo 赋予可执行权限\n此时就可已通过 git coo 4 切换对应的分支了\n","date":"2021-06-10T17:17:09Z","permalink":"https://dccmmtop.github.io/posts/%E6%89%A9%E5%B1%95git%E5%8A%9F%E8%83%BD%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F/","section":"posts","tags":["git"],"title":"扩展git功能的一种方式"},{"categories":null,"contents":"基本类型以及范围 选择优化的数据类型 MySQL 支持的数据类型非常多，选择正确的数据类型对于获得高性能至关重要。不管\n存储哪种类型的数据，下面几个简单的原则都有助于做出更好的选择。\n更小的通常更好。 一般情况下,应该尽量使用可以正确存储数据的最小数据类型二。更小的数据类型通\n常更快，因为它们占用更少的磁盘、内存和 CPU 缓存，并且处理时需要的 CPU 周\n期也更少。\n但是要确保没有低估需要存储的值的范围，因为在 schema 中的多个地方增加数据类\n型的范围是一个非常耗时和痛苦的操作。如果无法确定哪个数据类型是最好的，就\n选择你认为不会超过范围的最小类型。(如果系统不是很忙或者存储的数据量不多，\n或者是在可以轻易修改设计的早期阶段，那之后修改数据类型也比较容易) 。\n简单就好 简单数据类型的操作通常需要更少的 CPU 周期。例如，整型比字符操作代价更低，\n因为字符集和校对规则 (排序规则)使字符比较比整型比较更复杂。这里有两个例子:\n一个是应该使用 MySQL 内建的类型圭 而不是字符串来存储日期和时间，另外一个\n是应该用整型存储 卫 地址。稍后我们将专门讨论这个话题。\n尽量避免 NULL 很多表都包含可为 NULL (空值) 的列，即使应用程序并不需要保存 NULL 也是如此，\n这是因为可为 NULL 是列的默认属性主:。通常情况下最好指定列为 NOT NULL, 除非真\n的需要存储 NULL值。\n如果查询中包含可为 NULL 的列，对 MySQL 来说更难优化，因为可 NULL 的列使\n得索引、索引统计和值比较都更复杂。可为 NULL 的列会使用更多的存储空间，在\nMySQL 里也需要特殊处理。当可为 NULL 的列被索引时，每个索引记录需要一个额\n外的字节，在 MyISAM 里甚至还可能导致固定大小的索引(例如只有一个整数列的\n索引) 变成可变大小的索引。\n通常把可为 NULL 的列改为 NOT NULL 带来的性能提升比较小，所以 (调优时) 没有\n必要首先在现有 schema 中查找并修改掉这种情况，除非确定这会导致问题。但是，\n如果计划在列上建索引，就应该尽量避免设计成可为 NULL 的列。\n整数类型 类型 存储空间 TINYINT 8 SMALLINT 16 MEDIUMINT 24 INT 32 BIGINT 64 字符串 VARCHAR VARCHAR 类型用于存储可变长字符串，是最常见的字符串数据类型。它比定长类型\n更节省空间，因为它仅使用必要的空间 (例如，越短的字符串使用越少的空间) 。有\n一种情况例外，如果 MySQL 表使用 ROW_FORMAT=FIXED 创建的话，每一行都会使用\n定长存储，这会很浪费空间。\nVARCHAR需要使用 1 或 2 个额外字节记录字符串的长度 : 如果列的最大长度小于或\n等于 255 字节, 则只使用 1 个字节表示, 否则使用 2 个字节。假设采用 latinl 字符集，\n一个 VARCHAR(16) 的列需要 11 个字节的存储空间。VARCHAR(1069) 的列则需要 1002\n个字节，因为需要 2 个字节存储长度信息。\nVARCHAR节省了存储空间，所以对性能也有帮助。但是，由手答是变长的，在\nUPDATE 时可能使行变得比原来更长，这就导致需要做额外的工作。如果一个行占用\n的空间增长，并且在页内没有更多的空间可以存储，在这种情况下，不同的存储引\n人擎的处理方式是不一样的。例如，MyISAM 会将行拆成不同的片段存储，InnoDB\n则需要分裂页来使行可以放进页内。其他一些存储引擎也许从不在原数据位置更新\n数据。\n下面这些情况下使用VARCHAR是合适的 : 字符串列的最大长度比平均长度大很多 ，\n列的更新很少，所以碎片不是问题 ， 使用了像 UTF-8 这样复杂的字符集，每个字符\n都使用不同的字节数进行存储。\nCHAR CHAR 类型是定长的 : MySQL 总是根据定义的字符串长度分配足够的空间。当存储\n[CHAR值时全MYSQI研删除所有的末尾空格上(在 MySQL 4.1 和更老版本中 VARCHAR\n也是这样实现的一 也就是说这些版本中 CHAR 和 VARCHAR 在逻辑上是一样的，区\n别只是在存储格式上) 。 CHAR值佐根据需要采用空格进行填充以方便比较\nEEAR适咎存储很知的字符十|或者所有值都接近同一个长度。例如，CHAR 非常适\n合存储密码的 MD5 值，因为这是一个定长的值。对于经常变更的数据，CHAR也比\nVARCHAR更好，因为定长的 CHAR 类型不容易产生碎片。对于非常短的列，CHAR 比\nVARCHAR 在存储空间上也更有效率。例如用 CHAR(1) 来存储只有 Y 和N的值，如果\n采用单字节字符集二 只需要一个字节，但是 VARCHAR(1) 却需要两个字节，因为还有\n一个记录长度的额外字节。\n慷慨是不明智的 使用 VARCHAR(5) 和 VARCHAR(290) 存储“heLLo\u0026rsquo; 的空间开销是一样的。那么使用更\n短的列有什么优势吗?\n事实证明有很大的优势。更长的列会消耗更多的内存，因为 MYSQL 通常会分配固\n定大小的内存块来保存内部值。尤其是使用内存临时表进行排序或操作时会特别粳\n粒。在利用磁盘临时表进行排序时也同样糟楼。\n所以最好的策略是只分配真正需要的空间。\n日期和时间类型 MySQL 可以使用许多类型来保存日期和时间值，例如 YEAR 和 DATE。MySQL 能存储的\n最小时间粒度为秒 (MariaDB 支持微秒级别的时间类型) 。但是 MySQL 也可以使用微秒\n级的粒度进行临时运算，我们会展示怎么绕开这种存储限制。\n大部分时间类型都没有替代品，因此没有什么是最佳选择的问题。唯一的问题是保\n存日期和时间的时候需要做什么。MySQL 提供两种相似的日期类型 : DATETIME 和\nTIMESTAMP。对于很多应用程序，它们都能工作，但是在某些场景，一个比另一个工作\n得好。让我们来看一下。\nDATETIME 这个类型能保存大范围的值，从 1001 年到 9999 年，精度为秒。它把日期和时间封\n装到格式为YYYYMMDDHHMMSS 的整数中，与时区无关。使用8 个字节的存储\n空间。\n默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME值，例如\n“2008-01-16 22:37:08\u0026quot;。这是 ANSI 标准定义的日期和时间表示方法。\nTIMESTAMP 就像它的名字一样，TIMETAMP 类型保存了从 1970 年 1 月 1 日午夜 格林尼治标准\n时间) 以来的秒数,它和 UNIX 时间戳相同。TIMESTAMP 只使用 4 个字节的存储空间，\n因此它的范围比 DATETIME 小得多 : 只能表示从 1970 年到 2038 年。MySQL 提供了\nFROM_UNIXTINME() 函数把 Unix 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函\n数把日期转换为 Unix 时间发。\nMySQL 4.1 以及更新的版本按照 DATETIME 的方式格式化 TIMESTAMP 的值，但是\nMySQL 4.0 以及更老的版本不会在各个部分之间显示任何标点符号。这仅仅是显示\n格式上的区别，TIMESTAMP 的存储格式在各个版本都是一样的。\nTIMESTAMP 显示的值也依赖于时区。MyYSQL 服务器、操作系统，以及客户端连接都\n有时区设置。\n因此，存储值为 0 的 TIMESTAMP 在美国东部时区显示 “1969-12-31 19:00:00\u0026quot;, 45\n格林尼治时间差 5 个小时。有必要强调一下这个区别 : 如果在多个时区存储或访问\n数据，TIMESTAMP 和 DATETIME 的行为将很不一样。前者提供的值与时区有关系，后\n者则保留文本表示的日期和时间。\nTIMESTAMP 也有 DATETIME 没有的特殊属性。默认情况下，如果插和人时没有指定第一\n个TIMESTAMP 列的值,MySQL 则设置这个列的值为当前时间\u0026quot;。 在插入一行记录时，\nMySQL 默认也会更新第一个 TIMESTAMP 列的值 (除非在 UPDATE 语句中明确指定了\n值)。你可以配置任何 TIMESTAMP 列的插入和更新行为。最后，TIMESTAMP 列默认为\nNOT NULL，这也和其他的数据类型不一样。\n除了特殊行为之外, 通常也应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。\n有时候人们会将 Unix 时间截存储为整数值，但这不会带来任何收益。用整数保存时间\n截的格式通常不方便处理，所以我们不推荐这样做。\n如果需要存储比秒更小粒度的日期和时间值怎么办? MySQL 目前没有提供合适的数据\n类型，但是可以使用自己的存储格式 : 可以使用 BIGINT 类型存储微秒级别的时间截，或\n者使用DOUBLE 存储秒之后的小数部分。这两种方式都可以，或者也可以使用 MariaDB\n","date":"2021-04-13T23:59:32Z","permalink":"https://dccmmtop.github.io/posts/%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E4%BB%A5%E5%8F%8A%E8%8C%83%E5%9B%B4/","section":"posts","tags":["mysql"],"title":"基本类型以及范围"},{"categories":null,"contents":"先sort排序，再去重 :sort //直接排序 :g/^\\(.*\\)$\\n\\1$/d //去除重复行 :g/\\%(^\\1$\\n\\)\\@\u0026lt;=\\(.*\\)$/d //功能同上，也是去除重复行 :g/\\%(^\\1\\\u0026gt;.*$\\n\\)\\@\u0026lt;=\\(\\k\\+\\).*$/d //功能同上，也是去除重复行 使用awk awk '!a[$0]++' file\n解析：\nawk流程是逐行处理的，默认从文件的第一行一直处理到文件最后一行\n还要知道awk的基本命令格式是'pattern{action}'先匹配各种各样的样式，然后大括号里处理如何打印输出，\n默认的只要匹配了pattern就{print $0}，如果pattern未命中其判断值为假（0）那么就不会再去处理{action}了\npattern命中则为判断值为真（非0）就去处理{action}。\n举个最简单的例子：awk '1' file和awk '{print $0}' file 是一个道理，都是从头到尾依次打印文件的每一行。\n'!a[$0]++'\n分成几个部分简单解释下吧。\n这个命令没有{action}也就是说，只要pattern部分判断值为真（非0）就打印该行，否则就跳过不打印\n！在awk是取相反的意思，就是把对的变成错的把真的变成假的，放在这个命令中是神马作用一会解释；\na[$0] 这个非常好理解，建立数组a，其变量是文本中的每一行，awk里$1是第一列，$2是第二列，以此类推$NF是最后一列，而$0是代表所有列及分隔符，也就是一整行，这样如果pattern是真的那就打印一整行\n++的意思是a数组取变量完毕后，对该数组值+1\n找个最简单的文档来解释一下\ncat file xxx yyy xxx zzz 这个文件有4行，其中第一、三行是重复的。套用这个命令处理流程如下：\n获取第一行a[xxx]，因为这是第一行，数组a里从没见过xxx这个变量，那么自然他的值就是假（0）也就是说a[xxx]=0，这个时候！就有大作用了，他把a[xxx]假（0）变成了a[xxx]为真（!0）这个时候原本不该打印的第一行就变成了应该打印了，取逻辑反后对a[xxx]的值+1然后处理第二行\n第二行a[yyy]这个情况跟刚才第一行的a[xxx]一样，也应该打印他\n到第三行的时候情况变了，因为第一行已经出现过a[xxx]并且已经++过了，他的值已经是非0而不是前两行的0了，本应打印但这时候再由！取逻辑反就不必打印了\n第四行a[zzz]就又和第一、二两行一样了。\n所以执行完就是这个结果\nawk \u0026#39;!a[$0]++\u0026#39; file xxx yyy zzz 再把file搞稍微复杂点\nawk \u0026#39;{print NR,$0}\u0026#39; file 1 xxx 2 yyy 3 zzz 4 xxx 5 yyy 6 zzz 7 xxx 8 yyy 9 zzz 一共9行文本，3行一次重复。为了看得更清楚，本来默认的{print $0}稍微改下，变成{print NR,$0} NR表示行号。\n那么现在来执行下刚才讲的试试看\nawk \u0026#39;!a[$0]++{print NR,$0}\u0026#39; file 1 xxx 2 yyy 3 zzz awk \u0026#39;a[$0]++{print NR,$0}\u0026#39; file 4 xxx 5 yyy 6 zzz 7 xxx 8 yyy 9 zzz ","date":"2019-11-09T18:05:59Z","permalink":"https://dccmmtop.github.io/posts/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E8%A1%8C/","section":"posts","tags":["linux"],"title":"去除重复行"},{"categories":null,"contents":"根据项目的进展，我们需要实现后台进行定时读取信息的功能，而最关键的实现部分是周期性功能，根据调研，决定使用whenever来实现这一功能。\ngithub：https://github.com/javan/whenever\n开发前需要明确的问题 whenever是怎样一种周期性机制？ whenever能为我们提供什么功能？ whenever为周期性任务提供了哪些控制方式？ 问题解决\nwhenever周期性机制\n我们来看一下github上面是怎么说的：\nWhenever is a Ruby gem that provides a clear syntax for writing and deploying cron jobs.\n意思就是说，whenever是一个ruby gem，但同时它是基于cron jobs的。\n那么什么是cron jobs呢？我们来看一下维基百科的定义：\nCron crontab命令常见于Unix和类Unix的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。该词来源于希腊语chronos（χρόνος），原意是时间。\n通常，crontab储存的指令被守护进程激活，crond常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。\n也就是说，crontab是在unix和类unix系统中用来实现周期性功能的指令。在网上搜一下，我们就会看到很多crontab指令相关的语法。\n根据上述的分析，我们可以得出这样的结论：\nwhenever事实上是一个cron翻译器，它将rails中的ruby代码翻译成cron脚本，从而将周期性的任务交给cron来执行。 这样，通过whenever我们可以使用ruby语言来写周期性任务代码，在ruby层控制代码，而不需要与shell脚本进行切换；另一方面，我们会发现，由于cron命令的强大，它的语法也因此变得很复杂，通过whenever，我们可以很方便的实现周期性任务。\n一个十分简单的demo 1.添加whenever(Gemfile)\ngem 'whenever', :require =\u0026gt;false\n2.生成config/schedule.rb文件\n执行命令：\nwheneverize\n3.添加自己的周期性任务\n在config/schedule.rb文件中添加：\nset :environment, :development every 2.minutes do runner \u0026#34;Timetest.mytime\u0026#34; end 其中，set :environment, :development是设置执行任务时的环境，默认情况下环境为production\n上述代码实现的是每两分钟读取当前时间并存入到数据库的功能。其中，runner方法执行的方法如下：\nclassTimetest \u0026lt; ApplicationRecord def self.mytime a = Timetest.new a.time_now = Time.now a.save end end 这样，在rails中实现whenever的代码就算是写完了，真的是简单到不行啊！（实在忍不住感慨一句）\n下面就要执行周期性任务了。\n4.执行周期性任务\n在rails工程文件夹下进行一下操作\n更新schedule.rb中的任务到cronjob中 whenever -i\n可以看到这样的打印结果：\n[write]crontabfileupdated\n执行周期性任务 whenever -w\n可以看到：\n[write]crontabfilewritten\n此时我们的周期性任务便在后台运行了，此时查看我们的任务：\ncrontab -l\n可以看到以下打印：\n# Begin Whenever generated tasks for:\n/home/vito/rails/test_of_rails/test_rails/config/schedule.rb0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58 * * * * /bin/bash -l -c \u0026#39;cd /home/vito/rails/test_of_rails/test_rails \u0026amp;\u0026amp; bundle exec bin/rails runner -e development \u0026#39;\\\u0026#39;\u0026#39;Timetest.mytime\u0026#39;\\\u0026#39;\u0026#39;\u0026#39;# End Whenever generated tasks for: /home/vito/rails/test_of_rails/test_rails/config/schedule.rb 这样，我们的周期性任务就算是在顺利执行了。\n需要注意的一点是运行时crontab的环境（rails和crontab环境不匹配时whenever无法执行），一般调试时多使用的是development环境，而不设置时默认的是production环境，如果你使用crontab -l发现是production环境，可以使用\ncrontab -e\n直接修改为development，或者直接将-e production删掉即可。\n经过上述流程，我们便可以成功地实现周期性任务了。如果此时你发现自己的周期性任务还是没有执行，那你就得好好看看你自己的任务代码了，很可能是执行的任务代码本身有问题，而与whenever的实现没有太大的关系了。\n","date":"2019-10-31T22:04:20Z","permalink":"https://dccmmtop.github.io/posts/whenever%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F/","section":"posts","tags":["ruby"],"title":"whenever使用方式"},{"categories":null,"contents":"以前就遇到过的问题。有如下情景：\n1.假设现在我要将文件a的部分内容复制到文件b中，一般情况，我会用vs或者sp命令打开这两个文件然后用y和p进行复制粘贴。但是如果分别用vim打开这两个文件就不能完成上述动作。\n2.假设我先在要把vim打开的源代码中的部分内容复制到博客中，一般我会用vim编辑好以后，退出用gedit打开，或者cat一下，再复制到系统剪切板，再粘贴。\n今天，对于vim这个没办法跟“外界”交流的特性忍够了，决定解决一下。\n1.首先，查看vim版本是否支持clipboard vim --version | grep \u0026#34;clipboard\u0026#34;1 结果如下：\nclipboard 前面有一个小小的减号，说明不支持。\n2.如果不支持的话，需要安装图形化界面的vim，或者重新编译vim sudo apt-get install vim-gnome1 安装完成后再次执行：\nvim --version | grep \u0026#34;clipboard\u0026#34;1 发现已经支持clipboard\n3.vim的寄存器 打开vim输入:reg查看vim的寄存器，当支持clipboard之后，会多出\u0026quot;+寄存器，表示系统剪切板，在vim中进入visual视图后使用\u0026quot;Ny(N表示特定寄存器编好)，将内容复制到特定的剪切板，那么我们的目的是要复制到系统剪切板则需要选中内容后输入命令：\n\u0026#34;+y1 粘贴到特定的寄存器也是同理。例如\u0026quot;+p将系统剪切板的内容拷贝到vim中（非编辑模式下）。\n4. 映射\n\u0026#34; 从系统剪切板粘贴 nnoremap P \u0026#34;+p \u0026#34; 复制到系统剪切板 vmap Y \u0026#34;+y ","date":"2019-10-31T22:01:14Z","permalink":"https://dccmmtop.github.io/posts/vim%E4%B8%AD%E4%B8%8E%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%B2%98%E8%B4%B4%E5%92%8C%E5%A4%8D%E5%88%B6/","section":"posts","tags":["vim"],"title":"vim中与系统的粘贴和复制"},{"categories":null,"contents":"自定义脚本 编写自己的需要开启自启的脚本，如下：\n#!/usr/bin/env ruby `cd /home/mc/code/rails_app/master_crawler_trm \u0026amp;\u0026amp; nohup rake real_time_extract_word \u0026gt; /dev/null 2\u0026amp;\u0026gt;1` 启动脚本 #!/usr/bin/env ruby start = \u0026#34; sudo -u mc /home/mc/bin/real_time_extract_words \u0026#34; stop = \u0026#34;ps -ef | grep real_time_extract_word | grep -v grep | cut -c 9-15 | xargs kill -9\u0026#34; action = ARGV[0] if action == \u0026#39;start\u0026#39; system(start) puts \u0026#34;启动成功\u0026#34; elsif action == \u0026#39;stop\u0026#39; system(stop) puts \u0026#34;已停止\u0026#34; elsif action == \u0026#39;restart\u0026#39; system(stop) system(start) puts \u0026#34;已重启\u0026#34; end 这个文件可以被 ubuntu 下的服务读取并执行，保存该文件为 custom_sh 然后 sudo chmod +x custom_sh赋予可执行权限\n要注意自己的脚本需要在哪个用户下执行\n操作该服务 sudo systemctl daemon-reload # 添加新的 或者改动 服务之后要刷新 sudo service custom_sh start # 启动 sudo service custom_sh stop # 停止 sudo service custom_sh status # 查看状态 开启自启 sudo update-rc.d custom_sh defaults ","date":"2019-04-28T17:59:37Z","permalink":"https://dccmmtop.github.io/posts/ubuntu%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%9C%8D%E5%8A%A1%E4%BB%A5%E5%8F%8A%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/","section":"posts","tags":["linux"],"title":"ubuntu中添加服务以及开机自启"},{"categories":null,"contents":"假如已经连接的的数据库名是 word_development, 现在需要添加一个名为 trademark_development 和 trademark_test 的本地数据库\n步骤如下:\n添加配置信息 在 config/database.yml 添加如下信息\ntrademark_default: \u0026amp;trademark_default adapter: postgresql encoding: unicode pool: \u0026lt;%= ENV.fetch(\u0026#34;RAILS_MAX_THREADS\u0026#34;) { 5 } %\u0026gt; trademark_development: \u0026lt;\u0026lt;: *trademark_default database: trademark_development # 如果是远程的数据库添加如下信息 #username: username #password: 123456 #host: xxxx #port: xxx trademark_test: \u0026lt;\u0026lt;: *trademark_default database: trademark_test 关联 model 模仿 ApplicationRecord 这个抽象类，新建一个 TrademarkBase 抽象类，然后指明这个类连接的数据库的配置\nclass TrademarkBase \u0026lt; ActiveRecord::Base self.abstract_class = true establish_connection \u0026#34;trademark_#{Rails.env}\u0026#34;.to_sym end 继承抽象类 新建的 model 继承该类，然后指定表名\nclass TrademarkUser \u0026lt; TrademarkBase self.table_name = :users end 测试 进入控制台中\npry(main)\u0026gt; TrademarkUser =\u0026gt; TrademarkUser (call \u0026#39;TrademarkUser.connection\u0026#39; to establish a connection) 可以看到成功关联另一个数据库中的表对象\n","date":"2019-04-03T11:48:31Z","permalink":"https://dccmmtop.github.io/posts/rails%E8%BF%9E%E6%8E%A5%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BA%93/","section":"posts","tags":["rails"],"title":"Rails连接多个数据库"},{"categories":null,"contents":" 摘抄自 《posrgreSQL 修炼之道 从小工到专家》\n触发器（trigger）是一种由事件自动触发执行的特殊的存储过程，这些事件可以是对一个表进行 INSERT,UPDATE,DELETE 等操作\n触发器经常用于加强数据库的完整性约束和业务规则上的约束等\n创建触发器 创建触发器的语法如下\nCREATE [ CONSTRAINT ] TRIGGER name { BEFORE | AFTER | INSTEAD OF } { event [ OR ... ] } ON table_name [ FROM referenced_table_name ] [ NOT DEFERRABLE | [ DEFERRABLE ] [ INITIALLY IMMEDIATE | INITIALLY DEFERRED ] ] [ REFERENCING { { OLD | NEW } TABLE [ AS ] transition_relation_name } [ ... ] ] [ FOR [ EACH ] { ROW | STATEMENT } ] [ WHEN ( condition ) ] EXECUTE PROCEDURE function_name ( arguments ) 创建触发器的步骤 创建执行函数 先为触发器创建一个执行函数，此函数的返回类型为触发器类型，然后即可创建相应的触发器\n下面使用一个例子来讲解触发器的使用，假设有一张学生表（student），和一张考试成绩表（score）\nCREATE TABLE student( student_no int primary key, student_name varchar(40), age int ) CREATE TABLE score( student_no int, chinese_no int, math_score int, test_date date ) 如果想删除学生表的一条记录时，把这个学生在成绩表中的成绩也删除掉，这时就可以使用触发器。先建触发器的执行函数：\nCREATE OR REPLACE FUNCTION student_delete_trigger() RETURNS TRIGGER AS $$ BEGIN DELETE FROM score WHERE student_no = OLD.student_no; RETURN OLD; END; $$ LANGUAGE plpgsql; 创建触发器 CREATE TRIGGER delete_student_trigger AFTER DELETE ON student FOR EACH ROW EXECUTE PROCEDURE student_delete_trigger(); 测试 按照上面的语句创建好触发器后还需要相应的测试，先插入一些测试数据：\nINSERT INTO student VALUES(1, \u0026#39;张三\u0026#39;, 14); INSERT INTO student VALUES(2, \u0026#39;李四\u0026#39;, 13); INSERT INTO student VALUES(3, \u0026#39;王二\u0026#39;, 15); INSERT INTO score VALUES(1, 85, 75, date \u0026#39;2013-05-23\u0026#39;); INSERT INTO score VALUES(1, 89, 73, date \u0026#39;2013-09-18\u0026#39;); INSERT INTO score VALUES(2, 68, 83, date \u0026#39;2013-05-23\u0026#39;); INSERT INTO score VALUES(2, 73, 85, date \u0026#39;2013-09-18\u0026#39;); INSERT INTO score VALUES(3, 72, 79, date \u0026#39;2013-05-23\u0026#39;); INSERT INTO score VALUES(3, 78, 82, date \u0026#39;2013-05-23\u0026#39;); 现在把学好为 3 的学生 “王二” 从表 \u0026ldquo;student\u0026rdquo; 删掉：\nDELETE FROM stduent WHERE student_no = 3; 这时可以查询成绩表 \u0026lsquo;score\u0026rsquo; 可以发现学号（student_no ） 为 3 的学生成绩记录也被删除掉了\n语句级触发器与行级触发器 语句级触发器 语句级触发器是指执行每个 SQL 时，只执行一次，行级触发器是指每行就会执行一次。一个修改 0 行的操作任然会导致合适的语句级触发器被执行。下面来看看相应的示例。\n假设对 student 的更新情况记录 log。可以为 student 建一张 log 表，如下：\nCREATE TABLE log_student( update_time timetamp, --操作的时间 db_user varchar(40), --操作的数据库用户名 opr_type varchar(6), --操作类型：insert delete udate ); 创建记录 log 的触发器函数：\nCREATE FUNCTION log_student_trigger() RETURNS trigger AS $$ BEGIN INSERT INTO log_student values(now(), user, TG_OP); RETURN NULL; END; $$ LANGUAGE \u0026#34;plpgsql\u0026#34;; 上面函数中的 \u0026ldquo;TG_OP\u0026rdquo; 是触发器中的特殊变量，代表 DML 操作类型。\n然后在 student 表上创建一个语句级触发器：\nCREATE TRIGGER log_student_trigger AFTER INSERT OR UPDATE OR DELETE ON student FOR STATEMENT EXECUTE PROCEDURE log_student_trigger(); 删除触发器 drop trigger log_student_log on student; 语句级触发器即使在没有更新到数据时，也会被触发\n行级触发器 CREATE TRIGGER log_student_trigger_2 AFTER INSERT OR UPDATE OR DELETE ON student FOR ROW EXECUTE PROCEDURE log_student_trigger(); 行级触发器即使在没有更新到数据时，不会被触发\nBEFORE 触发器和 AFTER 触发器 通常，语句级别的 \u0026ldquo;before\u0026rdquo; 触发器是在语句开始做任何事之前被触发，而语句级别的\u0026quot;after\u0026quot; 触发器是在语句结束时才触发的。行级别的\u0026quot;before\u0026quot; 触发器是在对特定行进行操作之前触发的，而行级别的 \u0026ldquo;after\u0026rdquo; 触发器是在语句结束时才触发的，但是它会在任何语句级别的 \u0026ldquo;after\u0026rdquo; 触发器被触发之前触发\nBEFORE 触发器可以直接修改 \u0026ldquo;NEW\u0026rdquo; 值以改变实际更新的值，具体例子如下：\n先建一个触发器函数：\nCREATE FUNCTION student_new_name_trigger() RETURNS trigger AS \u0026#39; BEGIN NEW.student_name = NEW.student_name || NEW.student_no; RETURN NEW; END; \u0026#39; LANGUAGE \u0026#34;plpgsql\u0026#34;; 这个函数的作用是，插入或者更新时，在 \u0026ldquo;student_name\u0026rdquo; 后面加上 \u0026ldquo;student_no\u0026rdquo; 学号。也就是直接修改 \u0026ldquo;NEW.student_name\u0026rdquo; ,语句如下：\nNEW.student_name = NEW.student_name||NEW.student_no 在这中情况下只能使用 BEFORE 触发器，因为 BEFORE 触发器是在更新数据之前触发的，所以这时修改了\u0026quot;NEW.student_name\u0026quot;, 后面实际更新到数据库中的值就变成了 \u0026ldquo;student_name||student_no\u0026rdquo;\n如果使用了 AFTER ，则修改 \u0026ldquo;NEW\u0026rdquo; 是没用的\n删除触发器 删除触发器的语法如下：\nDROP TRIGGER [ IF EXISTS ] name ON table [CASCADE | RESTRICT ];\n其中的语法说明如下。\nIF EXISTS: 如果指定的触发器不存在，那么发出一个 notice 而不是跑出一个错误 CASCADE: 级联删除依赖此触发器的对象 RESTRICT: 这是默认值，如果有任何依赖对象存在，那么拒绝删除 **在 PostgresSQL 中要在删除触发器的语法中指定 \u0026ldquo;ON table\u0026rdquo;，而在其他一些数据库的语发可能直接是 \u0026ldquo;DROP TRIGGER name\u0026rdquo; **\n删除触发器时，触发器的函数不会被删除。不过，当表删除时，表上的触发器也会被删除\n触发器的行为 触发器函数与返回值。语句级触发器总是返回 NULL。 即必须显式的在触发器函数中写上 \u0026ldquo;RETURN NULL\u0026rdquo;, 如果没有写，将导致出错。\n对于 \u0026ldquo;BEFORE\u0026rdquo; 和 \u0026ldquo;INSTEAD OF\u0026rdquo; 这类行级触发器函数来说，如果返回的是 NULL， 则表示忽略对当前行的操作，如果返回的是非 NULL 行，对与 INSERT 和 UPDATE 来说，返回的行将成为被插入的行或将要是更新的行。\n对于　AFTER 这类行级触发器来说，其返回值将会被忽略。\n如果同一时间上有多个触发器，则将按触发器名字的顺序来触发。　如果是　\u0026ldquo;BEFORE\u0026rdquo; 和　\u0026ldquo;INTEAD OF\u0026rdquo; 行级触发器，每个触发器所返回的行（可能已经被修改）将成为下一个触发器的输入，如果\u0026quot;BEFORE\u0026quot; 和　\u0026ldquo;INSTEAD OF\u0026rdquo; 行级触发器返回的内容为空，那么该行上的其他行级触发器也不会被触发。\n触发器函数中的特殊变量 当把一个 PL/pgSQL 函数当做触发器函数调用的时候，系统会在顶层生命字段里自动创建几个特殊变量，比如在之前的几个例子当中　\u0026ldquo;NEW\u0026rdquo;, \u0026ldquo;OLD\u0026rdquo;, \u0026ldquo;TG_OP\u0026rdquo;, 变量等。可以使用的变量如下这些：\nNEW: 该变量为 INSERT/UPDATE 操作触发的行级触发器中存储的新数据行，数据类型是　\u0026ldquo;RECORD\u0026rdquo; ,在语句级别的触发器里没有分配次变量，　DELETE 操作触发的行级触发器中也没有分配此变量\nOLD：数据类型是 record。在 update、delete 操作触发时存储旧的数据行。\nTG_NAME：数据类型是 name。触发器名称。\nTG_WHEN：内容为\u0026quot;BEFORE\u0026quot;或“AFTER”，可以用来判断是 BEFORE 触发器还是 AFTER 触发器。\nTG_LEVEL：内容为“ROW”或“STATEMENT”，可以用来判断是语句级触发器还是行级触发器。\nTG_OP：内容为“INSERT”、“UPDATE”、“DELETE”、“TRUNCATE”，用于指定 DML 语句类型。\nTG_RELID：触发器所在表的 oid。\nTG_TABLE_NAME：触发器所在表的表名称。\nTG_SCHEMA_NAME：触发器所在表的模式。\nTG_NARGS：在创建触发器语句中赋予触发器过程的参数个数。\nTG_ARGV[]：text 类型的一个数组。创建触发器语句中指定的参数。\n","date":"2019-03-30T09:15:25Z","permalink":"https://dccmmtop.github.io/posts/%E8%A7%A6%E5%8F%91%E5%99%A8%E5%88%9B%E5%BB%BA%E7%A4%BA%E4%BE%8B/","section":"posts","tags":["SQL","触发器"],"title":"触发器创建示例"},{"categories":null,"contents":" 翻译并整理： https://collectiveidea.com/blog/archives/2016/07/22/solutions-to-potential-upgrade-problems-in-rails-5\nhttps://blog.bigbinary.com/2016/08/29/rails-5-disables-autoloading-after-booting-the-app-in-production.html\nautoload和eager_load autoload: 在常量使用之前不会加载，只有当使用一个当前不存在常量时，会在 autoload_paths 寻找，然后加载它，当在给定的目录中找不到这个常量时，就会触发错误。并且 autoload 是非线程安全的\neager_load 在使用常量之前，加载 eager_load_paths 中的所有常量\nRails5的变化 在 Rails5 中的生产环境下， autoload 默认是被禁用的， 有三种办法解决这个问题\n添加路径到 eager_load_paths 假如自己写的类在lib下\n在 config/application.rb 中\nconfig.eager_load_paths \u0026lt;\u0026lt; Rails.root.join(\u0026#39;lib\u0026#39;) 重新启用 autoload 你可以在任何环境中重新启用 autoload, 但是这种方法在高版本中可能被弃用\nconfig.enable_dependency_loading = true 把代码移动到 app/ 目录下 Rails 在默认的情况下会 autolaod 和 eager_load app/ 目录下的所有内容。这样可以减少你额外的配置。例如把 lib 目录 移动到 app/lib/\n译注： 这种方式使 rails 的目录变得混乱， 不建议\n","date":"2019-01-19T10:21:48Z","permalink":"https://dccmmtop.github.io/posts/rails5%E4%B8%AD%E7%9A%84autoload%E5%92%8Ceager_load/","section":"posts","tags":["rails"],"title":"Rails5中的autoload和eager_load"},{"categories":null,"contents":" 翻译自 https://medium.com/@florenceliang/some-notes-about-using-hash-sort-by-in-ruby-f4b3a700fc33\nruby中的sort和sort_by 是两个非常有用的工具，我们可以按照自己的想法排序。比如我们可以按照名字字母表或者名字的长短进行排序。但是ruby文档中却没有过多的说明，因此本文主要说明一下关于Hash的sort和sort_by的用法\n返回数组 首先要知道的是 Hash 对象调用 sort 或者 sort_by 后，会返回一个数组，而不是Hash。如果想得到 Hash，需要使用 to_h 方法\n假如有下面的 Hash\nhash = {a:1, b:2, c:4, d:3, e:2} 调用sort方法，将会返回一个嵌套数组\nhash.sort # 没有指定按什么值进行排序，所顺序不会发生变化 =\u0026gt; [[:a, 1], [:b, 2], [:c, 4], [:d, 3], [:e 2]] 按照 hash 的值排序 hash.sort_by {|k, v| v} =\u0026gt; [[:a, 1], [:b, 2], [:e, 2], [:d, 3], [:c, 4]] 按照 hash 的值倒序 hash.sort_by {|k, v| -v} =\u0026gt; [[:c, 4], [:d, 3], [:b, 2], [:e, 2], [:a, 1]] 先按值倒序再按 key 正序 hash = {“w”=\u0026gt;2, “k”=\u0026gt;1, “l”=\u0026gt;2, “v”=\u0026gt;5, “d”=\u0026gt;2, “h”=\u0026gt;4, “f”=\u0026gt;1, “u”=\u0026gt;1, “p”=\u0026gt;1, “j”=\u0026gt;1} hash.sort_by {|k, v| [-v, k]} =\u0026gt; [[“v”, 5], [“h”, 4], [“d”, 2], [“l”, 2], [“w”, 2], [“f”, 1], [“j”, 1], [“k”, 1], [“p”, 1], [“u”, 1]] ","date":"2019-01-15T11:45:27Z","permalink":"https://dccmmtop.github.io/posts/ruby%E4%B8%ADhash%E7%9A%84%E6%8E%92%E5%BA%8F/","section":"posts","tags":["ruby"],"title":"ruby中hash的排序"},{"categories":null,"contents":":g/xxx/d，删除包含xxx的行 :v/xxx/d，删除不含xxx的行 :%s/xxx//gn，统计xxx个数，n表示只报告匹配的个数而不进行实际的替换。 详见「:help :v」或「help :g」\n","date":"2019-01-09T15:25:29Z","permalink":"https://dccmmtop.github.io/posts/vim%E5%88%A0%E9%99%A4%E4%B8%8D%E5%8C%85%E5%90%AB%E7%89%B9%E5%AE%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E8%A1%8C/","section":"posts","tags":["vim"],"title":"vim删除不包含特定字符串的行"},{"categories":null,"contents":"反引号 返回标准输出 output = `ls` puts \u0026#34;output is #{output}\u0026#34; Result of above code is\n$ ruby main.rb output is lab.rb 反引号执行系统命令时，会把异常抛给主线程 反引号会从主进程新开一个进程执行命令，如果子进程发生异常，会传递给主进程，如果主进程没有对异常进行处理，主进程就会终止。\n下面的例子中 执行一个‘xxxxx’非法的命令\noutput = `xxxx` puts \u0026#34;output is #{output}\u0026#34; 执行结果：\n$ ruby main.rb main.rb:1:in ``\u0026#39;: No such file or directory - xxxxxxx (Errno::ENOENT) from main.rb:1:in `\u0026lt;main\u0026gt;\u0026#39; 阻塞进程 主进程会一直等待反引号中的子进程结束\n检查命令的执行状态 使用 $?.success? 来检查命令的执行状态\noutput = `ls` puts \u0026#34;output is #{output}\u0026#34; puts $?.success? 结果:\n$ ruby main.rb output is lab.rb main.rb true 允许使用字符串插值 例子：\ncmd = \u0026#39;ls\u0026#39; `#{cmd}` x% x% 和反引号一样，它可以使用不同的分隔符\noutput = %x[ ls ] output = %x{ ls } system system 也可以执行系统命令，他和反引号有点相像。\n阻塞进程\n隐藏异常\nsystem 不会向主进程传递异常。\noutput = system(\u0026#39;xxxxxxx\u0026#39;) puts \u0026#34;output is #{output}\u0026#34; 结果：\n$ ruby main.rb output is 检查命令的执行状态 如果命令成功执行，则系统返回true（退出状态为零）。对于非零退出状态，它返回false, 如果命令执行失败，返回nil\nsystem(\u0026#34;command that does not exist\u0026#34;) #=\u0026gt; nil system(\u0026#34;ls\u0026#34;) #=\u0026gt; true system(\u0026#34;ls | grep foo\u0026#34;) #=\u0026gt; false exec exec 会替换掉当前进程，请看下面的例子：\n在 irb 中执行exec(\u0026rsquo;ls\u0026rsquo;)：\n$ irb e1.9.3-p194 :001 \u0026gt; exec(\u0026#39;ls\u0026#39;) lab.rb main.rb nsingh ~/dev/lab 1.9.3 $ 可以发现，执行完exec(\u0026ldquo;ls\u0026rdquo;)命令以后，已经退出irb，回到shell。\n由于exec替换了当前进程，因此如果操作成功，则不会返回任何内容。如果操作失败，则引发SystemCallError\nsh sh实际上是在呼叫系统, FileUtils在rake中添加了此方法。它允许以简单的方式检查命令的退出状态。\nrequire \u0026#39;rake\u0026#39; sh %w(xxxxx) do |ok, res| if !ok abort \u0026#39;the operation failed\u0026#39; end end popen3 如果你要捕获stdout和stderr，那么你应该使用popen3，因为这个方法允许你与stdin，stdout和stderr进行交互。\n我想以编程方式执行git push heroku master，我想捕获输出。这是我的代码。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;git push heroku master\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| puts \u0026#34;stdout is:\u0026#34; + stdout.read puts \u0026#34;stderr is:\u0026#34; + stderr.read end 输出：\nstdout is: stderr is: -----\u0026gt; Heroku receiving push -----\u0026gt; Ruby/Rails app detected -----\u0026gt; Installing dependencies using Bundler version 1.2.1 这里需要注意的重要一点是，当我执行程序ruby lab.rb时，我的终端在前10秒内没有看到任何输出。然后我将整个输出视为一个转储。\n另外需要注意的是，heroku正在将所有这些输出写入stderr而不是stdout。\n所以我们应该捕获来自heroku的输出，因为它正在流式传输而不是在处理结束时将整个输出转储为一个单个的字符串块。\n这是修改后的代码。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;git push heroku master\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| while line = stderr.gets puts line end end 现在，当我使用ruby lab.rb执行上述命令时，我会逐步获得终端输出，就好像我输入了git push heroku master一样。 这是捕获流输出的另一个例子。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;ping www.google.com\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| while line = stdout.gets puts line end end 在上面的例子中，您将在终端上获得ping的输出，就像您在终端上键入ping www.google.com一样。\n现在让我们看看如何检查命令是否成功\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;ping www.google.com\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| exit_status = wait_thr.value unless exit_status.success? abort \u0026#34;FAILED !!! #{cmd}\u0026#34; end end popen2e popen2e类似于popen3，但合并了标准输出和标准错误。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;ping www.google.com\u0026#39; Open3.popen2e(cmd) do |stdin, stdout_err, wait_thr| while line = stdout_err.gets puts line end exit_status = wait_thr.value unless exit_status.success? abort \u0026#34;FAILED !!! #{cmd}\u0026#34; end end 在所有其他领域，此方法与popen3类似。\nProcess.spawn Kernel.spawn在子shell中执行给定的命令。它会立即返回进程ID。\nirb(main)\u0026gt; pid = Process.spawn(\u0026#34;ls -al\u0026#34;) =\u0026gt; 81001 ","date":"2019-01-03T09:50:00Z","permalink":"https://dccmmtop.github.io/posts/%E7%94%A8ruby%E6%89%A7%E8%A1%8C%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4/","section":"posts","tags":["ruby"],"title":"用ruby执行系统命令"},{"categories":null,"contents":"整理自https://devhints.io/xpath#prefixes\nDescendant selectors h1 //h1 div p //div//p ul \u0026gt; li //ul/li ul \u0026gt; li \u0026gt; a //ul/li/a div \u0026gt; * //div/* :root / :root \u0026gt; body /body Attribute selectors #id //*[@id=\u0026quot;id\u0026quot;] .class //*[@class=\u0026quot;class\u0026quot;] …kinda input[type=\u0026quot;submit\u0026quot;] //input[@type=\u0026quot;submit\u0026quot;] a#abc[for=\u0026quot;xyz\u0026quot;] //a[@id=\u0026quot;abc\u0026quot;][@for=\u0026quot;xyz\u0026quot;] a[rel] //a[@rel] a[href^='/'] //a[starts-with(@href, '/')] a[href$='pdf'] //a[ends-with(@href, '.pdf')] a[href*='://'] //a[contains(@href, '://')] a[rel~='help'] //a[contains(@rel, 'help')] …kinda Order selectors ul \u0026gt; li:first-child //ul/li[1] ul \u0026gt; li:nth-child(2) //ul/li[2] ul \u0026gt; li:last-child //ul/li[last()] li#id:first-child //li[@id=\u0026quot;id\u0026quot;][1] a:first-child //a[1] a:last-child //a[last()] Siblings h1 ~ ul //h1/following-sibling::ul h1 + ul //h1/following-sibling::ul[1] h1 ~ #id //h1/following-sibling::[@id=\u0026quot;id\u0026quot;] jQuery $('ul \u0026gt; li').parent() //ul/li/.. $('li').closest('section') //li/ancestor-or-self::section $('a').attr('href') //a/@href $('span').text() //span/text() Other things h1:not([id]) //h1[not(@id)] Text match //button[text()=\u0026quot;Submit\u0026quot;] Text match (substring) //button[contains(text(),\u0026quot;Go\u0026quot;)] Arithmetic //product[@price \u0026gt; 2.50] Has children //ul[*] Has children (specific) //ul[li] Or logic //a[@name or @href] Union (joins results) `//a Class check //div[contains(concat(\u0026#39; \u0026#39;,normalize-space(@class),\u0026#39; \u0026#39;),\u0026#39; foobar \u0026#39;)] Xpath doesn’t have the “check if part of space-separated list” operator, so this is the workaround (source).\n#Expressions Steps and axes // ul / a[@id='link'] Axis Step Axis Step Prefixes Prefix Example What // //hr[@class='edge'] Anywhere ./ ./a Relative / /html/body/div Root Begin your expression with any of these.\nAxes Axis Example What / //ul/li/a Child // //[@id=\u0026quot;list\u0026quot;]//a Descendant Separate your steps with /. Use two (//) if you don’t want to select direct children.\nSteps //div //div[@name=\u0026#39;box\u0026#39;] //[@id=\u0026#39;link\u0026#39;] A step may have an element name (div) and predicates ([...]). Both are optional. They can also be these other things:\n//a/text() #=\u0026gt; \u0026#34;Go home\u0026#34; //a/@href #=\u0026gt; \u0026#34;index.html\u0026#34; //a/* #=\u0026gt; All a\u0026#39;s child elements #Predicates Predicates //div[true()] //div[@class=\u0026#34;head\u0026#34;] //div[@class=\u0026#34;head\u0026#34;][@id=\u0026#34;top\u0026#34;] Restricts a nodeset only if some condition is true. They can be chained.\nOperators # Comparison //a[@id = \u0026#34;xyz\u0026#34;] //a[@id != \u0026#34;xyz\u0026#34;] //a[@price \u0026gt; 25] # Logic (and/or) //div[@id=\u0026#34;head\u0026#34; and position()=2] //div[(x and y) or not(z)] Use comparison and logic operators to make conditionals.\nUsing nodes # Use them inside functions //ul[count(li) \u0026gt; 2] //ul[count(li[@class=\u0026#39;hide\u0026#39;]) \u0026gt; 0] # This returns `\u0026lt;ul\u0026gt;` that has a `\u0026lt;li\u0026gt;` child //ul[li] You can use nodes inside predicates.\nIndexing //a[1] # first \u0026lt;a\u0026gt; //a[last()] # last \u0026lt;a\u0026gt; //ol/li[2] # second \u0026lt;li\u0026gt; //ol/li[position()=2] # same as above //ol/li[position()\u0026gt;1] # :not(:first-child) Use [] with a number, or last() or position().\nChaining order a[1][@href=\u0026#39;/\u0026#39;] a[@href=\u0026#39;/\u0026#39;][1] Order is significant, these two are different.\nNesting predicates //section[//h1[@id=\u0026#39;hi\u0026#39;]] This returns \u0026lt;section\u0026gt; if it has an \u0026lt;h1\u0026gt; descendant with id='hi'.\n#Functions Node functions name() # //[starts-with(name(), \u0026#39;h\u0026#39;)] text() # //button[text()=\u0026#34;Submit\u0026#34;] # //button/text() lang(str) namespace-uri() count() # //table[count(tr)=1] position() # //ol/li[position()=2] Boolean functions not(expr) # button[not(starts-with(text(),\u0026#34;Submit\u0026#34;))] String functions contains() # font[contains(@class,\u0026#34;head\u0026#34;)] starts-with() # font[starts-with(@class,\u0026#34;head\u0026#34;)] ends-with() # font[ends-with(@class,\u0026#34;head\u0026#34;)] concat(x,y) substring(str, start, len) substring-before(\u0026#34;01/02\u0026#34;, \u0026#34;/\u0026#34;) #=\u0026gt; 01 substring-after(\u0026#34;01/02\u0026#34;, \u0026#34;/\u0026#34;) #=\u0026gt; 02 translate() normalize-space() string-length() Type conversion string() number() boolean() #Axes Using axes //ul/li # ul \u0026gt; li //ul/child::li # ul \u0026gt; li (same) //ul/following-sibling::li # ul ~ li //ul/descendant-or-self::li # ul li //ul/ancestor-or-self::li # $(\u0026#39;ul\u0026#39;).closest(\u0026#39;li\u0026#39;) Steps of an expression are separated by /, usually used to pick child nodes. That’s not always true: you can specify a different “axis” with ::.\n// ul /child:: li Axis Step Axis Step Child axis # both the same //ul/li/a //child::ul/child::li/child::a child:: is the default axis. This makes //a/b/c work.\n# both the same # this works because `child::li` is truthy, so the predicate succeeds //ul[li] //ul[child::li] # both the same //ul[count(li) \u0026gt; 2] //ul[count(child::li) \u0026gt; 2] Descendant-or-self axis # both the same //div//h4 //div/descendant-or-self::h4 // is short for the descendant-or-self:: axis.\n# both the same //ul//[last()] //ul/descendant-or-self::[last()] Other axes Axis Abbrev Notes ancestor ancestor-or-self attribute @ @href is short for attribute::href child div is short for child::div descendant descendant-or-self // // is short for /descendant-or-self::node()/ namespace self . . is short for self::node() parent .. .. is short for parent::node() following following-sibling preceding preceding-sibling There are other axes you can use.\nUnions //a | //span Use | to join two expressions.\n#More examples Examples //* # all elements count(//*) # count all elements (//h1)[1]/text() # text of the first h1 heading //li[span] # find a \u0026lt;li\u0026gt; with an \u0026lt;span\u0026gt; inside it # ...expands to //li[child::span] //ul/li/.. # use .. to select a parent Find a parent //section[h1[@id=\u0026#39;section-name\u0026#39;]] Finds a \u0026lt;section\u0026gt; that directly contains h1#section-name\n//section[//h1[@id=\u0026#39;section-name\u0026#39;]] Finds a \u0026lt;section\u0026gt; that contains h1#section-name. (Same as above, but uses descendant-or-self instead of child)\nClosest ./ancestor-or-self::[@class=\u0026#34;box\u0026#34;] Works like jQuery’s $().closest('.box').\nAttributes //item[@price \u0026gt; 2*@discount] Finds \u0026lt;item\u0026gt; and check its attributes\n","date":"2019-01-02T17:56:48Z","permalink":"https://dccmmtop.github.io/posts/xpath%E7%94%A8%E6%B3%95/","section":"posts","tags":["xpath"],"title":"xpath用法"},{"categories":null,"contents":" 打开管理配置， \u0026ndash;\u0026gt; appearance \u0026ndash;\u0026gt; new\n给新的 color theme 命名(gruvbox)，然后点击应用，保存。\n打开 .kde/share/apps/konsole/gruvbox.colortheme,清空文件内容\n在 manjaro 中， 该文件位于 ~/.local/share/konsole/ 目录下\n复制 https://github.com/morhetz/gruvbox-contrib/blob/master/konsole/Gruvbox_dark.colorscheme, 到 gruvbox.colorscheme.\nGruvbox_dark.colorscheme\n[Background] Color=40,40,40 [BackgroundIntense] Color=40,40,40 [Color0] Color=40,40,40 [Color0Intense] Color=146,131,116 [Color1] Color=204,36,29 [Color1Intense] Color=251,73,52 [Color2] Color=152,151,26 [Color2Intense] Color=184,187,38 [Color3] Color=215,153,33 [Color3Intense] Color=250,189,47 [Color4] Color=69,133,136 [Color4Intense] Color=131,165,152 [Color5] Color=177,98,134 [Color5Intense] Color=211,134,155 [Color6] Color=104,157,106 [Color6Intense] Color=142,192,124 [Color7] Color=168,153,132 [Color7Intense] Color=235,219,178 [Foreground] Color=235,219,178 [ForegroundIntense] Color=235,219,178 [General] Description=Gruvbox Opacity=1 Wallpaper= ","date":"2018-12-28T11:31:57Z","permalink":"https://dccmmtop.github.io/posts/yakuake%E9%85%8D%E7%BD%AEcolor-theme/","section":"posts","tags":["yakuake","color"],"title":"yakuake配置color-theme"},{"categories":null,"contents":"rails 中使用邮件服务是非常方便的，直接加配置文件就可以，参考指南\nqq 邮箱正确的配置如下\nproduction.rb / development.rb\nActionMailer::Base.delivery_method = :smtp config.action_mailer.perform_deliveries = true config.action_mailer.raise_delivery_errors = true config.action_mailer.default :charset =\u0026gt; \u0026#34;utf-8\u0026#34; ActionMailer::Base.smtp_settings = { :address =\u0026gt; \u0026#39;smtp.qq.com\u0026#39;, :port =\u0026gt; 465, :domain =\u0026gt; \u0026#39;qq.com\u0026#39;, :user_name =\u0026gt; ENV[\u0026#39;qq_mail_address\u0026#39;] # 授权码 :password =\u0026gt; ENV[\u0026#39;qq_mail_address\u0026#39;] :authentication =\u0026gt; \u0026#39;plain\u0026#39;, :ssl =\u0026gt; true, :enable_starttls_auto =\u0026gt; true } 切记，qq 邮箱后台要开启 POP3/SMTP 服务，开启的时候需要通过发送短信息启用，启用的时候会生成一个授权码，配置文件的 password 只需要填写授权码即可。\n","date":"2018-11-28T17:26:31Z","permalink":"https://dccmmtop.github.io/posts/rails%E5%8F%91%E9%80%81qq%E9%82%AE%E4%BB%B6%E7%9A%84%E9%85%8D%E7%BD%AE/","section":"posts","tags":["rails"],"title":"rails发送qq邮件的配置"},{"categories":null,"contents":"在初始化数据库系统时，有一个预定义的超级用户，这用户的名称与初始化该数据库的操作系统用户名相同，默认是 postgres，在这个超级用户连接数据库，然后创建出更多的用户。\n创建用户和角色 创建用户与角色的语法如下：\nCREATE ROLE name [ [ WITH] option [ ... ] ] // 或 CREATE ROLE name [ [ WITH] option [ ... ] ] 在 postgres 中，用户与角色是没有区别的，除了 \u0026ldquo;CREATE USER\u0026rdquo; 默认创建出来的用户具有登录（LOGIN）权限，而 \u0026ldquo;CREATE ROLE\u0026quot;创建出来的用户默认没有登录权限之外，没有任何不同。\n上面的 option 可以是以下内容：\nSUPERUSER | NOSUPERUSER: 表示创建出来的用户是否是超级用户，只能是超级用户才能创建超级用户。 CREATEDB | NOCREATEDB: 指定创建出来的用户是否具有执行 \u0026ldquo;CREATE DATABASE\u0026quot;的权限 CREATEROLE | NOCREATEROLE: 指定创建出来的用户是否具有创建其他角色的权限 CREATEUSER | NOCREATEUSER: 指定创建出来的用户是否具有创建其他用户的权限 INHERIT | NOINHERIT: 如果创建的用户拥有某一个或者某几个角色，这是若是指定 INHERIT，则表示用户自动拥有相应角色的权限，否则这个用户没有该角色的权限。 LOGIN|NOLOGIN：指定创建出来的用户是否有“LOGIN 的权限，可以临时地禁止一个用户的“LOGIN”权限，这时这个用户就不能连接到数据库了。 CONNECTION LIMIT connlimit：指定该用户可以使用的并发连接数量。默认值是-1， 表示没有限制 [ENCRYPTED | UNENCRYPTED] PASSWORD \u0026lsquo;password\u0026rsquo;：用于控制存储在系统表里面的口令是否加密。 VALID UNTIL \u0026rsquo;timestamp\u0026rsquo;：密码失效时间，如果不指定这个子句，那么口令将永远有效。 IN ROLE rolename[，\u0026hellip;]：指定用户成为哪些角色的成员，请注意没有任何选项可以把新角色添加为管理员，必须使用独立的 GRANT 命令来做这件事情。 ROLE rolename[，\u0026hellip;]：rolename 将成为这个新建的角色的成员。 ADMIN rolename[,．．．]：rolename 将有这个新建角色的 WITH ADMIN OPTION 权限。 权限的管理 在数据库中，每个数据库的逻辑结构对象（包括数据库）都有一个所有者， 也就是说任何数据库对象都是属于某个用户的，所有者默认就拥有所有权限。所以不需要把 对象的权限再赋给所有者。这也很好理解，自己创建的数据库对象，自己当然有全部的权限 了。当然，所有者出于安全考虑也可以选择废弃一些自己的权限。在 PostsgreSQL 数据库中， 删除一个对象及任意修改它的权力都不能赋予别人，它是所有者固有的，不能被赋予或撤销。 所有者也隐含地拥有把操作该对象的权限赋给别人的权利。\n一个用户的权限分为两类，一类是在创建用户时就指定的权限，这些权限如下：\n超级用户的权限 创建数据库的权限 是否允许 LOGIN 的权限 这些权限是创建用户时指定的，后面可使用 ALTER ROLE 命令来修改。\n还有一类权限，是由命令 GRANT 和 REVOKE 来管理的，这些权限如下：\n在数据库中创建模式(SCHEMA) 允许在指定的数据库中创建临时表 连接某个数据库 在模式中创建数据库对象，如创建表、视图、函数等 在一些表中做 SELECT、UPDATE、INSERT、DELETE 等操作 在一张表的具体列上进行 SELECT、UPDATE、INSERT 操作 对序列进行查询（执行序列的 currval 函数）、使用（执行序列的 currval 函数和 nextval 函数）、更新等操作 在声明表上创建触发器 可以把表、索引等建到指定的表空间 在使用时，需要分清楚上述两类权限，如果要给用户赋予创建数据库的权限，则需要使用“ALTER ROLE”命令，而要给用户赋予创建模式的权限时，需要使用“GRANT”命令。\n“ALTER ROLE”命令的格式如下：\nALTER ROLE name [ [ WITH ] option] 命令中的\u0026quot;option\u0026rsquo;与“CREATEROLE”中的含义相同，这里就不再重复叙述了。\n总结 PostgreSQL 中的权限是按以下几个层次进行管理的：\n首先管理赋在用户特殊属性上的权限，如超级用户的权限、创建数据库的权限、创建 用户的权限、Login 的权限，等等。 然后是在数据库中创建模式的权限。 接着是在模式中创建数据库对象的权限，如创建表、创建索引，等等。 之后是查询表、往表中插人数据、更新表、删除表中数据的权限。 最后是操作表中某些字段的权限。 ","date":"2018-11-25T22:58:15Z","permalink":"https://dccmmtop.github.io/posts/postgres%E7%B3%BB%E5%88%97%E4%B9%8B%E7%94%A8%E6%88%B7%E5%8F%8A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/","section":"posts","tags":["database"],"title":"postgres系列之用户及权限管理"},{"categories":null,"contents":"整理自ruby-china 基础的用法 def print_heredoc puts \u0026lt;\u0026lt;EOF this is the first line this is the second line EOF end print_heredoc 输出：\nthis is the first line this is the second line 如果你觉得代码太难看（这根本不符合 Ruby 的风格），你可能会这样写：\ndef print_heredoc puts \u0026lt;\u0026lt;EOF this is the first line this is the second line EOF end print_heredoc 你会发现高亮显示已经不对了，它还会报这样的一个错误：\ntest.rb:6: can\u0026#39;t find string \u0026#34;EOF\u0026#34; anywhere before EOF test.rb:2: syntax error, unexpected end-of-input, expecting tSTRING_CONTENT or tSTRING_DBEG or tSTRING_DVAR or tSTRING_END 可以缩进的用法 希望代码写的漂亮一点的话，就得多做点工作，在第一个 EOF 前加上一个减号就 OK 了：\ndef print_heredoc puts \u0026lt;\u0026lt;-EOF this is the first line this is the second line EOF end print_heredoc 输出：\nthis is the first line this is the second line ruby 2.3 引入一个新的语法 def hello puts \u0026lt;\u0026lt;~HEREDOC I know I know You will like it. HEREDOC end hello 完美输出:\nI know I know You will like it. heredoc 的本质 有下面一个方法：\ndef a_method(string, integer) puts \u0026#34;the string is #{string} and the integer is #{integer}\u0026#34; end 一般这么用这个方法：\na_method \u0026#34;the string\u0026#34;, 1 如果想用 heredoc 呢？这里就需要说下那个\u0026lt;\u0026lt;-EOF到底是什么东西？其实\u0026lt;\u0026lt;-EOF只是个占位符，写上它以后，它就代表将要输入的字符串，这个字符串的判断是从\u0026lt;\u0026lt;-EOF的下一行开始计算，一直碰到只有EOF的一行（这一行只有一个EOF），这个字符串就这样计算出来的。如果上面的这个方法要用 heredoc 的话，就可以这样写：\na_method \u0026lt;\u0026lt;-EOF, 1 this is the first line this is the second line EOF 输出：\nthe string is this is the first line this is the second line and the integer is 1 可以看到，这个\u0026lt;\u0026lt;-EOF就好像一个实参一样，我们也可以把他完全当个实参来对待，它是个字符串，那么就可以调用字符串的方法，像这样：\na_method \u0026lt;\u0026lt;-EOF.gsub(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;), 1 this is the first line this is the second line EOF 输出：\nthe string is this is the first linethis is the second line and the integer is 1 我们把它掰直了，哈哈。换个写法更能体现 heredoc 的本质：\na_method(\u0026lt;\u0026lt;-EOF.gsub(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;), 1) this is the first line this is the second line EOF heredoc 的小技巧 那个\u0026lt;\u0026lt;-EOF为什么叫EOF，为什么不叫ABC，你可以试试，叫ABC也可以，但是末尾那个也要写 ABC，要保持配对。在有些编辑器（Atom，RubyMine）中甚至会根据占位符将 heredoc 中的内容高亮显示，比如可以写\u0026lt;\u0026lt;-RUBY, \u0026lt;\u0026lt;-HTML等等。\n假如前面的那个方法要传入两个字符串该怎么写呢？很简单：\na_method \u0026lt;\u0026lt;-STR1, \u0026lt;\u0026lt;-STR2 this is for STR1 STR1 this is for STR2 STR2 输出：\nthe string is this is for STR1 and the integer is this is for STR2 有一点点晕，解释一下。\u0026lt;\u0026lt;-STR1在前面，它就会一直找到只包含STR1的那一行，并把其中的内容替换掉\u0026lt;\u0026lt;-STR1，而\u0026lt;\u0026lt;-STR2在后面，它不会包括\u0026lt;\u0026lt;-STR1和STR1中的部分， 会一直找到只包含STR2的那一行，然后替换\u0026lt;\u0026lt;-STR2。只需记住是只包含占位符的一行，像this is for STR1中虽然包含STR1，但是这一行还有其他字符，heredoc 就不会在这一行结束，而是接着往下找。同时需要知道，虽然是占位符，但是可以调用字符串的方法，在 Rails5 中最近的一个 pull request 中 DHH 就用了很多 heredoc。不过我找了半天没找到，等以后找到再把链接付在这里。\n前面的输出结果中，大家一定发现一个问题，就是字符串的换行和行首的缩进，有时候你想要他们，有时候可不是，我们可以用gsub来替换。在 Rails 中已经有一个好用的方法了：\n2.times do puts \u0026lt;\u0026lt;-STR.strip_heredoc this is the first line this is the second line STR end 输出：\nthis is the first line this is the second line this is the first line this is the second line 注意行首是没有空格的，和输入的格式保持了一致，很方便，实现也是很简单的，参考 github 吧，strip.rb\n那些太奇怪的写法 其实还有很多奇怪的写法，比如下面这个：\nputs \u0026lt;\u0026lt;-\u0026#34;I am the content\u0026#34; line 1 line 2 line 3 I am the content 这是合法语法，但估计不会有人这么写，甚至有的编辑器都不能正常高亮显示它.\n","date":"2018-11-22T09:23:36Z","permalink":"https://dccmmtop.github.io/posts/ruby_heredoc%E7%9A%84%E7%94%A8%E6%B3%95/","section":"posts","tags":["ruby"],"title":"ruby_heredoc的用法"},{"categories":null,"contents":"什么是多态关联 假如有三个模型，分别是 用户， 产品， 图片。图片为用户所有，也为产品所有。我们可以创建两个 picture 的模型，如下\nrails g modle picture_user user_id:integer name:string url:string\nrails g modle picture_product product_id:integer name:string url:string\nclass PictureUser \u0026lt; ApplicationRecord belongs_to :user end class PictureProduct \u0026lt; ApplicationRecord belongs_to :product end 这样我们就可以使用user.prictures 和 product.pictures来分别获得用户下和产品下的图片了。但是我们发现，两个图片模型除了外键不一样，其他字段都是一样的，那么有没有一种办法只创建一个 picture 模型，同时属于 user 和 product 呢，这种既属于一个模型又属于另外一个模型（可以是很多个）的关联就是多态关联。\n多态关联的实现 为了能同时使用 user.pictures 和 product.pictures 来获得各自的图片，我们就需要对 picture 模型做一些修改，使其能够标识一张图片是属于 user 还是属于 product ，当然外键是必不可少的。我们还需要一个外键对应类的名称，如下：\nrails g modle picture pictureable_id:integer pictureable_type:string name:string url:string\nclass Picture \u0026lt; ApplicationRecord belongs_to :pictureable, polymorphic: true end class User \u0026lt; ApplicationRecord has_many :pictures, as: :pictureable end class Product \u0026lt; ApplicationRecord has_many :pictures, as: :pictureable end pictureable 相当于一个接口，凡是拥有图片的模型都可以像 User 那样使用关联。\n可以使用user.pictures.create(name: 'user_0', url: 'https://dcc.com')来创建一条关联对象，创建之后我们发现在 picture 表中多了一条记录：\nid: 1, pictureable_type: \u0026#39;User\u0026#39;, pictureable_id: 1, name:\u0026#34;user_0\u0026#34;, url:\u0026#39;https://dcc.com\u0026#39; pictureable_type: \u0026lsquo;User\u0026rsquo; 就是所属对象的标识，这样才可以使用 user.pictures 进行查询。由此我们知道，多态关联中，xxxable_type, xxxable_id字段是必不可少的。\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\n下面是关于多态 view 页面使用的讲解原文\n什么是多态 Rails 模型中的关系有一对一，一对多还有多对多，这些关联关系都比较直观，除此之外 Rails 还支持多态关联，所谓的多态关联其实可以概括为一个模型同时与多个其它模型之间发生一对多的关联。并且在实际的应用中这种关系也十分普遍，比如可以应用到站内消息模块，评论模块，标签模块等地方，下图就是多态关系下的评论模块的 E-R 图。\n通过 E-R 图，我们能直观的看到系统中的事件，文章以及照片都可以被用户评论，并且这些评论都被存储在一张叫 comments 表中。Ok，现在我们已经搞清楚了多态的含义，下面继续看下 Rails 中是如何实现多态关联的。\nRails 中实现多态的步骤 这里我们通过将 Rails Guides 中给出的例子线性化(转化为详细步骤)来说明这个问题。\nStep 1： 通过 Migration 创建表 执行下面命令来生成 Migration 文件\nrails g model picture name:string imageable_id:integer imageable_type:string 生成的 Migration 文件如下:\nclass CreatePictures \u0026lt; ActiveRecord::Migration def change create_table :pictures do |t| t.string :name t.integer :imageable_id t.string :imageable_type t.timestamps null: false end add_index :pictures, :imageable_id end end 其中需要特别关注， imageable_id 与 imageable_type 两个字段，前者用来存储相关联内容的外键键值，后者则用来存储相关联内容的类型名。后面在通过模型查找关联内容的时候，可以通过这两个值来定位到要找到的内容。特别是后者 imageable_type 的存在是多态实现的关键。\nStep 2: 修改各 Model 得关联关系 按照上面 E-R 图和代码修改模型结构，因为 Employee，Product 分别与 Picture 是一对多的关系，所以用到了 has_many 与 belongs_to 方法，再使用 polymorphic 与 as 来指明是多态关联。\nStep 3: Controller 中应用 上面的两步完成后就能在 Controller 中通过多态关联关系进行相互访问了，并且通过关联关系创建的新评论 Rails 也会自动帮你设置 commentable_id 与 commentable_type 两个字段的值。\nevent = Event.create name: \u0026#34;event1\u0026#34;\revent1= event.comments.create content: “comment1”\revent1.commentable_type #=\u0026gt; “Event” Done！到此就算完整应用到了多态关联关系，后续需要处理的就是如何来组织代码让多态关系更加灵活便捷的被你操作，不过这个就应该是另一篇文章的内容了。:)\n刚开始看 Rails Guide 的时候对多态的表关联真的是一头雾水。后来自己写了一个博客应用的时候用到了 acts_as_commentable 这个 gem，它就是用到了多态表的关联，然后我又看了 Terry 在 railscasts china 上的 视频 ，对多态的理解就深了很多。\n理解什么是多态 一般表的关联有一对一，一对多，多对多，这些都是非常好理解的，然后对于多态的表关联可能稍微有点不好理解。其实多态关键就是一个表关联到多个表上。就如 Comment（评论）表吧，一个 Topic 应该有 Comment（一个帖子应该有许多的评论），除此之外 Micropost（微博）也可能有很多的 Comment。然后一个网站中既有 Topic 的论坛功能，又有 Micropost 的功能，我们怎么处理 Comment 表呢？当然我们可以建两个独立的表比如 TopicComment 和 MicropostComment，再分别关联到 Topic 和 Micropost 上，但这不是一种好的选择，我们可以只建一个表，然后去关联这两个表，甚至多个表。这也就实现了多态的能力。\n一个例子 1.首先我们先生成一个 Comment 的 model，假设已经有 Topic 和 Micropost 这两个 model 了\nrails g model comment content:text commentable_id:integer comment_type:string 2.然后我们 会得到一个 migration\nclass CreateComments \u0026lt; ActiveRecord::Migration def change create_table :comments do |t| t.text :content t.integer :commentable_id t.string :commentable_type t.timestamps end end end 也可以通过 t.references 来简化上面的\nclass CreateComments \u0026lt; ActiveRecord::Migration def change create_table :comments do |t| t.text :content t.references :commentable, :polymorphic =\u0026gt; true #这里指明了多态，这样会生成comment_id和comment_type这两个字段的，如上 t.timestamps end end end 多态魔法就在这里，commentable_typle 字段用于指明 comment 所关联的表的类型，如 topic 或 micropost 等，而 comment_id 用于指定那个关联表的类型对象的 id。如：可以把一个 comment 关联到第一篇 topic 上，那么 comment_type 字段为 topic，而 comment_id 为对应 topic 对象的 id 1,同理这样就可以关联到不同表了，从而实现多态的关联。\n3,数据迁移 rake db:migrate 就能生成我们要的表了\n4,对 model 进行操作从而现实表的关联\n####comment model class Comment \u0026lt; ActiveRecord::Base belongs_to :commentable, :polymorphic =\u0026gt; true end 看到没有，这里的 comment belongs_to 没有写 topic，micropost 等，而写了 commentable,因为 commentable 中有 type 和 id 两个字段，可以指定任何其他 model 对象的，从而才能实现多态，如果这里写 belongs_to topic 的话就没办法实现多态了。然后我们看看 topic 和 mocropost 的 model 该如何写。\nclass Topic \u0026lt; ActiveRecord::Base has_many :comments, :as =\u0026gt; :commentable end class Micropost \u0026lt; ActiveRecord::Base has_many :comments, :as =\u0026gt; :commentable end 看到这里的 as 了吗？as 在这我们可以解释为：作为（我的理解，可能这种理解补科学，哈哈），也就是说 Topic 有许多的 comments，但是它是通过将自己作为 commentable，实现的。Micropost 同理。\n然后就是 controller 和 views 中（如 form 表单）的设计了，这也是我刚学的时候，最头疼这个了，因为对 params 参数通过表单到 controller 的传递没掌握好。\n在写这些之前，我们先看看如何写路由吧，因为一个 topic 有多个 comments，Micropost 同理。所以我们可以这样写\nresources :topics do resources :comments end resources :microposts do resources :comments end 然后我们通过命令 rake routes 就可以得到相应的路由了如：\ntopic_comments GET /topics/:topic_id/comments(.:format) comments#index POST /topics/:topic_id/comments(.:format) comments#create new_topic_comment GET /topics/:topic_id/comments/new(.:format) comments#new edit_topic_comment GET /topics/:topic_id/comments/:id/edit(.:format) comments#edit topic_comment GET /topics/:topic_id/comments/:id(.:format) comments#show PUT /topics/:topic_id/comments/:id(.:format) comments#update DELETE /topics/:topic_id/comments/:id(.:format) comments#destroy 这些待会我们会用到。\n然后我们再来分析 controller 和 views 之间的参数传递。我们通过完整的创建 comment 的过程进行说明\n(1)首先页面上肯定有一个创建 comment 的连接或按钮（假设创建 comment 的表单和 topic show 页面不在统一页面上），代码应该是这样的：\n\u0026lt;%= link_to \u0026#34;发表评论\u0026#34;, new_topic_comment_path%\u0026gt; (2)点击这个链接后，通过路由来到 controller 中的 new 方法(同时会将对应的 topic 相关的参数传给 controller)\ndef new @topic = Topic.find(parmas[:id]) #找到comment属于的topic @comment = @topic.comments.build #建立这个关系 end (3)经过这个方法（action）后，页面来到了 comments/new.html.erb,在这个页面中有一个评论的表单，大概是这样的\n\u0026lt;%= form_for([@comment.commentable, @comment]) do |f| %\u0026gt;\r......\r\u0026lt;%end%\u0026gt; 这个表的参数是一个数组，[email protected]\n@comment，如果没有关联的化，[email protected]，\n[email protected]\n��，还有一个就是 commentable，这里也就是 topic。\n还记得 new 中的 @comment = @topic.comments.build 的吗，这里就暂时将对应的 topic 对象写入 commentable（注意：只是暂时建立关系，还没有写入数据库），[email protected]\n@topic。\n(4)然后你填完表单后，按提交按钮后，表单中的参数（包括 commentable，@post 的 id 等信息），一起来到 controller 的 create 方法中\ndef create Topic.find(parmas[:topic_id]).comments.create(parmas[:comment]) ...... end 这样就真正创建了一个新的 comment。micropost 同理。\n其实多态讲的也差不多了，但在提一个地方\n**重要知识点:**假设一个 comment 已经建立了，它的 commentable_type 是:topic.comment_id 是 1。如果我们得到了这个 id 为 1 的 topic，@topic，那么我们怎么得到它的 comments 呢？是的很简单，直接 @topic.comments 就 ok 了。但是反过来呢，我们得到了这个 comment，@comment，我们如何得到对应的 topic 的信息呢？我以前刚学的时候，就用了@comment.topic ，呵呵，没错，得到的是一串错误，正确一概是 @comment.commentable\n关于多态我们已经讲的差不多了。\n补充：上面的例子 comment 的表单是独立在 comments/new.html.erb 中的，但是一般的应用 comment 的表单是在 topics/show.html.erb 中，也就是上面一个 topic，topic 下有一个 comment 表单。这样的话在 controller 中我们就不需要 new 这个方法了，那么我们在哪建立关系呢？\n@comment = @topic.comments.build #建立这个关系 我们就在表单的 \u0026lt;%= form_for ...%\u0026gt; 前面写 \u0026lt;@comment = @topic.comments.build\u0026gt;\n","date":"2018-11-21T15:35:41Z","permalink":"https://dccmmtop.github.io/posts/rails_%E5%A4%9A%E6%80%81%E5%85%B3%E8%81%94/","section":"posts","tags":["rails"],"title":"rails_多态关联"},{"categories":null,"contents":"安装 redis gem redis bundle install 修改 cable.yml development: adapter: redis 生成订阅 rails g channel block speak\n连接设置 连接是客户端-服务器通信的基础。每当服务器接受一个 WebSocket，就会实例化一个连接对象。所有频道订阅（channel subscription）都是在继承连接对象的基础上创建的。连接本身并不处理身份验证和授权之外的任何应用逻辑。WebSocket 连接的客户端被称为连接用户（connection consumer）。每当用户新打开一个浏览器标签、窗口或设备，对应地都会新建一个用户-连接对（consumer-connection pair）。 连接是 ApplicationCable::Connection 类的实例。对连接的授权就是在这个类中完成的，对于能够识别的用户，才会继续建立连接。\n例子：\nmodule ApplicationCable class Connection \u0026lt; ActionCable::Connection::Base identified_by :current_user def connect self.current_user = find_verified_user end private def find_verified_user if cu_user = User.find_by(id: cookies.signed[:user_id]) cu_user else reject_unauthorized_connection end end end end action cable 中不能使用 session，我们可以用 cookie 来验证用户。下面在 SessionHelper 中加入 cookie，方便 action cable 使用\nmodule SessionsHelper def log_in(user) session[:user_address] = user.address user = User.find_by(address: session[:user_address]) if user cookies.permanent.signed[:user_id] = user.id end end def current_user @current_user ||= User.find_by(address: session[:user_address]) end def logged_in? !current_user.nil? end def log_out session.delete(:user_address) @current_user = nil cookies.delete(:user_id) end end 订阅 App.block = App.cable.subscriptions.create channel: \u0026#34;BlockChannel\u0026#34;, connected: -\u0026gt; # Called when the subscription is ready for use on the server disconnected: -\u0026gt; # Called when the subscription has been terminated by the server received: (data) -\u0026gt; alert(data) 上述channel: 'BlockChannel'是必须的，声明像哪个频道订阅\n处理订阅 class BlockChannel \u0026lt; ApplicationCable::Channel def subscribed # 设置可以向哪些订阅者发布信息 stream_from current_user.address end def unsubscribed # Any cleanup needed when channel is unsubscribed end 测试 在 rails console 中\n# 向用户22发送一条通知 ActionCable.server.broadcast User.find(22).address, data: 22 关于 action cable 的部署 cable.yml action cable 默认为“异步”适配器，当涉及多个进程时，它不起作用。因此，您需要配置 Action Cable 以使用其他适配器，例如 Redis 或 PostgreSQL。\nproduction: adapter: redis url: redis://localhost:6379 staging: adapter: redis url: redis://localhost:6379 local: \u0026amp;local adapter: redis url: redis://localhost:6379 development: *local test: *local 不要忘记启动 redis\n在和 rails 相同的主机和端口使用 action cable 下面是 Rails 推荐的默认设置，也是最简单的设置.它的工作原理是将 ActionCable.server 挂载到 config / routes.rb 中的某个路径。这样，您的 Action Cable 服务器将在与您的应用程序相同的主机和端口上运行，但在子 URI 下运行。\n在 router.rb 文件中\nmount ActionCable.server =\u0026gt; \u0026#39;/cable\u0026#39; 你需要配置一个 location块 配置 cable 的请求,像下面这样：\nserver { listen 80; server_name www.foo.com; root /path-to-your-app/public; passenger_enabled on; ### INSERT THIS!!! ### location /cable { passenger_force_max_concurrent_requests_per_process 0; } } 为了应用的性能，必须添加passenger_force_max_concurrent_requests_per_process 0的配置，关于这个配置的详解请看 文档\n本文内容整理自 ruby-china 和 passenger 的配置文档\n","date":"2018-11-18T17:24:00Z","permalink":"https://dccmmtop.github.io/posts/rails_action_cable%E4%BD%BF%E7%94%A8/","section":"posts","tags":["rails"],"title":"rails_action_cable使用"},{"categories":null,"contents":"日积月累，自己写的 vim 脚本越来越多，大大的方便了日常编写任务，但是这些脚本没有做成插件的形式，导致换一台新机器时，不方便下载使用，下面就介绍一下如何把\n自己写的脚本做成一个插件，可以在vimrc中使用Plug xxx安装。\nbegin 新建文件夹，命名为vim_script 进入文件价，执行 git init初始化一个仓库 去 github 新建一个仓库，vim_scipt 设置本地仓库的 remote 信息 在 vim_script 下新建 autoload 文件夹，把自己写的 vim 脚本放到 autoload 下 在 vim_script 下新建 plugin 文件夹，新建script.vim(名字随意)，在该文件内设置脚本执行的命令，或者设置执行脚本的快捷键 如图：\n","date":"2018-10-28T15:48:33Z","permalink":"https://dccmmtop.github.io/posts/vim%E8%84%9A%E6%9C%AC%E6%8F%92%E4%BB%B6%E5%8C%96/","section":"posts","tags":["vim"],"title":"vim脚本插件化"},{"categories":null,"contents":"本文章为转载内容，点击查看原文章https://zhuanlan.zhihu.com/p/27389503\n使用脚本语言，可以更灵活地定制编辑器以完成复杂的任务。\n自定义命令 Vim 编辑器允许定义自己的命令，我们可以像执行内置命令一样来执行我们自己定义的命令。\n使用以下:command 命令自定义命令：\n:command Delete_first :1delete 注意自定义命令的名称，必须以大写字母开头，而且不能包含下划线；如果我们执行:Delete_first 自定义命令，那么 Vim 就会执行:1delete 命令，从而删除第一行。\n可以使用!来强制重新定义同名的自定义命令：\n:command! -nargs=+ Say :echo \u0026lt;args\u0026gt; 用户定义的命令可以指定一系列的参数，参数的个数由-nargs 选项在命令行中指定。例如定义 Delete_one 命令没有参数：\n:command Delete_one -nargs=0 1delete 默认情况下-nargs=0，所以可以省略。其他-nargs 选项值如下：\n-nargs=0 没有参数\n-nargs=1 1 个参数\n-nargs=* 任何个数的参数\n-nargs=? 零个或是一个参数\n-nargs=+ 一个或是更多个参数\n在命令定义中，参数是由关键字指定的：\n:command -nargs=+ Say :echo \u0026#34;\u0026lt;args\u0026gt;\u0026#34; 输入以下自定义命令：\n:Say Hello World 命令的执行结果显示：\nHello World 使用-range 选项，可以指定一个范围作为自定义命令的参数。-range 选项值如下：\n-range 允许范围，默认为当前行\n-range=%允许范围，默认为当前文件(while file)\n-range=count 允许范围，单一的数字\n当指定范围之后，就可以用关键字和得到这个范围的第一行和最后一行。\n例如以下定义了 SaveIt 命令，用于将指定范围的文件写入文件 save_file：\n:command -range=% SaveIt :\u0026lt;line1\u0026gt;,\u0026lt;line2\u0026gt;write! save_file 关键字含有与关键字相同的信息，所不同的是它用于调用函数。例如以下自定义命令：\n:command -nargs=* DoIt :call AFunction(\u0026lt;f-args\u0026gt;) 执行自定义命令：\n:DoIt a b c 将会传递参数给调用的函数：\n:call AFunction(\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;) 其他选项和关键字包括：\n-count=number 指定数量保存在关键字中\n-bang 指定!修饰符存放在关键字中\n-register 指定寄存器，默认为未命名寄存器，寄存器的定义保存在关键字中\n-bar 其他命令可以用|跟随在此命令之后\n-buffer 命令仅对当前缓冲区有效\n使用以下命令，首先分别创建一个用户自定义命令，然后再将两个命令组合起来。\ncommand! -bar DelTab %s/\t// command! DelLF %s/\\n// command! FmtCode DelTab|DelLF view raw\nScriptCommandMulti.vim\nhosted with ❤ by\nGitHub\n列示自定义命令 使用以下命令，可以列出用户定义的命令：\n:command 删除自定义命令 使用以下:delcommand 命令，可以删除用户定义的命令：\n:delcommand Delete_one 使用以下命令，清除所有的用户定义的命令：\n:comclear ","date":"2018-10-23T14:29:42Z","permalink":"https://dccmmtop.github.io/posts/vim%E8%87%AA%E5%AE%9A%E4%B9%89%E5%91%BD%E4%BB%A4/","section":"posts","tags":["vim"],"title":"vim自定义命令"},{"categories":null,"contents":"需求 给一段文字自动添加序号，要求本行的序号可以根据上一行的序号自动增一，若上一行没有序号，则从 1 开始\n实现 用 ruby 编写 vim 脚本非常容易实现\n\u0026#34; 每行的前面添加序号，根据上一行序号自动递增，若上一行没有序号，则从1开始 function! num#add_num() ruby \u0026lt;\u0026lt; EOF def get_current_line() count = 0 \u0026#34; 得到当前缓冲区 cb = Vim::Buffer.current \u0026#34; 得到上一行的行号 previousLine = cb.line_number - 1 \u0026#34; 如果行号存在，并且以数字开头 if previousLine \u0026gt;= 1 \u0026amp;\u0026amp; cb[previousLine] =~ /^\\d+/ \u0026#34; 得到上一行的序号 count = $\u0026amp;.to_i end \u0026#34; 修改本行内容 cb.line = \u0026#34;#{count + 1}. #{line}\u0026#34; end get_current_line() EOF endfunction 添加自定义命令 在.vimrc中，添加如下一行\ncommand! -range=% AddNum :\u0026lt;line1\u0026gt;,\u0026lt;line2\u0026gt; cal num#add_num()\n关于自定义命令请查看这篇文章：vim 添加自定义命令\n演示 ","date":"2018-10-23T12:58:16Z","permalink":"https://dccmmtop.github.io/posts/%E8%87%AA%E5%8A%A8%E6%B7%BB%E5%8A%A0%E5%BA%8F%E5%8F%B7/","section":"posts","tags":["vim"],"title":"自动添加序号"},{"categories":null,"contents":"ruby 中有个 range 对象，可以自动推测范围内的数据，比如：\n(1..100).each do |i| puts i end 会输出 1 到 100 内的所有数字\n自定义 如果我们有一个自定义的对象，假如名字为Ym\nclass Ym attr_accessor :year, :month def initialize @year, @month = year, month end end 若是想在Ym上使用((Ym.new(2009,1))..(Ym.new(2010,1))).each {|i| puts i},输出的结果按照正常的年月逻辑来显示，该如何实现呢？\n其实要实现类似(1..100)的方法很容易，只需在该类中include Compareable然后实现 succ和\u0026lt;=\u0026gt;方法就行了。\nclass Ym include Comparable attr_accessor :year, :month def initialize(year, month) @year, @month = year, month end def succ #如果月份满12，则年份增加一，月份再从一开始。 # 可以按需求定制更复杂的推测方法 yyy, mmm = @month == 12 ? [@year + 1, 1] : [@year, @month + 1] Ym.new(yyy, mmm) end def \u0026lt;=\u0026gt;(other) # 定义大小规则 (@year * 12 + @month) \u0026lt;=\u0026gt; (other.year * 12 + other.month) end def to_s sprintf \u0026#34;%4d-%02d\u0026#34;, @year, @month end end (Ym.new(2008,8)..(Ym.new(2019,9))).each do |y| puts y end 结果如图:\n","date":"2018-09-27T09:10:50Z","permalink":"https://dccmmtop.github.io/posts/%E8%87%AA%E5%AE%9A%E4%B9%89range%E5%AF%B9%E8%B1%A1/","section":"posts","tags":["ruby"],"title":"自定义range对象"},{"categories":null,"contents":"首先，在哪些情况下会用到正则表达式？\n使用正则表达式的命令最常见的就是 / 和 ? 命令。其格式如下：\n/正则表达式\r?正则表达式 另一个很有用的命令就是 :s（替换）命令，将第一个//之间的正则表达式替换成第二个//之间的字符串。\n:s/正则表达式/替换字符串/选项 在学习正则表达式时可以利用 / 命令来练习。\n元字符 元字符 说明 . 匹配任意字符 [abc] 匹配方括号中的任意一个字符。可以使用 - 表示字符范围，如[a-z0-9]匹 配小写字母和阿拉伯数字。 \\d 匹配阿拉伯数字，等同于[0-9]。 [^abc] 在方括号内开头使用^符号，表示匹配除方括号中字符之外的任意字符。 \\d 匹配阿拉伯数字，等同于[0-9]。 \\D 匹配阿拉伯数字之外的任意字符，等同于[^0-9]。 \\x 匹配十六进制数字，等同于[0-9A-Fa-f]。 \\X 匹配十六进制数字之外的任意字符，等同于[^0-9a-fa-f]。 \\w 匹配单词字母，等同于[0-9A-Za-z_]。 \\W 匹配单词字母之外的任意字符，等同于[^0-9a-za-z_]。 \\t 匹配字符。 \\s 匹配空白字符，等同于[ \\t]。 \\S 匹配非空白字符，等同于[^ \\t]。 如果需要查找一些特殊字符，如 *、.、/ 等，可以在这些字符前面添加 \\，表示这些不是元字符，而是普通字符。比如：\\/d 匹配的是 /d这两个字符，而不是匹配任意数字。\n表示数量的元字符\n元字符 说明 * 匹配 0-任意个 \\+ 匹配 1-任意个 \\? 匹配 0-1 个 \\{n,m} 匹配 n-m 个 \\{n} 匹配 n 个 \\{n,} 匹配 n-任意个 \\{,m} 匹配 0-m 个 表示位置的符号\n元字符 说明 $ 匹配行尾 ^ 匹配行首 \\\u0026lt; 匹配单词词首 \\\u0026gt; 匹配单词词尾 使用示例\n命令 描述 /char\\s\\+[A-Za-z_]\\w*; 查找所有以 char 开头，之后是一个以上的空白，最后是一个标识符和分号 /\\d\\d:\\d\\d:\\d\\d 查找如 17:37:01 格式的时间字符 :g/^\\s*$/d 删除只有空白的行 :s/\\\u0026lt;four\\\u0026gt;/4/g 将所有的 four 替换成 4，但是 fourteen 中的 four 不替换 替换变量 在正规表达式中使用 \\( 和 \\) 符号括起正规表达式，即可在后面使用\\1、\\2 等变量来访问 \\(和 \\) 中的内容。\n使用示例\n命令 描述 /\\(a\\+\\)[^a]\\+\\1 查找开头和结尾处 a 的个数相同的字符串，如 aabbbaa，aaacccaaa，但是不匹配 abbbaa :s/\\(http:\\/\\/[-a-z\\._~\\+%\\/]\\+\\)/\u0026lt;a href=\u0026quot;\\1\u0026quot;\u0026gt;\\1\u0026lt;\\/a\u0026gt;/ 将 url 替换为http://url的格式 :s/\\(\\w\\+\\)\\s\\+\\(\\w\\+\\)/\\2\\t\\1 将 data1 data2 修改为 data2 data1 函数式 在替换命令 :s/{pattern}/{string}/[flags] 中可以使用函数表达式来书写替换内容，格式为\n:s/替换字符串/\\=函数式 在函数式中可以使用 submatch(1)、submatch(2) 等来引用 \\1、\\2 等的内容，而submatch(0)可以引用匹配的整个内容。\n使用例\n:%s/\\\u0026lt;id\\\u0026gt;/\\=line(\u0026#34;.\u0026#34;) 将各行的 id 字符串替换为行号\n:%s/^\\\u0026lt;\\w\\+\\\u0026gt;/\\=(line(\u0026#34;.\u0026#34;)-10) .\u0026#34;.\u0026#34;. submatch(1) 将每行开头的单词替换为 (行号-10).单词 的格式，如第 11 行的 word 替换成 1. word\n与 Perl 正则表达式的区别 Vim 语法 Perl 语法 含义 \\+ + 1-任意个 \\? ? 0-1 个 \\{n,m} {n,m} n-m 个 \\( 和 \\) ( 和 ) 分组 贪婪模式和非贪婪模式 在 Vim 里，默认是贪婪模式，即 a.*b 会尽可能多滴匹配字符，在 ahdbjkbkls 中匹配 ahdbjkb 而不是 ahdb。\n如果是非贪婪的，可以使用 \\{-} 代替 *，即 a.\\{-}b 匹配 ahdb 而不是 ahdbjkb。\n作者：SpaceVim 链接：https://www.jianshu.com/p/03770041397c 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 ","date":"2018-09-21T14:28:51Z","permalink":"https://dccmmtop.github.io/posts/vim%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","section":"posts","tags":["vim"],"title":"vim中的正则表达式"},{"categories":null,"contents":"什么是跨域 理解跨域首先必须要了解同源策略。同源策略是浏览器上为安全性考虑实施的非常重要的安全策略。\n那么什么是同源？我们知道，URL 由协议、域名、端口和路径组成，如果两个 URL 的协议、域名和端口相同，则表示他们同源。\n我们用一个例子来说明：\nURL: http://www.example.com:8080/script/jquery.js\n在这个 url 中，各个字段分别代表的含义：\nhttp://——协议\nwww——子域名\nexample.com——主域名\n8080——端口号\nscript/jquery.js——请求的地址\n当协议、子域名、主域名、端口号中任意一各不相同时，都算不同的“域”。不同的域之间相互请求资源，就叫跨域。\n这里要注意，如果只是通过 AJAX 向另一个服务器发送请求而不要求数据返回，是不受跨域限制的。浏览器只是限制不能访问另一个域的数据，即不能访问返回的数据，并不限制发送请求。\n事实上，为了解决因同源策略而导致的跨域请求问题，解决方法有五种：\ndocument.domain Cross-Origin Resource Sharing(CORS) Cross-document messaging JSONP WebSockets 什么是 CORS(跨域资源共享，Cross-Origin Resource Sharing)？ 我们先来看看 wiki 上的定义：\n跨来源资源共享（CORS）是一份浏览器技术的规范，提供了 Web 服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，是 JSONP 模式的现代版。与 JSONP 不同，CORS 除了 GET 要求方法以外也支持其他的 HTTP 要求。用 CORS 可以让网页设计师用一般的 XMLHttpRequest，这种方式的错误处理比 JSONP 要来的好。另一方面，JSONP 可以在不支持 CORS 的老旧浏览器上运作。现代的浏览器都支持 CORS。\n由此我们可以知道， CORS 定义一种跨域访问的机制，可以让 AJAX 实现跨域访问。CORS 允许一个域上的网络应用向另一个域提交跨域 AJAX 请求。对于 CORS 来说，实现此功能非常简单，只需由服务器发送一个响应标头即可。服务器端对于 CORS 的支持，主要就是通过设置 Access-Control-Allow-Origin 来进行的。具体的关于 CORS 原理性的知识此处不再进行介绍，只在此对 CORS 和 JSONP 进行简单的比较.\nCORS 与 JSONP 比较\nCORS 与 JSONP 相比，更为先进、方便和可靠。\nJSONP 只能实现 GET 请求，而 CORS 支持所有类型的 HTTP 请求。 使用 CORS，开发者可以使用普通的 XMLHttpRequest 发起请求和获得数据，比起 JSONP 有更好的错误处理。 JSONP 主要被老的浏览器支持，它们往往不支持 CORS，而绝大多数现代浏览器都已经支持了 CORS。 rack-cors 怎样解决跨域问题？ Rack Middleware for handling Cross-Origin Resource Sharing (CORS), which makes cross-origin AJAX possible.\n也就是说，这个 gem 是基于 CORS 来实现 Ajax 的跨域请求功能的，我们可以添加这个 gem 来解决我们项目中遇到的问题。\n我们看到 gem 中给出的配置接口：\nRack\nIn config.ru, configure Rack::Cors by passing a block to the use command:\nuse Rack::Cors do allow do origins \u0026#39;localhost:3000\u0026#39;, \u0026#39;127.0.0.1:3000\u0026#39;, /\\Ahttp:\\/\\/192\\.168\\.0\\.\\d{1,3}(:\\d+)?\\z/ # regular expressions can be used here resource \u0026#39;/file/list_all/\u0026#39;, :headers =\u0026gt; \u0026#39;x-domain-token\u0026#39; resource \u0026#39;/file/at/_\u0026#39;, :methods =\u0026gt; [:get, :post, :delete, :put, :patch, :options, :head], :headers =\u0026gt; \u0026#39;x-domain-token\u0026#39;, :expose =\u0026gt; [\u0026#39;Some-Custom-Response-Header\u0026#39;], :max_age =\u0026gt; 600 # headers to expose end allow do origins \u0026#39;_\u0026#39; resource \u0026#39;/public/\\*\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; :get end end Rails\nPut something like the code below in config/application.rb of your Rails application. For example, this will allow GET, POST or OPTIONS requests from any origin on any resource.\nmodule YourApp class Application \u0026lt; Rails::Application # ... # Rails 3/4 config.middleware.insert_before 0, \u0026#34;Rack::Cors\u0026#34; do allow do origins \u0026#39;_\u0026#39; resource \u0026#39;_\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; [:get, :post, :options] end end # Rails 5 config.middleware.insert_before 0, Rack::Cors do allow do origins \u0026#39;_\u0026#39; resource \u0026#39;_\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; [:get, :post, :options] end end end end 可以看出，rack-cors 实际上直接给出了借口，我们在 bundle 这个 gem 后，直接在 config/application.rb 文件中添加配置信息即可，而无需自己在代码中添加有关跨域资源的策略信息。\n实际应用 这个 gem 是用在被访问的资源服务器上的，用来定义哪些域可以访问资源以及可以访问自己的哪些资源等策略信息。这个 gem 可以很轻松很方便地解决 ajax 跨域问题。\n安装 gem\ngem 'rack-cors', :require =\u0026gt; 'rack/cors'\n修改 config/application.rb\n我们使用的是 rails，因此只需要做以下修改即可： config.middleware.insert*before 0, Rack::Cors do allow do origins \u0026#39;*\u0026#39; resource \u0026#39;\\_\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; [:get, :post, :options] end end 其中，origins 用来配置可以请求自己资源的域，*表示任何域都可以请求；resource 用来配置自己的哪些资源可以被请求，*代表所有资源都可以被请求，methods 代表可以被请求的方法。\n做完这两部，我们就可以实现跨域请求资源了。此时重启服务器，本地再次请求资源就会成功，同时我们可以看到自己的请求中多了类似下面的一些信息：\nAccess-Control-Allow-Origin: http://localhost:3000\rAccess-Control-Allow-Methods: GET, POST, OPTIONS\rAccess-Control-Max-Age: 1728000\rAccess-Control-Allow-Credentials: true 这样，我们便在 rails 中解决了 Ajax 的跨域请求资源的问题，项目也可以继续向前开发了。\n作者：vito1994 链接：https://www.jianshu.com/p/c54a1dbaab24 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 ","date":"2018-09-19T09:25:49Z","permalink":"https://dccmmtop.github.io/posts/rack-cors%E8%A7%A3%E5%86%B3ajax%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98-cors/","section":"posts","tags":["rails"],"title":"rack-cors解决Ajax跨域问题-CORS"},{"categories":null,"contents":"在开始编写插件之前，你需要确认 Vim 是否支持 Ruby，通过以下命令来判别：\n$ vim --version | grep +ruby 如果输出为空，则表示你当前的 vim 不支持 Ruby，需要重新编译一下，并启用对 Ruby 的支持。\n如果没有问题那就开始吧！\n下面的示例是我用来把本地图片上传到七牛云图床。\n新建 在.vim/autoload中新建test.vim,复制以下代码\nfunction! qiniu#get_picture_url() ruby \u0026lt;\u0026lt; EOF class Qiniu def initialize @buffer = Vim::Buffer.current end def get_current_line s = @buffer.line # gets the current line # qiniu 是七牛云的一个gem，修改了部分代码，并重命名。 real_link = `qiniu #{s}` real_link = real_link.split(/\\n/).last Vim::Buffer.current.line = \u0026#34;![](#{real_link})\u0026#34; # sets the current line number end end gem = Qiniu.new gem.get_current_line EOF endfunction 绑定快捷键 在.vimrc中，映射快捷键\nnoremap \u0026lt;leader\u0026gt;qn :cal qiniu#get_picture_url()\u0026lt;cr\u0026gt;\u0026lt;CR\u0026gt; [\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;]\n我把图片的本地地址粘贴到 vim 中，然后使用qn快捷键，本地地址就会被修改成该图片在七牛云的外链，经常写博客用到\nvim API 在 vim 中执行:h ruby 查看 vim 提供 ruby 的 API\n演示 ","date":"2018-08-30T17:36:12Z","permalink":"https://dccmmtop.github.io/posts/%E7%94%A8ruby%E7%BC%96%E5%86%99vim%E8%84%9A%E6%9C%AC/","section":"posts","tags":["vim"],"title":"用ruby编写vim脚本"},{"categories":null,"contents":"在 rails 中,model 的属性是默认的可读可写的，有时我们需要重写某个字段的访问器。当查询某个字段的值时，需要进行其他操作；\n如： 当查询recommand_code的值时，若存在，则返回，若不存在则创建一个包含大写字母和数字的 6 为随机字符串\n主要是 read_attribute() 和 write_attribute() 的用法\ndef recommand_code # 重写 recommand_code 字段 _code = read_attribute(:recommand_code) # _code = self.recommand_code 错误，会引起无限递归 if self.block.empty? self.recommand_code=nil; return end return _code if _code loop do _code = ((\u0026#34;0\u0026#34;..\u0026#34;9\u0026#34;).to_a + (\u0026#34;A\u0026#34;..\u0026#34;Z\u0026#34;).to_a).sample(6).join break if User.find_by_recommand_code(_code).nil? end self.recommand_code = nil; return _code end def recommand_code=(value) write_attribute(:recommand_code,value) end ","date":"2018-08-20T18:09:07Z","permalink":"https://dccmmtop.github.io/posts/rails%E9%87%8D%E5%86%99%E5%AD%97%E6%AE%B5/","section":"posts","tags":["rails"],"title":"rails重写字段"},{"categories":null,"contents":"在开发微信公众号或小程序的时候，由于微信平台规则的限制，部分接口需要通过线上域名才能正常访问。但我们一般都会在本地开发，因为这能快速的看到源码修改后的运行结果。但当涉及到需要调用微信接口时，由于不和你在同一个局域网中的用户是无法访问你的本地开发机的，就必须把修改后的代码重新发布到线上域名所在的服务器才能去验证结果。每次修改都重新发布很繁琐也很浪费时间。\n本文将教你如何通过 SSH 隧道把本地服务映射到外网，以方便调试，通常把这种方法叫内网穿透。\n阅读完本文后，你能解决以下常见问题：\n开发微信公众号等应用时把本地服务映射到外网，加速调试流程； 把你正在开发的本地服务分享给互联网上其它人访问体验； 在任何地方通过互联网控制你家中在局域网里的电脑； 最终目的 把运行在本地开发机上的 HTTP 服务映射到外网，让全世界都能通过外网 IP 服务到你本地开发机上的 HTTP 服务。例如你本地的 HTTP 服务监听在 127.0.0.1:8080，你有一台公网 IP 为 12.34.56.78 的服务器，通过本文介绍的方法，可以让全世界的用户通过 http://12.34.56.78:8080 访问到你本地开发机上的 HTTP 服务。\n总结成一句话就是：把内网端口映射到外网。\n前提条件 为了把内网服务映射到外网，以下资源为必须的：\n一台有外网 IP 的服务器； 能在本地开发机上通过 ssh 登入到外网服务器。 要满足以上条件很简单：\n对于条件 1：购买一台低配 Linux 服务器，推荐国外的 DigitalOcean； 对于条件 2：对于 Mac、Linux 开发机是内置了 ssh 客户端的，对于 Windows 可以安装 Cygwin。 实现原理 要实现把内网端口映射到外网，最简单的方式就是通过 SSH 隧道。\nSSH 隧道就像一根管道，能把任何 2 台机器连接在一起，把发送到其中一台机器的数据通过管道传输到另一台机器。假如已经通过 SSH 隧道把本地开发机和外网服务器连接在了一起，外网服务器端监听在 12.34.56.78:8080，那么所有发给 12.34.56.78:8080 的数据都会通过 SSH 隧道原封不动地传输给本地开发机的 127.0.0.1:8080，如图所示：\n也就是说，去访问 12.34.56.78:8080 就像是访问本地开发机的 127.0.0.1:8080，本地开发机上的 8080 端口被映射到了外网服务器上的 8080 端口。\n如果你的外网服务器 IP 配置了域名解析，例如 yourdomin.com 会通过 DNS 解析为 12.34.56.78，那么也可以通过 yourdomin.com:8080 去访问本地开发机上的服务。\n这样就做到了访问外网地址时其实是本地服务返回的结果。\n通过 SSH 隧道传输数据时，数据会被加密，就算中间被劫持，黑客也无法得到数据的原内容。\n所以 SSH 隧道还有一个功能就是保证数据传输的安全性。\n实现步骤 把本地开机和外网服务器通过 SSH 隧道连接起来就和在本地开发机 SSH 登入远程登入到外网服务器一样简单。\n先来回顾以下 SSH 远程登入命令，假如想在本地远程登入到 12.34.56.78，可以在本地开发机上执行以下命令：\nssh username@12.34.56.78\n而实现 SSH 隧道只需在本地开发机上执行：\nssh -R 8080:127.0.0.1:8080 username@12.34.56.78\n可以看出实现 SSH 隧道的命令相对于 SSH 登入多出来 -R 8080:127.0.0.1:8080，多出的这部分的含义是：\n在远程机器(12.34.56.78)上启动 TCP 8080 端口监听着，再把远程机器(12.34.56.78)上 8080 端口映射到本地的 127.0.0.1:8080。\n执行完以上命令后，就可以通过 12.34.56.78:8080 去访问本地的 127.0.0.1:8080 了。\n通常把这种技术叫做 SSH 远程端口转发(remote forwarding)。\n其实不限于只能把本地开发机上运行的服务映射到外网服务器上去，还可以把任何本地开发机可以访问的服务映射到外网服务器上去。例如在本地开发机上能访问 github.com:80，在本地开发机上执行：\nssh -R 8080:github.com:80 username@12.34.56.78\n就能通过 12.34.56.78:8080 去访问 github.com:80 了。\n保持运行\n在执行完上面介绍的 SSH 隧道命令后，你会发现登入到了外网服务器上去了，如果你登出外网服务器，就会发现 12.34.56.78:8080 无法访问了。导致这个问题的原因是你登出外网服务器时，在外网服务器上本次操作对应的 SSH 进程也跟着退出了，而这个退出的进程曾负责监听在 8080 端口进行转发操作。\n为了让 SSH 隧道一直保持在后台执行，有以下方法。\n通过 SSH 自带的参数\nSSH 还支持这些参数：\nN 参数：表示只连接远程主机，不打开远程 shell； T 参数：表示不为这个连接分配 TTY； f 参数：表示连接成功后，转入后台运行； 因此要让 SSH 隧道一直保持在后台执行，可以通过以下命令：\nssh -NTf -R 8080:127.0.0.1:8080 username@12.34.56.78\n通过 AutoSSH SSH 隧道是不稳定的，在网络恶劣的情况下可能随时断开。如果断开就需要手动去本地开发机再次向外网服务器发起连接。\nAutoSSH 能让 SSH 隧道一直保持执行，他会启动一个 SSH 进程，并监控该进程的健康状况；当 SSH 进程崩溃或停止通信时，AutoSSH 将重启动 SSH 进程。\n使用 AutoSSH 只需在本地开发机上安装 AutoSSH ，方法如下：\nMac 系统：brew install autossh； Linux 系统：apt-get install autossh； 安装成功后，在本地开发机上执行：\nautossh -N -R 8080:127.0.0.1:8080 username@12.34.56.78\n就能完成和上面一样的效果，但本方法能保持 SSH 隧道一直运行。\n可以看出这行命令和上面的区别在于把 ssh 换成了 autossh，并且少了 -f 参数，原因是 autossh 默认会转入后台运行。\n常见问题\n如果你遇到通过以上方法成功启动 SSH 隧道后，还是无法访问 12.34.56.78:8080，那么很有可能是外网服务器上的 SSH 没有配置对。为此你需要去外网服务器上修改 /etc/ssh/sshd_config 文件如下：\nGatewayPorts yes\n这个选项的意思是，SSH 隧道监听的服务的 IP 是对外开放的 0.0.0.0，而不是只对本机的 127.0.0.1。不开 GatewayPorts 的后果是不能通过 12.34.56.78:8080 访问，只能在外网服务器上通过 127.0.0.1:8080 服务到本地开发机的服务。\n修改好配置文件后，你还需要重启 sshd 服务来加载新的配置，命令如下：\nservice sshd restart\n如果使用以上方法还是无法访问 12.34.56.78:8080，请检查你外网服务器的防火墙配置，确保 8080 端口是对外开放的。\n其它代替方案\n除了 SSH 隧道能实现内网穿透外，还有以下常用方法。\nfrp frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。\nfrp 有以下特性：\nfrp 比 SSH 隧道功能更多，配置项更多； frp 也需要一台外网服务器，并且需要在外网服务器上安装 frps，在本地开发机上安装 frpc； ngrok ngrok 是一个商用的内网穿透工具，它有以下特点：\n不需要有外网服务器，因为 ngrok 会为你提供； 只需要在本地开发机安装 ngrok 客户端，和注册 ngrok 账户； 按照服务收费； 这些代替方案的缺点在于都需要再额外安装其它工具，没有 SSH 隧道来的直接。\n想了解更多可以访问它们的主页。\n阅读原文\n","date":"2018-08-19T17:04:28Z","permalink":"https://dccmmtop.github.io/posts/ssh%E9%9A%A7%E9%81%93/","section":"posts","tags":["linux"],"title":"SSH隧道"},{"categories":null,"contents":"关于 Rails 的模型自关联有一个非常有意思的题目，大概是这样的：\nlisa = Person.create(name:\u0026#39;Lisa\u0026#39;) tom = Person.create(name:\u0026#39;Tom\u0026#39;,parent_id:lisa.id) andy = Person.create(name:\u0026#39;Andy\u0026#39;,parent_id:lisa.id) tom.parent.name =\u0026gt; \u0026#39;Lisa\u0026#39; lisa.children.map(\u0026amp;:name) =\u0026gt; [\u0026#39;Tom\u0026#39;,\u0026#39;Andy\u0026#39;] thomas = Person.create(name: \u0026#39;Thomas\u0026#39;,parent_id: tom.id) peter = Person.create(name:\u0026#39;Peter\u0026#39;,parent_id:tom.id) gavin = Person.create(name:\u0026#39;Gavin\u0026#39;, parent_id: andy.id) lisa.grandchildren.map(\u0026amp;:name) =\u0026gt; [\u0026#39;Thomas\u0026#39;,\u0026#39;Peter\u0026#39;,\u0026#39;Gavin\u0026#39;] 问如何定义 Person 模型来满足以上需求？\n题目考察了对模型自关联的理解，通过审题我们可以得出以下几点：\nPerson 对象的 Parent 同样是 Person 对象（自关联） Person 对象对 Parent 是多对一关系 Person 对象对 Children 是一对多关系 Person 对象通过 Children 与 GrandChildren 建立了一对多关系\n在不考虑 GrandChildren 时，不难得出模型定义如下： class Person \u0026lt; ActiveRecord::Base belongs_to :parent, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; has_many :children, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; end 其中 Person 包含两个自关联关系：\n第一个就是非常常见的从子到父的关系，在 Person 对象创建时指定 parent_id 来指向父对象；\n第二个关系用来指定 Person 对象对应的所有子对象\n接下来更近一步，我们要找到 Person 对象子对象的子对象，换句话说：孙子对象。\n如我们上面的分析，Person 对象通过 Children 与 GrandChildren 建立了一对多关系，其代码表现为：\nhas_many :grandchildren, :through =\u0026gt; :children, :source =\u0026gt; :children\n:source 选项的官方文档说明如下：\nThe :source option specifies the source association name for a has_many :through association. You only need to use this option if the name of the source association cannot be automatically inferred from the association name. —— rails guide\n在这里我们通过:source 选项告诉 Rails 在 children 对象上查找 children 关联关系。\n于是该题目完整的模型定义如下：\nclass Person \u0026lt; ActiveRecord::Base belongs_to :parent, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; has_many :children, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; has_many :grandchildren, :through =\u0026gt; :children, :source =\u0026gt; :children end 作者：李小西 033\n链接：https://www.jianshu.com/p/076b5fec4dad\n來源：简书\n简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。\n","date":"2018-08-15T12:22:06Z","permalink":"https://dccmmtop.github.io/posts/rails-%E8%87%AA%E5%85%B3%E8%81%94/","section":"posts","tags":["rails"],"title":"rails-自关联"},{"categories":null,"contents":"shuffle ((\u0026#34;0\u0026#34;..\u0026#34;9\u0026#34;).to_a + (\u0026#34;A\u0026#34;..\u0026#34;Z\u0026#34;).to_a).shuffle[0..6].to_a.join shuffle: 随机排列，中文名称是洗牌\nsample ((\u0026#34;0\u0026#34;..\u0026#34;9\u0026#34;).to_a + (\u0026#34;A\u0026#34;..\u0026#34;Z\u0026#34;).to_a).sample(6).join * [*\u0026#39;0\u0026#39;..\u0026#39;9\u0026#39;,*\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;].sample(6).join *的意思是将范围展开\n","date":"2018-08-12T10:23:46Z","permalink":"https://dccmmtop.github.io/posts/ruby%E9%9A%8F%E6%9C%BA%E7%94%9F%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/","section":"posts","tags":["ruby"],"title":"ruby随机生成字符串"},{"categories":null,"contents":"用 app 来调用 routes，比如 app.posts_path, app.topic_path(1)\nirb \u0026gt; app.topics_path =\u0026gt; \u0026#34;/topics\u0026#34; irb \u0026gt; app.get(app.root_path) ...... =\u0026gt; 200 用 helper 来调用 Helper 方法，比如:\nirb \u0026gt; helper.link_to(\u0026#34;Ruby China\u0026#34;, \u0026#34;http://ruby-china.org\u0026#34;) =\u0026gt; \u0026#34;\u0026lt;a href=\\\u0026#34;http://ruby-china.org\\\u0026#34;\u0026gt;Ruby China\u0026lt;/a\u0026gt;\u0026#34; irb \u0026gt; helper.truncate(\u0026#34;Here is Ruby China.\u0026#34;, length: 15) =\u0026gt; \u0026#34;Here is Ruby...\u0026#34; 使用 source_location 方法查看方法在那里定义的, 比如:\nirb \u0026gt;Topic.instance_method(:destroy).source_location =\u0026gt; [\u0026#34;/Users/jason/.rvm/gems/ruby-1.9.3-p0/gems/mongoid-2.4.8/lib/mongoid/persistence.rb\u0026#34;, 30] irb \u0026gt;Topic.method(:destroy_all).source_location =\u0026gt; [\u0026#34;/Users/jason/.rvm/gems/ruby-1.9.3-p0/gems/mongoid-2.4.8/lib/mongoid/persistence.rb\u0026#34;, 239] ","date":"2018-08-07T23:58:40Z","permalink":"https://dccmmtop.github.io/posts/rails_console%E5%A5%BD%E7%94%A8%E7%9A%84%E6%8A%80%E5%B7%A7/","section":"posts","tags":["rails"],"title":"rails_console好用的技巧"},{"categories":null,"contents":"转载 http://api.rubyonrails.org/classes/ActiveRecord/Store.html\n阅读 http://api.rubyonrails.org 相关的笔记\n使用 Model 里面的一个字段作为一个序列化的封装，用来存储一个 key/value\n文档里面提到，对应的存储字段的类型最好是 text， 以便确保有足够的存储空间\nMake sure that you declare the database column used for the serialized store as a text, so there\u0026#39;s plenty of room. 假设 Model 里面有一个字段 body\nclass CreatePosts \u0026lt; ActiveRecord::Migration[5.0] def change create_table :posts do |t| t.string :title t.text :body # 作为store序列化的字段 t.boolean :published t.integer :status t.timestamps end end end 接着设置对应的序列化属性\nclass Post \u0026lt; ApplicationRecord # enum status: [ :active, :archived ] # 这里使用数组 与之对应的数字从0依次增加 enum status: { active: 10, archived: 20 } # 明确指定对应的数字 store :body, accessors: [ :color, :homepage, :email ], coder: JSON # 序列化属性 end 这样设置后，在 body 这一个字段上就可以存储多个 key/value 了\nirb(main):001:0\u0026gt; p = Post.create (0.1ms) begin transaction SQL (1.2ms) INSERT INTO \u0026#34;posts\u0026#34; (\u0026#34;created_at\u0026#34;, \u0026#34;updated_at\u0026#34;) VALUES (?, ?) [[\u0026#34;created_at\u0026#34;, 2017-02-16 07:32:44 UTC], [\u0026#34;updated_at\u0026#34;, 2017-02-16 07:32:44 UTC]] (1.9ms) commit transaction =\u0026gt; #\u0026lt;Post id: 4, title: nil, body: {}, published: nil, status: nil, created_at: \u0026#34;2017-02-16 07:32:44\u0026#34;, updated_at: \u0026#34;2017-02-16 07:32:44\u0026#34;\u0026gt; irb(main):002:0\u0026gt; p.body =\u0026gt; {} irb(main):003:0\u0026gt; p.body.class =\u0026gt; ActiveSupport::HashWithIndifferentAccess irb(main):004:0\u0026gt; p.body[:color] = \u0026#34;red\u0026#34; =\u0026gt; \u0026#34;red\u0026#34; irb(main):005:0\u0026gt; p.body[:email] = \u0026#34;hello@126.com\u0026#34; =\u0026gt; \u0026#34;hello@126.com\u0026#34; irb(main):006:0\u0026gt; p.color =\u0026gt; \u0026#34;red\u0026#34; irb(main):007:0\u0026gt; p.email =\u0026gt; \u0026#34;hello@126.com\u0026#34; irb(main):008:0\u0026gt; p.body[:no_set] = \u0026#34;这个属性没有在model声明\u0026#34; =\u0026gt; \u0026#34;这个属性没有在model声明\u0026#34; irb(main):009:0\u0026gt; p.body[:no_set] =\u0026gt; \u0026#34;这个属性没有在model声明\u0026#34; irb(main):010:0\u0026gt; p.no_set #这个会报错 ","date":"2018-08-07T22:53:19Z","permalink":"https://dccmmtop.github.io/posts/activerecord_store%E7%94%A8%E6%B3%95%E7%A4%BA%E4%BE%8B/","section":"posts","tags":["rails"],"title":"ActiveRecord_Store用法示例"},{"categories":null,"contents":"这个命令可以以递归的方式下载整站，并可以将下载的页面中的链接转换为本地链接。\nwget 加上参数之后，即可成为相当强大的下载工具。\nwget -r -p -np -k http://xxx.com/abc/ -r, \u0026ndash;recursive（递归） specify recursive download.（指定递归下载） -k, \u0026ndash;convert-links（转换链接） make links in downloaded HTML point to local files.（将下载的 HTML 页面中的链接转换为相对链接即本地链接） -p, \u0026ndash;page-requisites（页面必需元素） get all images, etc. needed to display HTML page.（下载所有的图片等页面显示所需的内容） -np, \u0026ndash;no-parent（不追溯至父级） don\u0026rsquo;t ascend to the parent directory.\n另外断点续传用-nc 参数 日志 用-o 参数 ​ 熟练掌握 wget 命令，可以帮助你方便的使用 linux。\n命令用法详解 http://www.cnblogs.com/peida/archive/2013/03/18/2965369.html\nLinux 系统中的 wget 是一个下载文件的工具，它用在命令行下。对于 Linux 用户是必不可少的工具，我们经常要下载一些软件或从远程服务器恢复备份到本地服务器。wget 支持 HTTP，HTTPS 和 FTP 协议，可以使用 HTTP 代理。所谓的自动下载是指，wget 可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个 wget 下载任务，然后退出系统，wget 将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。\nwget 可以跟踪 HTML 页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循 Robot Exclusion 标准(/robots.txt). wget 可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。\nwget 非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget 会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。\n1．命令格式：\nwget [参数][url地址]\n2．命令功能：\n用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget 虽然功能强大，但是使用起来还是比较简单：\n1）支持断点下传功能；这一点，也是网络蚂蚁和 FlashGet 当年最大的卖点，现在，Wget 也可以使用此功能，那些网络不是太好的用户可以放心了；\n2）同时支持 FTP 和 HTTP 下载方式；尽管现在大部分软件可以使用 HTTP 方式下载，但是，有些时候，仍然需要使用 FTP 方式下载软件；\n3）支持代理服务器；对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能；\n4）设置方便简单；可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标；\n5）程序小，完全免费；程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。\n3．命令参数：\n启动参数：\n-V, –version 显示 wget 的版本后退出 -h, –help 打印语法帮助 -b, –background 启动后转入后台执行 -e, –execute=COMMAND 执行.wgetrc格式的命令，wgetrc 格式参见/etc/wgetrc 或~/.wgetrc 记录和输入文件参数：\n-o, –output-file=FILE 把记录写到 FILE 文件中 -a, –append-output=FILE 把记录追加到 FILE 文件中 -d, –debug 打印调试输出 -q, –quiet 安静模式(没有输出) -v, –verbose 冗长模式(这是缺省设置) -nv, –non-verbose 关掉冗长模式，但不是安静模式 -i, –input-file=FILE 下载在 FILE 文件中出现的 URLs -F, –force-html 把输入文件当作 HTML 格式文件对待 -B, –base=URL 将 URL 作为在-F -i 参数指定的文件中出现的相对链接的前缀 –sslcertfile=FILE 可选客户端证书 –sslcertkey=KEYFILE 可选客户端证书的 KEYFILE –egd-file=FILE 指定 EGD socket 的文件名 下载参数：\n–bind-address=ADDRESS 指定本地使用地址(主机名或 IP，当本地有多个 IP 或名字时使用) -t, –tries=NUMBER 设定最大尝试链接次数(0 表示无限制). -O –output-document=FILE 把文档写到 FILE 文件中 -nc, –no-clobber 不要覆盖存在的文件或使用.#前缀 -c, –continue 接着下载没下载完的文件 –progress=TYPE 设定进程条标记 -N, –timestamping 不要重新下载文件除非比本地文件新 -S, –server-response 打印服务器的回应 –spider 不下载任何东西 -T, –timeout=SECONDS 设定响应超时的秒数 -w, –wait=SECONDS 两次尝试之间间隔 SECONDS 秒 –waitretry=SECONDS 在重新链接之间等待 1…SECONDS 秒 –random-wait 在下载之间等待 0…2*WAIT 秒 -Y, –proxy=on/off 打开或关闭代理 -Q, –quota=NUMBER 设置下载的容量限制 –limit-rate=RATE 限定下载输率 目录参数：\n-nd –no-directories 不创建目录 -x, –force-directories 强制创建目录 -nH, –no-host-directories 不创建主机目录 -P, –directory-prefix=PREFIX 将文件保存到目录 PREFIX/… –cut-dirs=NUMBER 忽略 NUMBER 层远程目录 HTTP 选项参数：\n–http-user=USER 设定 HTTP 用户名为 USER. –http-passwd=PASS 设定 http 密码为 PASS -C, –cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许) -E, –html-extension 将所有 text/html 文档以.html 扩展名保存 –ignore-length 忽略 Content-Length头域 –header=STRING 在 headers 中插入字符串 STRING –proxy-user=USER 设定代理的用户名为 USER –proxy-passwd=PASS 设定代理的密码为 PASS –referer=URL 在 HTTP 请求中包含 Referer: URL头 -s, –save-headers 保存 HTTP 头到文件 -U, –user-agent=AGENT 设定代理的名称为 AGENT 而不是 Wget/VERSION –no-http-keep-alive 关闭 HTTP 活动链接 (永远链接) –cookies=off 不使用 cookies –load-cookies=FILE 在开始会话前从文件 FILE 中加载 cookie –save-cookies=FILE 在会话结束后将 cookies 保存到 FILE 文件中 FTP 选项参数：\n-nr, –dont-remove-listing 不移走 .listing文件 -g, –glob=on/off 打开或关闭文件名的 globbing 机制 –passive-ftp 使用被动传输模式 (缺省值). –active-ftp 使用主动传输模式 –retr-symlinks 在递归的时候，将链接指向文件(而不是目录) 递归下载参数：\n-r, –recursive 递归下载－－慎用! -l, –level=NUMBER 最大递归深度 (inf 或 0 代表无穷) –delete-after 在现在完毕后局部删除文件 -k, –convert-links 转换非相对链接为相对链接 -K, –backup-converted 在转换文件 X 之前，将之备份为 X.orig -m, –mirror 等价于 -r -N -l inf -nr -p, –page-requisites 下载显示 HTML 文件的所有图片 递归下载中的包含和不包含(accept/reject)：\n-A, –accept=LIST 分号分隔的被接受扩展名的列表 -R, –reject=LIST 分号分隔的不被接受的扩展名的列表 -D, –domains=LIST 分号分隔的被接受域的列表 –exclude-domains=LIST 分号分隔的不被接受的域的列表 –follow-ftp 跟踪 HTML 文档中的 FTP 链接 –follow-tags=LIST 分号分隔的被跟踪的 HTML 标签的列表 -G, –ignore-tags=LIST 分号分隔的被忽略的 HTML 标签的列表 -H, –span-hosts 当递归时转到外部主机 -L, –relative 仅仅跟踪相对链接 -I, –include-directories=LIST 允许目录的列表 -X, –exclude-directories=LIST 不被包含目录的列表 -np, –no-parent 不要追溯到父目录 wget -S –spider url 不下载只显示过程 4．使用实例：\n实例 1：使用 wget 下载单个文件\n命令：\nwget http://www.linuxidc.com/linuxidc.zip\n说明：\n以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。\n实例 2：使用 wget -O 下载并以不同的文件名保存\n命令：\nwget -O wordpress.zip http://www.linuxidc.com/download.aspx?id=1080\n说明：\nwget 默认会以最后一个符合”/”的后面的字符来命令，对于动态链接的下载通常文件名会不正确。\n错误：下面的例子会下载一个文件并以名称 download.aspx?id=1080 保存\nwget http://www.linuxidc.com/download?id=1\n即使下载的文件是 zip 格式，它仍然以 download.php?id=1080 命令。\n正确：为了解决这个问题，我们可以使用参数-O 来指定一个文件名：\nwget -O wordpress.zip http://www.linuxidc.com/download.aspx?id=1080\n实例 3：使用 wget –limit -rate 限速下载\n命令：\nwget \u0026ndash;limit-rate=300k http://www.linuxidc.com/linuxidc.zip\n说明：\n当你执行 wget 的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。\n实例 4：使用 wget -c 断点续传\n命令：\nwget -c http://www.linuxidc.com/linuxidc.zip\n说明：\n使用 wget -c 重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c 参数。\n实例 5：使用 wget -b 后台下载\n命令：\nwget -b http://www.linuxidc.com/linuxidc.zip\n说明：\n对于下载非常大的文件的时候，我们可以使用参数-b 进行后台下载。\nwget -b http://www.linuxidc.com/linuxidc.zip\nContinuing in background, pid 1840.\nOutput will be written to wget-log.\n你可以使用以下命令来察看下载进度：\ntail -f wget-log\n实例 6：伪装代理名称下载\n命令：\nwget \u0026ndash;user-agent=\u0026ldquo;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16\u0026rdquo; http://www.linuxidc.com/linuxidc.zip\n说明：\n有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过–user-agent 参数伪装。\n实例 7：使用 wget –spider 测试下载链接\n命令：\nwget \u0026ndash;spider URL\n说明：\n当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加–spider 参数进行检查。\nwget \u0026ndash;spider URL\n如果下载链接正确，将会显示\nwget --spider URL\rSpider mode enabled. Check if remote file exists.\rHTTP request sent, awaiting response... 200 OK\rLength: unspecified [text/html]\rRemote file exists and could contain further links,\rbut recursion is disabled -- not retrieving.\r这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误\rwget --spider url\rSpider mode enabled. Check if remote file exists.\rHTTP request sent, awaiting response... 404 Not Found\rRemote file does not exist -- broken link!!! 你可以在以下几种情况下使用 spider 参数：\n定时下载之前进行检查\n间隔检测网站是否可用\n检查网站页面的死链接\n实例 8：使用 wget –tries 增加重试次数\n命令：\nwget \u0026ndash;tries=40 URL\n说明：\n如果网络有问题或下载一个大文件也有可能失败。wget 默认重试 20 次连接下载文件。如果需要，你可以使用–tries 增加重试次数。\n实例 9：使用 wget -i 下载多个文件\n命令：\nwget -i filelist.txt\n说明：\n首先，保存一份下载链接文件\ncat \u0026gt; filelist.txt\nurl1\nurl2\nurl3\nurl4\n接着使用这个文件和参数-i 下载\n实例 10：使用 wget –mirror 镜像网站\n命令：\nwget \u0026ndash;mirror -p \u0026ndash;convert-links -P ./LOCAL URL\n说明：\n下载整个网站到本地。\n–miror:开户镜像下载\n-p:下载所有为了 html 页面显示正常的文件\n–convert-links:下载后，转换成本地的链接\n-P ./LOCAL：保存所有文件和目录到本地指定目录\n实例 11：使用 wget –reject 过滤指定格式下载\n命令：\nwget \u0026ndash;reject=gif ur\n说明：\n下载一个网站，但你不希望下载图片，可以使用以下命令。\n实例 12：使用 wget -o 把下载信息存入日志文件\n命令：\nwget -o download.log URL\n说明：\n不希望下载信息直接显示在终端而是在一个日志文件，可以使用\n实例 13：使用 wget -Q 限制总下载文件大小\n命令：\nwget -Q5m -i filelist.txt\n说明：\n当你想要下载的文件超过 5M 而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。\n实例 14：使用 wget -r -A 下载指定格式文件\n命令：\nwget -r -A.pdf url\n说明：\n可以在以下情况使用该功能：\n下载一个网站的所有图片\n下载一个网站的所有视频\n下载一个网站的所有 PDF 文件\n实例 15：使用 wget FTP 下载\n命令：\nwget ftp-url\nwget \u0026ndash;ftp-user=USERNAME \u0026ndash;ftp-password=PASSWORD url\n说明：\n可以使用 wget 来完成 ftp 链接的下载。\n使用 wget 匿名 ftp 下载：\nwget ftp-url\n使用 wget 用户名和密码认证的 ftp 下载\nwget \u0026ndash;ftp-user=USERNAME \u0026ndash;ftp-password=PASSWORD url\n备注：编译安装\n使用如下命令编译安装：\ntar zxvf wget-1.9.1.tar.gz cd wget-1.9.1 ./configure make make install ","date":"2018-06-22T10:56:46Z","permalink":"https://dccmmtop.github.io/posts/wget%E6%95%B4%E7%AB%99%E4%B8%8B%E8%BD%BD/","section":"posts","tags":["linux"],"title":"wget整站下载"},{"categories":null,"contents":"添加新用户 在服务器添加一个新的用户，用户名为 deploy教程\n执行命令sudo adduser 用户名\n按提示输入密码\n设置一些个人信息，可以直接按 enter 键，设为空\n添加权限\n在 root 用户下，打开/etc/sudoers文件\n# # This file MUST be edited with the \u0026#39;visudo\u0026#39; command as root. # # Please consider adding local content in /etc/sudoers.d/ instead of # directly modifying this file. # # See the man page for details on how to write a sudoers file. # Defaults env_reset Defaults mail_badpass Defaults secure_path=\u0026#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\u0026#34; # Host alias specification # User alias specification # Cmnd alias specification # User privilege specification root ALL=(ALL:ALL) ALL deploy ALL=(ALL:ALL) ALL # 添加这一行，使deploy具有使用sudo的权限 # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL # Allow members of group sudo to execute any command %sudo ALL=(ALL:ALL) ALL # See sudoers(5) for more information on \u0026#34;#include\u0026#34; directives: #includedir /etc/sudoers.d ruby 安装 安装rbenv 教程来源\nsudo deploy回到 deploy 下\ngit clone https://github.com/rbenv/rbenv.git ~/.rbenv # 用来编译安装 ruby git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build # 用来管理 gemset, 可选, 因为有 bundler 也没什么必要 git clone git://github.com/jamis/rbenv-gemset.git ~/.rbenv/plugins/rbenv-gemset # 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐 git clone git://github.com/rkh/rbenv-update.git ~/.rbenv/plugins/rbenv-update # 使用 Ruby China 的镜像安装 Ruby, 国内用户推荐 git clone git://github.com/AndorChen/rbenv-china-mirror.git ~/.rbenv/plugins/rbenv-china-mirror 然后把下面的代码放到 ~/.bashrc 里\nexport PATH=\u0026#34;$HOME/.rbenv/bin:$PATH\u0026#34; eval \u0026#34;$(rbenv init -)\u0026#34; 然后重开一个终端就可以执行 rbenv 了.\n安装 ruby\nrbenv install --list # 列出所有 ruby 版本 rbenv install 2.5.0 # 安装 2.5.0 安转过程可能出现缺少依赖的错误，可参考这篇文章解决\n一般解决办法:\nsudo apt-get install autoconf bison build-essential libssl-dev libyaml-dev libreadline6 libreadline6-dev zlib1g zlib1g-dev 验证安装是否成功\nrbenv versions # 列出安装的版本 rbenv version # 列出正在使用的版本 设置版本\nrbenv global 2.5.0 # 默认使用 2.5.0 rbenv shell 2.5.0 # 当前的 shell 使用 2.5.0, 会设置一个 `RBENV_VERSION` 环境变量 rbenv local jruby-1.7.3 # 当前目录使用 jruby-1.7.3, 会生成一个 `.rbenv-version` 文件 last\nrbenv rehash # 每当切换 ruby 版本和执行 bundle install 之后必须执行这个命令 rbenv which irb # 列出 irb 这个命令的完整路径 rbenv whence irb # 列出包含 irb 这个命令的版本 安装bundle\ngem install bundle 安装rails\ngem install rails 安装 nodejs\ncurl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash - sudo apt-get install -y nodejs 数据库 使用 postgresql 数据库教程来源\nsudo apt-get install postgresql 新建数据库用户\nsudo -i -u postgres //切换到数据库的超级管理员 psql //进入数据库控制台 create user deploy with password \u0026#39;xxxx\u0026#39;; //新建一个deploy用户，密码是xxx alter role deploy with createdb; //使deploy用户具有创建数据库的权限 alter role deploy with login；//使deploy用户具有登录数据库的权限 注意：\n在后面安装 pg gem 时，可能会出现You need to install postgresql-server-dev-X.Y for building a server-side extension or libpq-dev for building a client-side applic ation错误,依次执行：\nsudo apt-get install python-psycopg2 sudo apt-get install libpq-dev nginx passenger 安装 这里很详细了\nsudo apt-get install -y dirmngr gnupg sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7 sudo apt-get install -y apt-transport-https ca-certificates sudo sh -c \u0026#39;echo deb https://oss-binaries.phusionpassenger.com/apt/passenger xenial main \u0026gt; /etc/apt/sources.list.d/passenger.list\u0026#39; sudo apt-get update sudo apt-get install -y nginx-extras passenger passenger 的配置\nnginx 安装以后，打开/etc/nginx/passenger.conf会看到\npassenger_root /usr/lib/ruby/vendor_ruby/phusion_passenger/locations.ini;\rpassenger_ruby /home/deploy/.rbenv/shims/ruby; //这里需要修改ruby的安装路径 which ruby 可以查看 ruby 的路径\nCapistrano 配置原文教程 安装必要的包\ngroup :development do gem \u0026#39;capistrano\u0026#39; gem \u0026#39;capistrano-bundler\u0026#39; gem \u0026#39;capistrano-rails\u0026#39; gem \u0026#39;capistrano-rbenv\u0026#39; # Add this if you\u0026#39;re using rvm # gem \u0026#39;capistrano-rvm\u0026#39; end cap install\n我的 capfile 文件\n# Load DSL and set up stages require \u0026#34;capistrano/setup\u0026#34; # Include default deployment tasks require \u0026#34;capistrano/deploy\u0026#34; # Load the SCM plugin appropriate to your project: # # require \u0026#34;capistrano/scm/hg\u0026#34; # install_plugin Capistrano::SCM::Hg # or # require \u0026#34;capistrano/scm/svn\u0026#34; # install_plugin Capistrano::SCM::Svn # or require \u0026#34;capistrano/scm/git\u0026#34; install_plugin Capistrano::SCM::Git # Include tasks from other gems included in your Gemfile # # For documentation on these, see for example: # # https://github.com/capistrano/rvm # https://github.com/capistrano/rbenv # https://github.com/capistrano/chruby # https://github.com/capistrano/bundler # https://github.com/capistrano/rails # https://github.com/capistrano/passenger # # require \u0026#34;capistrano/rvm\u0026#34; require \u0026#34;capistrano/rbenv\u0026#34; # require \u0026#34;capistrano/chruby\u0026#34; require \u0026#34;capistrano/bundler\u0026#34; require \u0026#34;capistrano/rails/assets\u0026#34; require \u0026#34;capistrano/rails/migrations\u0026#34; require \u0026#34;capistrano/passenger\u0026#34; set :rbenv_type, :user set :rbenv_ruby, \u0026#39;2.5.0\u0026#39; # Load custom tasks from `lib/capistrano/tasks` if you have any defined Dir.glob(\u0026#34;lib/capistrano/tasks/*.rake\u0026#34;).each { |r| import r } 我的 deploy.rb 文件\n# config valid for current version and patch releases of Capistrano lock \u0026#34;~\u0026gt; 3.10.2\u0026#34; set :application, \u0026#34;script_blog\u0026#34; set :repo_url, \u0026#34;https://github.com/dccmmtop/script_blog.git\u0026#34; # Default branch is :master # ask :branch, `git rev-parse --abbrev-ref HEAD`.chomp # Default deploy_to directory is /var/www/my_app_name set :deploy_to, \u0026#34;/home/deploy/scrit_blog\u0026#34; # Default value for :format is :airbrussh. # set :format, :airbrussh # You can configure the Airbrussh format using :format_options. # These are the defaults. # set :format_options, command_output: true, log_file: \u0026#34;log/capistrano.log\u0026#34;, color: :auto, truncate: :auto # Default value for :pty is false # set :pty, true # Default value for :linked_files is [] # 在服务器\u0026lt;project-name\u0026gt;/share/config/ 下，要手动新建这两个文件， append :linked_files, \u0026#34;config/database.yml\u0026#34;,\u0026#34;config/secrets.yml\u0026#34; # Default value for linked_dirs is [] append :linked_dirs, \u0026#34;log\u0026#34;, \u0026#34;tmp/pids\u0026#34;, \u0026#34;tmp/cache\u0026#34;, \u0026#34;tmp/sockets\u0026#34;, \u0026#34;public/system\u0026#34; # Default value for default_env is {} # set :default_env, { path: \u0026#34;/opt/ruby/bin:$PATH\u0026#34; } # Default value for local_user is ENV[\u0026#39;USER\u0026#39;] # set :local_user, -\u0026gt; { `git config user.name`.chomp } # Default value for keep_releases is 5 # set :keep_releases, 5 # Uncomment the following to require manually verifying the host key before first deploy. # set :ssh_options, verify_host_key: :secure 注意append :linked_files, \u0026quot;config/database.yml\u0026quot;,\u0026quot;config/secrets.yml\u0026quot;\ndatabase.yml和secrets.yml是手动在,share/config/目录下新建的，一个是连接数据库的相关信息，一个是安全验证相关信息。我的部署目录是scriot_blog/ 就新建 script_blog/share/config/ 目录\n同时新建以上两个文件。\ndatabase.yml\nproduction: adapter: postgresql pool: \u0026lt;%= ENV.fetch(\u0026#34;RAILS_MAX_THREADS\u0026#34;) { 5 } %\u0026gt; timeout: 5000 database: production_blog username: \u0026#39;xxx\u0026#39; password: \u0026#39;xxx\u0026#39; secrets.yml\nproduction: secret_key_base: xxxxxx 其中secret_key_base的值是在本地项目下 执行rake secret 命令生成的。\ndeploy/production.rb\n# server-based syntax # ====================== # Defines a single server with a list of roles and multiple properties. # You can define all roles on a single server, or split them: # server \u0026#34;39.108.138.149\u0026#34;, user: \u0026#34;root\u0026#34;, roles: %w{app db web}, my_property: :my_value server \u0026#34;xxxx服务器的ip\u0026#34;, user: \u0026#34;deploy\u0026#34;, roles: %w{app db web} # server \u0026#34;example.com\u0026#34;, user: \u0026#34;deploy\u0026#34;, roles: %w{app web}, other_property: :other_value # server \u0026#34;db.example.com\u0026#34;, user: \u0026#34;deploy\u0026#34;, roles: %w{db} # role-based syntax # ================== # Defines a role with one or multiple servers. The primary server in each # group is considered to be the first unless any hosts have the primary # property set. Specify the username and a domain or IP for the server. # Don\u0026#39;t use `:all`, it\u0026#39;s a meta role. # role :app, %w{deploy@example.com}, my_property: :my_value # role :web, %w{user1@primary.com user2@additional.com}, other_property: :other_value # role :db, %w{deploy@example.com} # Configuration # ============= # You can set any configuration variable like in config/deploy.rb # These variables are then only loaded and set in this stage. # For available Capistrano configuration variables see the documentation page. # http://capistranorb.com/documentation/getting-started/configuration/ # Feel free to add new variables to customise your setup. # Custom SSH Options # ================== # You may pass any option but keep in mind that net/ssh understands a # limited set of options, consult the Net::SSH documentation. # http://net-ssh.github.io/net-ssh/classes/Net/SSH.html#method-c-start # # Global options # -------------- set :ssh_options, { keys: %w(/home/deploy/.ssh/id_rsa), port: xxx # forward_agent: false, # auth_methods: %w(password) } # # The server-based syntax can be used to override options: # ------------------------------------ # server \u0026#34;example.com\u0026#34;, # user: \u0026#34;user_name\u0026#34;, # keys: %w(/home/user_name/.ssh/id_rsa), # forward_agent: false, # auth_methods: %w(publickey password) # # password: \u0026#34;please use keys\u0026#34; # } 最后 本地执行cap production deploy\n","date":"2018-05-29T15:26:09Z","permalink":"https://dccmmtop.github.io/posts/%E9%83%A8%E7%BD%B2rails/","section":"posts","tags":["rails","部署"],"title":"部署Rails"},{"categories":null,"contents":" 生成 key\n在本地执行ssh-keygen 将本地的公钥拷贝到远程服务器 ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22222 username@ip ","date":"2018-05-29T11:24:54Z","permalink":"https://dccmmtop.github.io/posts/ssh%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/","section":"posts","tags":["linux"],"title":"ssh免密码登录"},{"categories":null,"contents":"管好自己 但行好事 不渡他人\n不谈内心秩序 只讲爱与和平\n我们一路奋战，不是为了能改变世界，而是为了不让世界改变我们\n","date":"1023-04-15T14:07:16Z","permalink":"https://dccmmtop.github.io/posts/%E5%A5%87%E8%91%A9/","section":"posts","tags":["文摘"],"title":"奇葩"},{"categories":null,"contents":"你要忍，忍到春暖花开；\n你要走，走到灯火通明；\n你要看过世界辽阔，再评判是好是坏；\n你要卯足劲变好，再旗鼓相当的站在不敢想象的人身边；\n你要变成想像中的样子，这件事，一步都不能退让。\n","date":"1023-04-11T21:51:57Z","permalink":"https://dccmmtop.github.io/posts/%E4%BD%A0%E8%A6%81/","section":"posts","tags":["文摘"],"title":"你要"},{"categories":null,"contents":"我想\n我想爱你而不用抓住你\n欣赏你而不需批判你\n和你一起参与而不强求你\n离开你亦无需言歉疚\n帮助你而没有半点低看你\n那么我俩的相处就是真诚的\n并且能彼此滋养\n————萨提亚\n","date":"1023-03-30T23:50:26Z","permalink":"https://dccmmtop.github.io/posts/%E6%88%91%E6%83%B3/","section":"posts","tags":["文摘"],"title":"我想"},{"categories":null,"contents":"你的眼睛真好看\n里面有晴雨日月\n山川江河云雾花鸟\n但我的眼睛更好看\n因为我的眼里有你\n———— 余光中\n遇见你我变得很低很低\n一直低到尘埃里去\n但我的心里是欢喜的\n并在那里开出一朵花来\n———— 张爱玲\n手，我是有的\n就是不知如何碰你\n———— 顾城\n我喜欢你\n所以希望你被簇拥包围\n所以你走的路\n要繁花盛开 要人生鼎沸\n———— 木苏里\n陪我到可可西里看一看海\n不要未来\n只要你来\n———— 大冰\n在你孤独，悲伤的日子\n请你悄悄的念一念我的名字\n并且说：“有人在思念我，在世间我活在一个人心里。”\n———— 普希金《我的名字》\n待我了无牵挂\n漂泊四海为家\n看看天山雪莲\n走走大漠黄沙\n泰山顶上饮酒\n西子湖畔浣纱\n蒙古草原纵马\n再访苗疆人家\n就此峰回路转\n去看大理三塔\n走过青石小巷\n伞下人面桃花\n此生心愿已了\n生老病死由他\n","date":"1023-03-26T16:48:35Z","permalink":"https://dccmmtop.github.io/posts/%E9%9A%90%E6%99%A6/","section":"posts","tags":["文摘"],"title":"隐晦"},{"categories":null,"contents":" 人并不受过去的原因所左右，而是朝着自己定下的目标前进\n我有一位年轻朋友，虽然梦想着成为小说家，但却总是写不出作品。他说是因为工作太忙、写小说的时间非常有限，所以才写不出来作品，也从未参加过任何比赛。　但真是如此吗？实际上，他是想通过不去比赛这一方式来保留一种“如果做的话我也可以”的可能性，即不愿出去被人评价，更不愿去面对因作品拙劣而落选的现实。\n他只想活在“只要有时间我也可以、只要环境具备我也能写、自己有这种才能”之类的可能性中。或许再过5年或者10年，他又会开始使用“已经不再年轻”或者“也已经有了家庭”之类的借口\n假若应征落选也应该去做。那样的话或许能够有所成长，或许会明白应该选择别的道路。总之，可以有所发展。所谓改变现在的生活方式就是这样。如果一直不去投稿应征，那就不会有所发展。　青年：梦也许会破灭啊！　哲人：但那又怎样呢？ 应该去做——这一简单的课题摆在面前，但却不断地扯出各种“不能做的理由”，你难道不认为这是一种很痛苦的生活方式吗？梦想着做小说家的他，正是“自己”把人生变得复杂继而难以获得幸福。\n","date":"1023-03-26T16:20:44Z","permalink":"https://dccmmtop.github.io/posts/%E8%A2%AB%E8%AE%A8%E5%8E%8C%E7%9A%84%E5%8B%87%E6%B0%94/","section":"posts","tags":["文摘"],"title":"被讨厌的勇气"},{"categories":null,"contents":"我说你是人间的四月天；\n笑响点亮了四面风；\n轻灵在春的光艳中交舞着变。\n你是四月早天里的云烟，\n黄昏吹着风的软，\n星子在无意中闪，\n细雨点洒在花前。\n那轻，那娉婷，你是，\n鲜妍百花的冠冕你戴着，\n你是天真，庄严，\n你是夜夜的月圆。\n雪化后那片鹅黄，你像；\n新鲜初放芽的绿，你是；\n柔嫩喜悦，\n水光浮动着你梦期待中白莲。\n你是一树一树的花开，\n是燕在梁间呢喃，\n——你是爱，是暖，是希望，\n你是人间的四月天！\n","date":"1023-03-26T16:01:37Z","permalink":"https://dccmmtop.github.io/posts/%E4%BD%A0%E6%98%AF%E4%BA%BA%E9%97%B4%E5%9B%9B%E6%9C%88%E5%A4%A9/","section":"posts","tags":["文摘"],"title":"你是人间四月天"},{"categories":null,"contents":"《当我开始爱自己》——查理 · 卓别林 当我开始爱自己\n我才发现所有的苦痛只是在警示\n我背离了本真\n今天我懂得，这是 \u0026ldquo;真实\u0026rdquo;\n当我开始爱自己\n我才明白有一种冒犯\n是把我的欲求强加于人\n尽管我知道时机不对\n那个人也没有准备好\n有时那个人正是我自己\n今天我懂得，这是“尊重”\n当我开始爱自己\n我不再渴求过另外一种人生\n我看到身边每一件事，都是在邀请我成长\n今天我懂得，这是“成熟”\n当我开始爱自己\n我才发现\n我一直都处在正确的时间和正确的地点\n发生的一切都恰如其分\n由此我归于平静\n今天我懂得，这是“自信”\n当我开始爱自己\n我便不再牺牲我的时间\n不再去勾画宏伟的明天\n此刻我只做有趣和快乐的事\n只做真正热爱和由衷喜欢之事\n以我的方式、以我的节奏\n今天我懂得，这是“单纯”\n当我开始爱自己\n我让自己远离了一切不健康的食物、人、事情和环境\n还有一切令我堕落，令我远离自我的东西\n以前我认为这种态度是健康的自私\n今天我懂得，这是“自爱”\n当我开始爱自己\n我不再奢望一直正确\n不再企图不错失时间\n今天我懂得，这是“谦逊”\n当我开始爱自己\n我不再缅怀过去，也不再忧虑未来\n我只活在当下\n进入正在发生的一切事物之中\n我充分的活在每一日\n如此日复一日\n而我懂得，这就是完美\n当我开始爱自己\n我才明白，头脑会让我混乱而病态\n然而当头脑与心相连\n它就成了可信赖的伙伴\n今天我将这份组合称之为“心之智慧”\n我们不必害怕自己与他人的任何分歧、矛盾和问题\n即便星球也会撞击到一起\n新世界就此而生\n今天我懂得，这就是“生命”\n","date":"1023-03-25T19:10:22Z","permalink":"https://dccmmtop.github.io/posts/%E5%BD%93%E6%88%91%E5%BC%80%E5%A7%8B%E7%88%B1%E8%87%AA%E5%B7%B1/","section":"posts","tags":["文摘"],"title":"当我开始爱自己"},{"categories":null,"contents":"fastJson序列化 Map \u0026lt; String , Object \u0026gt; jsonMap = new HashMap\u0026lt; String , Object\u0026gt;(); jsonMap.put(\u0026#34;a\u0026#34;,1); jsonMap.put(\u0026#34;b\u0026#34;,\u0026#34;\u0026#34;); jsonMap.put(\u0026#34;c\u0026#34;,null); jsonMap.put(\u0026#34;d\u0026#34;,\u0026#34;wuzhuti.cn\u0026#34;); String str = JSONObject.toJSONString(jsonMap); System.out.println(str); //输出结果:{\u0026#34;a\u0026#34;:1,\u0026#34;b\u0026#34;:\u0026#34;\u0026#34;,d:\u0026#34;wuzhuti.cn\u0026#34;} 从输出结果可以看出，null对应的key已经被过滤掉；这明显不是我们想要的结果，这时我们就需要用到fastjson的SerializerFeature序列化属性\n也就是这个方法：JSONObject.toJSONString(Object object, SerializerFeature\u0026hellip; features)\nFastjson的SerializerFeature序列化属性\nQuoteFieldNames———-输出key时是否使用双引号,默认为true WriteMapNullValue——–是否输出值为null的字段,默认为false WriteNullNumberAsZero—-数值字段如果为null,输出为0,而非null WriteNullListAsEmpty—–List字段如果为null,输出为[],而非null WriteNullStringAsEmpty—字符类型字段如果为null,输出为”“,而非null WriteNullBooleanAsFalse–Boolean字段如果为null,输出为false,而非null Map \u0026lt; String , Object \u0026gt; jsonMap = new HashMap\u0026lt; String , Object\u0026gt;(); jsonMap.put(\u0026#34;a\u0026#34;,1); jsonMap.put(\u0026#34;b\u0026#34;,\u0026#34;\u0026#34;); jsonMap.put(\u0026#34;c\u0026#34;,null); jsonMap.put(\u0026#34;d\u0026#34;,\u0026#34;wuzhuti.cn\u0026#34;); String str = JSONObject.toJSONString(jsonMap,SerializerFeature.WriteMapNullValue); System.out.println(str); //输出结果:{\u0026#34;a\u0026#34;:1,\u0026#34;b\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;c\u0026#34;:null,\u0026#34;d\u0026#34;:\u0026#34;wuzhuti.cn\u0026#34;} ","date":"0001-01-01T00:00:00Z","permalink":"https://dccmmtop.github.io/posts/fastjson%E5%BA%8F%E5%88%97%E5%8C%96/","section":"posts","tags":null,"title":""},{"categories":null,"contents":"HTTP 代理 代理使用同一种协议，网关使用不同的协议连接端 作用 监视流量并修改\n儿童过滤器\n安全防火墙\n缓存\n反向代理 假扮服务器\n文档访问控制，统一输入密码\n转码器 图片格式，语言\n匿名者\n从报文中删除身份特征，ip,首部，cookie等 层级结构 静态\n固定顺序转发 动态\n负载均衡 就近路由 协议类型路由 如何获取流量 修改客户端网络配置\n不设置代理 发送相对URI路径 有代理发送完整路径 修改路由设备网络\n在客户端不参与的情况下拦截网络流量，依赖流量交换设备和路由设备 修改DNS命名空间\n修改web服务器\n重定向 追踪报文 Via 首部\n记录经过的每个中间节点\n用来检测是否有路由循环\n组成\n协议名 协议版本 节点名 节点注释 trace 方法\nMax-Forwards\n整数 本报文还可以被转发的次数 每经过一个节点数量减一 代理认证 代理的互操作 保留不支持的首部和方法，向下转发\nOPTIONS方法\nURI 是*， 查看服务支持的所有方法 URLI 是具体资源，查看对该资源支持的方法 Allow 首部\n对资源所支持的方法列表 缓存 问题 冗余数据传输 带宽瓶颈 瞬间堵塞 距离时延 区分命中和未命中的情况 Date首部\n将响应中Date首部的值与当前时间进行比较，如果响应中的日期值比较早，客户端通常就可以认为这是一条缓存的响应。\n缓存的层次结构 在实际中，实现层次化（hierarchy）的缓存是很有意义的，在这种结构中，在较小缓存中未命中的请求会被导向较大的父缓存（parent cache），由它来为剩下的那些“提炼过的”流量提供服务。图7-9显示了一个两级的缓存层次结构。[插图]其基本思想是在靠近客户端的地方使用小型廉价缓存，而更高层次中，则逐步采用更大、功能更强的缓存来装载多用户共享的文档。\n网状缓存、内容路由以及对等缓存 处理步骤 接收 缓存从网络中读取抵达的请求报文 解析 缓存对报文进行解析，提取出URL和各种首部 查询 存查看是否有本地副本可用，如果没有，就获取一份副本（并将其保存在本地） 新鲜度检测 缓存查看已缓存副本是否足够新鲜，如果不是，就询问服务器是否有任何更新 创建相应 缓存会用新的首部和已缓存的主体来构建一条响应报文 发送 缓存通过网络将响应发回给客户端 日志 缓存可选地创建一个日志文件条目来描述这个事务 流程图\n保持副本的新鲜 说明\n可能不是所有的已缓存副本都与服务器上的文档一致。毕竟，这些文档会随着时间发生变化。报告可能每个月都会变化。在线报纸每天都会发生变化。财经数据可能每过几秒钟就会发生变化。如果缓存提供的总是老的数据，就会变得毫无用处。已缓存数据要与服务器数据保持一致。\n再验证\n缓存过期不代表文档变化，而是需要核对了\n有变化\n获取一份新的文档 无变化\n只需要获取新的首部，包括一个新的过期日期，并对缓存中的首部进行更新 条件再验证\nHTTP的条件方法可以高效地实现再验证。HTTP允许缓存向原始服务器发送一个“条件GET”，请求服务器只有在文档与缓存中现有的副本不同时，才回送对象主体。通过这种方式，将新鲜度检测和对象获取结合成了单个条件GET。向GET请求报文中添加一些特殊的条件首部，就可以发起条件GET。只有条件为真时，Web服务器才会返回对象。\nIf-Modified-Since:Date\nIf-Modified-Since首部可以与Last-Modified服务器响应首部配合工作。原始服务器会将最后的修改日期附加到所提供的文档上去。当缓存要对已缓存文档进行再验证时，就会包含一个If-Modified-Since首部，其中携带有最后修改已缓存副本的日期\n如果在此期间内容被修改了，最后的修改日期就会有所不同，原始服务器就会回送新的文档。否则，服务器会注意到缓存的最后修改日期与服务器文档当前的最后修改日期相符，会返回一个304 NotModified响应。\n自指定日期后，文档被修改了，为 true\n自指定日期后，文档没被修改过，为 false\n是否命中\n再验证命中\n服务器返回一个小的HTTP 304 Not Modified响应 再验证未命中\nHTTP 200 OK响应 对象被删除\n404 Not Found响应 无法使用该类型验证得场景\n内容不变，时间变 不重要得内容变化 无法得到修改时间 亚秒间隙发生变化，秒级粒度不够 If-None-Match：实体标签再验证\nHTTP允许用户对被称为实体标签（ETag）的“版本标识符”进行比较。实体标签是附加到文档上的任意标签（引用字符串）。它们可能包含了文档的序列号或版本名，或者是文档内容的校验和及其他指纹信息。当发布者对文档进行修改时，可以修改文档的实体标签来说明这个新的版本。这样，如果实体标签被修改了，缓存就可以用If-None-Match条件首部来GET文档的新副本了\n缓存中已有某些副本 If-None-Match:\u0026ldquo;v2.4,v2.5\u0026rdquo;\n强弱验证器\n弱: 改变不重要得内容时，缓存不失效 强: 任何改变都会失效 服务器会用前缀“W/”来标识弱验证器\nETag: W/\u0026ldquo;v2.6\u0026rdquo;\nIf-None-Match: W/\u0026ldquo;v2.6\u0026rdquo; 选择\n服务器只回送了一个Last-Modified，客户端就可以使用If-Modified-Since验证\n实体标签和最后修改日期都提供了，客户端就应该同时使用这两种再验证方案\n只有这两个条件都满足时，才能返回304 Not Modified响应 控制缓存的能力 首部 Cache-Control:max-age (推荐)\nmax-age=600 最大生存时间为600秒 Expires首部\n指定绝对日期，依赖机器本身时间 XMind: ZEN - Trial Version\n''\nSLL 握手\n服务器证书\n代理\n","date":"0001-01-01T00:00:00Z","permalink":"https://dccmmtop.github.io/posts/http%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/","section":"posts","tags":null,"title":""},{"categories":null,"contents":"java知识点 java 变量初始化顺序 总结一下对象的创建过程,假设有个名为Dog的类\r1.即使没有显式地使用 static关键字,构造器实际上也是静态方法。因此,当首次创建类型为Dog的对象时(构造器可以看成静态方法),或者Dog类的静态方法/静态域首次被访问时,\nava解释器必须査找类路径,以定位 Dog. class文件。\n2然后载入 Dog.class(后面会学到,这将创建一个Clas对象),有关静态初始化的所有动作都会执行。因此,静态初始化只在 Class对象首次加载的时候进行一次\n3.当用 new Dog0创建对象的时候,首先将在堆上为Dog对象分配足够的存储空间。\n4.这块存储空间会被清零,这就自动地将Dog对象中的所有基本类型数据都设置成了默认值 (对数字来说就是0,对布尔型和字符型也相同),而引用则被设置成了nul\n5执行所有出现于字段定义处的初始化动作。\n6.执行构造器。正如将在第7章所看到的,这可能会牵涉到很多动作,尤其是涉及继承的\n时候。\n","date":"0001-01-01T00:00:00Z","permalink":"https://dccmmtop.github.io/posts/java%E7%9F%A5%E8%AF%86%E7%82%B9/","section":"posts","tags":null,"title":""},{"categories":null,"contents":"缓存 2.2 使用 Redis 缓存\n注意：@Cacheable / @CachePut /@CacheEvict 注解需使用在 public 修饰的方法上\n2.2.2 缓存注解使用\n@Cacheable\n@Cacheable(value = \u0026#34;test\u0026#34;, key = \u0026#34;#msg\u0026#34;) public String put(String msg) { return msg; } 使用 @Cacheable 注解，会以 value::key 的形式作为 key，将方法的返回值做为 value，进行缓存。例如示例中传入的参数 msg 为 “ABC”，则 Redis 缓存中对应的 key-value 为 test::ABC - ABC.\n若在其它方法中需要从缓存中获取对应的数据信息，可以通过以下方式获取：\n@Autowired private StringRedisTemplate stringRedisTemplate; String value= stringRedisTemplate.opsForValue().get(key); // 如 test:ABC 如果存在多个参数，且要求多个参数同时做为key 时，参考如下配置，此时只有 id,name,age 同时一致时，才会从 Redis 获取数据\n@Cacheable(value = \u0026#34;test\u0026#34;, key = \u0026#34;#id + #name + #age\u0026#34;) public String testCache(String id, String name, String age) { System.out.println(\u0026#34;对数据 【\u0026#34; + id + name + age + \u0026#34;】 执行本地缓存\u0026#34;); return \u0026#34;nativeCache\u0026#34; + id + name + age; } @CacheaPut\n使用该注解，则不会从缓存中获取对应的数据，而是每次都执行相应的代码，并将执行结果存入指定的缓存。\n@CacheaEvict\n使用该注解，会清除对应的缓存信息。\n注：这里只是简单的描述了使用方法，更加详细的使用方法，可以自行百度。\n","date":"0001-01-01T00:00:00Z","permalink":"https://dccmmtop.github.io/posts/%E7%BC%93%E5%AD%98/","section":"posts","tags":null,"title":""},{"categories":null,"contents":"","date":"0001-01-01T00:00:00Z","permalink":"https://dccmmtop.github.io/search/","section":"","tags":null,"title":"Search Results"},{"categories":null,"contents":" 个人简介：喜爱编程 擅长 Java Ruby Golang Linux 对 Vim 情有独钟\n邮箱：dccmmtop@foxmail.com\nCSDN: https://blog.csdn.net/a141210104/\n要坚强\n你要克服懒惰\r你要克服游手好闲\r你要克服漫长的白日梦\r你要克服一蹴而就的妄想\r你要克服自以为是浅薄的幽默感\r你要独立生长在这世上\r不寻找不依靠\r因为冷漠寡情的人孤独一生\r你要坚强，振作，自立\r不能软弱，逃避，害怕\r不要沉溺在消极负面得情绪里\r要正面阳光得对待生活和爱你的人\r","date":"0001-01-01T00:00:00Z","permalink":"https://dccmmtop.github.io/about/","section":"","tags":null,"title":"你好"}]