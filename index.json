[{"categories":null,"contents":"从直觉上来看,一说到集群，我们就联想到高可用，一个节点宕机了，不会影响整体服务，客户端会从其他节点拿数据。比如 ES, Redis, mongoDB 等的集群都是符合直觉的架构。\n但是，事情到了rabbitMQ 这里，却完全不一样了。\n在将两个节点组成集群的那一刻，事情发生了巨大的变化：不是每一个节点都有所有队列的完全拷贝。\n在单一节点设置中，所有关于队列的信息（元数据、状态和内容）都完全存储在该节点上。但是如果在集群中创建队列的话，集群只会在单个节点而不是在所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息。所有其他非所有者节点只知道队列的元数据和指向该队列存在的那个节点的指针。因此当集群节点崩溃时，该节点的队列和关联的绑定就都消失了。附加在那些队列上的消费者丢失了其订阅信息，并且任何匹配该队列绑定信息的新消息也都丢失了。\n宕机后的队列 不要担心， 节点宕机后，我们可以让消费者重新连接到集群中并重新创建Q队列， 如果队列Q再一开始就是内存级别的，也就说我们可以承受节点宕机后，队列内容丢失的风险，重新创建一个全新的队列也没有什么不可。\n但是如果队列Q一开始是被设置成了持久化的呢？我们希望队列中的数据更可靠，不能重启节点就丢失了。所以如果允许重新创建队列，不就是把之前的Q队列给覆盖了吗？ rabbit 已经考虑这种情况， 所以在节点恢复之前， 不允许在集群中重新声明同名队列了， 如果非要这么做的话，你会得到一个 404 NOT_FOUND 的错误。除非将节点恢复，这种机制保证了持久化的队列中的消息不会丢失。\n产生上面现象的 “罪魁祸首” 就是因为，默认情况下，队列只存在单一节点上而不是每个节点丢复制一份呢？\nrabbit 这样设计肯定时有道理的嘛！但是为什么呢？\n一切为了性能 （1）存储空间——如果每个集群节点都拥有所有队列的完整拷贝，那么添加新的节点不会给你带来更多存储空间。举个例子，如果一个节点可以存储1GB的消息，那么添加两个节点只会给你带来两个一模一样的1GB消息的拷贝。\n（2）性能——消息的发布需要将消息复制到每一个集群节点。对于持久化消息来说，每一条消息都会触发磁盘活动。每次新增节点，网络和磁盘负载都会增加，最终只能保持集群性能的平稳（甚至更糟）。\n通过设置集群中的唯一节点来负责任何特定队列，只有该负责节点才会因队列消息而遭受磁盘活动的影响。所有其他节点只需要将接收到的该队列的消息传递给该队列的所有者节点。因此，往Rabbit集群添加更多的节点意味着你将拥有更多的队列，这些新增节点为你带来了性能的提升。当负载增加时，RabbitMQ集群是性能扩展的最佳方案。\n","date":"2022-07-07T00:22:11Z","permalink":"https://dccmmtop.github.io/posts/%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E9%98%9F%E5%88%97/","section":"posts","tags":["rabbitMQ"],"title":"集群中的队列"},{"categories":null,"contents":"本文采用docker-compose 的方式部署 rabbitMQ集群\ndocker-compose.yml version: \u0026#34;3\u0026#34; services: rabbitmq: restart: always container_name: rabbitMQ image: rabbitmq:3.7.7-management # 注意 hostname（本地域名）, rabbitMQ集群之间的通信就是靠此寻址的。物理机部署时也可以使用固定IP hostname: rabbitmq environment: RABBITMQ_NODE_PORT: 5672 # 指定虚拟主机的名称 RABBITMQ_DEFAULT_VHOST: my_vhost RABBITMQ_DEFAULT_USER: admin RABBITMQ_DEFAULT_PASS: admin # 指定 erlang_cookie, 集群中的所有节点的值必须保持一致， erlang 使用该值作为通信的密钥 RABBITMQ_ERLANG_COOKIE: rabbitmq_erlang_cookie ports: - 15672:15672 volumes: - ./data/rabbit:/var/lib/rabbitmq - ./conf:/etc/rabbitmq rabbitmq1: restart: always container_name: rabbitMQ_1 image: rabbitmq:3.7.7-management hostname: rabbitmq1 environment: RABBITMQ_NODE_PORT: 5672 RABBITMQ_DEFAULT_VHOST: my_vhost RABBITMQ_DEFAULT_USER: admin RABBITMQ_DEFAULT_PASS: admin RABBITMQ_ERLANG_COOKIE: rabbitmq_erlang_cookie ports: - 15673:15672 volumes: - ./data/rabbit_1:/var/lib/rabbitmq - ./conf:/etc/rabbitmq rabbitmq2: restart: always container_name: rabbitMQ_2 image: rabbitmq:3.7.7-management hostname: rabbitmq2 environment: RABBITMQ_NODE_PORT: 5672 RABBITMQ_DEFAULT_VHOST: my_vhost RABBITMQ_DEFAULT_USER: admin RABBITMQ_DEFAULT_PASS: admin RABBITMQ_ERLANG_COOKIE: rabbitmq_erlang_cookie ports: - 15674:15672 volumes: - ./data/rabbit_2:/var/lib/rabbitmq - ./conf:/etc/rabbitmq 集群节点之间如何通信 如上配置所示，我们并没有像之前的服务那样，使用 link 明确的标识与其他服务的链接关系。那么这是如何让 3 个节点互相通信的呢？\n原来docker 1.0 版本之后，会内置一个域名服务器，可以进入容器内部查看 /etc/reslv.conf 文件，会发现 nameserver 127.0.0.11 配置。 这就是默认域名解析服务器的地址。我们在配置文件中指定了 hostname 的值,在容器启动的时候，就会向域名服务器注册信息，域名服务器会记域名与该机器ip的对应关系，后面我们在容器中使用 hostname 去访问其他服务时，便会通过域名服务器找到对应的IP是什么，从而访问该服务\n目录结构 如上配置所示， 分别使用 ./data/rabbit ./data/rabbit_1 ./data/rabbit_2 作为每个节点的数据卷 使用 ./conf 作为配置文件的卷 所以有如下目录结构\n其中 rabbitmq.conf 配置如下\nloopback_users.guest = false\rlisteners.tcp.default = 5672\rdefault_pass = admin\rdefault_user = admin\rdefault_vhost = my_vhost\rhipe_compile = false\rmanagement.listener.port = 15672\rmanagement.listener.ssl = false\r# 下面配置针对 rabbitmq-mqtt 插件, 没有开启mqtt插件的，忽略\rmqtt.default_user = mqtt_user\rmqtt.default_pass = Aa111111\rmqtt.allow_anonymous = true\rmqtt.vhost = my_vhost enabled_plugins 配置如下\n[rabbitmq_management,rabbitmq_mqtt,rabbitmq_web_mqtt]. 开启了 web 端管理界面，mqtt 、 mqtt_web 插件\n启动容器 检查配置正确后，启动容器\ndocker-compose up -d 集群配置 可以分别进入三个容器内部检查容器日志，确定全部启动成功后，进入 rabbitMQ_1 容器中，执行\nrabbitmqctl stop_app 该命令会停止当前容器中的rabbitMQ 服务，rabbitMQ 是使用 erlang 语言编写的， rabbitMQ 自然要运行在 erlang 虚拟中，就像 java class 运行在 jvm 虚拟机中一样的道理。 所以上面的命令只是停止 rabbitMQ服务，erlang 虚拟机仍然在运行着。\n现在我们将 rabbitMQ 作为主节点， rabbitMQ_1, rabbitMQ_2 作为从节点，把 rabbtitMQ_1 加入 rabbitMQ 中:\nrabbitmqctl join_cluster rabbit@rabbitmq # rabbitmq 就是你的域名或ip地址, 前面的 rabbit@ 是固定写法 等待命令成功执行后，再执行如下命令启动 rabbit 服务\nrabbitmqctl start_app 然后可以执行 rabbitmqctl cluser_status 查看节点运行状态， rabbitMQ_2 容器中重复上述步骤即可\n集群状态如下:\n可以看到集群中已经由三个磁盘节点了。\n至于为什么叫磁盘节点，先按下不表，下篇文章再探讨， 下面我们先看一下如何让一个节点从集群中全身而退。\n退出集群 在节点上执行如下命令，即可安全的让节点退出集群\nrabbitmqctl stop_app rabbitmqctl reset rabbitmqctl start_app 这里的关键命令是 rabbitmqctl reset, 此命令清空节点状态，并将其恢复到空白状态，执行此命令时，节点会和集群中其他节点通信，告诉它们，我马上要退出集群了，这一步非常重要，不然其他节点以为当前节点是因为故障而断开的，并期望某一天一定会再回来的，同时阻止其他新的节点加入集群，直到该节点恢复。 所以当要离开集群时，务必先要重设节点状态!!!\n如下:\n从集群移除后，可以看到该节点已经称为独立节点了\n","date":"2022-07-04T22:27:10Z","permalink":"https://dccmmtop.github.io/posts/rabbitmq%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","section":"posts","tags":["rabbitMQ"],"title":"RabbitMQ集群部署"},{"categories":null,"contents":"生产者在某个队列上等待消费者返回的消息， 这个队列被称为应答队列,RabbitMQ 天生支持应答队列的机制。\n什么时候需要应答队列 当一条消息被消费者处理后，需要告知生产者一些信息时，消费者就需要在应答队列上发布一条应答消息。当应答消息被生产者处理后，应答队列就应该被删除 这时，生产者就像客户端的角色，而消费者就像服务器的角色。由此可知，应答队列应该与消息一对一存在，并且队列名称唯一，应答消息被处理后，队列就应该删除。很棒的是，RabbitMQ的匿名队列很好的满足了这个特性.\n上代码 客户端（生产者） 服务端(消费者) 从上代码中可以看出，利用应答队列是不是也可以实现 HTTP 服务器的功能，客户端发起请求， 服务器处理请求，返回数据，客户端处理返回数据\n我们知道，RabbitMQ中的队列要绑定到交换器上，才能通过路由将消息正确的投递到队列上，但是上面的服务端发送应答消息时，将消息头中的应答队列设置为消息的路由key，并没有绑定的动作，为什么也行得通呢？\n其实，不同于通过RabbitMQ发布的任何其他消息，这里没有交换器。这是关于通过Rabbit来实现RPC通信的唯一两处特别的地方：使用reply_to作为发布应答消息的目的地，同时发布的时候无须指定交换器\n","date":"2022-06-27T22:57:24Z","permalink":"https://dccmmtop.github.io/posts/%E5%BA%94%E7%AD%94%E9%98%9F%E5%88%97/","section":"posts","tags":["rabbitMQ"],"title":"RabbitMQ应答队列"},{"categories":null,"contents":"安装powershell 下载地址\n安装scoop 打开powershell 执行\n修改策略 set-executionpolicy remotesigned -s cu 安装scoop iex (new-object net.webclient).downloadstring('https://get.scoop.sh') 自动补全 PSReadLine 在 V5 或以上版本中自带\n命令 $profile 可见看见配置文件的路径，如果没有此文件，新建即可\n打配置文件 notepad $profile,输入一下内容\nImport-Module PSReadLine # Shows navigable menu of all options when hitting Tab Set-PSReadLineKeyHandler -Key Tab -Function MenuComplete # Autocompleteion for Arrow keys Set-PSReadLineOption -HistorySearchCursorMovesToEnd Set-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackward Set-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForward Set-PSReadLineOption -ShowToolTips Set-PSReadLineOption -PredictionSource History #Set the color for Prediction (auto-suggestion) Set-PSReadLineOption -Colors @{ Command = \u0026#39;Magenta\u0026#39; Number = \u0026#39;DarkBlue\u0026#39; Member = \u0026#39;DarkBlue\u0026#39; Operator = \u0026#39;DarkBlue\u0026#39; Type = \u0026#39;DarkBlue\u0026#39; Variable = \u0026#39;DarkGreen\u0026#39; Parameter = \u0026#39;DarkGreen\u0026#39; ContinuationPrompt = \u0026#39;DarkBlue\u0026#39; Default = \u0026#39;DarkBlue\u0026#39; InlinePrediction = \u0026#39;DarkGray\u0026#39; } oh-my-posh Oh My Posh是一个定制的提示引擎，适用于任何能够使用函数或变量调整提示字符串的shell。\n安装\nscoop install oh-my-posh\n配置\n在配置文件 $profile 输入：\noh-my-posh init pwsh --config ~\\scoop\\apps\\oh-my-posh\\current\\themes\\robbyrussel.omp.json | Invoke-Expression Get-ChildItemColor 安装\nInstall-Module -AllowClobber Get-ChildItemColor -Scope CurrentUser\n配置:\n在配置文件 $profile 输入：\nImport-Module Get-ChildItemColor\n完整配置 oh-my-posh init pwsh --config C:\\Users\\Administrator\\scoop\\apps\\oh-my-posh\\current\\themes\\robbyrussel.omp.json | Invoke-Expression Import-Module PSReadLine # Shows navigable menu of all options when hitting Tab Set-PSReadLineKeyHandler -Key Tab -Function MenuComplete # Autocompleteion for Arrow keys Set-PSReadLineOption -HistorySearchCursorMovesToEnd Set-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackward Set-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForward Set-PSReadLineOption -ShowToolTips Set-PSReadLineOption -PredictionSource History #Set the color for Prediction (auto-suggestion) Set-PSReadLineOption -Colors @{ Command = \u0026#39;Magenta\u0026#39; Number = \u0026#39;DarkBlue\u0026#39; Member = \u0026#39;DarkBlue\u0026#39; Operator = \u0026#39;DarkBlue\u0026#39; Type = \u0026#39;DarkBlue\u0026#39; Variable = \u0026#39;DarkGreen\u0026#39; Parameter = \u0026#39;DarkGreen\u0026#39; ContinuationPrompt = \u0026#39;DarkBlue\u0026#39; Default = \u0026#39;DarkBlue\u0026#39; InlinePrediction = \u0026#39;DarkGray\u0026#39; } Import-Module Get-ChildItemColor #Set-Alias ll Get-ChildItem -option AllScope #Set-Alias ls Get-ChildItemColorFormatWide -option AllScope Function vim {F:\\Neovim\\bin\\nvim.exe $args} ","date":"2022-05-13T17:28:48Z","permalink":"https://dccmmtop.github.io/posts/powershell%E9%85%8D%E7%BD%AE/","section":"posts","tags":["powershell"],"title":"powershell配置"},{"categories":null,"contents":"Shell学习笔记：awk实现group by分组统计功能 日常部分数据以 txt 的文件格式提供，为避免入库之后再进行统计的麻烦，故学习 shell 进行处理，减少工作量。\n1.样例数据\ntest.txt YD5Gxxx|6618151|6825449073|6476534190|36251|超级会员|0 YD5Gxxx|8968336|1445546463|6476534190|36251|超级会员|0 YD5Gxxx|2545939|6904742993|0858636804|36251|超级会员|80%以上 YD5Gxxx|3200810|6896525523|6501574903|36251|普通|0 YD5Gxxx|3378244|6926264463|6519442719|36251|超级会员|80%以上 YD5Gxxx|8075700|6854827783|0858523344|36251|普通|80%以上 YD5Gxxx|3368804|6934387193|0000487348|36251|超级会员|(0，50%] YD5Gxxx|2865288|6865082233|0859114957|36251|普通|(0，50%] YD5Gxxx|6655543|6930124273|6521876215|36251|超级会员|(0，50%] YD5Gxxx|2952781|6820973583|0858704189|36251|超级会员|0 每5行切分为一个文件 通过 split -l 对文件进行切分。\nsplit -l 5 super_user.txt\n分组统计 实现分组，count[$6]++ 实现计数。\nawk -F '|' '{count[$6]++;} END {for(i in count) {print i count[i]}}' test.txt\n普通3 超级会员7 根据第7列进行筛选之后，再按第6列进行分组统计。\nawk -F '|' '{if($7==\u0026quot;0\u0026quot;) {count[$6]++;}} END {for(i in count) {print i count[i]}}' test.txt\n普通1 超级会员3 分组求和 对所有进行求和。\nawk -F '|' '{sum += $2} END {print sum}' test.txt\n分组一般使用x[$2]=x[$3]的方式来实现，其中x[$2]中的$2为要分的组，可以多个分组，x[$3]为要处理的值。\n一次分组 awk -F '|' '{x[$6] += $2} END {for(i in x){print i, x[i]}}' test.txt\n普通 14141798 超级会员 34487798 二次分组 awk -F '|' '{x[$6\u0026quot;-\u0026quot;$7] += $2} END {for(i in x){print i, x[i]}}' test.txt\n超级会员-80%以上 5924183 超级会员-0 18539268 普通-(0，50%] 2865288 超级会员-(0，50%] 10024347 普通-0 3200810 普通-80%以上 8075700 格式化处理 分组求平均值 awk -F '|' '{sum += $2} END {print \u0026quot;Average = \u0026quot;, sum/NR}' test.txt\nAverage = 4.86296e+06 awk -F '|' '{a[$6] += $2; ca[$6]++} END {for(i in a){print(i,a[i]/ca[i])}}' test.txt\n普通 4.71393e+06 超级会员 4.92683e+06 分组求最大最小 awk -F '|' 'BEGIN {max=0} {if($2\u0026gt;max){max=$2}} END {print max}' test.txt\n最大值 awk -F '|' '{if($2\u0026gt;x[$6]){x[$6]=$2}} END {for(i in x) {print i, x[i]}}' test.txt\n普通 8075700 超级会员 8968336 分组整理字符 awk -F '|' '{x[$6]=x[$6]\u0026quot;\\n\u0026quot;$2} END {for(i in x){print i \u0026quot;:\u0026quot; x[i]}}' test.txt\n普通: 3200810 8075700 2865288 超级会员: 6618151 8968336 2545939 3378244 3368804 6655543 2952781 ","date":"2022-05-10T15:51:00Z","permalink":"https://dccmmtop.github.io/posts/awk%E5%88%86%E7%BB%84%E7%BB%9F%E8%AE%A1/","section":"posts","tags":null,"title":"awk分组统计"},{"categories":null,"contents":"OCP：开闭原则 软件的5大设计原则 “SOLID” 原则,其中之一就是 OCP(开闭原则)\n该设计原则是由Bertrand Meyer在20世纪80年代大力推广的，其核心要素是：如果软件系统想要更容易被改变，那么其设计就必须允许新增代码来修改系统行为，而非只能靠修改原来的代码。\n对原有代码改动越少，系统就越稳定。致力于实现增加新的功能只通过增加一个插件就可以实现\n案例 某程序想实现对输入参数的转换，参数目前是来自命令行，以后可能来自网页，或者从文本读取。如果把处理业务的逻辑与参数转换耦合到一起，那么以后修改参数来源时，势必会需改部分业务逻辑，造成系统的不稳定。本文将模仿 \u0026ldquo;database/sql\u0026rdquo; 的方式，只需更换不同的包，实现更换转换器的效果\n目录结构如下:\nmain.go\npackage main import ( \u0026#34;ocp/arg_parse\u0026#34; //安装命令行转换器 //_ \u0026#34;ocp/commandline_arg\u0026#34; // 或者安装网页参数转换器 _ \u0026#34;ocp/web_arg\u0026#34; \u0026#34;os\u0026#34; ) func main(){ // 从原代码层面看，就是通过接口进行参数转换。在运行时，一个具体的实现就注册到 Driver 上。 // 实则是通过该实现类的实例来完成转换的功能 arg_parse.Driver.ParseArg(os.Args) } parse.go\n该包中，大部分应该是接口，表明了各具体实现工具需要实现的一种方法，业务中通过调用接口的方式，最终调用具体实现。\n就像一个领导说，我公司有一个开发职位，该职位要求会实现某种转换参数的技能， 你们这些工人必须拥有这项技能，才能来我这里上班\n这里职位是虚拟的，职位本身不会干活，但是他要求了应聘该职位工人所拥有的技能\n最后领导会协调不同工种相互配合，完成整体任务，而不会关心具体执行任务的工人。\npackage arg_parse // 定义参数转换的接口，其他各种参数转换器都要实现本接口的功能。 // 具体的实现类的实例会赋值到 Driver，(一个职位) var Driver Parse type Args struct { Name string Age int Location string } // 待实现的方法(职位要求的技能) type Parse interface { ParseArg([] string) *Args } commandline_arg.go\n具体干活的工人1\npackage commandline_arg import ( \u0026#34;fmt\u0026#34; \u0026#34;ocp/arg_parse\u0026#34; ) type CommandlineParse struct {} func init(){ // 注册转换器, 该转换器拥有 parse 接口的方法， 就可以注册 (该工人应聘成功) fmt.Println(\u0026#34;注册 CommandlineParse\u0026#34;) arg_parse.Driver = \u0026amp;CommandlineParse{} } // 具体实现的功能， 实现了 ParseArg 方法 (该工人掌握的技能) func (c CommandlineParse) ParseArg(args []string) *arg_parse.Args{ fmt.Println(\u0026#34;我是命令行的参数转换器\u0026#34;) return \u0026amp;arg_parse.Args{} } web_arg.go\n具体干活的工人2\npackage web_arg import ( \u0026#34;fmt\u0026#34; \u0026#34;ocp/arg_parse\u0026#34; ) type WebParse struct {} func init(){ // 注册转换器, （工人应聘成功） fmt.Println(\u0026#34;注册 WebParse\u0026#34;) arg_parse.Driver = \u0026amp;WebParse{} } // 工人掌握的技能 func (c WebParse) ParseArg(args []string) *arg_parse.Args{ fmt.Println(\u0026#34;我是网页的参数转换器\u0026#34;) return \u0026amp;arg_parse.Args{} } ","date":"2022-05-08T22:06:51Z","permalink":"https://dccmmtop.github.io/posts/golnag%E6%8F%92%E4%BB%B6%E5%BC%8F%E5%BC%80%E5%8F%91%E7%9A%84%E4%B8%80%E7%A7%8D%E6%A1%88%E4%BE%8B/","section":"posts","tags":["go"],"title":"golnag插件式开发的一种案例"},{"categories":null,"contents":"想把自己写的一个控制台程序添加到右键，可以通过修改注册表的方式实现，但是修改起来比较麻烦，推荐使用右键管理器进行修改\n右侧的 ”文件“ ”文件夹“ ”目录“ 等，代表在该项右键时，右键菜单显示的内容\n点击 + 新建：\n点击确定\n由于是控制台程序不能直接运行，需要作为cmd.exe 参数来启动，如下:\ncmd.exe /c F:\\bin\\mix.exe push 另外，间接启动一个程序的时候也可以传入 /k 参数。与 /c 参数不同的是：\n/c 在执行完程序之后，cmd.exe 也会终止 /k 在执行完程序之后，cmd.exe 依然会继续运行 所以 /c 命令会更适用于自动化的脚本，而 /k 命令则更适用于半自动化的脚本。\n创建成功后可以修改命令以图标：\n效果：\n","date":"2022-05-08T16:39:06Z","permalink":"https://dccmmtop.github.io/posts/%E5%8F%B3%E9%94%AE%E6%B3%A8%E5%86%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E7%A8%8B%E5%BA%8F/","section":"posts","tags":["tools"],"title":"右键注册自定义程序"},{"categories":null,"contents":"软件架构这项工作的实质就是规划如何将系统切分成组件，并安排好组件之间的排列关系，以及组件之间互相通信的方式。 (拆分、组合、通信)\n目标 设计良好的架构可以让系统便于理解、易于修改、方便维护，并且能轻松部署。软件架构的终极目标就是最大化程序员的生产力，同时最小化系统的总运营成本。\n生命周期 设计良好架构的手段 始终保持多的可选项 软件架构师的目标是创建一种系统形态，该形态会以策略为最基本的元素，并让细节与策略脱离关系，以允许在具体决策过程中推迟或延迟与细节相关的内容。\n不应该过早的关注使用关系型，非关系型，还是分布式数据库 不应该过早的关注使用web服务，或者以某种形式发布（网页，app 等） 不应该确定是否使用 REST模式等 不应该过早的采用依赖注入框架 如果在开发高层策略时有意地让自己摆脱具体细节的纠缠，我们就可以将与具体实现相关的细节决策推迟或延后，因为越到项目的后期，我们就拥有越多的信息来做出合理的决策。\n组件的层次 离io越远，层次越高 我们对“层次”是严格按照“输入与输出之间的距离”来定义的。也就是说，一条策略距离系统的输入/输出越远，它所属的层次就越高。而直接管理输入/输出的策略在系统中的层次是最低的。\n","date":"2022-05-05T22:29:49Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E5%85%AB%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘八——软件架构"},{"categories":null,"contents":"SOLID 原则 SRP：单一职责原则。 该设计原则是基于康威定律（Conway\u0026rsquo;s Law）[1]的一个推论——一个软件系统的最佳结构高度依赖于开发这个系统的组织的内部结构。这样，每个软件模块都有且只有一个需要被改变的理由。\nOCP：开闭原则。 该设计原则是由Bertrand Meyer在20世纪80年代大力推广的，其核心要素是：如果软件系统想要更容易被改变，那么其设计就必须允许新增代码来修改系统行为，而非只能靠修改原来的代码。\nLSP：里氏替换原则。 该设计原则是Barbara Liskov在1988年提出的一个著名的子类型定义。简单来说，这项原则的意思是如果想用可替换的组件来构建软件系统，那么这些组件就必须遵守同一个约定，以便让这些组件可以相互替换。\nISP：接口隔离原则。 这项设计原则主要告诫软件设计师应该在设计中避免不必要的依赖。\n任何层次的软件设计如果依赖了它并不需要的东西，就会带来意料之外的麻烦。\nDIP：依赖反转原则。 该设计原则指出高层策略性的代码不应该依赖实现底层细节的代码，恰恰相反，那些实现底层细节的代码应该依赖高层策略性的代码。\n如果想要设计一个灵活的系统，在源代码层次的依赖关系中就应该多引用抽象类型，而非具体实现\n稳定的抽象层 我们每次修改抽象接口的时候，一定也会去修改对应的具体实现。但反过来，当我们修改具体实现时，却很少需要去修改相应的抽象接口。所以我们可以认为接口比实现更稳定。\n的确，优秀的软件设计师和架构师会花费很大精力来设计接口，以减少未来对其进行改动。毕竟争取在不修改接口的情况下为软件增加新的功能是软件设计的基础常识。\n应在代码中多使用抽象接口，尽量避免使用那些多变的具体实现类\n应避免在代码中写入与任何具体实现相关的名字，或者是其他容易变动的事物的名字。这基本上是DIP原则的另外一个表达方式。\n","date":"2022-05-05T10:45:55Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%B8%83%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘七——设计原则"},{"categories":null,"contents":"函数式编程所依赖的原理，在很多方面其实是早于编程本身出现的。因为函数式编程这种范式强烈依赖于Alonzo Church在20世纪30年代发明的λ演算。\n什么是函数式编程 举个例子：计算0-25范围内整数的平方\nJava版：\npublic class Squint { public static void main(String args[]){ for (int i=0; i＜25; i++) System.out.println(i*i); } } 下面我们改用Clojure语言来写这个程序，Clojure是LISP语言的一种衍生体，属于函数式编程语言。其代码如下：\n(println(take 25(map(fn[x](* x x))(range)))) 看不懂面的语言没有关系，我们讨论它的主要目标是要突显出Clojure和Java这两种语言之间的巨大区别。在Java程序中，我们使用的是可变量，即变量i，该变量的值会随着程序执行的过程而改变，故被称为循环控制变量。而Clojure程序中是不存在这种变量的，变量x一旦被初始化之后，就不会再被更改了。\n这句话有点出人意料：函数式编程语言中的变量（Variable）是不可变（Vary）的。\n不可变性与软件架构 为什么不可变性是软件架构设计需要考虑的重点呢？为什么软件架构师要操心变量的可变性呢？答案显而易见：所有的竞争问题、死锁问题、并发更新问题都是由可变变量导致的。如果变量永远不会被更改，那就不可能产生竞争或者并发更新问题。如果锁状态是不可变的，那就永远不会产生死锁问题。\n换句话说，一切并发应用遇到的问题，一切由于使用多线程、多处理器而引起的问题，如果没有可变变量的话都不可能发生。\n但是，能做到没有可变变量吗？即不可变性\n如果我们能忽略存储器与处理器在速度上的限制，那么答案是肯定的。否则的话，不可变性只有在一定情况下是可行的。\n可变性隔离 一种常见方式是将应用程序，或者是应用程序的内部服务进行切分，划分为可变的和不可变的两种组件。如下图：\n由于状态的修改会导致一系列并发问题的产生，所以我们通常会采用某种事务型内存来保护可变变量，避免同步更新和竞争状态的发生。\n要点是：一个架构设计良好的应用程序应该将状态修改的部分和不需要修改状态的部分隔离成单独的组件，然后用合适的机制来保护可变量。\n事件溯源 举个简单的例子，假设某个银行应用程序需要维护客户账户余额信息，当它执行存取款事务时，就要同时负责修改余额记录。\n如果我们不保存具体账户余额，仅仅保存事务日志，那么当有人想查询账户余额时，我们就将全部交易记录取出，并且每次都得从最开始到当下进行累计。当然，这样的设计就不需要维护任何可变变量了。\n但显而易见，这种实现是有些不合理的。因为随着时间的推移，事务的数目会无限制增长，每次处理总额所需要的处理能力很快就会变得不能接受。如果想使这种设计永远可行的话，我们将需要无限容量的存储，以及无限的处理能力。\n但是可能我们并不需要这个设计永远可行，而且可能在整个程序的生命周期内，我们有足够的存储和处理能力来满足它。\n这就是事件溯源，在这种体系下，我们只存储事务记录，不存储具体状态。当需要具体状态时，我们只要从头开始计算所有的事务即可。\n更重要的是，这种数据存储模式中不存在删除和更新的情况，我们的应用程序不是CRUD，而是CR。因为更新和删除这两种操作都不存在了，自然也就不存在并发问题。\n如果我们有足够大的存储量和处理能力，应用程序就可以用完全不可变的、纯函数式的方式来编程。\n","date":"2022-05-03T16:43:03Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E5%85%AD%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘六——函数式编程"},{"categories":null,"contents":"譬如封装（encapsulation）、继承（inheritance）、多态（polymorphism）。其隐含意思就是说面向对象编程是这三项的有机组合，或者任何一种支持面向对象的编程语言必须支持这三个特性。\n封装 通过采用封装特性，我们可以把一组相关联的数据和函数圈起来，使圈外面的代码只能看见部分函数，数据则完全不可见。\n而C++作为一种面向对象编程语言，反而破坏了C的完美封装性。\nC++通过在编程语言层面引入public、private、protected这些关键词，部分维护了封装性。但所有这些都是为了解决编译器自身的技术实现问题而引入的hack——编译器由于技术实现原因必须在头文件中看到成员变量的定义。\n而Java和C#则彻底抛弃了头文件与实现文件分离的编程方式，这其实进一步削弱了封装性。因为在这些语言中，我们是无法区分一个类的声明和定义的。 由于上述原因，我们很难说强封装是面向对象编程的必要条件。而事实上，有很多面向对象编程语言[3]对封装性并没有强制性的要求。 面向对象编程在应用上确实会要求程序员尽量避免破坏数据的封装性。但实际情况是，那些声称自己提供面向对象编程支持的编程语言，相对于C这种完美封装的语言而言，其封装性都被削弱了，而不是加强了。\n继承 既然面向对象编程语言并没有提供更好的封装性，那么在继承性方面又如何呢？\n嗯，其实也就一般般吧。简而言之，继承的主要作用是让我们可以在某个作用域内对外部定义的某一组变量与函数进行覆盖。这事实上也是C程序员[4]早在面向对象编程语言发明之前就一直在做的事了。\n虽然面向对象编程在继承性方面并没有开创出新，但是的确在数据结构的伪装性上提供了相当程度的便利性。\n回顾一下到目前为止的分析，面向对象编程在封装性上得0分，在继承性上勉强可以得0.5分（满分为1）。\n多态 在面向编程对象语言被发明之前，我们所使用的编程语言能支持多态吗？ 答案是肯定的，请注意看下面这段用C语言编写的copy程序：\n#include ＜stdio.h＞ void copy（）{ int c； while （（c=getchar（））!= EOF） putchar（c）； } 在上述程序中，函数getchar（）主要负责从STDIN中读取数据。但是STDIN究竟指代的是哪个设备呢？同样的道理，putchar（）主要负责将数据写入STDOUT，而STDOUT又指代的是哪个设备呢？很显然，这类函数其实就具有多态性，因为它们的行为依赖于STDIN和STDOUT的具体类型。\n这里的STDIN和STDOUT与Java中的接口类似，各种设备都有各自的实现。当然，这个C程序中是没有接口这个概念的，那么getchar（）这个调用的动作是如何真正传递到设备驱动程序中，从而读取到具体内容的呢？ 其实很简单，UNIX操作系统强制要求每个IO设备都要提供open、close、read、write和seek这5个标准函数。[6]也就是说，每个IO设备驱动程序对这5种函数的实现在函数调用上必须保持一致\ngetchar（）只是调用了STDIN所指向的FILE数据结构体中的read函数指针指向的函数。\n换句话说，getchar（）只是调用了STDIN所指向的FILE数据结构体中的read函数指针指向的函数。\n当然了，面向对象编程语言虽然在多态上并没有理论创新，但它们也确实让多态变得更安全、更便于使用了。 用函数指针显式实现多态的问题就在于函数指针的危险性。毕竟，函数指针的调用依赖于一系列需要人为遵守的约定。程序员必须严格按照固定约定来初始化函数指针，并同样严格地按照约定来调用这些指针。只要有一个程序员没有遵守这些约定，整个程序就会产生极其难以跟踪和消除的Bug。\n依赖反转 在安全和便利的多态支持出现之前，软件是什么样子的。下面有一个典型的调用树的例子，main函数调用了一些高层函数，这些高层函数又调用了一些中层函数，这些中层函数又继续调用了一些底层函数。在这里，源代码层面的依赖不可避免地要跟随程序的控制流。如下图\n如你所见，main函数为了调用高层函数，它就必须能够看到这个函数所在的模块。在C中，我们会通过#include来实现，在Java中则通过import来实现，而在C#中则用的是using语句。总之，每个函数的调用方都必须要引用被调用方所在的模块。 显然，这样做就导致了我们在软件架构上别无选择。在这里，系统行为决定了控制流，而控制流则决定了源代码依赖关系。\n但一旦我们使用了多态，情况就不一样了，如下图\n模块HL1调用了ML1模块中的F（）函数，这里的调用是通过源代码级别的接口来实现的。当然在程序实际运行时，接口这个概念是不存在的，HL1会调用ML1中的F（）函数[。\n请注意模块ML1和接口I在源代码上的依赖关系（或者叫继承关系），该关系的方向和控制流正好是相反的，我们称之为依赖反转\n当某个组件的源代码需要修改时，仅仅需要重新部署该组件，不需要更改其他组件，这就是独立部署能力。 如果系统中的所有组件都可以独立部署，那它们就可以由不同的团队并行开发，这就是所谓的独立开发能力。\n总结 面向对象编程到底是什么？业界在这个问题上存在着很多不同的说法和意见。然而对一个软件架构师来说，其含义应该是非常明确的：面向对象编程就是以多态为手段来对源代码中的依赖关系进行控制的能力，这种能力让软件架构师可以构建出某种插件式架构，让高层策略性组件与底层实现性组件相分离，底层组件可以被编译成插件，实现独立于高层组件的开发和部署。\n","date":"2022-05-02T23:29:24Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%BA%94%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘五——面向对象"},{"categories":null,"contents":"科学方法论不需要证明某条结论是正确的，只需要想办法证明它是错误的。如果某个结论经过一定的努力无法证伪，我们则认为它在当下是足够正确的。\n我们的程序也是如此,我们不能证明我们的代码是绝对正确的，而是做各种case覆盖证明它可能存在错误。如果没有，就认为相对稳定的\nDijkstra曾经说过“测试只能展示Bug的存在，并不能证明不存在Bug”，换句话说，一段程序可以由一个测试来证明其错误性，但是却不能被证明是正确的。测试的作用是让我们得出某段程序已经足够实现当前目标这一结论。\n这种证伪过程只能应用于可证明的程序上。某段程序如果是不可证明的，例如，其中采用了不加限制的goto语句，那么无论我们为它写多少测试，也不能够证明其正确性。\n结构化编程范式促使我们先将一段程序递归降解为一系列可证明的小函数，然后再编写相关的测试来试图证明这些函数是错误的。如果这些测试无法证伪这些函数，那么我们就可以认为这些函数是足够正确的，进而推导整个程序是正确的。\n结构化编程范式中最有价值的地方就是，它赋予了我们创造可证伪程序单元的能力。这就是为什么现代编程语言一般不支持无限制的goto语句。更重要的是，这也是为什么在架构设计领域，功能性降解拆分仍然是最佳实践之一。\n","date":"2022-05-02T23:13:43Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E5%9B%9B%E7%BB%93%E6%9E%84%E5%8C%96%E7%BC%96%E7%A8%8B/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘四——结构化编程"},{"categories":null,"contents":"编程范式指的是程序的编写模式，与具体的编程语言关系相对较小。这些范式会告诉你应该在什么时候采用什么样的代码结构。\n目前也只有三个编程范式：\n它们分别是结构化编程（structured programming）、面向对象编程（object-oriented programming）以及函数式编程（functional programming）。\n结构化编程 结构化编程是第一个普遍被采用的编程范式（但是却不是第一个被提出的），由Edsger Wybe Dijkstra于1968年最先提出。与此同时，Dijkstra还论证了使用goto这样的无限制跳转语句将会损害程序的整体结构。接下来的章节我们还会说到，也是这位 Dijkstra 最先主张用我们现在熟知的 if/then/else 语句和do/while/until语句来代替跳转语句的。\n我们可以将结构化编程范式归结为一句话： 结构化编程对程序控制权的直接转移进行了限制和规范。\n面向对象编程 就是面向对象编程了。事实上，这个编程范式的提出比结构化编程还早了两年，是在1966年由Ole Johan Dahl和Kriste Nygaard在论文中总结归纳出来的。这两个程序员注意到在ALGOL语言中，函数调用堆栈（call stack frame）可以被挪到堆内存区域里，这样函数定义的本地变量就可以在函数返回之后继续存在。这个函数就成为了一个类（class）的构造函数，而它所定义的本地变量就是类的成员变量，构造函数定义的嵌套函数就成为了成员方法（method）。这样一来，我们就可以利用多态（polymorphism）来限制用户对函数指针的使用。\n在这里，我们也可以用一句话来总结面向对象编程： 面向对象编程对程序控制权的间接转移进行了限制和规范。\n函数式编程 尽管第三个编程范式是近些年才刚刚开始被采用的，但它其实是三个范式中最先被发明的。事实上，函数式编程概念是基于与阿兰·图灵同时代的数学家Alonzo Church在1936年发明的λ演算的直接衍生物。1958年John Mccarthy利用其作为基础发明了LISP语言。众所周知，λ演算法的一个核心思想是不可变性——某个符号所对应的值是永远不变的，所以从理论上来说，函数式编程语言中应该是没有赋值语句的。大部分函数式编程语言只允许在非常严格的限制条件下，才可以更改某个变量的值。\n因此，我们在这里可以将函数式编程范式总结为下面这句话： 函数式编程对程序中的赋值进行了限制和规范。\n它们都从某一方面限制和规范了程序员的能力。没有一个范式是增加新能力的。也就是说，每个编程范式的目的都是设置限制。这些范式主要是为了告诉我们不能做什么，而不是可以做什么\n","date":"2022-05-02T22:52:25Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%B8%89%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘三——编程范式"},{"categories":null,"contents":"对于每个软件系统，我们都可以通过行为和架构两个维度来体现它的实际价值。软件研发人员应该确保自己的系统在这两个维度上的实际价值都能长时间维持在很高的状态\n大部分程序员认为这就是他们的全部工作。他们的工作是且仅是：按照需求文档编写代码，并且修复任何Bug。这真是大错特错。\n软件系统必须保持灵活。软件发明的目的，就是让我们可以以一种灵活的方式来改变机器的工作行为。对机器上那些很难改变的工作行为，我们通常称之为硬件（hardware）。\n变更实施的难度应该和变更的范畴（scope）成等比关系，而与变更的具体形状（shape）无关。\n如果系统的架构设计偏向某种特定的“形状”，那么新的变更就会越来越难以实施。所以，好的系统架构设计应该尽可能做到与“形状”无关。\n如果你问业务部门，是否想要能够变更需求，他们的回答一般是肯定的，而且他们会增加一句：完成现在的功能比实现未来的灵活度更重要。但讽刺的是，如果事后业务部门提出了一项需求，而你的预估工作量大大超出他们的预期，这帮家伙通常会对你放任系统混乱到无法变更的状态而勃然大怒。\n请记住：如果忽视软件架构的价值，系统将会变得越来越难以维护，终会有一天，系统将会变得再也无法修改。如果系统变成了这个样子，那么说明软件开发团队没有和需求方做足够的抗争，没有完成自己应尽的职责。\n","date":"2022-05-02T22:52:09Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%BA%8C/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘二"},{"categories":null,"contents":"架构是什么 按照Bob大叔的说法，所谓架构就是“用最小的人力成本来满足构建和维护系统需求”的设计行为。\n所谓软件架构，就是你希望在项目一开始就能做对，但是却不一定能够做得对的决策的集合。\n从人力成本的角度来定义架构，所以，架构是否合理，完全取决于是否节省人力，便于维护、扩展。\n第一性原理：技术是为商业价值和社会价值服务的。从这个原则出发，架构就是确保业务 短期快速发展、中期可延展、长期稳定，且低成本。在业务视角，对架构提出的要求就是“既要又要还要…” 架构，绝对不是“炫技”，现在大公司晋升汇报，都会需要讲技术能力，为了试点新技术，而强加给业务迭代，且往往业务和产品很难发现和知晓，这是非常本末倒置的行为。\n架构的衡量 一个软件架构的优劣，可以用它满足用户需求所需要的成本来衡量。如果该成本很低，并且在系统的整个生命周期内一直都能维持这样的低成本，那么这个系统的设计就是优良的。如果该系统的每次发布都会提升下一次变更的成本，那么这个设计就是不好的。就这么简单。\n现代的goto 大家对结构化编程的一般理解是，由if-else、switch-case之类的语句组织程序代码的编程方式，它杜绝了goto导致的混乱。但是从更深的层次上看，它也是一种设计范式，避免随意使用goto，使用if-else、switch-case之类控制语句和函数、子函数组织起来的程序代码，可以保证程序的结构是清楚的，自顶向下层层细化，消灭了杂错，杜绝了混淆。 联系如今的分布式系统，我们在设计的时候，真的能够做到自顶向下层层细化吗？有多少次，我看到的系统设计图里，根本没有“层次”的概念，各个模块没有一致的层次划分，与子系统交互的不是子系统，而是一盘散沙式的接口，甚至接口之间随意互调、关系乱成一团麻的情况也时常出现，带来的就是维护和调试的噩梦。吹散历史的迷雾，不正是古老的goto陷阱的再现吗？\n接口的设计 有多少次，我看到接口的设计非常随意，接口不是基于行为而是基于特定场景的实现，没有做适当的抽象，也没有为未来预留空间，直接导致契约僵硬死板。每新增一种终端呈现形式，整个内容生产流程就要大动干戈，这样的例子并不罕见。抹去历史的尘埃，这不正是“多态”出现之前的困境吗？\n接口定义需要基于行为不可以基于场景设计，否则接口会变得杂乱无章\n接口设计是站在自身角度，明确领域 应用定位，明确接口方法提供的能力，而不是基于需求方的需求（场景）\n软件架构是系统设计过程中的重要设计决定的集合，可以通过变更成本来衡量每个设计决定的重要程度。\n一个好的架构，不仅要在某一特定时刻满足软件用户、开发者和所有者的需求，更要在一段时间内持续满足他们的后续需求。如果你觉得好架构的成本太高，那你可以试试选择差的架构加上返工重来的成本。\n某个系统因为其组件错综复杂，相互耦合紧密，而导致不管多么小的改动都需要数周的恶战才能完成。又或是某个系统中到处充满了腐朽的设计和连篇累牍的恶心代码，处处都是障碍。再或者，你有没有见过哪个系统的设计如此之差，让整个团队的士气低落，用户天天痛苦，项目经理们手足无措？你有没有见过某个软件系统因其架构腐朽不堪，而导致团队流失，部门解散，甚至公司倒闭？作为一名程序员，你在编程时体会过那种生不如死的感觉吗？\n哈哈，深有体会，经历过这种痛苦之后，才会想着改变，想寻求更好的方法\n相信每个写代码的人都知道这些编码的准则，但是大部分人包括我自己在写代码的时候可能早就把这些代码准则抛之脑后了，可能因为各种原因，工期紧张或者就是自己懒惰的思想作怪，于是代码就越写越烂了，有一天连自己也懒得看这玩意了，当需求改动的时候就会变成所有人都痛苦的时候，所以在写代码的时候花一点时间是琢磨一下准备是非常有必要的，这些都是前人总结出来经验。\n","date":"2022-05-02T22:51:57Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%B6%E6%9E%84%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93%E4%B9%A6%E6%91%98%E4%B8%80/","section":"posts","tags":["架构"],"title":"架构整洁之道书摘一"},{"categories":null,"contents":"当查询命中缓存时，立即返回结果。跳过了解析 优化和执行阶段\n什么时候不会被缓存 查询涉及的相关表数据发生变化时 查询缓存系统会跟踪查询中涉及的每个表，如果这些表发生变化，那么和这个表相关的所有的缓存数据都将失效。这种机制效率看起来比较低，因为数据表变化时很有可能对应的查询结果并没有变更，但是这种简单实现代价很小，而这点对于一个非常繁忙的系统来说非常重要。\n查询语句任何细微变化时 MySql将查询结果存放在引用表中，通过一个哈希值引用，这个哈希值包含了如下因素，查询本身、要查询得数据库、客户端协议的版本等其他可能会影响返回结果的信息，\n判断缓存是否命中时，MySql不会解析、参数化、任何规整查询sql的操作，直接使用客户端发来的原始sql语句。任何字符上的不同，如空格、注释都会导致缓存不被命中。\n查询语句中有不确定数据时 包含 NOW() 、 CURRENT_DATE() 、CURRENT_USER() 、CONNECTION_ID() 等变化的信息 包含任何用户自定义函数，存储函数，用户变量，临时表，MySQL系统表 等 包含 子查询，存储过程（子查询的sql不是完整的，而是运行时被计算出来的） 查询结果太大 查询缓存内存用完 如果查询语句中包含任何的不确定函数，那么在查询缓存中是不可 能找到缓存结果的。因为即使之前刚刚执行了这样的查询，结果也不会放在查询缓存中。 MySQL在任何时候只要发现不能被缓存的部分，就会禁止这个查询被缓存。\n查询缓存的缺点 对读和写操作带来额外的消耗 读查询在开始之前必须先检查是否命中缓存 MySql执行完SQL时，如果该SQL可以缓存，但是此时还没被缓存，会将数据写入缓存中 影响写操作，执行写入操作时，将此表对应的所有缓存都设置失效。对查询缓存失效的操作是靠全局锁保护的。防止此时又被缓存了旧数据。所有与该表相关的查询都要等待该锁。无论此查询是否命中缓存，以及检测缓存是否失效。 如果缓存大，或者碎片很多，那么就会有很大的系统消耗。（设置了很大的查询缓存的时候） 事务对查询缓存的影响 对InnoDB用户来说，事务的一些特性会限制查询缓存的使用。当一个语句在事务中修改了某个表，MySQL会将这个表的对应的查询缓存都设置失效，而事实上，InnoDB的多版本特性会暂时将这个修改对其他事务屏蔽。在这个事务提交之前，这个表的相关查询是无法被缓存的，所以所有在这个表上面的查询一内部或外部的事务——都只能在该事务提交后才被缓存。因此，长时间运行的事务，会大大降低查询缓存的命中率。\n缓存对系统的影响 只有当缓存带来的资源节约大于其本身的资源消耗时才会给系统带来性能提升 适合做缓存的查询 汇总查询，如 count, max 等 复杂的查询，但结果少。如多表关联后需要分组，排序在分页的查询。同时涉及的表更新操作少于查询操作，防止缓存频发失效 命中率的计算 一个判断查询缓存是否有效的直接数据是命中率，就是使用查询缓存返回结果占总查询的比率。当MySQL接收到一个SELECT查询的时候，要么增加Qcache hits的值，要么增加Com select的值。所以查询缓存命中率可以由如下公式计算：Qcache hits / (Qcache hits+Com select)\n命中率低不代表性能提升少 不过，查询缓存命中率是一个很难判断的数值。命中率多大才是好的命中率？具体情况要具体分析。只要查询缓存带来的效率提升大于查询缓存带来的额外消耗，即使30%命中率对系统性能提升也有很大好处。另外，缓存了哪些查询也很重要，例如，被缓存的查询本身消耗非常巨大，那么即使缓存命中率非常低，也仍然会对系统性能提升有好处。所以，没有一个简单的规则可以判断查询缓存是否对系统有好处。\n命中和写入的比率 即Qcache hits和Qcache inserts的比值。根据经验来看，当这个比值大于3：1时通常查询缓存是有效的，不过这个比率最好能够达到10：1。如果你的应用没有达到这个比率，那么就可以考虑禁用查询缓存了，除非你能够通过精确的计算得知：命中带来的性能提升大于缓存失效的消耗，并且查询缓存并没有成为系统的瓶颈。\n缓存失效的一些指标检查 更新导致 可以通过参数Com*来查看数据修改的情况（包括Com update,Com delete,等等) 缓存空间不足 通过Qcache lowmem prunes来查看有多少次失效是由于内存不足导致的. 缓存的数据没有被查询 查看Com select和Qcache inserts的相对值。 如果每次查询操作都是缓存未命中，然后需要将查询结果放到缓存中，那么Qcache inserts的大小应该和Com select相当。所以在缓存完成预热后，我们总希 望看到Qcache inserts远远小于Com select。不过由于缓存和服务器内部的复杂和多样性，仍然很难说，这个比率是多少才是一个合适的值。 缓存空间的设置和使用 并非越大越好 每一个应用程序都会有一个“最大缓存空间”，甚至对一些纯读的应用来说也一样。最大缓存空间是能够缓存所有可能查询结果的缓存空间总和。理论上，对多数应用来说， 这个数值都会非常大。而实际上，由于缓存失效的原因，大多数应用最后使用的缓存空间都比预想的要小。即使你配置了足够大的缓存空间，由于不断地失效，导致缓存空间 一直都不会接近“最大缓存空间”\n设置一个合理值 通常可以通过观察查询缓存内存的实际使用情况，来确定是否需要缩小或者扩大查询缓存。如果查询缓存空间长时间都有剩余，那么建议缩小；如果经常由于空间不足而导致查询缓存失效，那么则需要增大查询缓存。不过需要注意，如果查询缓存达到了几十兆 这样的数量级，是有潜在危险的。（这和硬件以及系统压力大小有关)。\n查询缓存的一些配置参数 未完待续\n参考资料 《高性能MySql》 ","date":"2022-04-24T22:56:45Z","permalink":"https://dccmmtop.github.io/posts/mysql%E4%B8%AD%E7%9A%84%E6%9F%A5%E8%AF%A2%E7%BC%93%E5%AD%98/","section":"posts","tags":["mysql"],"title":"mysql中的查询缓存"},{"categories":null,"contents":"基准测试 http_load 可以通过一个输入文件提供多个 URL，Hpttp_load 在这些 URL 中随机选择进行测试。 也可以定制 axtp_1oad，使其按照时间比率进行测试，而不仅仅是测试最大请求处理\n下面通过一个简单的例子来演示如何使用 http_1oad。首先创建一个 urls.txt文件，输入 如下的 URL :\nhttp://ww.mysqlperformanceblog.com/\rhttp: //www.mysqlperformanceblog.com/page/2/\rhttp: //www.mysqlperformanceblog .com/mysql-patches/\rhttp: //www.mysqlperformanceblog . com/mysql-performance-presentations/\rhttp: //www.mysqlperformanceblog . com/2006/09/06/slow-query-log-analyzes-tools/ hitp_load 最简单的用法，就是循环请求给定的 URL 列表。测试程序将以最快的速度请 求这些 URL :\n$ http_load -parallel 1 -seconds 10 urls.txt\r19 fetches, 1 max parallel, 837929 bytes, in 10.0003 seconds\r44101.5 mean bytes/connection\r1.89995 fetches/sec, 83790.7 bytes/sec\rmsecs/connect: 41.6647 mean, 56.156 max, 38.21 min\rmsecs/first-response: 320.207 mean, 508.958 max, 179.308 min\rHTTP response codes:\rcode 200 - 19 测试的结果很容易理解，只是简单地输出了请求的统计信息。下面是另外一个稍微复杂 的测试，还是尽可能快地循环请求给定的 URL 列表，不过模拟同时有五个并发用户在 进行请求 ;\n$ http_load -parallel 5 -seconds 10 urls.txt\r94 fetches, 5 max parallel, 4.75565e+06 bytes, in 10.0005 seconds\r50592 mean bytes/connection\r9.39953 fetches/sec, 475541 bytes/sec\rmsecs/connect: 65.1983 mean, 169.991 max, 38.189 min\rmsecs/first-response: 245.014 mean, 993.059 max, 99.646 min\rHTTP response codes:\rcode 200 - 94 另外，除了测试最快的速度，也可以根据预估的访问请求率 〈比如每秒 5 次) 来做压力 模拟测试。\n$ http_load -rate 5 -seconds 10 urls.txt\r48 fetches, 4 max parallel, 2.50104e+06 bytes, in 10 seconds\r52105 mean bytes/connection\r4.8 fetches/sec, 250104 bytes/sec\rmsecs/connect: 42.5931 mean, 60.462 max, 38.117 min\rmsecs/first-response: 246.811 mean, 546.203 max, 108.363 min\rHTTP response codes:\rcode 200 - 48 最后，还可以模拟更大的负载，可以将访问请求率提高到每秒 20 次请求。请注意，连 接和请求响应时间都会随着负载的提高而增加。\n$ http_load -rate 20 -seconds 10 urls.txt\r111 fetches, 89 max parallel, 5.91142e+06 bytes, in 10.0001 seconds\r53256.1 mean bytes/connection\r11.0998 fetches/sec, 591134 bytes/sec\rmsecs/connect: 100.384 mean, 211.885 max, 38.214 min\rmsecs/first-response: 2163.51 mean, 7862.77 max, 933.708 min\rHTTP response codes:\rcode 200 -- 111 sysbench sysbench 的 CPU 基准测试\n最典型的子系统测试就是 CPU 基准测试。该测试使用 64 位整数，测试计算素数直到某 个最大值所需要的时间。 下面的例子将比较两台不同的GNU/Linux 服务器上的测试结果。 第一台机器的 CPU 配置如下 :\n[servert ~]$ cat /proc/cpuinfo\rmodel nane + AMD Opteron(tm) Processor 246\rstepping a\rcpu Miz + 192.857\rcache size: 1024 KB 这台服务器上运行如下的测试 :\n[serverl ~]$ sysbench -testccpu -cpu-max-prime=20000 run\rsysbench v0.4.8: multithreaded system evaluation benchnark\rTest execution summary: total tine: 121.74048 第二台服务器配置了不同的 CPU ，\n[server2 ~]$ cat /proc/cpuinfo\rmodel name: Intel(R) Xeon(R) CPU 5130 @ 2.00GH2\rstepping\rcpu Miz 测试结果如下 :\n[serverl ~]$ sysbench --test=cpu --cpu-max-prime=20000 run\rsysbench v0.4.8: multithreaded system evaluation benchnark\rTest execution summary: total time: 6.85965 测试的结果简单打印出了计算出素数的时间，很容易进行比较。在上面的测试中，第二 人台服务器的测试结果显示比第一台快两倍。\n","date":"2022-04-14T00:00:02Z","permalink":"https://dccmmtop.github.io/posts/%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/","section":"posts","tags":["mysql"],"title":"基准测试"},{"categories":null,"contents":"+++ hidden = true +++\n你要克服懒惰\n你要克服游手好闲\n你要克服漫长的白日梦\n你要克服一蹴而就的妄想\n你要克服自以为是浅薄的幽默感\n你要独立生长在这世上\n不寻找不依靠\n因为冷漠寡情的人孤独一生\n你要坚强,振作,自立\n不能软弱,逃避,害怕\n不要沉溺在消极负面得情绪里\n要正面阳光得对待生活和爱你的人\ndccmmtop@foxmail.com\n","date":"2022-04-13T23:42:17+08:00","permalink":"https://dccmmtop.github.io/about/","section":"","tags":null,"title":"你好!陌生人"},{"categories":null,"contents":"什么是字符集，什么是排序规则 字符集是只从二进制编码到某类字符符号的映射，该字符集的排序规则简称校对， 如 ASCII码，一个字节就可以表示一个英文字母，参照ASCII码表\n为什么需要字符集 因为人类无法直接理解二进制所表示的含义，需要转换\n多字节字符集 一个字符占用超过一个字节的字符集叫做多字节字符集 例如 UTF-8\n常见字符集 1，ASCII码：一个英文字母（不分大小写）占一个字节的空间，一个中文汉字占两个字节的空间。\n2，UTF-8编码：一个英文字符等于一个字节，一个中文（含繁体）等于三个字节。中文标点占三个字节，英文标点占一个字节\n3，Unicode编码：一个英文等于两个字节，一个中文（含繁体）等于两个字节。中文标点占两个字节，英文标点占两个字节\nMySQL 为什么提供那么多的字符集，为什么不统一使用UTF-8 为了性能\n在某些情况下可以节省空间，提高效率，例如某个字段只有阿拉伯语，直接使用 cp1256 字符集，该字符集只需一个字节就可以存下所有阿拉伯字符。 如果使用UTF-8字符集，会消耗更多的空间\nMySQL 是如何设置字符集的 创建对象时（建库、建表、建列） 库 \u0026gt; 表 \u0026gt; 列，每个范围上都有自己的字符集设置，采用的时继承关系。如果没有明确指定字符集，则继承上一级的。\n客户端和服务器通信时 服务器端总是假设客户端是按照character set client设置的字符来传输数据和SQL语句的。 当服务器收到客户端的SQL语句时，它先将其转换成字符集character_set_connection。它还使用这个设置来决定如何将数据转换成字符串。 当服务器端返回数据或者错误信息给客户端时，它会将其转换成character_set_result。 如图： MySQL 如何比较字符串大小 先将字符串转换成相同的字符集，如果字符集不兼容，会抛出错误，在MySQL 5.0 及以后，这个转换动作是自动进行的。\n字符集对查询的影响 可能无法使用索引排序，退化到文件排序 只有排序查询要求的字符集与保存到服务器上数据的字符集相同的时候，才会使用索引排序。 例子：film 表有字段 title, 使用的是 utf8_general_ci 排序规则。现有查询要求 title 字段按照 utf8_bin 排序 这中情况下无法使用索引排序，只能用 文件排序\n可能无法使用索引进行关联表 为了适应各种字符集，MySQL在查询的时候可能会进行字符集转换，例如，在使用不同字符集的列去关联两张表的时候，mysql会尝试转换其中一个列的字符集，这和在列外面封装一个函数一样，会让mysql无法使用该列上的索引\n对字符长度的处理 UTF-8 是一种多字节编码，一个字符所占的空间可能是 1、2、3个字节(不是固定的)。但是在MySQL内部，通常使用固定的空间来存储，这样做的目的是希望总是保证缓存中有足够的空间来存储字符串，例如一个 UTF-8的char(10)需要30个字节。 LENGTH() \u0026gt;= CHAR_LENGTH(), LENGTH() 计算字节长度， CHAR_LENGTH() 返回字符的个数\n","date":"2022-04-13T00:22:41Z","permalink":"https://dccmmtop.github.io/posts/mysql%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E9%9B%86/","section":"posts","tags":["mysql"],"title":"mysql中的字符集"},{"categories":null,"contents":"第一种 设置一个滑动窗口，左下标记 l， 右下标记r r 向右移动，记录每个字符的最后一次出现的位置 m 如果当前字符在 m 中存在,并且重复字符出现的位置在l右侧,让l移动到重复字符的下一个位置,跳过重复的字符 r每次移动时，计算r与l的距离,记录最大值\nfunc lengthOfLongestSubstring(s string) int { c := []rune(s) size := len(c) if size \u0026lt;= 1 { return size } l, r, maxLen, k := 0, 0, 0, 0 ok := false m := make(map[rune]int, size) for ; r \u0026lt; size; r++ { if k, ok = m[c[r]]; ok \u0026amp;\u0026amp; k \u0026gt;= l { l = k + 1 } if r-l+1 \u0026gt; maxLen { maxLen = r - l + 1 } m[c[r]] = r } return maxLen } 第二种 速度与第一种差不多，但内存占用少\nfunc lengthOfLongestSubstring(s string) int { c := []rune(s) size := len(c) if size \u0026lt;= 1 { return size } l, r, maxLen := 0, 0, 0 // 标记字符最后一次出现的位置,不可以使用默认0值，因为下标是从0开始的 m := [128]int{} for i := 0; i \u0026lt; 128; i++ { m[i] = -1 } for ; r \u0026lt; size; r++ { // 当出现重复字符时，左标记跳到该字符的下一个位置 l = max(l, m[c[r]]+1) // 记录窗口最大长度 maxLen = max(maxLen, r-l+1) // 记录字符出现的位置 m[c[r]] = r } return maxLen } func max(a int, b int) int { if a \u0026gt; b { return a } return b } ","date":"2022-04-08T00:04:33Z","permalink":"https://dccmmtop.github.io/posts/%E6%9C%80%E9%95%BF%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/","section":"posts","tags":["go","算法"],"title":"最长无重复子串"},{"categories":null,"contents":"InnoDB 是MySQL中唯一支持外键约束的内置引擎\n缺点 多一次查询 在每次修改数据时，都要在另外一张表执行一次查询操作，如果外键列的选择性很低，会导致存在一个很大但是选择性很低的索引。 比如在一个很大的users表中，有一个 status 列，该列只有2个值，虽然列本身很小，但是如果主键很大，那么这个索引就会很大，而这个索引除了做外键限制，也没有其他作用了\n外键维护效率低 在外键相关数据的更新和删除时，外键的维护时逐行进行的，比批量删除和批量更新效率低\n额外的锁等待 外键约束使查询需要访问一些额外的表，就意味着需要额外的锁：比如 学生表有一列存储班级的主键，当新增一条学生记录时，需要在对应班级的行加锁，防止在事务结束前，该班级被删除。这会导致额外的锁等待，甚至死锁。由于没有直接访问该表，往往难以排查该问题\n优点 保持数据一致性时，比在应用中做效率高 总结 有时可以使用触发器做外键约束，对于相关数据的同时更新，外键更合适，但是如果只对列做数值约束，使用触发器或者限制取值会更好， 如果只是做外键约束，在应用程序内做会更好，外键会带来很大的额外消耗，往往会成为性能瓶颈\n","date":"2022-04-06T23:09:21Z","permalink":"https://dccmmtop.github.io/posts/mysql%E5%A4%96%E9%94%AE%E7%BA%A6%E6%9D%9F/","section":"posts","tags":["mysql"],"title":"MySQL外键约束"},{"categories":null,"contents":"并发编程下，如何将goroutine中发生的错误传递给其他程序，从而进行优雅的处理呢， 一种解决方案是,将异步任务中产生的错误写入通道中，在另一个程序中读取该通道，从而实现通信，二次处理错误信息\n例子\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; ) // 错误信息的封装 type Result struct { Error error Response *http.Response } func main() { // 又不可达的链接，会触发错误 urls := []string{\u0026#34;http://10.102.49.2/web_client/#/main\u0026#34;, \u0026#34;http://10.102.204.36/console-acp/workspace/eop~region-k1~eop-uat8/deployment/detail/eop-dpl-cir-u6\u0026#34;, \u0026#34;http://123.com\u0026#34;} for result := range checkStatus(urls) { if result.Error != nil { // 在此进行错误处理 fmt.Printf(\u0026#34;Error: %v\\n\u0026#34;, result.Error) continue } fmt.Printf(\u0026#34;Response: %v\\n\u0026#34;, result.Response) } } // 将任务的处理结果放入通道中，并返回 func checkStatus(urls []string) \u0026lt;-chan Result { results := make(chan Result) go func() { defer close(results) var wg sync.WaitGroup for _, url := range urls { log.Println(\u0026#34;visist: \u0026#34;, url) wg.Add(1) go func(url string) { defer wg.Done() resp, err := http.Get(url) // 将结果写入通道 results \u0026lt;- Result{Error: err, Response: resp} }(url) } wg.Wait() }() return results } ","date":"2022-02-21T19:43:14Z","permalink":"https://dccmmtop.github.io/posts/%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E9%94%99%E8%AF%AF%E4%BC%A0%E9%80%92/","section":"posts","tags":["go"],"title":"异步任务中的错误传递"},{"categories":null,"contents":"一个通道的输出，作为下一个通道的输入，连绵不绝\n下面实现了加法 乘法的流水线\n// 流水线通道 package main import \u0026#34;fmt\u0026#34; func main() { done := make(chan interface{}) defer close(done) // 数据源 numStream := generate(done, 1, 2, 3, 4, 5) // 乘法 加法 乘法 pipeline := multi(done, add(done, multi(done, numStream, 2), 1), 2) for num := range pipeline { fmt.Println(num) } } // 接收一个中止信号，防止泄露 // 返回只读通道 func generate(done \u0026lt;-chan interface{}, num ...int) \u0026lt;-chan int { numStream := make(chan int) go func() { defer close(numStream) for _, i := range num { select { case \u0026lt;-done: return // 不断的向通道中写入数据 case numStream \u0026lt;- i: } } }() // 注意：隐式将通道转换成只读通道 return numStream } // 乘法器，从一个通道中接收数据，然后 乘以factor,将结果写入另一个通道中 //仍然要接收一个终止信号 func multi(done \u0026lt;-chan interface{}, numStream \u0026lt;-chan int, factor int) \u0026lt;-chan int { multiStream := make(chan int) go func() { defer close(multiStream) for i := range numStream { select { case \u0026lt;-done: return case multiStream \u0026lt;- (factor * i): } } }() return multiStream } // 加法器， 同上 func add(done \u0026lt;-chan interface{}, numStream \u0026lt;-chan int, factor int) \u0026lt;-chan int { addStream := make(chan int) go func() { defer close(addStream) for i := range numStream { select { case \u0026lt;-done: return case addStream \u0026lt;- (factor + i): } } }() return addStream } ","date":"2022-02-21T19:32:54Z","permalink":"https://dccmmtop.github.io/posts/%E9%80%9A%E9%81%93%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F/","section":"posts","tags":["go"],"title":"通道流水线工作模式"},{"categories":null,"contents":"goroutine 泄露 当 goroutine 被永远阻塞，或者只有主 goroutine 终止时，子 goroutine 才会终止， 即子goroutine 没有自行终止的时机 goroutine 便无法释放其所占的内存空间\n一般解决方案: 由父goroutine告知子goroutine终止时机\n准则: 父 goroutine 创建子 goroutine,那么父要确保子能够停止\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; ) func main() { done := make(chan interface{}) randStream := newRandStream(done) fmt.Println(\u0026#34;3 random ints:\u0026#34;) for i := 1; i \u0026lt;= 3; i++ { fmt.Printf(\u0026#34;%d: %d\\n\u0026#34;, i, \u0026lt;-randStream) } // 通知子goroutine停止 close(done) // 模拟正在运行的工作 time.Sleep(1 * time.Second) } // 生产者 // 只在生产者作用域内声明 chan, 并在内部进行写入逻辑，然后返回只读的通道, 防止在生产者外部向该通道中写入数据 // 维护了该通道的纯净 func newRandStream(done \u0026lt;-chan interface{}) \u0026lt;-chan int { randStream := make(chan int) go func() { // 当此goroutine结束时，打印。如果是main groutine 终止时，导致该goroutine终止，则不会打印 defer fmt.Println(\u0026#34;newRandStream closure existed\u0026#34;) defer close(randStream) for { select { case randStream \u0026lt;- rand.Int(): case \u0026lt;-done: // 接收到关闭信号,避免通道泄露 return } } }() // 有一个隐式转换，将可读可写的 randStream 转换成只读的 return randStream } ","date":"2022-02-21T19:23:11Z","permalink":"https://dccmmtop.github.io/posts/%E9%98%B2%E6%AD%A2goroutine%E6%B3%84%E9%9C%B2%E7%9A%84%E4%B8%80%E8%88%AC%E6%9C%BA%E5%88%B6/","section":"posts","tags":["go"],"title":"防止goroutine泄露的一般机制"},{"categories":null,"contents":"Go中的select和channel配合使用，通过select可以监听多个channel的I/O读写事件，当 IO操作发生时，触发相应的动作。\n基本使用 // 常规示例 func example() { done := make(chan interface{}) // 一段时间后发送关闭信号 go func() { time.Sleep(5 * time.Second) close(done) }() workCounter := 0 breakLoop := false for { select { case \u0026lt;-done: breakLoop = true default: } if breakLoop { break } workCounter++ time.Sleep(1 * time.Second) } fmt.Printf(\u0026#34;收到结束信号，任务执行了 %d 次\u0026#34;, workCounter) } 超时机制 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { ch := make(chan int) quit := make(chan bool) //新开一个协程 go func() { for { select { case num := \u0026lt;-ch: fmt.Println(\u0026#34;num = \u0026#34;, num) case \u0026lt;-time.After(3 * time.Second): fmt.Println(\u0026#34;超时\u0026#34;) quit \u0026lt;- true } } }() for i := 0; i \u0026lt; 5; i++ { ch \u0026lt;- i time.Sleep(time.Second) } // 收到超时信号，停止阻塞 \u0026lt;-quit fmt.Println(\u0026#34;程序结束\u0026#34;) } 最快返回 多个 goroutine 做同一件工作，取最快的返回结果\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/kirinlabs/HttpRequest\u0026#34; ) func main() { ch1 := make(chan int) ch2 := make(chan int) ch3 := make(chan int) go Getdata(\u0026#34;https://www.baidu.com\u0026#34;,ch1) go Getdata(\u0026#34;https://www.baidu.com\u0026#34;,ch2) go Getdata(\u0026#34;https://www.baidu.com\u0026#34;,ch3) select{ case v:=\u0026lt;- ch1: fmt.Println(v) case v:=\u0026lt;- ch2: fmt.Println(v) case v:=\u0026lt;- ch3: fmt.Println(v) } } func Getdata(url string,ch chan int){ req,err := HttpRequest.Get(url) if err != nil{ }else{ ch \u0026lt;- req.StatusCode() } } 死锁与默认情况 package main func main() { ch := make(chan string) select { case \u0026lt;-ch: } } 在第 4 行创建了一个信道 ch。我们在 select 内部（第 6 行），试图读取信道 ch。由于没有 Go 协程向该信道写入数据，因此 select 语句会一直阻塞，导致死锁。该程序会触发运行时 panic\n空select package main func main() { select {} } 除非有 case 执行，select 语句就会一直阻塞着。在这里，select 语句没有任何 case，因此它会一直阻塞，导致死锁。该程序会触发 panic\n","date":"2022-02-21T19:06:18Z","permalink":"https://dccmmtop.github.io/posts/select%E7%94%A8%E6%B3%95%E7%A4%BA%E4%BE%8B/","section":"posts","tags":["go"],"title":"select用法示例"},{"categories":null,"contents":"安装 samba yum install samba 修改配置文件 修改配置文件 vim /etc/samba/smb.conf 添加要共享的目录\n[opt] # 被共享目录的别名 path = /home/dccmmtop/opt # 要共享的目录 browseable = yes writable = yes # 可以写入 valid users = dccmmtop # 用户，该用户要有目录的权限 create mode=0777 directory mode=0777 force create mode=0777 force directory mode=0777 添加用户 将用户 dccmmtop 设置为可以登录 samba\nsmbpasswd -a dccmmtop // 设置 dccmmtop 的samba服务密码，可与用户密码相同 smbpasswd -e dccmmtop // 启用samba用户，显示Enable则成功 关闭网络防火墙或添加端口 sudo systemctl stop firewalld.service 或者开放端口\nsudo firewall-cmd --zone=public --add-port=139/tcp --permanent sudo firewall-cmd --zone=public --add-port=445/tcp --permanent sudo firewall-cmd --zone=public --add-port=137/udp --permanent sudo firewall-cmd --zone=public --add-port=138/udp --permanent sudo firewall-cmd --reload sudo systemctl restart firewalld.service 关闭SELinux 安全增强型Linux（SELinux）是一个Linux内核的功能，它提供支持访问控制的安全政策保护机制 打开 /etc/selinux/config ,将 SELINUX 设置为 disabled 重启生效\n临时修改 执行命令 setenforce 0 临时关闭SELinux\n运行命令getenforce，验证SELinux状态为disabled，表明SELinux已关闭。\n重启服务 systemctl restart smb.service // 不报错说明没问题 windows 挂载 开启windows SMB 服务，如下 映射网络位置 之后输入上面设置的用户名密码就可以了\n","date":"2022-01-19T11:27:26Z","permalink":"https://dccmmtop.github.io/posts/linux%E6%90%AD%E5%BB%BAsamba%E6%9C%8D%E5%8A%A1/","section":"posts","tags":["linux"],"title":"Linux搭建samba服务"},{"categories":null,"contents":"初始化项目 新建 myapp 目录，在下面添加 Dockerfile 文件，如下:\nDockerfile FROM ruby:2.5\rRUN apt-get update -qq \u0026amp;\u0026amp; apt-get install -y nodejs default-mysql-client\rADD . /myapp\rWORKDIR /myapp\rRUN bundle install\rEXPOSE 3000\rCMD [\u0026#34;bash\u0026#34;] Gemfile 再新建 Gemfile 文件\nsource \u0026#39;https://gems.ruby-china.com\u0026#39; # 安装 Rails gem \u0026#39;rails\u0026#39;, \u0026#39;~\u0026gt; 5.1.3\u0026#39; docker-compose.yml version: \u0026#39;3.3\u0026#39; # 使用已经存在的外部网络 networks: default: external: name: dev_network services: web: build: . command: bash -c \u0026#34;rm -f tmp/pids/server.pid \u0026amp;\u0026amp; bundle exec rails s -p 3000 -b \u0026#39;0.0.0.0\u0026#39;\u0026#34; volumes: - .:/myapp ports: - \u0026#34;3000:3000\u0026#34; # 链接已经存在的mysql external_links: - mysql:mysql 加入网络 加入已经存在的 dev_net 网络，以便访问 mysql 服务, 如果没有 dev_net,可以通过下面命令创建:\ndocker network create dev_net 在 mysql 的 docker-compose.yml 中同样添加下面内容，可以加入网络 dev_net\n# 使用已经存在的外部网络 networks: default: external: name: dev_network 生成项目骨架 docker-compose run web rails new . --force --database=mysql --skip-bundle Compose 会先使用 Dockerfile 为 web 服务创建一个镜像，接着使用这个镜像在容器里运行 rails new 和它之后的命令。一旦这个命令运行完后，应该就可以看一个崭新的应用已经生成了\n再次构建 新生成的 Gemfile 覆盖了原来gem源的配置, 修改 Gemfile 的源为 source 'https://gems.ruby-china.com', 另外可以根据需要添加 gem 由于 修改 Gemfile , 需要再次构建， 将 gem 包安装到镜像中，以后每次修改 Gemfile,都要重新构建。 或者可以将 gem 的安装目录通过添加卷的方式映射到本地。\ndocker-compose build 修改数据库配置 修改 config/database.yml\n# MySQL. Versions 5.1.10 and up are supported. # # Install the MySQL driver # gem install mysql2 # # Ensure the MySQL gem is defined in your Gemfile # gem \u0026#39;mysql2\u0026#39; # # And be sure to use new-style password hashing: # http://dev.mysql.com/doc/refman/5.7/en/old-client.html # default: \u0026amp;default adapter: mysql2 encoding: utf8 pool: \u0026lt;%= ENV.fetch(\u0026#34;RAILS_MAX_THREADS\u0026#34;) { 5 } %\u0026gt; username: root password: 123456 host: mysql development: \u0026lt;\u0026lt;: *default database: videobot # Warning: The database defined as \u0026#34;test\u0026#34; will be erased and # re-generated from your development database when you run \u0026#34;rake\u0026#34;. # Do not set this db to the same as development or production. test: \u0026lt;\u0026lt;: *default database: myapp_test # As with config/secrets.yml, you never want to store sensitive information, # like your database password, in your source code. If your source code is # ever seen by anyone, they now have access to your database. # # Instead, provide the password as a unix environment variable when you boot # the app. Read http://guides.rubyonrails.org/configuring.html#configuring-a-database # for a full rundown on how to provide these environment variables in a # production deployment. # # On Heroku and other platform providers, you may have a full connection URL # available as an environment variable. For example: # # DATABASE_URL=\u0026#34;mysql2://myuser:mypass@localhost/somedatabase\u0026#34; # # You can use this database configuration with: # # production: # url: \u0026lt;%= ENV[\u0026#39;DATABASE_URL\u0026#39;] %\u0026gt; # production: \u0026lt;\u0026lt;: *default database: myapp_production username: myapp password: \u0026lt;%= ENV[\u0026#39;MYAPP_DATABASE_PASSWORD\u0026#39;] %\u0026gt; 启动 docker-compose up ","date":"2022-01-13T09:05:07Z","permalink":"https://dccmmtop.github.io/posts/%E6%9E%84%E5%BB%BAdocker%E7%8E%AF%E5%A2%83%E5%BC%80%E5%8F%91rails/","section":"posts","tags":["docker","rails"],"title":"构建docker环境开发Rails"},{"categories":null,"contents":"为什么需要池 用来约束创建和复用昂贵的场景，比如数据库连接\nGo是怎么实现的池 通过 sync.Pool 包实现，并发安全\n怎么使用 Get 方法，首先检查池中是否有可用的实例返回给调用者,如果没有，调用New 方法创建新的实例,并返回 使用完该实例后，调用 Put 方法，将实例归还到池中 示例 package main import( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main(){ myPool := sync.Pool{ New: func() interface{} { fmt.Println(\u0026#34;创建一个新的实例\u0026#34;) return struct{}{} }, } // 第一次取，池中没有实例,要新建 myPool.Get() // 第二次取，池中也没有实例，因为没有把第一次取得实例放入到池中, 要新建 instance := myPool.Get() // 将取到得实例放入池中 myPool.Put(instance) // 池中已经有实例，不用新建 myPool.Get() // 所以会输出 2 次结果 } 结果: 第二个例子 package main import( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;os\u0026#34; ) func main(){ numCalcsCreated := 0 calcPool := \u0026amp;sync.Pool{ New: func() interface{} { numCalcsCreated ++ // 使用了 1Kb 的内存 mem := make([]byte,1024) return \u0026amp;mem }, } calcPool.Put(calcPool.New()) calcPool.Put(calcPool.New()) calcPool.Put(calcPool.New()) calcPool.Put(calcPool.New()) const numWorks = 1024 var wg sync.WaitGroup wg.Add(numWorks) sleepTime, err := strconv.Atoi(os.Args[1]) if err != nil { fmt.Println(err) return } for i := 0; i \u0026lt; numWorks ; i ++ { go func(){ defer wg.Done() // 使用池化技术，每次用完之后，立即放入池中，永远只会使用少量的实例 // 也有可能归还太慢，导致用池中暂时没有实例可用，从而继续生成新的实例 mem := calcPool.Get() time.Sleep(time.Duration(sleepTime) * time.Nanosecond) defer calcPool.Put(mem) }() } wg.Wait() fmt.Println(\u0026#34;创建了\u0026#34;,numCalcsCreated, \u0026#34;次\u0026#34;) } 调整休眠时间，运行结果如下 ","date":"2021-12-15T19:56:44Z","permalink":"https://dccmmtop.github.io/posts/go%E6%B1%A0/","section":"posts","tags":["go","池化"],"title":"Go池"},{"categories":null,"contents":"概念 测试是编程工作中非常重要的一环，但很多人却忽视了这一点，又或者只是把测试看作是一种可有可无的补充手段。Go语言提供了一些基本的测试功能，这些功能初看上去可能会显得非常原始，但正如将要介绍的那样，这些工具实际上已经能够满足程序员对自动测试的需要了。\n除了Go语言内置的testing包之外，还会介绍check和Ginkgo这两个流行的Go测试包，它们提供的功能比testing包更为丰富。\ntesting testing包需要与go test命令以及源代码中所有以-test.go后缀结尾的测试文件一同使用。尽管Go并没有强制要求，但一般来说，测试文件的名字都会与被测试源码文件的名字相对应。\n举个例子，对于源码文件server.go，我们可以创建出一个名为server-test.go的测试文件，这个测试文件包含我们想对server.go进行的所有测试。另外需要注意的一点是，被测试的源码文件和测试文件必须位于同一个包之内。\n为了测试源代码，用户需要在测试文件中创建具有以下格式的测试函数，其中xxx可以是任意英文字母以及数字的组合，但是首字符必须是大写的英文字母：\nfunc TestXxx（*testing.T）{..}\n在测试函数的内部，用户可以使用Error，Fail等一系列方法表示测试失败。当用户在终端里面执行go test命令的时候，所有符合上述格式的测试函数就会被执行。如果一个测试在执行的时候没有任何失败，就认为通过了测试\n一般来说，一个单元通常会与程序中的一个函数或者一个方法相对应，但这并不是必须的。程序中的一个部分能否独立地进行测试，是评判这个部分能否被归纳为“单元”的一个重要指标。\n检测错误 func TestDecode(t *testing.T){ post := Decode(\u0026#34;./tsconfig.json\u0026#34;) if post.Id != 1{ t.Error(\u0026#34;错误的postId, 期望得到1，实际是: \u0026#34;, post.Id) } if post.Content != \u0026#34;Hello world!\u0026#34; { t.Error(\u0026#34;错误的post content, 期望得到\u0026#39;Hello world\u0026#39;，实际是: \u0026#34;, post.Content) } } testing.T 有几个常用的方法:\nLog 将给定的文本记录到错误日志里， 与 fmt.Println 类似 Logf 根据给定的格式,将文本记录到错误日志，与 fmt.Printf 类似 Fail 将测试函数标记已失败，但允许测试函数继续执行 FailNow 将测试函数标记已失败，并停止执行测试函数 除此之外。还提供了其他便利的方法 Error 先执行 Log 函数，再执行 Fail 函数, 其他3个类似\n执行 go test 这条命令会执行当前目录中名字以_test.go为后缀的所有文件。结果如下\ngo test 输出很精简, 可以加 -v 参数使输出内容更详细, -cover 获知测试用例对代码的覆盖率\n跳过测试用例 Skip 在进行测试驱动开发（test-driven development，TDD）的时候，通常会让测试用例持续地失败，直到函数被真正地实现出来为止；\n但是，为了避免测试用例在函数尚未实现之前一直打印烦人的失败信息，用户也可以使用testing.T 结构提供的skip函数，暂时跳过指定的测试用例。\n此外，如果某个测试用例的执行时间非常长，我们也可以在实施完整性检查（sanity check）的时候，使用Skip函数跳过该测试用例。\n示例代码:\nfunc TestEncode(t *testing.T){ // Skip 暂时跳过对某个方法的测试 t.Skip(\u0026#34;暂时跳过对Encode方法的测试\u0026#34;) } 再次执行测试： Short 除了可以直接跳过整个测试用例，用户还可以通过向go test命令传入短暂标志 -short，并在测试用例中使用某些条件逻辑来跳过测试中的指定部分。 注意，这种做法跟在go test命令中通过选项来选择性地执行指定的测试不一样：选择性执行只会执行指定的测试，并跳过其他所有测试，而-short标志则会根据用户编写测试代码的方式，跳过测试中的指定部分或者跳过整个测试用例。\n示例：\nfunc TestLongTime(t *testing.T) { // 如果带有 -short 参数 if testing.Short() { t.Skip(\u0026#34;跳过长时间的测试\u0026#34;) } time.Sleep(10 * time.Second) } 并行测试 正如之前所说，单元测试的目的是独立地进行测试。尽管有些时候，测试套件会因为内部存在依赖关系而无法独立地进行单元测试，但是只要单元测试可以独立地进行，用户就可以通过并行地运行测试用例来提升测试的速度了\ntesting 通过 Parallel 方法实现并行\n示例：\nfunc TestSleep5(t *testing.T) { t.Parallel() time.Sleep(5 * time.Second) } func TestSleep3(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } func TestSleep1(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } 通过 -parallel n 控制可以同时运行几个单元测试 基准测试 Go 的 testing 包支持两种测试，一种是上面的功能测试，还一种就是检验程序的性能的基准测试\n基准测试用例也许要放到以 _test.go 结尾的文件中 基准测试函数需要符合 func BenchmarkXXXX(b *testing.B) {\u0026hellip;} 格式 测试用例的迭代次数是Go自行决定的 使用 go test -bench . 执行本目录下的所有基准测试 示例：\n// 基准测试 func BenchmarkDecode(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Decode(\u0026#34;./tsconfig.json\u0026#34;) } 结果: 执行了 21745 次。花费了 55310 纳秒，即 55 毫秒\n完整代码 json文件\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;content\u0026#34;: \u0026#34;Hello world!\u0026#34;, \u0026#34;author\u0026#34;:{ \u0026#34;id\u0026#34;:2, \u0026#34;name\u0026#34;: \u0026#34;Sau Sheong\u0026#34; }, \u0026#34;comments\u0026#34;: [ { \u0026#34;id\u0026#34;: 3, \u0026#34;content\u0026#34;: \u0026#34;Have a great day!\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Adam\u0026#34; }, { \u0026#34;id\u0026#34;: 4, \u0026#34;content\u0026#34;: \u0026#34;How are you today?\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Betty\u0026#34; } ] } package main import ( \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; ) func TestDecode(t *testing.T){ post := Decode(\u0026#34;./tsconfig.json\u0026#34;) if post.Id != 1{ t.Error(\u0026#34;错误的postId, 期望得到1，实际是: \u0026#34;, post.Id) } if post.Content != \u0026#34;Hello world!\u0026#34; { t.Error(\u0026#34;错误的post content, 期望得到\u0026#39;Hello world\u0026#39;，实际是: \u0026#34;, post.Content) } } func TestSleep5(t *testing.T) { t.Parallel() time.Sleep(5 * time.Second) } func TestSleep3(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } func TestSleep1(t *testing.T) { t.Parallel() time.Sleep(3 * time.Second) } func TestEncode(t *testing.T){ // Skip 暂时跳过对某个方法的测试 t.Skip(\u0026#34;暂时跳过对Encode方法的测试\u0026#34;) } func TestLongTime(t *testing.T) { // 如果带有 -short 参数 if testing.Short() { t.Skip(\u0026#34;跳过长时间的测试\u0026#34;) } time.Sleep(10 * time.Second) } // 基准测试 func BenchmarkDecode(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Decode(\u0026#34;./tsconfig.json\u0026#34;) } } ","date":"2021-11-23T23:21:17Z","permalink":"https://dccmmtop.github.io/posts/go%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","section":"posts","tags":["go","单元测试"],"title":"Go单元测试"},{"categories":null,"contents":"这个封装程序使用的结构和之前分析JSON时使用的结构是相同的。\n程序首先会创建一些结构，然后通过调用MarshalIndent函数将结构封装为由字节切片组成的JSON数据 最后，程序会将封装所得的JSON数据存储到指定的文件中。 也可以通过编码器手动将Go结构编码为json数据\n流程如下: 示例 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) type Post struct { // 处理对象属性与json字段的映射关系 // 如果对象属性与json字段名称相同。可以省略 Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author Author `json:\u0026#34;author\u0026#34;` Comments []Comment `json:\u0026#34;comments\u0026#34;` } type Author struct { Id int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type Comment struct { Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author string `json:\u0026#34;author\u0026#34;` } func main(){ post := Post{ Id: 1, Content: \u0026#34;Hello World\u0026#34;, Author: Author{ Id: 2, Name: \u0026#34;dc1\u0026#34;, }, Comments: []Comment{ { Id: 3, Content: \u0026#34;Have a great day\u0026#34;, Author: \u0026#34;Adam\u0026#34;, }, { Id: 4, Content: \u0026#34;How are you today\u0026#34;, Author: \u0026#34;Betty\u0026#34;, }, }, } fmt.Printf(\u0026#34;post: %v\\n\u0026#34;, post) // 将 json 结构封装为由字节切片组成的Json数据 output, err := json.MarshalIndent(\u0026amp;post,\u0026#34;\u0026#34;,\u0026#34;\\t\\t\u0026#34;) if err != nil { panic(err) } // 将数据写入文件中 err = ioutil.WriteFile(\u0026#34;post.json\u0026#34;,output,0666) if err != nil { panic(err) } // 使用Encoder将结构编码到文件中 // 创建用于储存json的文件 jsonFile, err := os.Create(\u0026#34;./post1.json\u0026#34;) if err != nil { panic(err) } // 创建解码器 encoder := json.NewEncoder(jsonFile) // 把结构编码到json文件中 err = encoder.Encode(\u0026amp;post) if err != nil { panic(err) } } ","date":"2021-11-18T23:08:29Z","permalink":"https://dccmmtop.github.io/posts/go%E5%86%99%E5%85%A5json/","section":"posts","tags":["go"],"title":"Go写入json"},{"categories":null,"contents":"可以使用Unmarshal函数来解封JSON，还可以使用Decoder手动地将JSON数据解码到结构里面，以此来处理流式的JSON数据， 流程如下 要解析的json文件 { \u0026#34;id\u0026#34;: 1, \u0026#34;content\u0026#34;: \u0026#34;Hello world!\u0026#34;, \u0026#34;author\u0026#34;:{ \u0026#34;id\u0026#34;:2, \u0026#34;name\u0026#34;: \u0026#34;Sau Sheong\u0026#34; }, \u0026#34;comments\u0026#34;: [ { \u0026#34;id\u0026#34;: 3, \u0026#34;content\u0026#34;: \u0026#34;Have a great day!\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Adam\u0026#34; }, { \u0026#34;id\u0026#34;: 4, \u0026#34;content\u0026#34;: \u0026#34;How are you today?\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Betty\u0026#34; } ] } 示例 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) type Post struct { // 处理对象属性与json字段的映射关系 // 如果对象属性与json字段名称相同。可以省略 Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author Author `json:\u0026#34;author\u0026#34;` Comments []Comment `json:\u0026#34;comments\u0026#34;` } type Author struct { Id int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type Comment struct { Id int `json:\u0026#34;id\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Author string `json:\u0026#34;author\u0026#34;` } func main(){ jsonFile, err := os.Open(\u0026#34;./tsconfig.json\u0026#34;) defer jsonFile.Close() if err != nil { panic(err) } var post Post jsonContent, err := ioutil.ReadAll(jsonFile) if err != nil { panic(err) } // 将json数据解封至结构 err = json.Unmarshal(jsonContent, \u0026amp;post) if err != nil { panic(err) } fmt.Printf(\u0026#34;post: %v\\n\u0026#34;, post) // 另一种方式 使用解码器 jsonFile1, err := os.Open(\u0026#34;./tsconfig.json\u0026#34;) defer jsonFile1.Close() if err != nil { panic(err) } decoder := json.NewDecoder(jsonFile1) var post1 Post err = decoder.Decode(\u0026amp;post1) if err != nil \u0026amp;\u0026amp; err != io.EOF { panic(err) } fmt.Printf(\u0026#34;post1: %v\\n\u0026#34;, post1) } 选择 最后，在面对JSON数据时，我们可以根据输入决定使用Decoder还是Unmarshal：\n如果JSON数据来源于io.Reader流， 如http.Request的Body，那么使用Decoder更好；\n如果JSON数据来源于字符串或者内存的某个地方，那么使用Unmarshal更好。\n","date":"2021-11-18T22:40:07Z","permalink":"https://dccmmtop.github.io/posts/go%E8%A7%A3%E6%9E%90json%E6%96%87%E4%BB%B6/","section":"posts","tags":["go"],"title":"Go解析json文件"},{"categories":null,"contents":"自动迁移 因为Gorm可以通过自动数据迁移特性来创建所需的数据库表，并在用户修改相应的结构时自动对数据库表进行更新， 当我们运行这个程序时，程序所需的数据库表就会自动生成\n负责执行数据迁移操作的AutoMigrate方法是一个变长参数方法，这种类型的方法和函数能够接受一个或多个参数作为输入。\n在下面展示的代码中，AutoMigrate方法接受的是Post结构和Comment结构。得益于自动数据迁移特性的存在，当用户向结构里面添加新字段的时候，Gorm就会自动在数据库表里面添加相应的新列。\n自动设置创建时间 Comment结构里面出现了一个类型为time.Time的CreatedAt字段，包含这样一个字段意味着Gorm每次在数据库里创建一条新记录的时候，都会自动对这个字段进行设置。 此外，Comment结构的其中一些字段还用到了结构标签，以此来指示Gorm应该如何创建和映射相应的字段。比如，Comment结构的Author字段就使用了结构标签、sql：\u0026ldquo;not null\u0026rdquo;，以此来告知Gorm，该字段对应列的值不能为null\n处理映射关系 在Comment结构里设置了一个PostId字段。Gorm会自动把这种格式的字段看作是外键，并创建所需的关系。\n示例代码 package main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; \u0026#34;github.com/jinzhu/gorm\u0026#34; \u0026#34;time\u0026#34; ) type Post struct { // 会被自动设置成主键 Id int Content string // 数据库层面不可以为空约束 Author string `sql:\u0026#34;not null\u0026#34;` Comments []Comment // 约定。 创建时会被自动赋值 CreatedAt time.Time } type Comment struct { Id int Content string Author string `sql:\u0026#34;not null\u0026#34;` // 以Id结尾的字段被视为外键,在关联对象时起作用. index 会在此字段上创建索引 PostId int `sql:\u0026#34;index\u0026#34;` CreatedAt time.Time } var Db *gorm.DB func init(){ var err error Db, err = gorm.Open(\u0026#34;mysql\u0026#34;,\u0026#34;esns:dccmmtop@tcp(192.168.32.128:3306)/chitchat?parseTime=true\u0026#34;) if err != nil { panic(err) } // 开启详细日志，会把执行的sql打印出来 Db.LogMode(true) // 执行数据库迁移。包括新增表，新增字段，修改字段 Db.AutoMigrate(\u0026amp;Post{},\u0026amp;Comment{}) } func main() { post := Post{ Content: \u0026#34;乱花渐欲迷人眼\u0026#34;, Author: \u0026#34;李商隐\u0026#34;, } // 创建post,并映射 Db.Create(\u0026amp;post) comment := Comment{ Content: \u0026#34;太棒了\u0026#34;, Author: \u0026#34;李白\u0026#34;, } // 创建关联的对象。这里通过外键Id 自动查找映射关系 Db.Model(\u0026amp;post).Association(\u0026#34;Comments\u0026#34;).Append(comment) var readPost Post // 查询 Db.Where(\u0026#34;id = ?\u0026#34;, post.Id).First(\u0026amp;readPost) var comments []Comment // 关联查询 Db.Model(\u0026amp;readPost).Related(\u0026amp;comments) fmt.Printf(\u0026#34;post: %v\\n\u0026#34;,readPost) fmt.Printf(\u0026#34;comments: %v\\n\u0026#34;,comments) } gorm 官方文档有详细的教程 https://gorm.io/zh_CN/\n","date":"2021-11-16T23:14:23Z","permalink":"https://dccmmtop.github.io/posts/gorm%E5%8C%85%E4%BD%BF%E7%94%A8/","section":"posts","tags":["go"],"title":"gorm包使用"},{"categories":null,"contents":"sqlx是一个第三方库，它为database/sql包提供了一系列非常有用的扩展功能。\n因为sqlx和database/sql包使用的是相同的接口，所以sqlx能够很好地兼容使用database/sql包的程序， 除此之外，sqlx还提供了以下这些额外的功能：\n通过结构标签（struct tag）将数据库记录（即行）封装为结构、映射或者切片； 为预处理语句提供具名参数支持。 表结构 create table blog ( id int auto_increment primary key, title varchar(255) not null, content text not null, creator_id int not null ); 示例 package main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; ) type Blog struct { Id int Title string Content string // 当属性与字段名不一致时，使用 struct_tag 进行映射 Creator int `db:\u0026#34;creator_id\u0026#34;` } var db *sqlx.DB func init(){ var err error db, err = sqlx.Open(\u0026#34;mysql\u0026#34;,\u0026#34;esns:dccmmtop@tcp(192.168.32.128:3306)/chitchat?parseTime=true\u0026#34;) if err != nil { panic(err) } } func (blog *Blog)save(){ result, err := db.Exec(\u0026#34;insert into blog(title, content, creator_id) values (?,?,?)\u0026#34;,blog.Title,blog.Content, blog.Creator) if err != nil { panic(err) } // mysql 不支持返回插入后的自增Id,需要额外处理 id, err := result.LastInsertId() if err != nil { panic(err) } blog.Id = int(id) return } func findById(id int)(blog Blog){ blog = Blog{} // StructScan 会自动赋值属性 err := db.QueryRowx(\u0026#34;select id, title, content, creator_id from blog where id = ?\u0026#34;, id).StructScan(\u0026amp;blog) if err != nil { panic(err) } return } func main(){ blog := Blog{ Title: \u0026#34;go语言学习\u0026#34;, Content: \u0026#34;开启go编程之旅吧\u0026#34;, Creator: 1, } blog.save() blog = findById(blog.Id) fmt.Printf(\u0026#34;blog: %v\\n\u0026#34;, blog) } ","date":"2021-11-16T22:07:23Z","permalink":"https://dccmmtop.github.io/posts/sqlx%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/","section":"posts","tags":["go"],"title":"sqlx包的使用"},{"categories":null,"contents":"连接数据库 sq1.DB结构是一个数据库句柄（handle），它代表的是一个包含了零个或任意多个数据库连接的连接池（pool），这个连接池由sql包管理。程序可以通过调用Open函数，并将相应的数据库驱动名字（driver name）以及数据源名字（data source name）传递给该函数来建立与数据库的连接。\n比如，在下面展示的例子中，程序使用的是mysql驱动。数据源名字是一个特定于数据库驱动的字符串，它会告诉驱动应该如何与数据库进行连接。\nOpen函数在执行之后会返回一个指向sq1.DB结构的指针作为结果。Open函数在执行时，不会真正的与数据库连接，甚至不会检查参数.\nOpen函数真正的作用是设置好连接数据库所需要的结构，并以惰性的方式，等真正需要的时候才建立与数据库连接\nvar Db *sql.DB func init() { var err error Db, err = sql.Open(\u0026#34;mysql\u0026#34;, \u0026#34;esns:dccmmtop@tcp(192.168.32.128:3306)/chitchat\u0026#34;) if err != nil { log.Fatal(err) } return } 创建用户 type User struct { Id int64 Uuid string Name string Email string Password string CreatedAt time.Time } func (u *User) Create() (err error) { statement := \u0026#34;insert into users(uuid,name,email,password, created_at) value(?,?,?,?,?)\u0026#34; // 预编译 stmt, err := Db.Prepare(statement) if err != nil { return err } defer stmt.Close() // 加密密码 u.Password = Encrypt(u.Password) // 生成UUID u.Uuid = CreateUUID() u.CreatedAt = time.Now() // 执行 result, err := stmt.Exec(u.Uuid,u.Name,u.Email,u.Password,u.CreatedAt) if err != nil { return err } // 返回插入后的自增ID u.Id, err = result.LastInsertId() if err != nil { util.Danger.Println(\u0026#34;创建用户返回Id错误: \u0026#34;,err) return err } util.Info.Println(\u0026#34;新增用户: \u0026#34;, fmt.Sprintf(\u0026#34;%v\u0026#34;,*u)) userJson, err := json.Marshal(*u) if err != nil { return err } util.Info.Println(\u0026#34;新增用户: \u0026#34;, string(userJson)) return } 查询用户 // 根据ID查询用户 func FindUserById(id int64)(u User, err error) { sql := \u0026#34;select id, uuid, `name`, email, password, created_at from users where id = ?\u0026#34; u = User{} // scan 将查询出来的每一列赋值给对应的属性 err = Db.QueryRow(sql, id).Scan(\u0026amp;u.Id, \u0026amp;u.Uuid, \u0026amp;u.Name, \u0026amp;u.Email, \u0026amp;u.Password, \u0026amp;u.CreatedAt) if err != nil { util.Danger.Println(\u0026#34;查询用户错误: \u0026#34;, err) return } return } 获取多个对象 type Thread struct { Id int64 Uuid string UserId int64 Topic string CreatedAt time.Time } // 获取用户发布多个帖子 func ThreadsList(userId int64)(threads []Thread){ sql := \u0026#34;select id, uuid, user_id, topic,created_at from threads where user_id = ? order by created_at desc\u0026#34; rows, err := Db.Query(sql,userId) if err != nil { util.Danger.Println(\u0026#34;查询 threads 错误, 返回空数据,err:\u0026#34;, err) return } defer rows.Close() for rows.Next() { thread := Thread{} err := rows.Scan(\u0026amp;thread.Id,\u0026amp;thread.Uuid,\u0026amp;thread.UserId,\u0026amp;thread.Topic,\u0026amp;thread.CreatedAt) if err != nil { util.Danger.Println(err) continue } threads = append(threads,thread) } return } ","date":"2021-11-13T22:21:13Z","permalink":"https://dccmmtop.github.io/posts/go%E4%B8%8Esql/","section":"posts","tags":["go"],"title":"Go与SQL"},{"categories":null,"contents":"\n","date":"2021-11-12T08:57:15Z","permalink":"https://dccmmtop.github.io/posts/sql%E4%BC%98%E5%8C%96/","section":"posts","tags":["sql"],"title":"SQL优化"},{"categories":null,"contents":"什么是数据的一致性 “数据一致”一般指的是：缓存中有数据，缓存的数据值 = 数据库中的值。\n但根据缓存中是有数据为依据，则”一致“可以包含两种情况：\n缓存中有数据，缓存的数据值 = 数据库中的值（需均为最新值，本文将“旧值的一致”归类为“不一致状态”） 缓存中本没有数据，数据库中的值 = 最新值（有请求查询数据库时，会将数据写入缓存，则变为上面的“一致”状态） ”数据不一致“：缓存的数据值 ≠ 数据库中的值；缓存或者数据库中存在旧值，导致其他线程读到旧数据\n数据不一致情况及应对策略 根据是否主动向缓存中写值，可以把缓存分成读写缓存和只读缓存。\n只读缓存：只在缓存进行数据查找，即使用 更新数据库+删除缓存策略； 读写缓存：需要在缓存中对数据进行增删改查，即使用 更新数据库+更新缓存策略。 只读缓存（更新数据库+删除缓存） 新增数据时 ，写入数据库；访问数据时，缓存缺失，查数据库，更新缓存（始终是处于”数据一致“的状态，不会发生数据不一致性问题) 更新（修改/删除）数据时 ，会有个时序问题：更新数据库与删除缓存的顺序（这个过程会发生数据不一致性问题）, 如下图 在更新数据的过程中，可能会有如下问题：\n无并发请求下，其中一个操作失败的情况\n并发请求下，其他线程可能会读到旧值 因此，要想达到数据一致性，需要保证两点：\n无并发请求下，保证 A 和 B 步骤都能成功执行\n并发请求下，在 A 和 B 步骤的间隔中，避免或消除其他线程的影响\n接下来，我们针对有/无并发场景，进行分析并使用不同的策略。\nA. 无并发情况 无并发请求下，在更新数据库和删除缓存值的过程中，因为操作被拆分成两步，那么就很有可能存在“步骤 1 成功，步骤 2 失败” 的情况发生（由于单线程中步骤 1 和步骤 2 是串行执行的，不太可能会发生 “步骤 2 成功，步骤 1 失败” 的情况）。\n先删除缓存，再更新数据库,如图\n先更新数据库，再删除缓存\n两种方案执行情况对比:\n执行时序 潜在问题 结果 是否存在一致性问题 先删除缓存，在更新数据库 删除缓存成功，更新数据库失败 请求无法命中缓存，读取数据库旧值 是 先更新数据库,后删除缓存 更新数据库成功，删除缓存失败 请求命中缓存，读取到旧值 是 解决策略：\na.消息队列+异步重试\n无论使用哪一种执行时序，可以在执行步骤 1 时，将步骤 2 的请求写入消息队列，当步骤 2 失败时，就可以使用重试策略，对失败操作进行 “补偿”。 如图:\n具体步骤如下：\n把要删除缓存值或者是要更新数据库值操作生成消息，暂存到消息队列中（例如使用 Kafka 消息队列）； 当删除缓存值或者是更新数据库值操作成功时，把这些消息从消息队列中去除（丢弃），以免重复操作； 当删除缓存值或者是更新数据库值操作失败时，执行失败策略，重试服务从消息队列中重新读取（消费）这些消息，然后再次进行删除或更新； 删除或者更新失败时，需要再次进行重试，重试超过的一定次数，向业务层发送报错信息 b.订阅 Binlog 变更日志\n创建更新缓存服务，接收数据变更的 MQ 消息，然后消费消息，更新/删除 Redis 中的缓存数据； 使用 Binlog 实时更新/删除 Redis 缓存。利用 Canal，即将负责更新缓存的服务伪装成一个 MySQL 的从节点，从 MySQL 接收 Binlog，解析 Binlog 之后，得到实时的数据变更信息，然后根据变更信息去更新/删除 Redis 缓存； MQ+Canal 策略，将 Canal Server 接收到的 Binlog 数据直接投递到 MQ 进行解耦，使用 MQ 异步消费 Binlog 日志，以此进行数据同步； 不管用 MQ/Canal 或者 MQ+Canal 的策略来异步更新缓存，对整个更新服务的数据可靠性和实时性要求都比较高，如果产生数据丢失或者更新延时情况，会造成 MySQL 和 Redis 中的数据不一致。因此，使用这种策略时，需要考虑出现不同步问题时的降级或补偿方案。 B. 高并发情况 使用以上策略后，可以保证在单线程/无并发场景下的数据一致性。但是，在高并发场景下，由于数据库层面的读写并发，会引发的数据库与缓存数据不一致的问题（本质是后发生的读请求先返回了）\n(1) 先删除缓存，再更新数据库\n假设线程 A 删除缓存值后，由于网络延迟等原因导致未及更新数据库，而此时，线程 B 开始读取数据时会发现缓存缺失，进而去查询数据库。而当线程 B 从数据库读取完数据、更新了缓存后，线程 A 才开始更新数据库，此时，会导致缓存中的数据是旧值，而数据库中的是最新值，产生“数据不一致”。其本质就是，本应后发生的“B 线程-读请求” 先于 “A 线程-写请求” 执行并返回了。\n时序如下：\n时间 线程A 线程B 问题 T1 删除数据X的缓存值 T2 1. 读取缓存值X，缓存缺失，从数据库读取X值 线程B读到旧值 T3 2. 将数据X值写入缓存 导致其他线程读到旧值 T4 更新数据库中X的值 缓存时旧值，数据库是新值。数据不一致 解决方案\na.设置缓存过期时间 + 延时双删\n通过设置缓存过期时间，若发生上述淘汰缓存失败的情况，则在缓存过期后，读请求仍然可以从 DB 中读取最新数据并更新缓存，可减小数据不一致的影响范围。虽然在一定时间范围内数据有差异，但可以保证数据的最终一致性。\n此外，还可以通过延时双删进行保障：在线程 A 更新完数据库值以后，让它先 sleep 一小段时间，确保线程 B 能够先从数据库读取数据，再把缺失的数据写入缓存（此时有可能写入的是旧值），然后，线程 A 再进行删除，确保缓存中最终会将是最新的值。后续，其它线程读取数据时，发现缓存缺失，会从数据库中读取最新值\n延时删除只是确保最终缓存中的值与数据库保持一致。不能防止中间的不一致\nredis.delKey(X) db.update(X) Thread.sleep(N) redis.delKey(X) sleep 时间：在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算(较难)\n注意：如果难以接受 sleep 这种写法，可以使用延时队列进行替代。\n先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力，也就是缓存穿透的问题。针对缓存穿透问题，可以用缓存空结果、布隆过滤器进行解决。\n(2) 先更新数据库，再删除缓存\n如果线程 A 更新了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。其本质也是，本应后发生的“B 线程-读请求” 先于 “A 线程-删除缓存” 执行并返回了。\n如下:\n时间 线程A 线程B 问题 T1 更新数据库中的数据X T2 读取数据X,命中缓存。读取旧值 线程A尚未删除缓存。导致线程B读到旧值 T3 更新数据库中X的值 导致其他线程读到旧值 或者，在”先更新数据库，再删除缓存”方案下，“读写分离 + 主从库延迟”也会导致不一致：\n时间 线程A 线程B MySQL集群 问题 T1 更新主库X=2(原值 X=1) T2 删除缓存 T3 查询缓存，没有命中。查询从库，得到旧值 X=1 T4 将X=1写入缓存 T5 从库同步完成 X=2 缓存中是旧值X=1,数据库(主+从)是新值 X=2。数据不一致 解决方案：\na.延迟消息 凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率\nb.订阅 binlog，异步删除 通过数据库的 binlog 来异步淘汰 key，利用工具(canal)将 binlog 日志采集发送到 MQ 中，然后通过 ACK 机制确认处理删除缓存。\nc.删除消息写入数据库 通过比对数据库中的数据，进行删除确认 先更新数据库再删除缓存，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力，也就是缓存穿透的问题。针对缓存穿透问题，可以用缓存空结果、布隆过滤器进行解决。\nd.加锁 更新数据时，加写锁；查询数据时，加读锁 保证两步操作的“原子性”，使得操作可以串行执行。“原子性”的本质是什么？不可分割只是外在表现，其本质是多个资源间有一致性的要求，操作的中间状态对外不可见。\n建议： 优先使用“先更新数据库再删除缓存”的执行时序，原因主要有两个：\n先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力； 业务应用中读取数据库和写缓存的时间有时不好估算，进而导致延迟双删中的 sleep 时间不好设置。 读写缓存（更新数据库+更新缓存） 读写缓存：增删改在缓存中进行，并采取相应的回写策略，同步数据到数据库中\n同步直写：使用事务，保证缓存和数据更新的原子性，并进行失败重试（如果 Redis 本身出现故障，会降低服务的性能和可用性）\n异步回写：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库（没写回数据库前，缓存发生故障，会造成数据丢失） 该策略在秒杀场中有见到过，业务层直接对缓存中的秒杀商品库存信息进行操作，一段时间后再回写数据库。\n一致性：同步直写 \u0026gt; 异步回写 因此，对于读写缓存，要保持数据强一致性的主要思路是：利用同步直写 同步直写也存在两个操作的时序问题：更新数据库和更新缓存\n无并发情况 执行顺序 潜在问题 结果 是否存在一致性问题 解决策略 先更新缓存，后更新数据库 缓存更新成功，数据库更新失败 数据库为旧值 是 消息队列+重试机制 先更新数据库，后更新缓存 数据库更新成功，缓存更新失败 请求命中缓存，读取缓存中的旧值 是 消息队列+重试机制；订阅Binlog日志 高并发情况 有一下四种情况\n时序 并发类型 潜在问题 影响程度 先更新数据库，后更新缓存 写+读 并发 1. 线程A先更新数据库\n2. 线程B读取数据，命中缓存，读取到旧值\n3. 线程A更新缓存成功，后续请求会命中缓存，得到新值 线程A未更新完缓存之前，这期间的读请求会读到短暂旧值。对业务影响短暂 先更新缓存，后更新数据库 写+读 并发 1. 线程A先更新缓存成功\n2. 线程B读取数据，此时线程B命中缓存，读取到最新值\n3.线程A更新数据库成功 虽然线程A还未更新完数据库，数据库与缓存会存在短暂的不一致。但在这之前进来的读请求都能命中缓存，获取到最新值，对业务影响较小 先更新数据库，后更新缓存 写+写 并发 1.线程A,B同时更新同一条数据\n2. 更新缓数据库的顺序是先A后B\n3. 更新缓存的顺序是先B后A 会导致数据库与缓存不一致。对业务影响较大 先更新缓存，后更新数据库 写+写 并发 1.线程A,B同时更新同一条数据\n2. 更新缓存的顺序是先A后B\n3. 更新数据库的顺序是先B后A 会导致数据库与缓存不一致。对业务影响较大 针对场景 1 和 2 的解决方案是：保存请求对缓存的读取记录，延时消息比较，发现不一致后，做业务补偿\n针对场景 3 和 4 的解决方案是： 对于写请求，需要配合分布式锁使用。写请求进来时，针对同一个资源的修改操作，先加分布式锁，保证同一时间只有一个线程去更新数据库和缓存；没有拿到锁的线程把操作放入到队列中，延时处理。用这种方式保证多个线程操作同一资源的顺序性，以此保证一致性。 如图： 其中，分布式锁的实现可以使用以下策略：\n乐观锁 使用版本号、updatetime；缓存中，只允许高版本覆盖低版本 Watach实现Redis乐观锁 watch监控rediskey的状态值，创建redis事务，key+1，执行事务，key被修改过则回滚 setnx 获取锁：set/setnx；释放锁：del命令/Lua脚本 Redisson分布式锁 利用Redis的Hash结构作为储存单元，将业务指定的名称作为key，将随机UUID和线程ID作为field，最后将加锁的次数作为value来储存；线程安全 强一致性策略 上述策略只能保证数据的最终一致性。要想做到强一致，最常见的方案是 2PC、3PC、Paxos、Raft 这类一致性协议，但它们的性能往往比较差，而且这些方案也比较复杂，还要考虑各种容错问题。如果业务层要求必须读取数据的强一致性，可以采取以下策略：\n（1）暂存并发读请求\n在更新数据库时，先在 Redis 缓存客户端暂存并发读请求，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。\n（2）串行化\n读写请求入队列，工作线程从队列中取任务来依次执行\n修改服务 Service 连接池，id 取模选取服务连接，能够保证同一个数据的读写都落在同一个后端服务上 修改数据库 DB 连接池，id 取模选取 DB 连接，能够保证同一个数据的读写在数据库层面是串行的 （3）使用 Redis 分布式读写锁\n将淘汰缓存与更新库表放入同一把写锁中，与其它读请求互斥，防止其间产生旧数据。读写互斥、写写互斥、读读共享，可满足读多写少的场景数据一致，也保证了并发性。并根据逻辑平均运行时间、响应超时时间来确定过期时间。\n总结 针对读写缓存时：同步直写，更新数据库+更新缓存： 针对只读缓存时：更新数据库+删除缓存： 较为通用的一致性策略拟定： 在并发场景下，使用 “更新数据库 + 更新缓存” 需要用分布式锁保证缓存和数据一致性，且可能存在”缓存资源浪费“和”机器性能浪费“的情况；一般推荐使用 “更新数据库 + 删除缓存” 的方案。如果根据需要，热点数据较多，可以使用 “更新数据库 + 更新缓存” 策略。\n在 “更新数据库 + 删除缓存” 的方案中，推荐使用推荐用 “先更新数据库，再删除缓存” 策略，因为先删除缓存可能会导致大量请求落到数据库，而且延迟双删的时间很难评估。在 “先更新数据库，再删除缓存” 策略中，可以使用“消息队列+重试机制” 的方案保证缓存的删除。并通过 “订阅 binlog” 进行缓存比对，加上一层保障。\n此外，需要通过初始化缓存预热、多数据源触发、延迟消息比对等策略进行辅助和补偿。【多种数据更新触发源：定时任务扫描，业务系统 MQ、binlog 变更 MQ，相互之间作为互补来保证数据不会漏更新】\n数据一致性中需要注意的其他问题有哪些？ k-v 大小的合理设置 Redis key 大小设计：由于网络的一次传输 MTU 最大为 1500 字节，所以为了保证高效的性能，建议单个 k-v 大小不超过 1KB，一次网络传输就能完成，避免多次网络交互；k-v 是越小性能越好Redis 热 key：（1） 当业务遇到单个读热 key，通过增加副本来提高读能力或是用 hashtag 把 key 存多份在多个分片中；（2）当业务遇到单个写热 key，需业务拆分这个 key 的功能，属于设计不合理- 当业务遇到热分片，即多个热 key 在同一个分片上导致单分片 cpu 高，可通过 hashtag 方式打散——[引自腾讯云技术分享] 避免其他问题导致缓存服务器崩溃，进而简直导致数据一致性策略失效 缓存穿透、缓存击穿、缓存雪崩、机器故障等问题： 方案选定的思路 确定缓存类型（读写/只读） 确定一致性级别 确定同步/异步方式 选定缓存流程 补充细节\n参考资料 https://mp.weixin.qq.com/s/GU3cbUkI84IMwttDz16P3w ","date":"2021-11-11T20:29:30Z","permalink":"https://dccmmtop.github.io/posts/mysql%E5%92%8Credis%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/","section":"posts","tags":["架构"],"title":"MySQL 和 Redis 的数据一致性问题"},{"categories":null,"contents":"#ruby的分布式锁实现，基于redis class Redlock DefaultRetryCount=3 DefaultRetryDelay=200 ClockDriftFactor = 0.01 UnlockScript=\u0026#39; if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end\u0026#39; def initialize(*server_urls) @servers = [] server_urls.each{|url| @servers \u0026lt;\u0026lt; Redis.new(:url =\u0026gt; url) } @quorum = server_urls.length / 2 + 1 @retry_count = DefaultRetryCount @retry_delay = DefaultRetryDelay @urandom = File.new(\u0026#34;/dev/urandom\u0026#34;) end def set_retry(count,delay) @retry_count = count @retry_delay = delay end def lock_instance(redis,resource,val,ttl) begin return redis.client.call([:set,resource,val,:nx,:px,ttl]) rescue return false end end def unlock_instance(redis,resource,val) begin redis.client.call([:eval,UnlockScript,1,resource,val]) rescue # Nothing to do, unlocking is just a best-effort attempt. end end def get_unique_lock_id val = \u0026#34;\u0026#34; bytes = @urandom.read(20) bytes.each_byte{|b| val \u0026lt;\u0026lt; b.to_s(32) } val end def lock(resource,ttl) val = get_unique_lock_id @retry_count.times { n = 0 start_time = (Time.now.to_f*1000).to_i @servers.each{|s| n += 1 if lock_instance(s,resource,val,ttl) } # Add 2 milliseconds to the drift to account for Redis expires # precision, which is 1 milliescond, plus 1 millisecond min drift # for small TTLs. drift = (ttl*ClockDriftFactor).to_i + 2 validity_time = ttl-((Time.now.to_f*1000).to_i - start_time)-drift if n \u0026gt;= @quorum \u0026amp;\u0026amp; validity_time \u0026gt; 0 return { :validity =\u0026gt; validity_time, :resource =\u0026gt; resource, :val =\u0026gt; val } else @servers.each{|s| unlock_instance(s,resource,val) } end # Wait a random delay before to retry sleep(rand(@retry_delay).to_f/1000) } return false end def unlock(lock) @servers.each{|s| unlock_instance(s,lock[:resource],lock[:val]) } rescue =\u0026gt;e puts \u0026#34;RedLock err:\u0026#34; + e.to_s end end #初始化分布式锁（一般在初始化程序中 config/initializers/xxx.rb） $distributed_locks = Redlock.new(\u0026ldquo;redis://#{REDIS_HOST}:6379\u0026rdquo;)\n使用示例\ndef self.apply_join(user_id, tag_info_id) # 设置重试次数和每次重试的间隔时间 $distributed_locks.set_retry(1, 100) # 持有锁的时间 tag_user_lock = $distributed_locks.lock(\u0026#34;#{user_id}_#{tag_info_id}\u0026#34;, 60 * 1000) result = false begin if tag_user_lock unless TagUserTag.where(user_id: user_id, tag_id: tag_info_id).first # 并发导致创建多条相同记录 TagUserTag.where(user_id: user_id, tag_id: tag_info_id, status: 1).delete_all TagUserTag.create(user_id: user_id, tag_id: tag_info_id, status: 1) update_user_tag_cache_status(user_id, tag_info_id, 1) result = true else Rails.logger.info(\u0026#34;该用户已经在本系统标签下，或 已提出申请\u0026#34;) end end # 释放锁 $distributed_locks.unlock(tag_user_lock) rescue =\u0026gt; e # 释放锁 $distributed_locks.unlock(tag_user_lock) end result end ","date":"2021-11-11T20:24:42Z","permalink":"https://dccmmtop.github.io/posts/ruby%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","section":"posts","tags":["ruby"],"title":"ruby分布式锁"},{"categories":null,"contents":"对Go语言来说，CSV文件可以通过encoding/csv包进行操作，下面通过这个包来读写CSV文件。\n由于程序在接下来的代码中立即就要对写入的posts.csv文件进行读取，而刚刚写入的数据有可能还滞留在缓冲区中，所以程序必须调用写入器的Flush方法来保证缓冲区中的所有数据都已经被正确地写入文件里面了。\n读取CSV文件的方法和写人文件的方法类似。首先，程序会打开文件，并通过将文件传递给NewReader函数来创建出一个读取器（reader），接着，程序会将读取器的FieldsPer Record字段的值设置为负数，这样的话，即使读取器在读取时发现记录（record）里面缺少了某些字段，读取进程也不会被中断。\n反之，如果FieldsPerRecord字段的值为正数，那么这个值就是用户要求从每条记录里面读取出的字段数量，当读取器从CsV文件里面读取出的字段数量少于这个值时，Go就会抛出一个错误。\n最后，如果FieldsPerRecord字段的值为0，那么读取器就会将读取到的第一条记录的字段数量用作FieldsPerRecord的值。\n在设置好FieldsPerRecord字段之后，程序会调用读取器的ReadAl1方法，一次性地读取文件中包含的所有记录；但如果文件的体积较大，用户也可以通过读取器提供的其他方法，以每次一条记录的方式读取文件。\npackage main import ( \u0026#34;encoding/csv\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; ) type Blog struct { Id int Content string } func main(){ csvFile, err := os.Create(\u0026#34;testCsv.csv\u0026#34;) if err != nil { panic(err) } defer csvFile.Close() csvWriter := csv.NewWriter(csvFile) allPost := []Blog{ {Id: 1, Content: \u0026#34;昨夜西风凋敝树\u0026#34;}, {Id: 2, Content: \u0026#34;忽如一夜春风来\u0026#34;}, {Id: 3, Content: \u0026#34;千树万树梨花开\u0026#34;}, {Id: 4, Content: \u0026#34;卷我屋上三重茅\u0026#34;}, } for _, blog := range allPost { csvWriter.Write([]string{strconv.Itoa(blog.Id), blog.Content}) } csvWriter.Flush() // 读取csv file,err := os.Open(\u0026#34;./testCsv.csv\u0026#34;) if err != nil { panic(nil) } defer file.Close() csvReader := csv.NewReader(file) // 设置每行至少的列数,遇到少于此数的行数会报错。-1 代表不检查列数 csvReader.FieldsPerRecord = -1 record, err := csvReader.ReadAll() if err != nil { panic(err) } var posts []Blog for _, item := range record { id ,_ := strconv.ParseInt(item[0],0,0) post := Blog{Id: int(id), Content: item[1]} posts = append(posts,post) } fmt.Println(posts[0]) } ","date":"2021-11-09T22:47:15Z","permalink":"https://dccmmtop.github.io/posts/go%E8%AF%BB%E5%86%99csv/","section":"posts","tags":["go"],"title":"Go读写CSV"},{"categories":null,"contents":"概念 幂等这个概念，是一个数学上的概念，即：f……(f(f(x))) = f(x)。用在计算机领域，指的是系统里的接口或方法对外的一种承诺，使用相同参数对同一资源重复调用某个接口或方法的结果与调用一次的结果相同。\n业务场景 从业务场景上来说，如：现在互联网电商的下单服务，同一个用户在短时间内调用某一个下单服务，只能下单成功一次；银行账户之间的转账，A账户给B账户转账，无论系统出现什么问题或故障，也只能转账成功一次；前端页面对相同表单的内容多次向后端发起提交请求，后端只能给出一个相同的结果等都属于幂等的范畴。 试想一下，如果提供的这些服务不是幂等的，客户在下单时由于网络不稳定或是连续点了几次下单按钮，实际客户只下了一单，结果系统里给客户生成了多单，那平台/商家将是无法承受的，如果被“羊毛党”盯上，损失是无可估量的；银行之间的转账，A账户本来实际给B账户只转了一百万，结果B账户收到了几百万，这在业务上是不可接受的。分析这些业务场景，开发者发现，无论是下单服务、转账服务还是表单提交都是一个个业务请求，提供这些业务服务的接口或方法都应该保证无论服务是超时、重试或有故障等异常情况，都要满足业务上的处理结果是正确的。业务上的一次或多次请求，最终的处理结果是一致的，即：在一定时间内，服务的幂等其实就是请求的幂等。\n架构分析 从系统架构上进行分析，幂等该在哪一层去做，怎么做？ 上图为一个最常见的经典系统框架图，Web端发起一个请求到后端，幂等该在哪一层来处理呢？不妨一层一层的分析。\nNginx是否需要做幂等，Nginx的主要功能是做Web服务器、反向代理、负载均衡等，把请求转发到后端的服务器上，本身不参与具体的业务，所以Nginx是不需要做幂等处理的；Gateway是负责权限校验、安全防御、认证鉴权、流量控制、协议转换、日志审计、监控等，本身也不含对任何业务的处理，所以其也不需要做幂等处理；Service层通常是对业务逻辑进行处理、编排，可能会改变数据，但对于数据的改变结果，最终也还是需要通过数据访问层，写入到数据库，所以Service层也不需要做数据幂等；DAO层主要是和数据库交互，把Service层的结果写入数据库，对Service层提供读取、写入数据库的功能。\n在写入数据库的时候，针对每一次的写入，可能返回不同的结果，此时就需要按场景进行具体的分析对待；DataBase层，主要提供数据的存储，并不参与具体的业务逻辑计算。所以，通过对该架构的每一层的功能分析，得出对于请求的幂等处理，需要在DAO层做处理，以便保证多次请求和一次请求的结果是一致的。\n数据库操作分析 通过上面的分析，得出幂等需要在DAO层来处理，再进一步分析，得出DAO层的操作主要就是CRUD。下面逐一对每一种操作分析是否需要做幂等，以及怎么做。\nR（read）：对应的操作SQL语句为select。只要查询条件不变，在一定的时间内，执行一次和执行多次返回的结果肯定是相同的，所以其本身是幂等的，不需要再做处理。\nselect * from user where id = 1; 查询一次或多次结果是一致的，所以是幂等的。\nC（create）：对应的操作SQL语句为insert。此时，需要分情况，如果用到的数据库主键为数据库自增，不考虑业务主键防重的情况下，每一次写入数据库就不是幂等的，所以为了保证幂等，需要在数据insert前做业务防重或是在数据库表上对业务主键加唯一索引。如果数据库主键不是自增，是由业务系统写入的，需要在业务系统里把数据库主键和业务主键做一对一映射，或是由独立服务提供数据库主键和业务主键的映射关系，保证多次请求获取到的数据库主键和业务主键是一致的，确保写入数据库操作是幂等的。综合来说，就是相同的数据多次写入数据库后，能否保证只有一条数据。\ninsert into user (id,age,sex,ts) values(1,10,‘male’,2021-07-20 10:22:23); U（update）：对应的操作SQL语句为update。更新操作时，一定是要用绝对值进行更新操作，而不要用相对值进行更新，相对值更新可能导致更新操作不幂等。\n幂等：\nupdate user set age = 10 where id = 1; 非幂等：\nupdate user set age++ where id = 1; D（delete）：对应的操作SQL语句为delete。删除操作时，如果删除的是一个范围，生产上最好是禁止该类操作；比较推荐的做法是把按范围操作删除转换为先按范围查询，再按查询的主键进行删除。而且按范围删除的操作不是幂等的。\n幂等：\ndelete from user where id = 1; 非幂等：该类操作要禁止。\ndelete from user where id in （select id from user order by id desc limit 10); 常见业务场景 保证幂等的实现方式有多种，此处例举几类常见的业务场景，在实际应用中，根据业务场景进行选用。\n页面token机制 进入页面时，从服务器获取token，在服务器端把token进行存储，提交时把token带到服务器端进行验证,这里的token相当于业务ID 常见的处理流程如下 乐观锁机制 使用数据库的版本号实现乐观锁，数据库更新时，判断版本号是否与查询时保持一致，一致更新成功，否则更新失败；\nselect+insert 数据写入前，先查询数据是否存在，存在直接返回，不存在则写入数据，保证写入数据库的数据正确性；常用于并发不高的一些后台系统或是防止任务的重复执行；\n悲观锁机制 一般id为主键或唯一索引，仅锁定当前记录；\nselect * from table where id = \u0026#39;1234\u0026#39; for update; 去重表 每一次写入或更新业务表时，先查询去重表是否已经存在记录，再操作业务表。\n数据库唯一索引 为业务表建立唯一索引，避免业务数据多次写入；\n状态机 务状态在变更之前是有条件的，必须按设定的状态条件进行更新；\n在实际开发中，保证提供的接口或服务的幂等（性），是一个最基本的技术要求，希望通过该分析，能对还未理解幂等（性）的研发人员有所帮助。\n参考资料 https://mp.weixin.qq.com/s/_Nn5F98PvoWk_xjVrIpFNQ ","date":"2021-11-09T21:52:29Z","permalink":"https://dccmmtop.github.io/posts/%E5%B9%82%E7%AD%89%E8%AE%BE%E8%AE%A1/","section":"posts","tags":["架构"],"title":"幂等设计"},{"categories":null,"contents":"logrotate 在很多 Linux 发行版上都是默认安装的。系统会定时运行 logrotate，一般是每天一次。系统是这么实现按天执行的。crontab 会每天定时执行 /etc/cron.daily 目录下的脚本，而这个目录下有个文件叫 logrotate。在 centos 上脚本内容是这样的：\n系统自带 cron task：/etc/cron.daily/logrotate，每天运行一次。\n[root@gop-sg-192-168-56-103 logrotate.d]# cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \u0026#34;ALERT exited abnormally with [$EXITVALUE]\u0026#34; fi exit 0 可以看到这个脚本主要做的事就是以 /etc/logrotate.conf 为配置文件执行了 logrotate。就是这样实现了每天执行一次 logrotate。\n很多程序的会用到 logrotate 滚动日志，比如 nginx。它们安装后，会在 /etc/logrotate.d 这个目录下增加自己的 logrotate 的配置文件。logrotate 什么时候执行 /etc/logrotate.d 下的配置呢？看到 /etc/logrotate.conf 里这行，一切就不言而喻了。\ninclude /etc/logrotate.d logrotate 原理 ogrotate 是怎么做到滚动日志时不影响程序正常的日志输出呢？logrotate 提供了两种解决方案。\ncreate copytruncate Linux 文件操作机制 介绍一下相关的 Linux 下的文件操作机制。\nLinux 文件系统里文件和文件名的关系如下图 目录也是文件，文件里存着文件名和对应的 inode 编号。通过这个 inode 编号可以查到文件的元数据和文件内容。文件的元数据有引用计数、操作权限、拥有者 ID、创建时间、最后修改时间等等。文件件名并不在元数据里而是在目录文件中。因此文件改名、移动，都不会修改文件，而是修改目录文件。\ncreate 这也就是默认的方案，可以通过 create 命令配置文件的权限和属组设置；这个方案的思路是重命名原日志文件，创建新的日志文件。详细步骤如下：\n重命名正在输出日志文件，因为重命名只修改目录以及文件的名称，而进程操作文件使用的是 inode，所以并不影响原程序继续输出日志。 创建新的日志文件，文件名和原日志文件一样，注意，此时只是文件名称一样，而 inode 编号不同，原程序输出的日志还是往原日志文件输出。 最后通过某些方式通知程序，重新打开日志文件；由于重新打开日志文件会用到文件路径而非 inode 编号，所以打开的是新的日志文件。 如上也就是 logrotate 的默认操作方式，也就是 mv+create 执行完之后，通知应用重新在新文件写入即可。mv+create 成本都比较低，几乎是原子操作，如果应用支持重新打开日志文件，如 syslog, nginx, mysql 等，那么这是最好的方式。比如通过 kill 命令向程序发送一个 HUP 信号，使之重新加载\n不过，有些程序并不支持这种方式，压根没有提供重新打开日志的接口；而如果重启应用程序，必然会降低可用性，为此引入了如下方式。\ncopytruncate 该方案是把正在输出的日志拷 (copy) 一份出来，再清空 (trucate) 原来的日志；详细步骤如下：\n将当前正在输出的日志文件复制为目标文件，此时程序仍然将日志输出到原来文件中，此时，原文件名也没有变。 清空日志文件，原程序仍然还是输出到预案日志文件中，因为清空文件只把文件的内容删除了，而 inode 并没改变，后续日志的输出仍然写入该文件中。 如上所述，对于 copytruncate 也就是先复制一份文件，然后清空原有文件。 通常来说，清空操作比较快，但是如果日志文件太大，那么复制就会比较耗时，从而可能导致部分日志丢失。不过这种方式不需要应用程序的支持即可。\n配置 logrotate 执行文件： /usr/sbin/logrotate 主配置文件: /etc/logrotate.conf 自定义配置文件: /etc/logrotate.d/*.conf\n修改配置文件后，并不需要重启服务。 由于 logrotate 实际上只是一个可执行文件，不是以 daemon 运行。\n运行 logrotate logrotate [OPTION...] \u0026lt;configfile\u0026gt; -d, --debug ：debug 模式，测试配置文件是否有错误。 -f, --force ：强制转储文件。 -m, --mail=command ：压缩日志后，发送日志到指定邮箱。 -s, --state=statefile ：使用指定的状态文件。 -v, --verbose ：显示转储过程 crontab 定时 通常惯用的做法是配合 crontab 来定时调用。默认是一天执行一次，可以自己添加 crontab 规则\ncrontab -e */30 * * * * /usr/sbin/logrotate /etc/logrotate.d/rsyslog \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 手动运行 debug 模式：指定 [-d|\u0026ndash;debug]\nlogrotate -d 并不会真正进行 rotate 或者 compress 操作，但是会打印出整个执行的流程，和调用的脚本等详细信息。\nverbose 模式： 指定 [-v|\u0026ndash;verbose]\nlogrotate -v 会真正执行操作，打印出详细信息（debug 模式，默认是开启 verbose）\nlogrotate 参数 详细介绍请自行 man logrotate\n主要介绍下完成常用需求会用到的一些参数。\n一个典型的配置文件如下：\n[root@localhost ~]# vim /etc/logrotate.d/log_file /var/log/log_file { monthly rotate 5 compress delaycompress missingok notifempty create 644 root root postrotate /usr/bin/killall -HUP rsyslogd endscript } monthly: 日志文件将按月轮循。其它可用值为 daily，weekly 或者 yearly。 rotate 5: 一次将存储 5 个归档日志。对于第六个归档，时间最久的归档将被删除。 compress: 在轮循任务完成后，已轮循的归档将使用 gzip 进行压缩。 delaycompress: 总是与 compress 选项一起用，delaycompress 选项指示 logrotate 不要将最近的归档压缩，压缩 将在下一次轮循周期进行。这在你或任何软件仍然需要读取最新归档时很有用。 missingok: 在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。 notifempty: 如果日志文件为空，轮循不会进行。 create 644 root root: 以指定的权限创建全新的日志文件，同时 logrotate 也会重命名原始日志文件。 postrotate/endscript: 在所有其它指令完成后，postrotate 和 endscript 里面指定的命令将被执行。在这种情况下，rsyslogd 进程将立即再次读取其配置并继续运行。 上面的模板是通用的，而配置参数则根据你的需求进行调整，不是所有的参数都是必要的。\n/var/log/log_file {\rsize=50M\rrotate 5\rdateext\rcreate 644 root root\rpostrotate\r/usr/bin/killall -HUP rsyslogd\rendscript\r} 在上面的配置文件中，我们只想要轮询一个日志文件，size=50M 指定日志文件大小可以增长到 50MB,不满50MB不会被分割 dateext 指示让旧日志文件以创建日期命名。\n常见配置参数\ndaily: 指定转储周期为每天 weekly: 指定转储周期为每周 monthly: 指定转储周期为每月 rotate count: 指定日志文件删除之前转储的次数，0 指没有备份，5 指保留 5 个备份 tabooext [+] list：让 logrotate 不转储指定扩展名的文件，缺省的扩展名是：.rpm-orig, .rpmsave, v, 和～ missingok：在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。 size size：当日志文件到达指定的大小时才转储，bytes (缺省) 及 KB (sizek) 或 MB (sizem) compress： 通过 gzip 压缩转储以后的日志 nocompress： 不压缩 copytruncate：用于还在打开中的日志文件，把当前日志备份并截断 nocopytruncate： 备份日志文件但是不截断 create mode owner group: 转储文件，使用指定的文件模式创建新的日志文件 nocreate: 不建立新的日志文件 delaycompress： 和 compress 一起使用时，转储的日志文件到下一次转储时才压缩 nodelaycompress： 覆盖 delaycompress 选项，转储同时压缩。 errors address: 专储时的错误信息发送到指定的 Email 地址 ifempty: 即使是空文件也转储，这个是 logrotate 的缺省选项。 notifempty: 如果是空文件的话，不转储 mail address: 把转储的日志文件发送到指定的 E-mail 地址 nomail: 转储时不发送日志文件 olddir directory：储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统 noolddir： 转储后的日志文件和当前日志文件放在同一个目录下 prerotate/endscript： 在转储以前需要执行的命令可以放入这个对，这两个关键字必须单独成行 nginx 日志轮换示例 /var/log/nginx/*.log /var/log/nginx/*/*.log{\rdaily\rmissingok\rrotate 14\rcompress\rdelaycompress\rnotifempty\rcreate 640 root adm\rsharedscripts\rpostrotate\r[ ! -f /var/run/nginx.pid ] || kill -USR1 `cat /var/run/nginx.pid`\rendscript\r} 关于 USR1 信号解释 USR1 亦通常被用来告知应用程序重载配置文件；例如，向 Apache HTTP 服务器发送一个 USR1 信号将导致以下步骤的发生：停止接受新的连接，等待当前连接停止，重新载入配置文件，重新打开日志文件，重启服务器，从而实现相对平滑的不关机的更改。\n对于 USR1 和 2 都可以用户自定义的，在 POSIX 兼容的平台上，SIGUSR1 和 SIGUSR2 是发送给一个进程的信号，它表示了用户定义的情况。它们的符号常量在头文件 signal.h 中定义。在不同的平台上，信号的编号可能发生变化，因此需要使用符号名称。\nkill -HUP pid killall -HUP pName 其中 pid 是进程标识，pName 是进程的名称。\n如果想要更改配置而不需停止并重新启动服务，可以使用上面两个命令。在对配置文件作必要的更改后，发出该命令以动态更新服务配置。根据约定，当你发送一个挂起信号 (信号 1 或 HUP) 时，大多数服务器进程 (所有常用的进程) 都会进行复位操作并重新加载它们的配置文件。\n参考资料 Linux 日志切割神器 logrotate 原理介绍和配置详解\n","date":"2021-11-08T22:25:00Z","permalink":"https://dccmmtop.github.io/posts/%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2logrotate%E5%8E%9F%E7%90%86%E5%92%8C%E9%85%8D%E7%BD%AE/","section":"posts","tags":null,"title":"日志切割logrotate原理和配置"},{"categories":null,"contents":"上下文感知 Go语言的模板引擎可以根据内容所处的上下文改变其显示. 上下文感知的一个显而易见的用途就是对被显示的内容实施正确的转义（escape）：这意味着，如果模板显示的是HTML格式的内容，那么模板将对其实施HTML转义；如果模板显示的是JavaScript格式的内容，那么模板将对其实施JavaScript转义；诸如此类。除此之外，Go模板引擎还可以识别出内容中的URL或者css样式。\n示例 package main import ( \u0026#34;html/template\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/testContextAware\u0026#34;, testContextAware) server.ListenAndServe() } func testContextAware(w http.ResponseWriter, r *http.Request) { t, err := template.ParseFiles(\u0026#34;./testContextAware.tmpl\u0026#34;) if err != nil { panic(err) } content := `我问: \u0026lt;i\u0026gt; \u0026#34;发生了什么\u0026#34; \u0026lt;/i\u0026gt;` err = t.Execute(w,content) if err != nil { panic(err) } } 上下文感知模板 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt;{{ . }}\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/{{ . }}\u0026#34;\u0026gt;Path\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/?q={{ . }}\u0026#34;\u0026gt;Query\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a onclick=\u0026#34;f (\u0026#39;{{ .}}\u0026#39;) \u0026#34;\u0026gt;Onclick\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `` ## 结果 ```txt HTTP/1.1 200 OK Date: Mon, 08 Nov 2021 13:52:59 GMT Content-Length: 569 Content-Type: text/html; charset=utf-8 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt;我问: \u0026amp;lt;i\u0026amp;gt; \u0026amp;#34;发生了什么\u0026amp;#34; \u0026amp;lt;/i\u0026amp;gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/%e6%88%91%e9%97%ae:%20%3ci%3e%20%22%e5%8f%91%e7%94%9f%e4%ba%86%e4%bb%80%e4%b9%88%22%20%3c/i%3e\u0026#34;\u0026gt;Path\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/?q=%e6%88%91%e9%97%ae%3a%20%3ci%3e%20%22%e5%8f%91%e7%94%9f%e4%ba%86%e4%bb%80%e4%b9%88%22%20%3c%2fi%3e\u0026#34;\u0026gt;Query\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;\u0026lt;a onclick=\u0026#34;f (\u0026#39;我问: \\u003ci\\u003e \\u0022发生了什么\\u0022 \\u003c\\/i\\u003e\u0026#39;) \u0026#34;\u0026gt;Onclick\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 原本有可能会被浏览器执行的js已经被转义了，原样展示\n应用场景 由上可见，上下文感知特性可以很方便的避免XSS攻击 上下文感知功能不仅能够自动对HTML进行转义，它还能够防止基于JavaScript，Css甚至URL的XSS攻击。那么这是否意味着我们只要使用Go的模板引擎就可以无忧无虑地进行开发了呢？并非如此，上下文感知虽然很方便，但它并非灵丹妙药，而且有不少方法可以绕开上下文感知。\n实际上，如果需要，用可以完全不使用上下文感知特性的。\n不使用上下文感知 可以使用类型转换，把内容转换成html\nfunc testContextAware(w http.ResponseWriter, r *http.Request) { t, err := template.ParseFiles(\u0026#34;./testContextAware.tmpl\u0026#34;) if err != nil { panic(err) } content := `我问: \u0026lt;i\u0026gt; \u0026#34;发生了什么\u0026#34; \u0026lt;/i\u0026gt;` err = t.Execute(w,template.HTML(content)) if err != nil { panic(err) } } ","date":"2021-11-08T21:32:35Z","permalink":"https://dccmmtop.github.io/posts/go%E6%A8%A1%E6%9D%BF%E4%B9%8B%E4%B8%8A%E4%B8%8B%E6%84%9F%E7%9F%A5/","section":"posts","tags":["go"],"title":"Go模板之上下文感知"},{"categories":null,"contents":"Go的模板动作就是嵌入模板的命令\n条件动作 {{ if arg }} some content {{ else }} other content {{ end }} 迭代动作 迭代动作可以对数组，切片，映射，或者通道进行迭代, 在迭代循环内部， 点(.) 会被设置正在当前迭代内容 设置动作 设置动作允许为指定的范围的点(.) 设置指定的值。减少重复代码 包含动作 包含动作（include action）允许用户在一个模板里面包含另一个模板，从而构建出嵌套的模板。 包含动作的格式为{{ template \u0026ldquo;name\u0026rdquo; arg }}，其中name参数为被包含模板的名字。没有特别指定名称时，就是文件名， arg 是参数名 参数、变量和管道 参数 一个参数（argument）就是模板中的一个值。它可以是布尔值、整数、字符串等字面量，也可以是结构、结构中的一个字段或者数组中的一个键。除此之外，参数还可以是一个变量、一个方法（这个方法必须只返回一个值，或者只返回一个值和一个错误）或者一个函数。最后，参数也可以是一个点（.），用于表示处理器向模板引擎传递的数据。\n变量 除了参数之外，用户还可以在动作中设置变量。变量以美元符号（$）开头，就像这样： 在这个例子中，点（.）是一个映射，而动作range在迭代这个映射的时候，会将变量$key和$value分别初始化为当前被迭代映射元素的键和值。\n管道 模板中的管道（pipeline）是多个有序地串联起来的参数、函数和方法，它的工作方式和语法跟Unix的管道也非常相似： {{p1 | p2 | p3}} 这里的p1,p2和p3可以是参数或者函数。 管道允许用户将一个参数的输出传递给下一个参数，而各个参数之间则使用 | 分隔。 函数 Go的模板引擎函数都是受限制的：尽管这些函数可以接受任意多个参数作为输入，但它们只能返回一个值，或者返回一个值和一个错误。 为了创建一个自定义模板函数，需要：\n创建一个名为FuncMap的映射，并将映射的键设置为函数的名字，而映射的值则设置为实际定义的函数 将FuncMap 与模板绑定 示例：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;html/template\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/testFuncMap\u0026#34;, testFuncMap) server.ListenAndServe() } func testFuncMap(w http.ResponseWriter, re *http.Request) { funcMap := template.FuncMap{ \u0026#34;fdate\u0026#34;: formatDate, } t := template.New(\u0026#34;testFuncMap.tmpl\u0026#34;).Funcs(funcMap) // 前后模板的名字必须相同 t, err := t.ParseFiles(\u0026#34;./testFuncMap.tmpl\u0026#34;) if err != nil { fmt.Println(err) return } err = t.Execute(w,time.Now()) if err != nil { fmt.Println(err) return } } func formatDate(t time.Time) string{ layout := \u0026#34;2006-01-02\u0026#34; return t.Format(layout) } 模板文件 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; the date/time is {{. | fdate}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 结果 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; the date/time is 2021-11-08 \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","date":"2021-11-02T22:44:33Z","permalink":"https://dccmmtop.github.io/posts/go%E6%A8%A1%E6%9D%BF%E4%B9%8B%E5%8A%A8%E4%BD%9C/","section":"posts","tags":["go"],"title":"Go模板之动作"},{"categories":null,"contents":"Go的模板都是文本文档（其中Web应用的模板通常都是HTML），它们都嵌入了一些称为动作（action）的指令。从模板引擎的角度来说，模板就是嵌入了动作的文本（这些文本通常包含在模板文件里面），而模板引擎则通过分析并执行这些文本来生成出另外一些文本。Go语言拥有通用模板引擎库 text/template，它可以处理任意格式的文本，除此之外，Go语言还拥有专门为HTML格式而设的模板引擎库 html/template 模板中的动作默认使用两个大括号 {{}}）包围，如果用户有需要，也可以通过模板引擎提供的方法自行指定其他定界符（delimiter）。\n使用步骤 使用 Go 的模板引擎需要两个步骤：\n对文本格式的模板源进行语法分析，创建一个经过语法分析的模板结构，其中模板源既可以是一个字符串，也可以是模板文件包含的内容， 执行经过语法分析的模板，将 ResponseWriter 和模板所需要的动态数据传递给模板引擎，被调用的模板引擎会把分析后的模板结构和数据结合起来，生成最终的HTML,并将HTML写入 ResponseWriter. 示例: tmp.html: \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{.}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; package main import ( \u0026#34;html/template\u0026#34; \u0026#34;net/http\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/process\u0026#34;,process) server.ListenAndServe() } func process(w http.ResponseWriter, request *http.Request) { t, _ := template.ParseFiles(\u0026#34;./tmpl.html\u0026#34;) t.Execute(w, \u0026#34;你好哇！李银河\u0026#34;) // 第二种方式 /** t = template.New(\u0026#34;tmpl.html\u0026#34;) t.ParseFiles(\u0026#34;./tmpl.html\u0026#34;) t.Execute(w,\u0026#34;hello\u0026#34;) */ } 对模板进行语法分析 ParseFiles是一个独立的（standalone）函数，它可以对模板文件进行语法分析，并创建出一个经过语法分析的模板结构以供Execute方法执行。实际上，ParseFiles函数只是为了方便地调用Template结构的ParseFiles方法而设置的一个函数-当用户调用Parseriles函数的时候，Go会创建一个新的模板，并将用户给定的模板文件的名字用作这个新模板的名字：\nt, _ := template.ParseFiles(\u0026#34;tmpl.html\u0026#34;) 这相当于创建一个新模板，然后调用它的ParseFiles方法\nt := template.New(\u0026#34;tmpl.html\u0026#34;) t, _ := t.ParseFiles(\u0026#34;tmpl.html\u0026#34;) 上面两种方式都可以接受多个模板参数，但只返回第一个参数对应的模板结构 当用户向ParseFiles函数或Parseriles方法传入多个文件时，ParseFiles只会返回用户传入的第一个文件的已分析模板，并且这个模板也会根据用户传入的第一个文件的名字进行命名；至于其他传入文件的已分析模板则会被放置到一个映射里面，这个映射可以在之后执行模板时使用。 换句话说，我们可以这样认为：在向Parseriles传入单个文件时，Parseriles返回的是一个模板；而在向ParseFiles传入多个文件时，ParseFiles返回的则是一个模板集合，理解这一点能够帮助我们更好地学习嵌套模板技术。\n对字符串分析 在绝大多数情况下，程序都是对模板文件进行语法分析，但是在需要时，程序也可以直接对字符串形式的模板进行语法分析。实际上，所有对模板进行语法分析的手段最终都需要调用Parse方法来执行实际的语法分析操作。比如说，在模板内容相同的情况下，语句\nt, _ := template.ParseFiles(\u0026#34;./tmpl.html\u0026#34;) 和代码\ntmpl := `\u0026lt;! DOCTYPE html\u0026gt; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34;content=\u0026#34;text/html; charset=utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Go Web programming\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {.}} \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;` t := template. New (\u0026#34;tmpl. html\u0026#34;) t, = t. Parse (tmpl). t. Execute (w, \u0026#34;Hello world!\u0026#34;) 将产生相同的效果\n对错误的处理 一直都没有处理分析模板时可能会产生的错误。虽然Go语言的一般做法是手动地处理错误，但Go也提供了另外一种机制，专门用于处理分析模板时出现的错误： t := template.Must(template.ParseFiles(\u0026quot;tmpl.html\u0026quot;)) Must函数可以包裹起一个函数，被包裹的函数会返回一个指向模板的指针和一个错误，如果这个错误不是nil，那么Must函数将产生一个panic。\n在Go里面，panic会导致正常的执行流程被终止：如果panic是在函数内部产生的，那么函数会将这个panic返回给它的调用者。panic会一直向调用栈的上方传递，直至main函数为止，并且程序也会因此而崩溃。\n执行模板 执行模板最常用的方法就是调用模板的Execute方法，并向它传递Responsewriter以及模板所需的数据。在只有一个模板的情况下，上面提到的这种方法总是可行的，但如果模板不止一个，那么当对模板集合调用Execute方法的时候，Execute方法只会执行模板集合中的第一个模板。如果想要执行的不是模板集合中的第一个模板而是其他模板，就需要使用ExecuteTemplate 方法\nt,_ := template.ParseFiles(\u0026#34;t1. html\u0026#34;, \u0026#34;t2. html\u0026#34;) 变量t就是一个包含了两个模板的模板集合，其中第一个模板名为t1.html，而第二个模板则名为t2.html （正如前面所说，除非显式地对模板名进行修改，否则模板的名字和后缀名将由传入的模板文件决定），如果对这个模板集合调用Execute方法：\nt.Execute(w,\u0026#34;你好哇！\u0026#34;) 就只有模板t1.html会被执行。如果想要执行的是模板t2.html而不是t1.html，则需要执行以下语句：\nt.ExecuteTemplate(w,\u0026#34;t2.html\u0026#34;,\u0026#34;你好哇!\u0026#34;) `` ","date":"2021-11-01T22:51:35Z","permalink":"https://dccmmtop.github.io/posts/go%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E/","section":"posts","tags":["go"],"title":"Go的模板引擎"},{"categories":null,"contents":"为了向用户报告某个动作的执行情况，应用程序有时候会向用户展示一条简短的通知消息， 比如说，如果一个用户尝试在论坛上发表一篇帖子，但是这篇帖子因为某种原因而发表失败了，那么论坛应该向这个用户展示一条帖子发布失败的消息。 这种通知消息应该出现在用户当前所在的页面，但是在通常情况下，用户在访问这个页面时却不应该看到这样的消息。因此程序实际上要做的是在某个条件被满足时，才在页面上显示一条临时出现的消息，这样用户在刷新页面之后就不会再看见相同的消息了-我们把这种临时出现的消息称为闪现消息（flash message）\n实现闪现消息的方法有很多种，但最常用的方法是把这些消息存储在页面刷新时就会被移除的会话cookie里面\n添加闪现消息到cookie setMessage处理器函数的定义跟之前展示过的setCookie处理器函数的定义非常相似，主要的区别在于setMessage对消息使用了Base64URL编码，以此来满足响应首部对cookie值的URL编码要求。在设置cookie时，如果cookie的值没有包含诸如空格或者百分号这样的特殊字符，那么不对它进行编码也是可以的；但是因为在发送闪现消息时，消息本身通常会包含诸如空格这样的字符，所以对cookie的值进行编码就成了一件必不可少的事情了。\nfunc setMessage(w http.ResponseWriter, r *http.Request) { msg := []byte(\u0026#34;创建失败，缺少必填字段!\u0026#34;) cookie := \u0026amp;http.Cookie{ Name: \u0026#34;flash\u0026#34;, // 必须对 cookie 进行url编码 Value: base64.URLEncoding.EncodeToString(msg), } http.SetCookie(w, cookie) } 展示闪现消息 // 展示闪现消息 func showMessage(w http.ResponseWriter, r *http.Request) { msg, err := r.Cookie(\u0026#34;flash\u0026#34;) if err != nil { if err == http.ErrNoCookie { fmt.Fprintln(w, \u0026#34;not found message\u0026#34;) return } } // 使cookie过期，让浏览器删除cookie cookie := http.Cookie{ Name: \u0026#34;flash\u0026#34;, MaxAge: -1, Expires: time.Unix(1,0), } http.SetCookie(w, \u0026amp;cookie) fmt.Fprintln(w, msg) } 这个函数首先会尝试获取指定的cookie，如果没有找到该cookie，它就会把变量err设置成一个http.ErrNoCookie值，并向浏览器返回一条\u0026quot;No message found\u0026quot;消息。如果找到了这个cookie，那么它必须完成以下两个操作\n创建一个同名的cookie，将它的MaxAge值设置为负数，并且将Expires值也设置成一个已经过去的时间； 使用SetCookie方法将刚刚创建的同名cookie发送至客户端。 初看上去，这两个操作的目的似乎是要替换已经存在的cookie，但实际上，因为新cookie的MaxAge值为负数，并且Expires值也是一个已经过去的时间，所以这样做实际上就是要完全地移除这个cookie。在设置完新cookie之后，程序会对存储在旧cookie中的消息进行解码，并通过响应返回这条消息。\n完整代码 package main import ( \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) /** * 利用cookie实现闪现消息 */ func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;setMessage\u0026#34;,setMessage) http.HandleFunc(\u0026#34;showMessage\u0026#34;,showMessage) server.ListenAndServe() } // 展示闪现消息 func showMessage(w http.ResponseWriter, r *http.Request) { msg, err := r.Cookie(\u0026#34;flash\u0026#34;) if err != nil { if err == http.ErrNoCookie { fmt.Fprintln(w, \u0026#34;not found message\u0026#34;) return } } // 使cookie过期，让浏览器删除cookie cookie := http.Cookie{ Name: \u0026#34;flash\u0026#34;, MaxAge: -1, Expires: time.Unix(1,0), } http.SetCookie(w, \u0026amp;cookie) fmt.Fprintln(w, msg) } func setMessage(w http.ResponseWriter, r *http.Request) { msg := []byte(\u0026#34;创建失败，缺少必填字段!\u0026#34;) cookie := \u0026amp;http.Cookie{ Name: \u0026#34;flash\u0026#34;, // 必须对 cookie 进行url编码 Value: base64.URLEncoding.EncodeToString(msg), } http.SetCookie(w, cookie) } ","date":"2021-10-28T23:20:50Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%A9%E7%94%A8cookie%E5%AE%9E%E7%8E%B0%E9%97%AA%E7%8E%B0%E6%B6%88%E6%81%AF/","section":"posts","tags":["go"],"title":"利用cookie实现闪现消息"},{"categories":null,"contents":"解析请求头 // 解析请求头 func headers(w http.ResponseWriter, request *http.Request) { // 获取所欲请求头，Header 是个 map, key 是字符串，value 是字符串切片 headers := request.Header fmt.Printf(\u0026#34;所有请求头headers: %v\\n\u0026#34;, headers) // 获取单个请求头,返回的是字符串切片 // [gzip, deflate] coding := headers[\u0026#34;Accept-Encoding\u0026#34;] fmt.Printf(\u0026#34;Accept-Encoding: %v\\n\u0026#34;, coding) // 获取单个请求头,如果请求头的值是多个，用逗号拼接 // gzip, deflate coding1 := headers.Get(\u0026#34;Accept-Encoding\u0026#34;) fmt.Printf(\u0026#34;Accept-Encoding1: %v\\n\u0026#34;, coding1) fmt.Fprintln(w, headers) } 从流中解析请求体 直接读取 body 字节流，再转换成自己想要的格式，比较麻烦，但可以处理任何请求类型的参数\n// 接请求体-原始 // 可以处理 application/json 类型的参数 func body(w http.ResponseWriter, request *http.Request) { // 获取请求体的长度 len := request.ContentLength body := make([]byte, len) //　将数据读取到字节数组中 request.Body.Read(body) fmt.Fprintln(w,string(body)) } 解析表单 调用ParseForm方法或者ParseMultipartForm方法，对请求进行语法分析。 根据步骤1调用的方法，访问相应的Form字段、PostForm字段或MultipartForm字段 // 解析表单-手动解析语法 func form(w http.ResponseWriter, request *http.Request) { // 手动先进行语法分析 request.ParseForm() // 再访问Form 或PostForm 字段 // Form 会同时返回 URL 上的参数值 和 form 表单中的参数值，如果两处有相同的参数，值是字符串切片 fmt.Printf(\u0026#34;Form: %v\\n\u0026#34;,request.Form) // PostForm 只会包含表单中的参数 fmt.Printf(\u0026#34;PostForm: %v\\n\u0026#34;,request.PostForm) // multipart/form-data 类型参数 // 取出1024字节数据 request.ParseMultipartForm(1024) fmt.Printf(\u0026#34;multipart: %v\\n\u0026#34;,request.MultipartForm) } 直接获取表单值 因为Formvalue方法即使在给定键拥有多个值的情况下，也只会从Form结构中取出给定键的第一个值，所以如果想要获取给定键包含的所有值，那么就需要直接访问Form结构\n// 解析表单-自动解析语法 func formValue(w http.ResponseWriter, request *http.Request) { // FormValue 方法会自动调用 ParseForm 或 ParseMultipartForm 方法 // 如果参数有多个值，只会取第一个, 如果需要获取全部值，用Form字段 request.FormValue(\u0026#34;userId\u0026#34;) // PostFormValue 同上，但只包含表单中的参数，包含URl中参数 request.PostFormValue(\u0026#34;name\u0026#34;) } 获取文件 func uploadFile(w http.ResponseWriter, request *http.Request) { // 第一种方法，解析文件流 request.ParseMultipartForm(1024) // 取出文件头 fileHeader := request.MultipartForm.File[\u0026#34;uploaded\u0026#34;][0] // 打开文件 file, err := fileHeader.Open() if err != nil { data ,err := ioutil.ReadAll(file) if err != nil { // 将文件写入响应体 fmt.Println(\u0026#34;file: \u0026#34;, string(data)) } } // 第二种，FormFile 方法 file1,_, err := request.FormFile(\u0026#34;uploaded1\u0026#34;) if err != nil { data, err := ioutil.ReadAll(file1) if err != nil { fmt.Println(\u0026#34;file1: \u0026#34;, string(data)) } } } 完整代码 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/header\u0026#34;,headers) http.HandleFunc(\u0026#34;/body\u0026#34;,body) http.HandleFunc(\u0026#34;/form\u0026#34;,form) http.HandleFunc(\u0026#34;/formValue\u0026#34;,formValue) http.HandleFunc(\u0026#34;/uploadFile\u0026#34;,uploadFile) server.ListenAndServe() } func uploadFile(w http.ResponseWriter, request *http.Request) { // 第一种方法，解析文件流 request.ParseMultipartForm(1024) // 取出文件头 fileHeader := request.MultipartForm.File[\u0026#34;uploaded\u0026#34;][0] // 打开文件 file, err := fileHeader.Open() if err != nil { data ,err := ioutil.ReadAll(file) if err != nil { // 将文件写入响应体 fmt.Println(\u0026#34;file: \u0026#34;, string(data)) } } // 第二种，FormFile 方法 file1,_, err := request.FormFile(\u0026#34;uploaded1\u0026#34;) if err != nil { data, err := ioutil.ReadAll(file1) if err != nil { fmt.Println(\u0026#34;file1: \u0026#34;, string(data)) } } } // 解析表单-自动解析语法 func formValue(w http.ResponseWriter, request *http.Request) { // FormValue 方法会自动调用 ParseForm 或 ParseMultipartForm 方法 // 如果参数有多个值，只会取第一个, 如果需要获取全部值，用Form字段 request.FormValue(\u0026#34;userId\u0026#34;) // PostFormValue 同上，但只包含表单中的参数，包含URl中参数 request.PostFormValue(\u0026#34;name\u0026#34;) } // 解析表单-手动解析语法 func form(w http.ResponseWriter, request *http.Request) { // 手动先进行语法分析 request.ParseForm() // 再访问Form 或PostForm 字段 // Form 会同时返回 URL 上的参数值 和 form 表单中的参数值，如果两处有相同的参数，值是字符串切片 fmt.Printf(\u0026#34;Form: %v\\n\u0026#34;,request.Form) // PostForm 只会包含表单中的参数 fmt.Printf(\u0026#34;PostForm: %v\\n\u0026#34;,request.PostForm) // multipart/form-data 类型参数 // 取出1024字节数据 request.ParseMultipartForm(1024) fmt.Printf(\u0026#34;multipart: %v\\n\u0026#34;,request.MultipartForm) } // 接请求体-原始 // 可以处理 application/json 类型的参数 func body(w http.ResponseWriter, request *http.Request) { // 获取请求体的长度 len := request.ContentLength body := make([]byte, len) //　将数据读取到字节数组中 request.Body.Read(body) fmt.Fprintln(w,string(body)) } // 解析请求头 func headers(w http.ResponseWriter, request *http.Request) { // 获取所欲请求头，Header 是个 map, key 是字符串，value 是字符串切片 headers := request.Header fmt.Printf(\u0026#34;所有请求头headers: %v\\n\u0026#34;, headers) // 获取单个请求头,返回的是字符串切片 // [gzip, deflate] coding := headers[\u0026#34;Accept-Encoding\u0026#34;] fmt.Printf(\u0026#34;Accept-Encoding: %v\\n\u0026#34;, coding) // 获取单个请求头,如果请求头的值是多个，用逗号拼接 // gzip, deflate coding1 := headers.Get(\u0026#34;Accept-Encoding\u0026#34;) fmt.Printf(\u0026#34;Accept-Encoding1: %v\\n\u0026#34;, coding1) fmt.Fprintln(w, headers) } ","date":"2021-10-28T00:19:44Z","permalink":"https://dccmmtop.github.io/posts/%E8%A7%A3%E6%9E%90%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E8%AF%B7%E6%B1%82%E4%BD%93/","section":"posts","tags":["go"],"title":"解析请求头和请求体"},{"categories":null,"contents":"在HTTP协议中，首部和请求体是分开传输的，将一些认证信息参数放在请求头中，服务端先解析请求头，如果认证不通过，可以直接返回认证失败，不用再传输请求体，从而提高服务器的性能。 下面做实验验证,实验思路： 编写一个带有身份验证的上传文件接口，此接口先解析请求头中的 token参数，如果token正确，继续解析请求体中的附件，如果token错误，直接返回401， 上传一个超大的文件，比较两种情况的接口耗时。\n接口 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; ) func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/test\u0026#34;,test) server.ListenAndServe() } func test(w http.ResponseWriter, request *http.Request) { token := request.Header.Get(\u0026#34;Authorization\u0026#34;) if token != \u0026#34;Bearer 12345\u0026#34; { w.WriteHeader(401) fmt.Fprintln(w,\u0026#34;认证失败\u0026#34;) return } fmt.Println(\u0026#34;解析文件。。。\u0026#34;) data, _ ,err := request.FormFile(\u0026#34;file\u0026#34;) if err != nil { file, _ := ioutil.ReadAll(data) fmt.Fprintln(w, string(file)) } } 测试 通过 curl 调用该接口, 为使效果明显，上传了一个较大的镜像文件。\ntoken 错误的情况 curl -i --location --request GET \u0026#39;127.0.0.1:8080/test\u0026#39; --header \u0026#39;Authorization:Bearer 123456\u0026#39; --form \u0026#39;file=@\u0026#34;/home/dc/windows10.iso\u0026#34;\u0026#39; 几毫秒内就返回了失败的结果:\nHTTP/1.1 401 Unauthorized Date: Wed, 27 Oct 2021 14:45:54 GMT Content-Length: 13 Content-Type: text/plain; charset=utf-8 Connection: close 认证失败 token 正确的情况\ncurl -i --location --request GET \u0026#39;127.0.0.1:8080/test\u0026#39; --header \u0026#39;Authorization:Bearer 12345\u0026#39; --form \u0026#39;file=@\u0026#34;/home/dc/windows10.iso\u0026#34;\u0026#39; 通过后台日志可以看到正在读取文件。由于测试的附件有3G多，需要漫长的等待。\n总结 由此证明，请求头和请求体时分开传输的, 我们往往把一些身份认证信息等放在首部，便于服务快速的响应。此外还需注意一点，在一些web框架中提供的通用身份校验中间件，或者自己编写的请求过滤器，需要先解析请求头，再解析请求体。才能利用上此特性。\n","date":"2021-10-27T22:12:48Z","permalink":"https://dccmmtop.github.io/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8A%8A%E6%9F%90%E4%BA%9B%E5%8F%82%E6%95%B0%E6%94%BE%E5%9C%A8%E8%AF%B7%E6%B1%82%E5%A4%B4%E4%B8%AD/","section":"posts","tags":["HTTTP"],"title":"为什么把某些参数放在请求头中"},{"categories":null,"contents":"将 cookie 发送给至客户端 Cookie结构的string方法可以返回一个经过序列化处理的cookie，其中Set-Cookie响应首部的值就是由这些序列化之后的cookie组成的。\npackage main import \u0026#34;net/http\u0026#34; func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/setCookie\u0026#34;,setCookie) server.ListenAndServe() } func setCookie(w http.ResponseWriter, request *http.Request) { c1 := http.Cookie{ Name: \u0026#34;first_cookie\u0026#34;, Value: \u0026#34;吃饭了吗\u0026#34;, HttpOnly: true, } c2 := http.Cookie{ Name: \u0026#34;second_cookie\u0026#34;, Value: \u0026#34;吃啥呢\u0026#34;, HttpOnly: true, } // String() 方法返回序列化后得 cookie w.Header().Set(\u0026#34;Cookie\u0026#34;,c1.String()) w.Header().Add(\u0026#34;Cookie\u0026#34;,c2.String()) // 第二种设置 cookie 的方法 c3 := http.Cookie{ Name: \u0026#34;cookie3\u0026#34;, Value: \u0026#34;天气怎么样\u0026#34;, HttpOnly: true, } http.SetCookie(w, \u0026amp;c3) } ","date":"2021-10-26T23:32:10Z","permalink":"https://dccmmtop.github.io/posts/cookie%E6%93%8D%E4%BD%9C/","section":"posts","tags":["go"],"title":"Cookie操作"},{"categories":null,"contents":"会话 cookie 与持久 cookie 没有设置Expires字段的cookie通常称为会话cookie或者临时cookie，这种cookie在浏览器关闭的时候就会自动被移除。相对而言，设置了Expires字段的cookie通常称为持久cookie，这种cookie会一直存在，直到指定的过期时间来临或者被手动删除为止。\ncookie 过期时间 Expires字段和MaxAge字段都可以用于设置cookie的过期时间，其中Expires字段用于明确地指定cookie应该在什么时候过期，而MaxAge字段则指明了cookie在被浏览器创建出来之后能够存活多少秒。之所以会出现这两种截然不同的过期时间设置方式，是因为不同浏览器使用了各不相同的cookie实现机制，跟Go语言本身的设计无关。虽然HTTP 1.1中废弃了Expires，推荐使用MaxAge来代替Expires，但几乎所有浏览器都仍然支持Expires；而且，微软的IE6，IE7和IE8都不支持MaxAge。为了让cookie在所有浏览器上都能够正常地运作，一个实际的方法是只使用Expires，或者同时使用Expires和 МаxAge.\n","date":"2021-10-26T23:28:06Z","permalink":"https://dccmmtop.github.io/posts/cookie%E6%A6%82%E8%A7%88/","section":"posts","tags":["http"],"title":"Cookie概览"},{"categories":null,"contents":"返回体 func writeExample(w http.ResponseWriter, request *http.Request) { // 没有手动设置响应类型，会通过检测响应的前 512 个字节自动判断响应类型 // 这里是 Content-Type: text/html; charset=utf-8 str:= `\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Go Web Programming\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;hl\u0026gt;Hello World\u0026lt;/hl\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` w.Write([]byte(str)) } 设置状态码 func writeHeaderExample(w http.ResponseWriter, request *http.Request) { // 设置HTTP 状态码, WriteHeader 方法名有误导，只能设置状态码，而不是其他响应首部， // 默认是200 // 调用 WriteHeader 之后不能在对响应首部做任何操作。但是可以继续写入响应体 w.WriteHeader(500) // 在 WriteHeader 之后对首部的设置无效 //w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;json\u0026#34;) fmt.Fprintln(w,\u0026#34;服务异常\u0026#34;) } 设置重定向 func headerExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Location\u0026#34;,\u0026#34;http://www.baidu.com\u0026#34;) w.WriteHeader(302) } 返回JSON func jsonExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;application/json\u0026#34;) post := \u0026amp;Post{ User: \u0026#34;He Dong\u0026#34;, Thread: []string{\u0026#34;First\u0026#34;,\u0026#34;Second\u0026#34;,\u0026#34;Three\u0026#34;}, } json, _ := json.Marshal(post) w.Write(json) } 完整代码 package main import ( json \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) type Post struct { User string Thread []string } // 通过编写响应首部重定向 func main(){ server := http.Server{ Addr: \u0026#34;127.0.0.1:8080\u0026#34;, } http.HandleFunc(\u0026#34;/write\u0026#34;,writeExample) http.HandleFunc(\u0026#34;/writeHeader\u0026#34;,writeHeaderExample) http.HandleFunc(\u0026#34;/redirect\u0026#34;,headerExample) http.HandleFunc(\u0026#34;/jsonExample\u0026#34;,jsonExample) server.ListenAndServe() } func writeExample(w http.ResponseWriter, request *http.Request) { // 没有手动设置响应类型，会通过检测响应的前 512 个字节自动判断响应类型 // 这里是 Content-Type: text/html; charset=utf-8 str:= `\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Go Web Programming\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;hl\u0026gt;Hello World\u0026lt;/hl\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` w.Write([]byte(str)) } func writeHeaderExample(w http.ResponseWriter, request *http.Request) { // 设置HTTP 状态码, WriteHeader 方法名有误导，只能设置状态码，而不是其他响应首部， // 默认是200 // 调用 WriteHeader 之后不能在对响应首部做任何操作。但是可以继续写入响应体 w.WriteHeader(500) // 在 WriteHeader 之后对首部的设置无效 //w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;json\u0026#34;) fmt.Fprintln(w,\u0026#34;服务异常\u0026#34;) } func headerExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Location\u0026#34;,\u0026#34;http://www.baidu.com\u0026#34;) w.WriteHeader(302) } // 返回json func jsonExample(w http.ResponseWriter, request *http.Request) { // Header 设置响应首部 w.Header().Set(\u0026#34;Content-Type\u0026#34;,\u0026#34;application/json\u0026#34;) post := \u0026amp;Post{ User: \u0026#34;He Dong\u0026#34;, Thread: []string{\u0026#34;First\u0026#34;,\u0026#34;Second\u0026#34;,\u0026#34;Three\u0026#34;}, } json, _ := json.Marshal(post) w.Write(json) } ","date":"2021-10-26T22:54:48Z","permalink":"https://dccmmtop.github.io/posts/%E8%AE%BE%E7%BD%AE%E5%93%8D%E5%BA%94%E9%A6%96%E9%83%A8/","section":"posts","tags":["go"],"title":"设置响应首部及响应体示例"},{"categories":null,"contents":"package main import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;crypto/rsa\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;crypto/x509/pkix\u0026#34; \u0026#34;encoding/pem\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main(){ max := new(big.Int).Lsh(big.NewInt(1), 128) serialNumber, _ := rand.Int(rand.Reader,max) subject := pkix.Name{ Organization: []string {\u0026#34;YX\u0026#34;}, OrganizationalUnit: []string {\u0026#34;YX\u0026#34;}, CommonName: \u0026#34;DC\u0026#34;, } template := x509.Certificate{ SerialNumber: serialNumber, Subject: subject, NotBefore: time.Now(), NotAfter: time.Now().Add(365 * 24 * time.Hour), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth}, IPAddresses: []net.IP{net.ParseIP(\u0026#34;127.0.0.1\u0026#34;)}, } pk, _ := rsa.GenerateKey(rand.Reader,2048) derBytes , _ := x509.CreateCertificate(rand.Reader, \u0026amp;template, \u0026amp;template, \u0026amp;pk.PublicKey, pk) cerOut, _ := os.Create(\u0026#34;cert.pem\u0026#34;) pem.Encode(cerOut, \u0026amp;pem.Block{Type: \u0026#34;CERTIFICATE\u0026#34;, Bytes: derBytes}) cerOut.Close() keyOut , _ := os.Create(\u0026#34;key.pem\u0026#34;) pem.Encode(keyOut, \u0026amp;pem.Block{Type: \u0026#34;RAS PRIVATE KEY\u0026#34;, Bytes: x509.MarshalPKCS1PrivateKey(pk)}) keyOut.Close() } ","date":"2021-10-25T22:02:11Z","permalink":"https://dccmmtop.github.io/posts/%E7%94%9F%E6%88%90%E8%AF%81%E4%B9%A6%E5%8F%8A%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%A7%81%E9%92%A5/","section":"posts","tags":null,"title":"生成证书及服务端私钥"},{"categories":null,"contents":"nobody在linux中是一个不能登陆的帐号，一些服务进程如apache，aquid等都采用一些特殊的帐号来运行，比如nobody,news,games等等，这是就可以防止程序本身有安全问题的时候，不会被黑客获得root权限\n1、Windows系统在安装后会自动建立一些用户帐户，在Linux系统中同样有一些用户帐户是在系统安装后就有的，就像Windows系统中的内置帐户一样。\n2、它们是用来完成特定任务的，比如nobody和ftp等，我们访问 www.111cn.net的网页程序时，官网的服务器就是让客户以 nobody 身份登录的(相当于Windows系统中的匿名帐户);我们匿名访问ftp时，会用到用户ftp或nobody。\n3、首先，nobody是一个普通用户，非特权用户。 使用nobody用户名的\u0026rsquo;目的\u0026rsquo;是，使任何人都可以登录系统，但是其 UID 和 GID 不提供任何特权，即该uid和gid只能访问人人皆可读写的文件。 4、其次，许多系统中都按惯例地默认创建一个nobody，尽量\u0026rsquo;限制它的权限至最小\u0026rsquo;，当服务器向外服务时，可能会让client以nobody的身份登录。\n5、nobody就是一个普通账户，因为默认登录shell是 \u0026lsquo;/sbin/nologin\u0026rsquo;，所以这个用户是无法直接登录系统的，也就是黑客很难通过漏洞连接到你的服务器来做破坏。此外这个用户的权限也给配置的很低。因此有比较高的安全性。一切都只给最低权限。这就是nobody存在的意义。\n","date":"2021-10-21T17:30:36Z","permalink":"https://dccmmtop.github.io/posts/linux_nobody%E7%94%A8%E6%88%B7/","section":"posts","tags":null,"title":"Linux_nobody用户"},{"categories":null,"contents":"概览 为了理解Elasticsearch中数据是如何组织的，从以下两个角度来观察\n逻辑设计 搜索应用所要注意的。用于索引和搜索的基本单位是文档，可以将其认为 文档以类型来分组，类型包含若干文档， 类似表格包含若干 行。 是关系数据库里的一行。 最终一个或多个类型存在于同一索引中，索引是更大的容器， 类似于SQL中的数据库\nES SQL 文档 行 类型 表 索引 数据库 物理设计 在后台Elasticsearch是如何处理数据的。 Elasticsearch将每个索引划分为分片，每份分片可以在集群中的不同服务器间迁移。通常，应用程序无须关心这些，因为无论Elasticsearch是单台还是多台服务器，应用和Elasticsearch的交互基本保持不变。但是，开始管理集群的时候，就需要留心了。 原因是，物理设计的配置方式决定了集群的性能可扩展性和可用性\n逻辑设计 这个索引一类型-ID的组合唯一确定了Elasticsearch中的某篇文档。当进行搜索的时候，可以查找特定索引、特定类型中的文档，也可以跨多个类型甚至是多个索引进行搜索。 文档 无模式 Elasticsearch中的文档是无模式的，也就是说并非所有的文档都需要拥有相同的字段，它们不是受限于同一个模式。例如，在所有信息完备之前就要使用组织者数据时，你可以彻底忽略位置数据。 映射包含某个类型中当前索引的所有文档的所有字段。但是不是所有的文档必须要有所有的字段 动态添加字段 如果一篇新近文档拥有一个映射中尚不存在的字段, Elasticsearch会自动地将新字段加入映射 自动推导类型 如果值是7, Elasticsearch会假设字段是长整型。 这种新字段的自动检测也有缺点,因为 Elasticsearch可能猜得不对。例如,在索引了值7之后,你可能想再索引he11owor1a,这时由于它是 string而不是1ong,索引就会失败。对于线上环境、最安全的方式是在索引数据之前,就定义好所需的映射。 类型 类型是文档的逻辑容器,类似于表格是行的容器。在不同的类型中,最好放入不同结构(模式)的文档 没有物理实体与类型系相对应 ，从物理角度来看,同一索引中的文档都是写入磁盘,而不考虑它们所属的映射类型。 索引 准实时 每个索引有一个称为 refresh interva1的设置,定义了新近索引的文档对于搜索可见的时间间隔。从性能的角度来看,刷新操作的代价是非常昂贵的,这也是为什么更新只是偶尔进行。默认是每秒更新一次,而不是每来一篇新的文档就更新一次。如果看到 Elasticsearch被称为准实时的,就是指的这种刷新过程。\n物理设计 分片 默认情况下，每个索引由5个主分片组成，每个主分片有一个副本。一共10个分片。（至少一个主分片，0个或多个副分片）\n一份分片是一个包含倒排索引的目录中 也是 Lucene的索引\n分片也是 Elasticsearch将数据从一个节点迁移到另一个节点的最小单位\n副分片可以在运行的时候添加或者移除，而主分片不可以，创建索引之前必须确定主分片的数量\n过少的分片将限制可扩展性,但是过多的分片会影响性能。默认设置的5份是一个不错开始\n存储文档 默认情况下,当存储一篇文档的时候,系统首先根据文档ID的散列值选择一个主分片,并将文档发送到该主分片 然后文档被发送到该主分片的所有副本分片进行存储。这使得副本分片和主分片之间保持数据的同步。 数据同步使得副本分片可以服务于搜索请求,并在原有主分片无法访问时自动升级为主分片。\n默认地,文档在分片中均匀分布:对于每篇文档,分片是通过其ID字符串的散列决定的。每份分片拥有相同的散列范围,接收新文档的机会均等 搜索文档 在搜索的时候,接受请求的节点将请求转发到一组包含所有数据的分片。 Elasticsearch使用 round-robin的轮询机制选择可用的分片(主分片或副本分片),并将搜索请求转发过去。 Elasticsearch然后从这些分片收集结果,将其聚集到单一的回复,然后将回复返回给客户端应用程序。 倒排索引 倒排索引的结构使得 Elasticsearch在不扫描所有文档的情况下,就能告诉你哪些文档包含特定的词条(单词)。\n词条字典 词条字典将每个词条和包含该词条的文档映射起来，搜索时没有必要为了某个词条扫描所有的文档,而是根据这个字典快速地识别匹配的文档。 词频 词频使得 Elasticsearch可以快速地获取某篇文档中某个词条岀现的次数。这对于计算结果的相关性得分非常重要。例如,如果搜索“ denver”,包含多个“ denver\u0026quot;”的文档通常更为相关。 水平扩展 增加更多的节点，工作负载被分摊，抵御高并发。无法提高单个搜搜索速度\n垂直扩展 给机器增加更多的内存或cpu核心，更快的磁盘\n工具 有几个图形化界面\nElasticsearch Head Elasticsearch Head可以通过 Elasticsearch插件的形式来安装这个工具,一个单 机的HTTP服务器,或是可以从文件系统打开的网页。可以从那里发送HTTP请求, 但是Head作为监控工具是最有用的,向你展示集群中分片是如何分布的。 参考资料 《ElasticSearch实战》\n","date":"2021-10-18T23:23:43Z","permalink":"https://dccmmtop.github.io/posts/elasticsearch%E8%AE%A4%E7%9F%A5/","section":"posts","tags":null,"title":"ElasticSearch认知"},{"categories":null,"contents":"插入并保存文档 db.foo.insert({\u0026#34;bar\u0026#34;: \u0026#34;baz\u0026#34;}) 这个操作会给文档自动增加一个\u0026quot;_id\u0026quot;键（要是原来没有的话），然后将其保存到MongoDB中。\n批量插入 如果要向集合中插入多个文档，使用批量插入会快一些。使用批量插入，可以将一组文档传递给数据库。 在shell中，可以使用batchInsert函数实现批量插入，它与insert函数非常像，只是它接受的是一个文档数组作为参数：\ndb.foo.batchInsert([{\u0026#34;_id\u0026#34;: 1},{\u0026#34;_id\u0026#34;: 2}, {\u0026#34;_id\u0026#34;: 3}]) 要是只导入原始数据（例如，从数据feed或者MySQL中导入），可以使用命令行工具，如mongoimport，而不是批量插入\n当前版本的MongoDB能接受的最大消息长度是48 MB，所以在一次批量插入中能插入的文档是有限制的。如果试图插入48 MB以上的数据，多数驱动程序会将这个批量插入请求拆分为多个48 MB的批量插入请求。具体可以查看所使用的驱动程序的相关文档。\n如果在执行批量插入的过程中有一个文档插入失败，那么在这个文档之前的所有文档都会成功插入到集合中，而这个文档以及之后的所有文档全部插入失败。\n在批量插入中遇到错误时，如果希望batchInsert忽略错误并且继续执行后续插入，可以使用continueOnError选项\n插入校验 检查文档的基本结构，如果没有\u0026quot;_id\u0026quot;字段，就自动增加一个。 所有文档都必须小于16 MB（这个值是MongoDB设计者人为定的，未来有可能会增加）。作这样的限制主要是为了防止不良的模式设计，并且保证性能一致 ","date":"2021-09-05T17:46:38Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%9B%E5%BB%BA%E6%96%87%E6%A1%A3/","section":"posts","tags":["mongoDB"],"title":"Mongo创建文档"},{"categories":null,"contents":"运行shell MongoDB自带JavaScript shell，可在shell中使用命令行与MongoDB实例交互。 shell是一个功能完备的JavaScript解释器，可运行任意JavaScript程序 另外，可充分利用JavaScript的标准库: 再者，可定义和调用JavaScript函数：[插图] 需要注意，可使用多行命令。shell会检测输入的JavaScript语句是否完整，如没写完可在下一行接着写。在某行连续三次按下回车键可取消未输入完成的命令，并退回到＞-提示符。 启动mongo shell时不连接到任何mongod有时很方便。通过\u0026ndash;nodb参数启动shell，启动时就不会连接任何数据库,启动mongo shell时不连接到任何mongod有时很方便。通过\u0026ndash;nodb参数启动shell，启动时就不会连接任何数据库 可以通过db.help()查看数据库级别的帮助，使用db.foo.help()查看集合级别的帮助。\n如果想知道一个函数是做什么用的，可以直接在shell输入函数名（函数名后不要输入小括号），这样就可以看到相应函数的JavaScript实现代码。例如，如果想知道update函数的工作机制，或者是记不清参数的顺序，就可以像下面这样做： 使用shell执行脚本 如果希望使用指定的主机/端口上的mongod运行脚本，需要先指定地址，然后再跟上脚本文件的名称 也可以使用load()函数，从交互式shell中运行脚本： 在脚本中可以访问db变量，以及其他全局变量。然而，shell辅助函数（比如\u0026quot;usedb\u0026quot;和\u0026quot;show collections\u0026quot;）不可以在文件中使用。这些辅助函数都有对应的JavaScript函数\n编辑复合变量 shell的多行支持是非常有限的：不可以编辑之前的行。如果编辑到第15行时发现第1行有个错误，那会让人非常懊恼。因此，对于大块的代码或者是对象，你可能更愿意在编辑器中编辑。为了方便地调用编辑器，可以在shell中设置EDITOR变量（也可以在环境变量中设置）：\nEDITOR = \u0026#34;/usr/bin/vim\u0026#34; 现在，如果想要编辑一个变量，可以使用\u0026quot;edit变量名\u0026quot;这个命令，比如： 用户主目录下创建一个名为.mongorc.js的文件,在.mongorc.js文件中添加一行内容，EDITOR=\u0026ldquo;编辑器路径\u0026rdquo;;，以后就不必单独设置EDITOR变量了。\n","date":"2021-09-05T17:36:12Z","permalink":"https://dccmmtop.github.io/posts/mongo_shell%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/","section":"posts","tags":null,"title":"Mongo_Shell基础操作"},{"categories":null,"contents":"文档是MongoDB中数据的基本单元，非常类似于关系型数据库管理系统中的行，但更具表现力。 类似地，集合（collection）可以看作是一个拥有动态模式（dynamic schema）的表。 MongoDB的一个实例可以拥有多个相互独立的数据库（database），每一个数据库都拥有自己的集合。 每一个文档都有一个特殊的键\u0026quot;_id\u0026quot;，这个键在文档所属的集合中是唯一的。\n文档 MongoDB而且区分大小写， 下面两个文档是不同的\n{ \u0026#34;foo\u0026#34;: 3 } { \u0026#34;Foo\u0026#34;: 3 } 文档中的键/值对是有序的：{\u0026ldquo;x\u0026rdquo; : 1, \u0026ldquo;y\u0026rdquo;:2}与{\u0026ldquo;y\u0026rdquo;: 2, \u0026ldquo;x\u0026rdquo;: 1}是不同的。 通常，字段顺序并不重要，无须让数据库模式依赖特定的字段顺序（MongoDB会对字段重新排序）。在某些特殊情况下，字段顺序变得非常重要，\n集合 文档中的键/值对是有序的：{\u0026ldquo;x\u0026rdquo; : 1, \u0026ldquo;y\u0026rdquo;:2}与{\u0026ldquo;y\u0026rdquo;: 2, \u0026ldquo;x\u0026rdquo;: 1}是不同的。通常，字段顺序并不重要，无须让数据库模式依赖特定的字段顺序（MongoDB会对字段重新排序）。在某些特殊情况下，字段顺序变得非常重要，\n集合是动态模式的。这意味着一个集合里面的文档可以是各式各样的\n命名 可以使用db.collectionName获取一个集合的内容，但是，如果集合名称中包含保留字或者无效的JavaScript属性名称，db.collectionName就不能正常工作了。 假设要访问version集合，不能直接使用db.version，因为db.version是db的一个方法（会返回当前MongoDB服务器的版本）： 在JavaScript中，x.y等同于x[\u0026lsquo;y\u0026rsquo;]。也就是说，除了名称的字面量之外，还可以使用变量访问子集合。因此，如果需要对blog的每一个子集合进行操作，可以使用如下方式进行迭代 相同数据结构归为一个文档 如果把各种各样的文档不加区分地放在同一个集合里，无论对开发者还是对管理员来说都将是噩梦。\n在一个集合里查询特定类型的文档在速度上也很不划算，分开查询多个集合要快得多。\n把同种类型的文档放在一个集合里，数据会更加集中\n创建索引时，需要使用文档的附加结构（特别是创建唯一索引时）。索引是按照集合来定义的。在一个集合中只放入一种类型的文档，可以更有效地对集合进行索引。\n命名 集合名不能以“system.”开头，这是为系统集合保留的前缀。例如，system.users这个集合保存着数据库的用户信息，而system.namespaces集合保存着所有数据库集合的信息。 用户创建的集合不能在集合名中包含保留字符’$\u0026rsquo;。因为某些系统生成的集合中包含$，很多驱动程序确实支持在集合名里包含该字符。除非你要访问这种系统创建的集合，否则不应该在集合名中包含$。 子集合 组织集合的一种惯例是使用“.”分隔不同命名空间的子集合。例如，一个具有博客功能的应用可能包含两个集合，分别是blog.posts和blog.authors\n数据库 据库最终会变成文件系统里的文件，而数据库名就是相应的文件名，这是数据库名有如此多限制的原因。 另外，有一些数据库名是保留的，可以直接访问这些有特殊语义的数据库。这些数据库如下所示。 · admin从身份验证的角度来讲，这是“root”数据库。如果将一个用户添加到admin数据库，这个用户将自动获得所有数据库的权限。再者，一些特定的服务器端命令也只能从admin数据库运行，如列出所有数据库或关闭服务器。 local这个数据库永远都不可以复制，且一台服务器上的所有本地集合都可以存储在这个数据库中 config MongoDB用于分片设置时，分片信息会存储在config数据库中。 启动MongoDB 通常，MongoDB作为网络服务器来运行，客户端可连接到该服务器并执行操作。下载MongoDB（http://www.mongodb.org/downloads）并解压，运行mongod命令，启动数据库服务器 mongod在没有参数的情况下会使用默认数据目录/data/db（Windows系统中为C:\\data\\db） 默认情况下，MongoDB监听27017端口 mongod还会启动一个非常基本的HTTP服务器，监听数字比主端口号高1000的端口，也就是28017端口。这意味着，通过浏览器访问http://localhost:28017，能获取数据库的管理信息 shell中的基本操作 要查看db当前指向哪个数据库，可以使用db命令 选择数据库 use 读取 find和findOne方法可以用于查询集合里的文档。若只想查看一个文档，可用findOne： find和findOne可以接受一个查询文档作为限定条件。这样就可以查询符合一定条件的文档。使用find时，shell会自动显示最多20个匹配的文档，也可获取更多文档\n更新 使用update修改博客文章。update接受（至少）两个参数：第一个是限定条件（用于匹配待更新的文档），第二个是新的文档\n例子: 修改变量post，增加\u0026quot;comments\u0026quot;键：\npost.comment = [] db.blog.update({title: \u0026#34;my blog\u0026#34;},post) 删除 使用remove方法可将文档从数据库中永久删除。如果没有使用任何参数，它会将集合内的所有文档全部删除。它可以接受一个作为限定条件的文档作为参数 ","date":"2021-09-05T16:47:25Z","permalink":"https://dccmmtop.github.io/posts/mongodb%E5%9F%BA%E7%A1%80/","section":"posts","tags":["mongoDB"],"title":"MongoDB基础"},{"categories":null,"contents":"Go 的包管理方式是逐渐演进的， 最初是 monorepo 模式，所有的包都放在 GOPATH 里面，使用类似命名空间的包路径区分包，不过这种包管理显然是有问题，由于包依赖可能会引入破坏性更新，生产环境和测试环境会出现运行不一致的问题。\n从 v1.5 开始开始引入 vendor 包模式，如果项目目录下有 vendor 目录，那么 go 工具链会优先使用 vendor 内的包进行编译、测试等，这之后第三方的包管理思路都是通过这种方式来实现，比如说由社区维护准官方包管理工具 dep。\n不过官方并不认同这种方式，在 v1.11 中加入了 Go Module 作为官方包管理形式，就这样 dep 无奈的结束了使命。最初的 Go Module 提案的名称叫做 vgo，下面为了介绍简称为 gomod。不过在 v1.11 和 v1.12 的 Go 版本中 gomod 是不能直接使用的。可以通过 go env 命令返回值的 GOMOD 字段是否为空来判断是否已经开启了 gomod，如果没有开启，可以通过设置环境变量 export GO111MODULE=on 开启。\n目前 gomod 在 Go v1.12 功能基本稳定，到下一个版本 v1.13 将默认开启，是时候开始在项目中使用 gomod 了。\nHello,World Go 维护者 Russ Cox 写一个简单的库，用于说明 gomod 的使用，下文我将使用这个库开始介绍。\n首先在个人包命名空间目录新建一个文件夹，然后直接使用 go mod init 即可。\nmkdir $GOPATH/github.com/islishude/gomodtest cd $GOPATH/github.com/islishude/gomodtest go mod init 更新：现在不允许在 GOPATH 下使用 gomod，需要更改成以下命令：\nmkdir -p ~/gopher/gomodtest cd ~/gopher/gomodtest go mod init github.com/islishude/gomodtest 这时可看到目录内多了 go.mod 文件，内容很简单只有两行：\nmodule github.com/islishude/gomodtest\ngo 1.12 首行为当前的模块名称，接下来是 go 的使用版本。这两行和 npm package.json 的 name 和 engine 字段的功能很类似。\n然后新建一个 main.go 写入以下内容，这里我们引用了 rsc.io/quote 包，注意我们现在还没有下载这个包。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;rsc.io/quote\u0026#34; ) func main() { fmt.Println(quote.Hello()) } 如果是默认情况下，使用 go run main.go 肯定会提示找不到这个包的错误，但是当前 gomod 模式，如果没有此依赖回先下载这个依赖。\n$ go run main.go go: finding rsc.io/quote v1.5.2 go: finding rsc.io/sampler v1.3.0 go: finding golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: downloading rsc.io/quote v1.5.2 go: extracting rsc.io/quote v1.5.2 go: downloading rsc.io/sampler v1.3.0 go: extracting rsc.io/sampler v1.3.0 go: downloading golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c go: extracting golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c Hello, world. 因为包含 http://golang.org 下的包，记得设置代理。这个时候当前包目录除了 go.mod 还有一个 go.sum 的文件，这个类似 npm package-lock.json。\ngomod 不会在 $GOPATH/src 目录下保存 rsc.io 包的源码，而是包源码和链接库保存在 $GOPATH/pkg/mod 目录下。\n$ ls $GOPATH/pkg/mod cache golang.org rsc.io 除了 go run 命令以外，go build、go test 等命令也能自动下载相关依赖包。\n包管理命令 当然我们平常都不会直接先写代码，写上引入的依赖名称和路径，然后在 build 的时候在下载。\n安装依赖 如果要想先下载依赖，那么可以直接像以前那样 go get 即可，不过 gomod 下可以跟语义化版本号，比如 go get foo@v1.2.3，也可以跟 git 的分支或 tag，比如go get foo@master，当然也可以跟 git 提交哈希，比如 go get foo@e3702bed2。需要特别注意的是，gomod 除了遵循语义化版本原则外，还遵循最小版本选择原则，也就是说如果当前版本是 v1.1.0，只会下载不超过这个最大版本号。如果使用 go get foo@master，下次在下载只会和第一次的一样，无论 master 分支是否更新了代码，如下所示，使用包含当前最新提交哈希的虚拟版本号替代直接的 master 版本号。\n$ go get golang.org/x/crypto/sha3@master go: finding golang.org/x/crypto/sha3 latest go: finding golang.org/x/crypto latest $ cat go.mod module github.com/adesight/test go 1.12 require ( golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a // indirect rsc.io/quote v1.5.2 ) 如果下载所有依赖可以使用 go mod download 命令。\n升级依赖 查看所有以升级依赖版本：\n$ go list -u -m all go: finding golang.org/x/sys latest go: finding golang.org/x/crypto latest github.com/adesight/test golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a [v0.0.0-20190316082340-a2f829d7f35f] golang.org/x/text v0.3.0 rsc.io/quote v1.5.2 rsc.io/sampler v1.99.99 ###升级次级或补丁版本号：\ngo get -u rsc.io/quote 仅升级补丁版本号： go get -u=patch rscio/quote 升降级版本号，可以使用比较运算符控制：\ngo get foo@\u0026#39;\u0026lt;v1.6.2\u0026#39; 移除依赖 当前代码中不需要了某些包，删除相关代码片段后并没有在 go.mod 文件中自动移出。 运行下面命令可以移出所有代码中不需要的包：\ngo mod tidy 如果仅仅修改 go.mod 配置文件的内容，那么可以运行 go mod edit --droprequire=path，比如要移出 golang.org/x/crypto 包\ngo mod edit --droprequire=golang.org/x/crypto\n查看依赖包 可以直接查看 go.mod 文件，或者使用命令行：\n$ go list -m all github.com/adesight/test golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a golang.org/x/text v0.3.0 rsc.io/quote v1.5.2 rsc.io/sampler v1.99.99 $ go list -m -json all # json 格式输出 { \u0026#34;Path\u0026#34;: \u0026#34;golang.org/x/text\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;v0.3.0\u0026#34;, \u0026#34;Time\u0026#34;: \u0026#34;2017-12-14T13:08:43Z\u0026#34;, \u0026#34;Indirect\u0026#34;: true, \u0026#34;Dir\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/golang.org/x/text@v0.3.0\u0026#34;, \u0026#34;GoMod\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/cache/download/golang.org/x/text/@v/v0.3.0.mod\u0026#34; } { \u0026#34;Path\u0026#34;: \u0026#34;rsc.io/quote\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;v1.5.2\u0026#34;, \u0026#34;Time\u0026#34;: \u0026#34;2018-02-14T15:44:20Z\u0026#34;, \u0026#34;Dir\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/rsc.io/quote@v1.5.2\u0026#34;, \u0026#34;GoMod\u0026#34;: \u0026#34;/Users/lishude/go/pkg/mod/cache/download/rsc.io/quote/@v/v1.5.2.mod\u0026#34; } 模块配置文本格式化 由于可手动修改 go.mod 文件，所以可能此文件并没有被格式化，使用下面命令进行文本格式化。\ngo mod edit -fmt 发布版本 发布包新版本和其它包管理工具基本一致，可以直接打标签，不过打标签之前需要在 go.mod 中写入相应的版本号：\n$ go mod edit --module=github.com/islishude/gomodtest/v2 $ cat go.mod module github.com/islishude/gomodtest/v2 go 1.12 require ( golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a // indirect rsc.io/quote v1.5.2 ) 官方推荐将上述过程在一个新分支来避免混淆，那么类如上述例子可以创建一个 v2 分支，但这个不是强制要求的。\n还有一种方式发布新版本，那就是在主线版本种加入 v2 文件夹，相应的也需要内置 go.mod 这个文件。\n从老项目迁移 从很多第三方的包管理工具迁移到 gomod 特别简单，直接运行 go mod init 即可。\n如果没有使用任何第三方包管理工具，除了运行 go mod init 初始化以外，还要使用 go get ./\u0026hellip; 下载安装所有依赖包，并更新 go.mod 和 go.sum 文件。\n默认情况下，go get 命令使用 @latest 版本控制符对所有依赖进行下载，如果想要更改某一个包的版本，可以使用 go mod edit \u0026ndash;require 命令，比如要更新 rsc.io/quote 到 v3.1.0 版本。\ngo mod edit --require=rsc.io/quote@v3.1.0 ","date":"2021-08-18T16:23:14Z","permalink":"https://dccmmtop.github.io/posts/go_mod%E7%9A%84%E4%BD%BF%E7%94%A8/","section":"posts","tags":["go"],"title":"go_mod的使用"},{"categories":null,"contents":"count(1) and count(*) **当表的数据量大些时，对表作分析之后，使用count(1)还要比使用count(*)用时多了！ **\n从执行计划来看，count(1)和count()的效果是一样的。 但是在表做过分析之后，count(1)会比count()的用时少些（1w以内数据量），不过差不了多少。\n如果count(1)是聚索引,id,那肯定是count(1)快。但是差的很小的。 因为count(),自动会优化指定到那一个字段。所以没必要去count(1)，用count()，sql会帮你完成优化的 因此： count(1)和count(*)基本没有差别！\ncount(1) and count(字段) 两者的主要区别是 （1） count(1) 会统计表中的所有的记录数， 包含字段为null 的记录。 （2） count(字段) 会统计该字段在表中出现的次数，忽略字段为null 的情况。即 不统计字段为null 的记录。\ncount(*) 和 count(1)和count(列名)区别\n执行效果上 count(*)包括了所有的列，相当于行数，在统计结果的时候， 不会忽略列值为NULL count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候， 不会忽略列值为NULL count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数， 即某个字段值为NULL时，不统计。\n执行效率上 列名为主键，count(列名)会比count(1)快 列名不为主键，count(1)会比count(列名)快 如果表多个列并且没有主键，则 count（1） 的执行效率优于 count（） 如果有主键，则 select count（主键）的执行效率是最优的 如果表只有一个字段，则 select count（）最优。\n","date":"2021-08-15T10:41:01Z","permalink":"https://dccmmtop.github.io/posts/count1%E4%B8%8Ecountx%E7%9A%84%E5%8C%BA%E5%88%AB/","section":"posts","tags":["mysql"],"title":"count(1)与count(x)的区别"},{"categories":null,"contents":"唯一索引 唯一索引是索引具有的一种属性，让索引具备唯一性，确保这张表中，该条索引数据不会重复出现。在每一次insert和update操作时，都会进行索引的唯一性校验，保证该索引的字段组合在表中唯一。\ndb.containers.createIndex({name: 1},{unique:true, background: true}) db.packages.createIndex({ appId: 1, version: 1 },{unique:true, background: true}) 知识点一：\n创建索引时,1表示按升序存储,-1表示按降序存储。\n知识点二:\nMongo提供两种建索引的方式foreground和background。 前台操作，它会阻塞用户对数据的读写操作直到index构建完毕； 后台模式，不阻塞数据读写操作，独立的后台线程异步构建索引，此时仍然允许对数据的读写操作。 创建索引时一定要写{background: true} 创建索引时一定要写{background: true} 创建索引时一定要写{background: true}\n复合索引 概念：指的是将多个键组合到一起创建索引，终极目的是加速匹配多个键的查询。\ndb.flights.createIndex({ flight: 1, price: 1 },{background: true}) 内嵌索引 可以在嵌套的文档上建立索引，方式与建立正常索引完全一致。\n个人信息表结构如下,包含了省市区三级的地址信息，如果想要给城市（city）添加索引，其实就和正常添索引一样\ndb.personInfos.createIndex({“address.city”:1}) 数组索引 MongoDB支持对数组建立索引，这样就可以高效的搜索数组中的特定元素。\n知识点四：\n但是！对数组建立索引的代价是非常高的，他实际上是会对数组中的每一项都单独建立索引，就相当于假设数组中有十项，那么就会在原基础上，多出十倍的索引大小。如果有一百个一千个呢？ 所以在mongo中是禁止对两个数组添加复合索引的，对两个数组添加索引那么索引大小将是爆炸增长，所以谨记在心。 过期索引（TTL） 可以针对某个时间字段，指定文档的过期时间（经过指定时间后过期 或 在某个时间点过期）\n哈希索引（Hashed Index） 是指按照某个字段的hash值来建立索引，hash索引只能满足字段完全匹配的查询，不能满足范围查询等\n地理位置索引（Geospatial Index） 能很好的解决一些场景，比如『查找附近的美食』、『查找附近的加油站』等\n文本索引（Text Index） 能解决快速文本查找的需求，比如，日志平台，相对日志关键词查找，如果通过正则来查找的话效率极低，这时就可以通过文本索引的形式来进行查找\nExplain查询计划 提到查的慢，二话不说直接看查询计划好么？具体每一个字段的含义我就不做赘述了很容易查到，我截取winningPlan的部分和大家一起看一下。WinningPlan就是在查询计划中胜出的方案，那肯定就有被淘汰的方案，是在rejectPlan里。\n// 查询计划中的winningPlan部分 \u0026#34;winningPlan\u0026#34;: { \u0026#34;stage\u0026#34;: \u0026#34;FETCH\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;createdAt\u0026#34;: { \u0026#34;$gte\u0026#34;: ISODate(\u0026#34;2019-07-22T12:00:44.000Z\u0026#34;) } }, \u0026#34;inputStage\u0026#34;: { \u0026#34;stage\u0026#34;: \u0026#34;IXSCAN\u0026#34;, \u0026#34;keyPattern\u0026#34;: { \u0026#34;load\u0026#34;: 1 }, \u0026#34;indexName\u0026#34;: \u0026#34;load_1\u0026#34;, \u0026#34;isMultiKey\u0026#34;: false, \u0026#34;multiKeyPaths\u0026#34;: { \u0026#34;load\u0026#34;: [] }, \u0026#34;isUnique\u0026#34;: false, \u0026#34;isSparse\u0026#34;: false, \u0026#34;isPartial\u0026#34;: false, \u0026#34;indexVersion\u0026#34;: 2, \u0026#34;direction\u0026#34;: \u0026#34;backward\u0026#34;, \u0026#34;indexBounds\u0026#34;: { \u0026#34;load\u0026#34;: [ \u0026#34;[MaxKey, MinKey]\u0026#34; ] } } }, 知识点六： explain 结果将查询计划以阶段树的形式呈现。 每个阶段将其结果（文档或索引键）传递给父节点。 中间节点操纵由子节点产生的文档或索引键。 根节点是MongoDB从中派生结果集的最后阶段。 对于新人一定要特别注意：在看查询结果的阶段树的时候一定一定是从最里层一层一层往外看的，不是直接顺着读下来的。\n知识点七： 在查询计划中出现了很多stage，下面列举的经常出现的stage以及他的含义： COLLSCAN：全表扫描 IXSCAN：索引扫描 FETCH：根据前面扫描到的位置抓取完整文档 SORT：进行内存排序，最终返回结果 SORT_KEY_GENERATOR：获取每一个文档排序所用的键值 LIMIT：使用limit限制返回数 SKIP：使用skip进行跳过 IDHACK：针对_id进行查询 COUNTSCAN：count不使用用Index进行count时的stage返回 COUNT_SCAN：count使用了Index进行count时的stage返回 TEXT：使用全文索引进行查询时候的stage返回\n最期望看到的查询组合 Fetch+IDHACK Fetch+ixscan Limit+（Fetch+ixscan） PROJECTION+ixscan 最不期望看到的查询组合 COLLSCAN（全表扫） SORT（使用sort但是无index） COUNTSCAN（不使用索引进行count）\n最左前缀原则 假定索引(a，b，c) 它可能满足的查询如下：\n1. a 2. a，b 3. a，b，c 4. a，c [该组合只能用a部分] 5. a, c, b [cb在查询时会被优化换位置] 显然，最左前缀的核心是查询条件字段必须含有索引第一个字段\n最左值尽可能用最精确过滤性最好的值，不要用那种可能会用于范围模糊查询，用于排序的字段\n效率极低的操作符 $where和$exists：这两个操作符，完全不能使用索引。 $ne和$not:通常来说取反和不等于,可以使用索引，但是效率极低，不是很有效，往往也会退化成扫描全表。 $nin:不包含，这个操作符也总是会全表扫描 对于管道中的索引，也很容易出现意外，只有在管道最开始时的match sort可以使用到索引，一旦发生过project投射，group分组，lookup表关联，unwind打散等操作后，就完全无法使用索引。 索引设计和优化原则 最后祭出李丹老师的索引设计和优化原则\n1.主键的设置 业务无关、显示指定、递增属性\n2.数据区分度\n原则上区分度高的字段优先做索引字段，如果是组合索引优先放前面\n3.字段更新频率 频繁更新的字段是否做索引字段需要综合考虑对业务的影响及查询的代价\n4.前缀索引问题 需要注意的是因前缀索引只包含部分值因此无法通过前缀索引优化排序\n5.适当冗余设计 对于存储较长字符串字段可额外增加字段存储原字段计算(如hash)后的值\n创建索引时只需要对额外字段创建索引即可\n6.避免无效索引 通常类似表已经含有主键ID就无需再创建额外唯一性的ID索引\n7.查询覆盖率 设计一个索引我们需要考虑尽量覆盖更多的查询场景\n8.控制字段数 如果你设计的索引例如含有7、8个字段通常需要考虑设计是否合理\n优化原则 1.减少网络带宽 按需返回所需字段、尽量避免返回大字段\n2.减少内存计算 减少无必要中间结果存储、减少内存计算\n3.减少磁盘IO 添加合适的索引、关注SQL改写\n","date":"2021-08-12T23:13:39Z","permalink":"https://dccmmtop.github.io/posts/%E7%B4%A2%E5%BC%95/","section":"posts","tags":["mongo"],"title":"索引"},{"categories":null,"contents":"开启查询日志 方法一：执行MongoDB命令 这个命令只能设置单个组件的日志等级，如果想要一次性设置多个组件的日志等级，可以使用下面的方法：\n新建 start_log.js， 内容如下\ndb.adminCommand( { setParameter: 1, logComponentVerbosity: { verbosity: 1, query: { verbosity: 2 }, storage: { verbosity: 2, journal: { verbosity: 1 } } } } ); 执行命令：\n如果开启了认证,还需要加上用户和密码信息 ./mongo 127.0.0.1:27019/eop_task ./start_log.js\n上面例子中的方法，\n将全局的日志等级设置成1；\n将query的日志等级设置成2；\n将storage的日志等级设置成2；\n将storage.journal的日志等级设置成1；\n恢复原级别：\n新建 close_log.js,内容如下：\ndb.auth(\u0026#34;eop\u0026#34;,\u0026#34;password\u0026#34;); db.adminCommand( { setParameter:0, logComponentVerbosity: { verbosity: 0, query: { verbosity: -1 } } }); 执行命令 ./mongo 127.0.0.1:27019/eop_task ./close_log.js\n方法二：写入配置文件 执行这个命令，等同于在配置文件中写入：\nsystemLog: verbosity: 1 component: query: verbosity: 2 storage: verbosity: 2 journal: verbosity: 1 日志轮换 有时候，长时间没有清理日志，日志的数据量会变的很大，这个时候我们可以通过两种方法来对日志进行滚动：\n利用日志轮滚的方法，直接在MongoDB的命令行里面输入：\nuse admin //切换到admin数据库 db.runCommand({logRotate:1}) 这种方法采用了命令来切换日志文件，不需要关闭mongodb服务，是一个比较推荐的做法。\n当然，如果需要人手工的定期执行这个命令，好像也不够优雅，所以可以配合crontab去做这个事情，每天定时执行一次，达到日志文件轮滚的目的。\n查询日志分析 调整日志等级后，在日志文件中会发现如下类似日志\n{ aggregate: \u0026#34;zyb_work_task\u0026#34;, pipeline: [ { $match: { level: 0 } }, { $match: { $or: [ { \u0026#34;creator.id\u0026#34;: 811 }, { assigneeType: 0, assigneeId: 811 }, { assigneeType: 1, assigneeId: { $in: [ 5778 ] } }, { assigneeType: 2, assigneeId: { $in: [ 29295, 28087, 28118 ] } }, { \u0026#34;ccUser.id\u0026#34;: 811 }, { subAssigneeList: 811 }, { subCcList: 811 }, { subCreatorList: 811 } ] } }, { $addFields: { currentEmpFocus: { $ifNull: [ \u0026#34;$empFocus.811\u0026#34;, 0 ] } } }, { $match: { finishFlag: false } }, { $sort: { currentEmpFocus: -1, createTime: -1 } }, { $limit: 20 } ], cursor: {} } 然后将其复制到js文件中：\nfind_task_list.js\n//认证 db.auth(\u0026#34;eop\u0026#34;,\u0026#34;pass\u0026#34;); // 格式化输出 print(JSON.stringify( // 执行命令 db.runCommand( // 直接复制日志中的查询命令 { aggregate: \u0026#34;zyb_work_task\u0026#34;, pipeline: [ { $match: { level: 0 } }, { $match: { $or: [ { \u0026#34;creator.id\u0026#34;: 811 }, { assigneeType: 0, assigneeId: 811 }, { assigneeType: 1, assigneeId: { $in: [ 5778 ] } }, { assigneeType: 2, assigneeId: { $in: [ 29295, 28087, 28118 ] } }, { \u0026#34;ccUser.id\u0026#34;: 811 }, { subAssigneeList: 811 }, { subCcList: 811 }, { subCreatorList: 811 } ] } }, { $addFields: { currentEmpFocus: { $ifNull: [ \u0026#34;$empFocus.811\u0026#34;, 0 ] } } }, { $match: { finishFlag: false } }, { $sort: { currentEmpFocus: -1, createTime: -1 } }, { $limit: 20 } ], cursor: {} } ) ) ); 执行: ./mongo 127.0.0.1:27019/eop_task ./find_task_list.js\n此时就可以将应用的查询语句对应的结果显示出来，进行下一步的调试\n","date":"2021-08-12T23:13:15Z","permalink":"https://dccmmtop.github.io/posts/%E5%BC%80%E5%90%AF%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97/","section":"posts","tags":["mongoDB"],"title":"开启查询日志"},{"categories":null,"contents":"require \u0026#34;socket\u0026#34; local_ip = UDPSocket.open {|s| s.connect(\u0026#34;1.1.1.1\u0026#34;, 1);s.addr.last} ","date":"2021-08-12T23:12:27Z","permalink":"https://dccmmtop.github.io/posts/%E8%8E%B7%E5%8F%96%E6%9C%AC%E6%9C%BAip%E5%9C%B0%E5%9D%80/","section":"posts","tags":["ruby"],"title":"获取本机ip地址"},{"categories":null,"contents":"sidekiq清空队列里任务的方式主要有两种，一是使用sidekiq的api，二是直接操作redis\n一、使用sidekiq的api清空队列的任务 sidekiq里有提供操作队列的api，首先引入\nrequire\u0026rsquo;sidekiq/api'\n获取所有队列：Sidekiq::Queue.all\n获取默认队列：Sidekiq::Queue.new# the \u0026ldquo;default\u0026rdquo; queue\n按名称获取队列：Sidekiq::Queue.new(\u0026ldquo;mailer\u0026rdquo;)\n清空队列的所有任务：Sidekiq::Queue.new.clear\n按条件来删除队列的任务：\nqueue=Sidekiq::Queue.new(\u0026#34;mailer\u0026#34;) queue.eachdo |job| job.klass# =\u0026gt; \u0026#39;MyWorker\u0026#39; job.args# =\u0026gt; [1, 2, 3] job.delete if job.jid==\u0026#39;abcdef1234567890\u0026#39; end 二、直接操作redis来删除队列里的任务 首先获取配置文件config，再连接redis，这里使用了redis的Gem包 redis= Redis.new(:host =\u0026gt; config[\u0026lsquo;host\u0026rsquo;], :port =\u0026gt; config[\u0026lsquo;port\u0026rsquo;], :db=\u0026gt; config[\u0026lsquo;db\u0026rsquo;], :password =\u0026gt; config[\u0026lsquo;password\u0026rsquo;]) 由于queues用的是set类型的数据，所以要用srem来删除相应的数据\nredis.srem(‘queues’, ‘队列的名称’) # 这种情况会直接删除该名称的队列 ","date":"2021-08-12T23:12:03Z","permalink":"https://dccmmtop.github.io/posts/%E6%B8%85%E9%99%A4sidekiq%E4%BB%BB%E5%8A%A1/","section":"posts","tags":["rails"],"title":"清除sidekiq任务"},{"categories":null,"contents":"例子如下：\n用sort_by\ndef order_weight_sort_by(string) string.split(\u0026#34; \u0026#34;).sort_by do |a| sum_a = a.split(\u0026#34;\u0026#34;).inject(0) { |mem, var| mem += var.to_i } first_number = a[0].to_i [a.size, sum_a, first_number] end.join(\u0026#34; \u0026#34;) end string = \u0026ldquo;56 65 74 100 99 68 86 980 90\u0026rdquo; p order_weight_sort_by(string ) 结果是\u0026quot;90 56 65 74 68 86 99 100 980\u0026quot;\n上面的方法是先将字符串变成一个由数字字符串组成的数组。然后先按照字符串的长度进行排序，再按照字符串各数字之和进行排序，最后按照字符串的第一个数字大小进行排序。\n关键代码[a.size, sum_a, first_number]\n当最后是一个条件数组时，sort_by会按照该条件数组的顺序依次排序。\n用sort\ndef order_weight_sort(string) string.split(\u0026#34; \u0026#34;).sort do |a, b| sum_a = a.split(\u0026#34;\u0026#34;).inject(0) { |mem, var| mem += var.to_i } first_number_a = a[0].to_i size_a = a.size sum_b = b.split(\u0026#34;\u0026#34;).inject(0) { |mem, var| mem += var.to_i } first_number_b = b[0].to_i size_b = b.size [size_a, sum_a, first_number_a] \u0026lt;=\u0026gt; [size_b, sum_b, first_number_b] end.join(\u0026#34; \u0026#34;) end string = \u0026ldquo;56 65 74 100 99 68 86 980 90\u0026rdquo; p order_weight_sort(string ) 结果与上面是一样的。\n关键代码[size_a, sum_a, first_number_a] \u0026lt;=\u0026gt; [size_b, sum_b, first_number_b]\n这样看起来sort_by比sort简洁很多。 确实sort_by只需要一个参数，而sort需要两个参数。但他们实现多层排序是一样的，最后都是用一个条件数组来表示。\n区别\n但sort要比sort_by灵活。因为最后的排序条件还可以这样写：\n[size_b, sum_a, first_number_b] \u0026lt;=\u0026gt; [size_a, sum_b, first_number_a] 这样就相当于先按长度倒序排列，然后再按照字符串各数字之和进行排序，最后再按照首个字符的大小倒序排列。\n作者：kamionayuki 链接：http://www.jianshu.com/p/319d0174f246 來源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n","date":"2021-08-12T23:10:45Z","permalink":"https://dccmmtop.github.io/posts/ruby%E5%A4%9A%E5%B1%82%E6%95%B0%E7%BB%84%E6%8E%92%E5%BA%8F/","section":"posts","tags":["ruby"],"title":"ruby多层数组排序"},{"categories":null,"contents":"rails验证码 1.安装包\ngem \u0026#39;rucaptcha\u0026#39; gem \u0026#39;dalli\u0026#39; 2.配置路由 （最新版本的不用配置路由）\nmount RuCaptcha::Engine =\u0026gt; \u0026#34;/rucaptcha\u0026#34; 3.controller部分\ndef create @user = User.new(user_params) if verify_rucaptcha?(@user)\u0026amp;\u0026amp;@user.save ...... 4.view部分\n\u0026lt;div class=\u0026#34;form-group \u0026#34;\u0026gt; \u0026lt;%= rucaptcha_input_tag( class:\u0026#39;form-control rucaptcha-text\u0026#39;) %\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#39;rucaptcha-image-box\u0026#39;\u0026gt;\u0026lt;%= rucaptcha_image_tag(class:\u0026#39;rucaptcha-image\u0026#39;, alt: \u0026#39;Captcha\u0026#39;) %\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; 5.实现点击图片刷新验证码\n#点击验证码刷新 $(\u0026#34;.rucaptcha-image\u0026#34;).click -\u0026gt; img = $(this) currentSrc = img.attr(\u0026#39;src\u0026#39;); img.attr(\u0026#39;src\u0026#39;, currentSrc.split(\u0026#39;?\u0026#39;)[0] + \u0026#39;?\u0026#39; + (new Date()).getTime()); return false ","date":"2021-08-12T23:08:20Z","permalink":"https://dccmmtop.github.io/posts/rails%E9%AA%8C%E8%AF%81%E7%A0%81/","section":"posts","tags":["rails"],"title":"rails验证码"},{"categories":null,"contents":"有时候我需要写一个页面能向show edit 那样可以接受参数的路由,弄了好久不知道怎样解决,今天恍然大悟\n我们执行 rake routes 就会看到如下\nwechat_nodes GET /wechat/nodes(.:format) wechat/nodes#index POST /wechat/nodes(.:format) wechat/nodes#create new_wechat_node GET /wechat/nodes/new(.:format) wechat/nodes#new edit_wechat_node GET /wechat/nodes/:id/edit(.:format) wechat/nodes#edit wechat_node GET /wechat/nodes/:id(.:format) wechat/nodes#show PATCH /wechat/nodes/:id(.:format) wechat/nodes#update PUT /wechat/nodes/:id(.:format) wechat/nodes#update DELETE /wechat/nodes/:id(.:format) wechat/nodes#destroy 这是使用resources 生成一些路由,我们可以模仿写出自己的路由\n我们看最后一列的内容, 对于show 来说 他的格式为/wechat/nodes/:id 再看edit 他的格式是/wechat/nodes/:id/edit\n加入我们要想写一个/wechat/node/edit/34/topic/23 类似的路由,我们可以直接这样写\nresources nodes do collection do get \u0026#39;edit/:id/topic/:node_id\u0026#39;, to: \u0026#39;nodes#topic\u0026#39; , as: \u0026#39;topic_edit\u0026#39; end end 这里为什么会有一个as呢,因为没有as和其后面的名称的话,这样是没有前面的路由信息的.\n筛选路由-路由冲突的解决方案 Rails项目有一个Article模型,对应ArticlesController控制器,其路由设置如下:\nresources :articles do end 这样它的CRUD路径就都自动创建出来了 ;)\n现在我想再添加一个对Article模型搜索的页面,那么首先要在控制器中添加对应的search方法:\ndef search render text:\u0026#34;hello search!!!\u0026#34; end 然后在Article默认路由集合后面添加一行新路由:\nget \u0026quot;articles/search\u0026quot;,to:\u0026quot;articles#search\u0026quot;1\n现在我们访问一下articles/search页面,咦?怎么出错了:\n仔细看出错信息,原来Article之前的show路由恰恰可以匹配新的search路由,只不过原来的:id变成了search这个字符串哦.这就是为什么报Couldn’t find Article with id=search的原因了!\n下面给出解决,我们只需要先禁用默认的show路由:\nresources :articles,except:[:show] do resources :comments end 然后再生成一条筛选路由即可,所谓筛选路由就是对该路由内容进行细粒度匹配的方法:\nget \u0026#34;articles/:id\u0026#34;,to:\u0026#34;articles#show\u0026#34;,constraints:{id:/\\d+/} 路由都是从上之下依次匹配的,如果上面一条被匹配则路由匹配结束!这里只匹配id为数字的articles/xxx路径,所以search就会默认被忽略从而被后面search正确的路由所匹配!\n","date":"2021-08-12T22:53:06Z","permalink":"https://dccmmtop.github.io/posts/%E5%8F%82%E6%95%B0%E8%B7%AF%E7%94%B1%E4%B8%8E%E8%B7%AF%E7%94%B1%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3%E4%B8%8E%E7%AD%9B%E9%80%89%E8%B7%AF%E7%94%B1/","section":"posts","tags":["rails"],"title":"参数路由与路由冲突解决与筛选路由"},{"categories":null,"contents":"存储过程添加列 create procedure add_col( in model_name text, in col_name text, in col_info text, out result text ) begin if not exists( select * from information_schema.COLUMNS where TABLE_NAME = model_name and COLUMN_NAME = col_name) then set @ddl=CONCAT(\u0026#39;alter table \u0026#39;, model_name, \u0026#39; add column \u0026#39;, col_name, col_info); prepare stmt from @ddl; execute stmt; set result = \u0026#39;success\u0026#39;; else set result = \u0026#39;exists\u0026#39;; end if; end; set @result = \u0026#39;\u0026#39;; call add_col(\u0026#39;zyb_employee\u0026#39;,\u0026#39;senior_1 \u0026#39;,\u0026#39;tinyint(1) NULL DEFAULT 0 COMMENT \u0026#34;是否是高级用户，0 不是 1 是\u0026#34;\u0026#39;,@result); select @result; ","date":"2021-08-10T18:19:22Z","permalink":"https://dccmmtop.github.io/posts/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E6%B7%BB%E5%8A%A0%E5%88%97/","section":"posts","tags":["mysql"],"title":"存储过程添加列"},{"categories":null,"contents":"MySQL隔离级别 隔离级别是针对数据库 ACID 中的I(隔离性)来说的\n原子性 一致性 隔离性 持久性 隔离性 通常来说，一个事务所做的修改，在最终提交以前，对其他事务是不可见的，为什么是 “通常来说”，因为这种特性和隔离级别有关\n未提交读 READ UNCOMMITTED 事务中的修改，即使没有提交，其他事务也可以读到。被称为脏读。 从性能上来说，并没有比其他级别好太多，却有很多问题，一般很少使用。\n读提交： 事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。造成了不可重复读（虚读）。\n可重复读： 事务A读取与搜索条件相匹配的若干行。事务B以插入或删除行等方式来修改事务A的结果集，然后再提交。事务A再读取时，却发现数据发生了变化。造成了幻读。\n不可重复读真正含义应该包含虚读和幻读。 所谓的虚读，也就是大家经常说的不可重复读，是指在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。比如事务T1读取某一数据，事务T2读取并修改了该数据，T1为了对读取值进行检验而再次读取该数据，便得到了不同的结果。\n一种更易理解的说法是：在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。\n所谓幻读，是指事务A读取与搜索条件相匹配的若干行。事务B以插入或删除行等方式来修改事务A的结果集，然后再提交。 幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样.一般解决幻读的方法是增加范围锁RangeS，锁定检锁范围为只读，这样就避免了幻读。\n简单来说，幻读是由插入或者删除引起的。\n","date":"2021-08-10T18:17:48Z","permalink":"https://dccmmtop.github.io/posts/mysql%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","section":"posts","tags":["mysql"],"title":"MySQL隔离级别"},{"categories":null,"contents":"开启慢日志 在 MySQL 中，慢查询日志默认为OFF状态，通过如下命令进行查看： show variables like \u0026quot;slow_query_log\u0026quot;;\n通过如下命令进行设置为 ON 状态：\nset global slow_query_log = \u0026quot;ON\u0026quot;;\n日志存储位置 其中slow_query_log_file属性，表示慢查询日志存储位置，其日志默认名称为 host 名称。 如下所示： show variables like \u0026quot;slow_query_log_file\u0026quot;;\n+---------------------+----------------------------------------------+ | Variable_name | Value | +---------------------+----------------------------------------------+ | | slow_query_log_file | /usr/local/mysql/data/hostname.log | +---------------------+----------------------------------------------+ 2 rows in set (0.01 sec) 也可使用 以下命令进行修改：\nset global slow_query_log_file = ${path}/${filename}.log;\n设置阀值 慢查询 查询时间，当SQL执行时间超过该值时，则会记录在slow_query_log_file 文件中，其默认为 10 ，最小值为 0，(单位： 秒)。\nmysql\u0026gt; show variables like \u0026#34;long_query_time\u0026#34;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 1 row in set (0.00 sec) 可通过以下命令进行修改：\nmysql\u0026gt; set global long_query_time = 5; 记录为走索引的sql 在 MySQL 中，还可以设置将未走索引的SQL语句记录在慢日志查询文件中(默认为关闭状态)。通过下述属性即可进行设置：\nmysql\u0026gt; set global log_queries_not_using_indexes = \u0026#34;ON\u0026#34;; Query OK, 0 rows affected (0.00 sec) 注意事项 设置该属性后，只要SQL未走索引，即使查询时间小于long_query_time值，也会记录在慢SQL日志文件中。 该设置会导致慢日志快速增长，开启前建议检查慢查询日志文件所在磁盘空间是否充足。 在生产环境中，不建议开启该参数。 解析日志文件 慢查询日志以#作为起始符。 User@Host：表示用户 和 慢查询查询的ip地址。 如上所述，表示 root用户 localhost地址。 Query_time: 表示SQL查询持续时间， 单位 (秒)。 Lock_time: 表示获取锁的时间， 单位(秒)。 Rows_sent: 表示发送给客户端的行数。 Rows_examined: 表示：服务器层检查的行数。 set timestamp ：表示 慢SQL 记录时的时间戳。 慢sql日志分析工具pt-query-digest 下载安装 yum install percona-toolkit-3.0.3-1.el7.x86_64.rpm 下载地址：https://www.percona.com/downloads/percona-toolkit/3.0.3/\n推荐用法 查询保存到query_history表查看慢sql，数据结构清晰，方便分析，方便与其他系统集成。\npt-query-digest --user=root --password=epPfPHxY --history h=10.8.8.66,D=testDb,t=query_review--create-history-table mysql_slow.log --since \u0026#39;2020-10-01 09:30:00\u0026#39; --until \u0026#39;2020-10-21 18:30:00\u0026#39; 常见用法 直接分析慢查询文件\npt-query-digest slow.log \u0026gt; slow_report.log 分析某个用户的慢sql\npt-query-digest \u0026ndash;filter \u0026lsquo;($event-\u0026gt;{user} || \u0026ldquo;\u0026rdquo;) =~ m/^root/i\u0026rsquo; slow.log 分析某个数据库的慢sql\npt-query-digest \u0026ndash;filter \u0026lsquo;($event-\u0026gt;{db} || \u0026ldquo;\u0026rdquo;) =~ m/^sonar/i\u0026rsquo; slow.log\n分析某段时间内的慢sql pt-query-digest mysql_slow.log --since \u0026#39;2020-09-21 09:30:00\u0026#39; --until \u0026#39;2020-09-21 18:30:00\u0026#39; 输出结果说明\n第一部分：总体统计结果 Overall：总共有多少条查询 Time range：查询执行的时间范围 unique：唯一查询数量，即对查询条件进行参数化以后，总共有多少个不同的查询 total：总计 min：最小 max：最大 avg：平均 95%：把所有值从小到大排列，位置位于95%的那个数，这个数一般最具有参考价值 median：中位数，把所有值从小到大排列，位置位于中间那个数 \u0026hellip;\u0026hellip; #语句执行时间 #锁占用时间 #发送到客户端的行数 #select语句扫描行数 #查询的字符数\n第二部分：查询分组统计结果 Rank：所有语句的排名，默认按查询时间降序排列，通过--order-by指定 Query ID：语句的ID，（去掉多余空格和文本字符，计算hash值） Response：总的响应时间 time：该查询在本次分析中总的时间占比 calls：执行次数，即本次分析总共有多少条这种类型的查询语句 R/Call：平均每次执行的响应时间 V/M：响应时间Variance-to-mean的比率 Item：查询对象 第三部分：每一种查询的详细统计结果 由下面查询的详细统计结果，最上面的表格列出了执行次数、最大、最小、平均、95%等各项目的统计。 ID：查询的ID号，和上图的Query ID对应 Databases：数据库名 Users：各个用户执行的次数（占比） Query_time distribution ：查询时间分布, 长短体现区间占比，本例中1s-10s之间查询数量是10s以上的两倍。 Tables：查询中涉及到的表 Explain：SQL语句 ","date":"2021-08-10T18:16:04Z","permalink":"https://dccmmtop.github.io/posts/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","section":"posts","tags":["mysql"],"title":"mysql慢日志"},{"categories":null,"contents":"安装 sudo apt-get install mysql-server mysql-client 配置远程可连接 你想myuser使用mypassword（密码）从任何主机连接到mysql服务器的话。\nmysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO \u0026#39;用户名\u0026#39;@\u0026#39;%\u0026#39;IDENTIFIED BY \u0026#39;你的密码\u0026#39; WITH GRANT OPTION; 如果你想允许用户myuser从ip为192.168.1.6的主机连接到mysql服务器，并使用mypassword作为密码\nmysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO \u0026#39;用户名\u0026#39;@\u0026#39;192.168.1.3\u0026#39;IDENTIFIED BY \u0026#39;你的密码\u0026#39; WITH GRANT OPTION; 最后\nmysql\u0026gt;FLUSH PRIVILEGES; 使修改生效，就可以了\n在远程主机上开放防火墙端口 sudo ufw allow 3306 或者关闭防火墙（不推荐）sudo ufw disable\n修改mysql配置文件 [mysqld] character-set-server = utf8 bind-address = 0.0.0.0 //修改ip地址 port = 3306 配置文件在/etc/mysql/mysql.conf.d/mysqld.cnf\n重启mysql服务：service mysql restart\n查看处于监听的服务状态：sudo netstat -aptn\n阿里云主机 如果你要连接的远程主机是阿里云服务器还需要配置安全组规则！！！ 开放入口，端口为3306/3306 优先级1 远程访问地址：0.0.0.0/0 点击保存\n","date":"2021-08-10T18:12:14Z","permalink":"https://dccmmtop.github.io/posts/ubuntu%E5%AE%89%E8%A3%85mysql%E5%8F%8A%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE/","section":"posts","tags":["mysql"],"title":"Ubuntu 安装 mysql 及配置远程访问"},{"categories":null,"contents":"GoAccess的多种展示方式 goaccess有多种数据可视化的方式,分别为:\n命令行输出格式化数据 利用access.log生成静态的可视化数据 生成实时可视化数据 注意，如果是编译安装且选择了 –enable-geoip=mmdb的话需要编辑配置文件，并在使用命令的时候带上参数 –config-file=/usr/local/etc/goaccess/goaccess.conf，如果是用包管理器安装的话则不需要\n命令行输出GoAccess goaccess /var/log/nginx/access.log -c，会先询问你数据的格式，我这里的日志使用的是第一种。\n解析accesslog生成静态html GoAccess还可以解析access.log生成静态html，以更加直观的方式来展示数据。\ngoaccess /var/log/nginx/access.log -o report.html \u0026ndash;log-format=COMBINED，之后再使用浏览器访问report.html即可查看报告，各种数据一应俱全。 实时解析访问日志 GoAccess除了可以生成静态的html文件，还可以生成实时网站访问数据！\ngoaccess /var/log/nginx/access.log -o /var/www/html/report.html \u0026ndash;log-format=COMBINED \u0026ndash;real-time-html \u0026ndash;config-file=/usr/local/etc/goaccess/goaccess.conf\n添加中文支持 Goaccess 1.3之后的版本提供了多语言支持，先在命令行中执行 apt install language-pack-zh-hans 安装中文包，再使用export LANG=zh_CN.UTF-8修改环境变量，再次使用 goaccess /var/log/nginx/access.log -o /var/www/html/report.html \u0026ndash;log-format=COMBINED \u0026ndash;real-time-html \u0026ndash;config-file=/usr/local/etc/goaccess/goaccess.conf启动GoAccess可以发现已经是中文界面了。\n关于实时模式，可以查看官网的demo https://rt.goaccess.io/?20200209201008\n异常退出 如果实时模式没有正常退出，可能无法再次正常启动，GoAccess默认使用7890 websocket端口，所以使用lsof -i:7890查看占用该端口的进程号并kill即可。\nssl支持 如果需要在加密连接上输出实时数据，则需要使用 \u0026ndash;ssl-cert= 和 \u0026ndash;ssl-key=,我在设置之后访问report.html发现数据依旧是静态的，突然想起我用了cloudflare cdn，而7890端口并不在cloudflare的支持端口列表里面，所以我使用参数 \u0026ndash;ws-url=wss://服务器域名(我们的浏览器会尝试与该域名的8443端口见了ws连接):8443 \u0026ndash;port=8443 把端口改成了8443。令人没想到的是，此时的report.html使用代理链接的时候是可以连接的，并可以查看实时信息，而直接连接的时候依旧是静态数据，tcping一看。\n去cloudflare的官网可以发现如下内容\n只有端口 80 和 443 可兼容以下服务：\n对于启用了中国网络的域名的中国境内数据中心 HTTP/HTTPS 流量， 也就是说，国内是没办法通过cloudflare连接非80/443端口的…\n反向代理 但是也不是没有办法连接，最后我想到了反向代理的方案。\n将启动参数改为\u0026ndash;ws-url=wss://你的域名.com/goaccess \u0026ndash;port=7890\n修改nginx站点配置文件 /etc/nginx/site-available/default,添加下面内容\nlocation /goaccess {\rproxy_redirect off;\rproxy_pass https://127.0.0.1:7890;\rproxy_http_version 1.1;\rproxy_set_header Upgrade $http_upgrade;\rproxy_set_header Connection \u0026quot;upgrade\u0026quot;;\rproxy_set_header Host $http_host;\r}\r注意，如果你的站点配置文件里面开启了url重写，为了避免 /goaccess 受到影响，我们需要把该路径排除重写。\n把重写规则都放到location / 里面去 location / { if (-f $request_filename/index.html){ rewrite (.*) $1/index.html break; } if (-f $request_filename/index.php){ rewrite (.*) $1/index.php; } if (!-f $request_filename){ rewrite (.*) /index.php; } } #下面什么都不需要做\nlocation /goaccess/ { } 之后重启nginx，再访问report.html，发现左边齿轮处终于显示connect了。\n如果你只是自己看或者不在意ip暴露，其实直接使用ip直接连接不走cdn就没那么麻烦了。\n","date":"2021-08-10T18:10:24Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%86%E6%9E%90nginx%E6%97%A5%E5%BF%97/","section":"posts","tags":["nginx"],"title":"分析nginx日志"},{"categories":null,"contents":"现在有需求，想要开机就运行一些东西，方法有两个，一个是桌面级的启动，一个是系统级的启动\n桌面级别，就是进入桌面后，自动打开一些软件 系统级别就是再没有进入桌面就运行一些东西。 先谈桌面级的，比如 gnome，启动 gnome-tweak 工具就可以看到开机启动项目，添加进去即可。\n系统级别的，我觉得最简单的方式就是创建 systemctl 的 service 脚本。这个脚本放什么位置呢？ 我们运行一个命令就看到了：\n$ systemctl enable sshd Created symlink /etc/systemd/system/multi-user.target.wants/sshd.service → /usr/lib/systemd/system/sshd.service. 我们开起来 sshd 服务，显示出来 service 的位置，我们模仿这个做一个开机启动出来。 还记得很早的 linux 中有一个 rc.local 么？有什么需要开机启动的脚本直接丢进去就行了。 升级到了systemd 之后，这个玩意就消失了，我们尝试恢复他。\n先建立一个 rc-local.service， sudo vim /usr/lib/systemd/system/rc-local.service 然后，我们模仿其他的 service ，来写一下：\n[Unit]\rDescription=\u0026#34;/etc/rc.local Compatibility\u0026#34; #Wants=sshdgenkeys.service\r#After=sshdgenkeys.service\r#After=network.target\r[Service]\rType=forking\rExecStart=/etc/rc.local start\rTimeoutSec=0\rStandardInput=tty\rRemainAfterExit=yes\rSysVStartPriority=99\r[Install]\rWantedBy=multi-user.target 然后，我们创建 /etc/rc.local 文件： sudo touch /etc/rc.local sudo chmod +x /etc/rc.local 设置 rc.local 开机自启 sudo systemctl enable rc-local.service ","date":"2021-08-10T17:55:56Z","permalink":"https://dccmmtop.github.io/posts/arch%E6%B7%BB%E5%8A%A0rc.local%E5%AE%9E%E7%8E%B0%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/","section":"posts","tags":["linux"],"title":"arch添加rc.local 实现开机自启"},{"categories":null,"contents":"笔记本可以同时连接无线和有线，如果可以指定哪些ip使用无线，哪些ip地址可以使用有线，\n假设无线连接的是内网， 有线连接的是外网，可以设置静态路由，使访问不同的地址使用不同的网络\n首先确定 本机 无线和有线 的网关： sudo ip route show 如上图 wlp3s0 是我的无线设备，enp0s20f0u1 是有线设备， 可以把网线拔了来确定哪个是什么设备 无线设备使用 192.168.1.1 作为网关 有线设备使用 192.168.42.129 作为网关\n设置路由 假设 某公司的内网都是 10 开头的，\nip route add 10.0.0.0/8 via 192.168.1.1 dev wlp3s0 10.0.0.0/8 是DICP表示法，不懂可以使用 站长工具 计算\n此时在使用 sudo ip route show 查看路由表 永久生效 上述方法只能临时生效，可以写成脚本开机自启arch 添加rc.local 实现开机自启\n本方法已在 arch 中验证 其他系统 大同小异，寻找 代替 ip route 的命令即可\nwin 下设置方法 使用管理员打开 powershell\nipconfig 查看优先和无线的网关 route -p add 10.0.0.0 mask 255.0.0.0 192.168.2.1 添加 10 开头的ip 走内网\n","date":"2021-08-10T17:50:41Z","permalink":"https://dccmmtop.github.io/posts/%E5%90%8C%E6%97%B6%E8%BF%9E%E6%8E%A5%E5%86%85%E7%BD%91%E5%92%8C%E5%A4%96%E7%BD%91/","section":"posts","tags":["linux"],"title":"同时连接内网和外网"},{"categories":null,"contents":"Git配置多个SSH-Key 当有多个git账号时，比如： a. 一个gitee，用于公司内部的工作开发； b. 一个github，用于自己进行一些开发活动；\n解决方法 生成一个公司用的SSH-Key\n$ ssh-keygen -t rsa -C \u0026#39;xxxxx@company.com\u0026#39; -f ~/.ssh/gitee_id_rsa 生成一个github用的SSH-Key\n$ ssh-keygen -t rsa -C \u0026#39;xxxxx@qq.com\u0026#39; -f ~/.ssh/github_id_rsa 在 ~/.ssh 目录下新建一个config文件，添加如下内容（其中Host和HostName填写git服务器的域名，IdentityFile指定私钥的路径）\n# gitee Host gitee.com HostName gitee.com PreferredAuthentications publickey IdentityFile ~/.ssh/gitee_id_rsa # github Host github.com HostName github.com PreferredAuthentications publickey IdentityFile ~/.ssh/github_id_rsa 用ssh命令分别测试\n$ ssh -T git@gitee.com $ ssh -T git@github.com ","date":"2021-08-10T17:50:06Z","permalink":"https://dccmmtop.github.io/posts/git%E7%94%9F%E6%88%90%E5%85%AC%E9%92%A5/","section":"posts","tags":["git"],"title":"git生成公钥"},{"categories":null,"contents":"docSet 文档可用于 zeal dash 软件中。 zeal 在win 下 和 Linux 均有可用版本 dash 则只在 Mac 可用\n制作 dcocSet 文档主要分 3 步\n镜像文档网站 做镜像网站就是把整个网站爬下来，并且把 css 和 js 图片等静态资源文件转换成本地的路径， 主要使用工具是 wget\n以 vue 中文文档 为例:\nwget -r -p -np -k https://cn.vuejs.org/ 制作索引文件 zeal 可以快速的搜索文档主要利用了 sqlite 数据库，在数据库中有一张 serchIndex 表， 这张表常用的字段有三个，分别是 name , type, path\nname 关键词\ntype 关键词的类型，代表该关键词是函数，还是类，等等可选的字段有 Sections, Fun, classes\npath 点击关键词要跳转的路径\n所以，关键是制作一个这种合理的索引表 下面是使用 ruby 实现制作索引表的功能，以及一些目录的生成\nrequire \u0026#39;nokogiri\u0026#39; require \u0026#34;sqlite3\u0026#34; require \u0026#34;fileutils\u0026#34; class HtmlToDoc def initialize(html_dir, docset_name) @html_path = html_dir @docset_name = \u0026#34;#{docset_name}.docset\u0026#34; @name = docset_name mkdir_file create_plist @con = SQLite3::Database.new(@dsidx) @con.execute(\u0026#34;CREATE TABLE IF NOT EXISTS searchIndex(id INTEGER PRIMARY KEY, name TEXT, type TEXT, path TEXT)\u0026#34;); end # 插入数据 def update_db(name, path, type = \u0026#39;Classes\u0026#39;) @con.execute(\u0026#39;INSERT OR IGNORE INTO searchIndex(name, type, path) VALUES (?,?,?)\u0026#39;,[name,type,path]) puts name,path end # 提取url，根据你的需求更改提取规则 def add_urls(html_path) doc = Nokogiri::HTML(File.open(html_path).read) doc.css(\u0026#34;h3\u0026gt;a\u0026#34;).each do |tag| name = tag.parent.text.strip if name.size \u0026gt; 0 \u0026amp;\u0026amp; tag[:href] path = tag[:href].strip.split(\u0026#34;#\u0026#34;).last update_db(name,html_path + \u0026#34;#\u0026#34; + path) end end end # 生成目录 def mkdir_file FileUtils.rm_r(@docset_name) if File.exists?(@docset_name) @doc_dir = \u0026#34;#{@docset_name}/Contents/Resources/Documents\u0026#34; FileUtils.mkdir_p(@doc_dir) @dsidx = \u0026#34;#{@docset_name}/Contents/Resources/docSet.dsidx\u0026#34; FileUtils.touch(@dsidx) @plist = \u0026#34;#{@docset_name}/Contents/info.plist\u0026#34; FileUtils.touch(@plist) puts \u0026#34;目录创建成功\u0026#34; end # 制作plist 文件 # 各种key 的意思请参考 dash 官方文档 def create_plist plist = \u0026lt;\u0026lt;-EOF \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;CFBundleIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;CFBundleName\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;DashDocSetFamily\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;DocSetPlatformFamily\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;requests\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;isDashDocset\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;isJavaScriptEnabled\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;dashIndexFilePath\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;#{@name}\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; EOF File.open(@plist,\u0026#34;w\u0026#34;).write(plist) end # 移动文档 def copy_files FileUtils.cp_r(@html_path.split(\u0026#34;/\u0026#34;).first, @doc_dir) # 将docSet 文档移动到 zeal 目录下 # local_doc_dir = \u0026#34;/home/dccmmtop/.local/share/Zeal/Zeal/docsets\u0026#34; # FileUtils.cp_r(@docset_name, local_doc_dir) end def start Dir.open(@html_path).each do |file| next unless file =~ /.html$/ add_urls(File.join(@html_path, file)) end copy_files end end if ARGV[0] == \u0026#34;-h\u0026#34; puts \u0026#39;ruby ./convert.rb \u0026#34;要生成文档的html地址(要包含整个网站的根目录)\u0026#34; \u0026#34;生成文档的名字\u0026#34;\u0026#39; puts \u0026#34;例子： ruby convert.rb cn.vuejs.org/v2/guide vue\u0026#34; else HtmlToDoc.new(ARGV[0],ARGV[1]).start end 移动docSet目录 最后将 制作好的 docSet 文件夹移动到 zeal 的文档目录下, 也可以将上面脚本中 copy_files 方法最后两行去掉\n","date":"2021-08-10T17:33:16Z","permalink":"https://dccmmtop.github.io/posts/%E5%88%B6%E4%BD%9Cdocset%E6%96%87%E6%A1%A3/","section":"posts","tags":["linux"],"title":"制作docSet文档"},{"categories":null,"contents":"git 库中已经有文件被跟踪，如何忽略本地改动后的跟踪 放入到.gitinore 并没有什么用，还是会显示改动，是否要提交，看着很烦\n忽略本地文件，且不会对线上库里的文件造成影响，执行此命令:\ngit update-index --assume-unchanged filename\n如果想撤销忽略，提交此文件的改动，执行此命令：\ngit update-index --no-assume-unchanged filename\n如果忽略的文件多了，可以使用以下命令查看忽略列表\ngit uls-files -v | grep '^h\\ '\n提取文件路径，方法如下\ngit ls-files -v | grep '^h\\ ' | awk '{print $2}'\n所有被忽略的文件，取消忽略的方法，如下\ngit ls-files -v | grep '^h' | awk '{print $2}' |xargs git update-index --no-assume-unchanged ","date":"2021-08-10T17:10:57Z","permalink":"https://dccmmtop.github.io/posts/%E5%BF%BD%E7%95%A5%E6%94%B9%E5%8A%A8/","section":"posts","tags":["git"],"title":"忽略改动"},{"categories":null,"contents":"为什么要使用tmux? 对我个人而言，tmux 对我最大的吸引力就是 多窗口，以及会话的保持与恢复，我可以很方便的切换窗口，以及快速恢复工作环境\n安装 我使用的是源码安装，\n下载源码 下载链接 解压 进入到解压目录，执行 ./configure --prefix=/usr/local/tmux3.1c \u0026amp;\u0026amp; make 然后执行 sudo make install 我个人喜欢指定目录，方便日后卸载，或切换版本,中间可能会有缺少依赖包的错误，根据错误提示信息取网上搜索对应的包安装即可，其实linux发展到现在，很多错误提示信息都非常详细，英语不好的可以翻译下，就知道失败的原因了，有时反而比直接复制错误信息取网上搜索快的多。 此时已经安装完毕，可执行文件在 /usr/lcoal/tmux3.1c/bin/tmux, 可以在 /usr/bin/ 下创建 tmux 的软链，或者直接在 ~/.bash_profile 文件中编写 alias tmux='/usr/local/tmux3.1c/tmux' 然后在终端执行 source ~/.bash_profile 就可以让配置文件生效了。 配置及安装插件 下面是我的配置文件,以及说明\n##-- bindkeys --# ### 修改前缀键为 ctrl a set -g prefix ^a unbind ^b bind a send-prefix # 窗口切换 我设置的键位和 vim 相通 # ctrl + j 上一个窗口 bind -n C-j previous-window # ctrl + k 下一个窗口 bind -n C-k next-window # 设置松开鼠标不会自动跳到屏幕底部，这个特性需要稍微高一点的版本才支持 set -g mouse on unbind -T copy-mode-vi MouseDragEnd1Pane # 快速加载配置文件，不用重启 tmux bind-key r source-file ~/.tmux.conf \\; display-message \u0026#34;tmux.conf reloaded\u0026#34; # 下面是底部状态栏的配置，个人不喜欢花里胡哨，简单够用即可 set -g message-style \u0026#34;bg=#00346e, fg=#ffffd7\u0026#34; # tomorrow night blue, base3 set -g status-style \u0026#34;bg=#00346e, fg=#ffffd7\u0026#34; # tomorrow night blue, base3 set -g status-left \u0026#34;#[bg=#0087ff] ❐ Hi!\u0026#34; # blue set -g status-left-length 400 set -g status-right \u0026#34;\u0026#34; #set -g status-right \u0026#34;#[bg=red] %Y-%m-%d %H:%M \u0026#34; #set -g status-right-length 600 set -wg window-status-format \u0026#34; #I #W \u0026#34; set -wg window-status-current-format \u0026#34; #I #W \u0026#34; set -wg window-status-separator \u0026#34;|\u0026#34; set -wg window-status-current-style \u0026#34;bg=red\u0026#34; # red set -wg window-status-last-style \u0026#34;fg=red\u0026#34; # split window unbind % # 水平分隔窗口 快捷键是 prefix \\ (我的前缀是 ctrl + A) bind \\\\ split-window -h unbind \u0026#39;\u0026#34;\u0026#39; # 垂直分隔窗口 快捷键是 prefix - bind - split-window -v # select pane 面板之间跳转 bind k selectp -U # above (prefix k) bind j selectp -D # below (prefix j) bind h selectp -L # left (prefix h) bind l selectp -R # right (prefix l) # 下面是插件部分 # 用来保存会话 和 回复会话的 # 保存会话快捷键是 prefix + ctrl + s (对应我的就是 ctrl + A ctrl + s 中间不要松开 ctrl 键) # 恢复会话： prefix + ctrl + r set -g @plugin \u0026#39;tmux-plugins/tmux-resurrect\u0026#39; set -g @continuum-restore \u0026#39;on\u0026#39; set -g @resurrect-save-bash-history \u0026#39;on\u0026#39; set -g @resurrect-capture-pane-contents \u0026#39;on\u0026#39; # 设置保存 vim 的工作状态 set -g @resurrect-strategy-vim \u0026#39;session\u0026#39; # 每隔60秒自动保存一次 set -g @continuum-save-interval \u0026#39;60\u0026#39; # 鼠标选中即复制的插件 set -g @plugin \u0026#39;tmux-plugins/tmux-yank\u0026#39; set -g @yank_action \u0026#39;copy-pipe\u0026#39; set -g @yank_with_mouse on # 安装插件的插件 # ctrl + I 即自动下载插件安装。 run \u0026#39;~/.tmux/plugins/tpm/tpm\u0026#39; 结束！ 有任何问题，欢迎下面留言，或者加我好友讨论哟！或者请我吃个糖果 (^_^) ","date":"2021-08-10T11:21:12Z","permalink":"https://dccmmtop.github.io/posts/%E4%BD%BF%E7%94%A8tmux/","section":"posts","tags":null,"title":"Linux tmux 安装及配置"},{"categories":null,"contents":"Linux修改时区的正确方法 CentOS和Ubuntu的时区文件是/etc/localtime，但是在CentOS7以后localtime以及变成了一个链接文件\n[root@centos7 ~]# ll /etc/localtime lrwxrwxrwx 1 root root 33 Oct 12 11:01 /etc/localtime -\u0026gt; /usr/share/zoneinfo/Asia/Shanghai 如果采用直接cp的方法修改系统时区，那么就会把它所链接的文件修改掉，例如把美国的时区文件内容修改成了上海的时区内容，有可能会导致有些编程语言或程序在读取系统时区的时候发生错误，因此正确的修改方法是：\nCentOS6、Ubuntu16\ncp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime CentOS7、RHEL7、Scientific Linux 7、Oracle Linux 7 最好的方法是使用timedatectl命令\ntimedatectl list-timezones |grep Shanghai #查找中国时区的完整名称 Asia/Shanghai\ntimedatectl set-timezone Asia/Shanghai #其他时区以此类推\n或者直接手动创建软链接(验证过) ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ","date":"2021-08-10T11:18:51Z","permalink":"https://dccmmtop.github.io/posts/%E4%BF%AE%E6%94%B9%E6%97%B6%E5%8C%BA/","section":"posts","tags":null,"title":"修改时区"},{"categories":null,"contents":"在目标机器中修改/etc/ssh/sshd_conf文件 将UseDNS 的缺省值由yes修改为no，并重启ssh\n","date":"2021-08-10T10:48:44Z","permalink":"https://dccmmtop.github.io/posts/ssh%E6%85%A2/","section":"posts","tags":null,"title":"ssh慢"},{"categories":null,"contents":"通过rename命令批量重命名文件 基本语法 示例\n改变文件扩展名 大写改成小写 更改文件名模式 通过rename命令批量重命名文件 基本语法\nrename [-n -v -f] \u0026lt;pcre\u0026gt; \u0026lt;files\u0026gt; \u0026lsquo;pcre’是Perl兼容正则表达式，它表示的是要重命名的文件和该怎么做。正则表达式的形式是‘s/old-name/new-name/’。 ‘-v’选项会显示文件名改变的细节（比如：XXX重命名成YYY）。 ‘-n’选项告诉rename命令在不实际改变名称的情况下显示文件将会重命名的情况。这个选项在你想要在不改变文件名的情况下模拟改变文件名的情况下很有用。 ‘-f’选项强制覆盖存在的文件。 示例\n改变文件扩展名 假设你有许多.jpeg的图片文件，你想要把它们的名字改成.jpg。下面的命令就会将.jpeg 文件改成 *.jpg。 rename 's/\\.jpeg/\\.jpg/' *.jpeg 大写改成小写 有时你想要改变文件名的大小写，你可以使用下面的命令。 把所有的文件改成小写 rename 'y/A-Z/a-z/' 把所有的文件改成大写 rename 'y/a-z/A-Z/' * 更改文件名模式 现在让我们考虑更复杂的包含子模式的正则表达式。在PCRE中，子模式包含在圆括号中，符后接上数字（比如1，$2）。 下面的命令会将‘imgNNNN.jpeg’变成‘danNNNN.jpg’。\nroot@root:~$ rename -v \u0026#39;s/img_(\\d{4})\\.jpeg/dan_$1.jpg/\u0026#39; *.jpeg img_5417.jpeg renamed as dan_5417.jpg img_5418.jpeg renamed as dan_5418.jpg img_5419.jpeg renamed as dan_5419.jpg img_5420.jpeg renamed as dan_5420.jpg img_5421.jpeg renamed as dan_5421.jpg img_5422.jpeg renamed as dan_5422.jpg 下面的命令会将‘img_000NNNN.jpeg’变成‘dan_NNNN.jpg’。\nroot@root:~$ rename -v \u0026#39;s/img_\\d{3}(\\d{4})\\.jpeg/dan_$1.jpg/\u0026#39; *.jpeg img_0005417.jpeg renamed as dan_5417.jpg img_0005418.jpeg renamed as dan_5418.jpg img_0005419.jpeg renamed as dan_5419.jpg img_0005420.jpeg renamed as dan_5420.jpg img_0005421.jpeg renamed as dan_5421.jpg img_0005422.jpeg renamed as dan_5422.jpg 上面的例子中，子模式‘\\d{4}’会捕捉4个连续的数字，捕捉的四个数字匹配模式对应$1, 将会用于新的文件名。\n","date":"2021-08-10T10:12:57Z","permalink":"https://dccmmtop.github.io/posts/linux%E6%89%B9%E9%87%8F%E9%87%8D%E5%91%BD%E5%90%8D/","section":"posts","tags":null,"title":"linux批量重命名"},{"categories":null,"contents":"设置ss git config --global http.proxy \u0026#39;socks5://127.0.0.1:1080\u0026#39; git config --global https.proxy \u0026#39;socks5://127.0.0.1:1080\u0026#39; 设置代理 git config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 取消代理 git config --global --unset http.proxy git config --global --unset https.proxy ","date":"2021-08-08T17:18:35Z","permalink":"https://dccmmtop.github.io/posts/git%E8%AE%BE%E7%BD%AE%E5%92%8C%E5%8F%96%E6%B6%88%E4%BB%A3%E7%90%86/","section":"posts","tags":["git"],"title":"git 设置和取消代理"},{"categories":null,"contents":" ","date":"2021-08-03T23:02:05Z","permalink":"https://dccmmtop.github.io/posts/%E5%8D%95%E5%90%91%E9%80%9A%E9%81%93/","section":"posts","tags":null,"title":"单向通道"},{"categories":null,"contents":"var 方式 var name type = expiression\nvar name string = \u0026#34;zhangsan\u0026#34; var name = \u0026#34;zhangsan\u0026#34; var name string // 默认值是 \u0026#34;\u0026#34; 变量列表声明 var 方式通常用户和初始化类型不一致的局部变量，或则初始化值不重要的情况\n短变量声明 多变量声明 i,j := 0,1\n重点 := 代表声明 = 标识赋值\n交换值 i,j = j,i\n第二次声明等同赋值 第二行 err 等同于赋值\n至少声明一个变量 ","date":"2021-08-01T11:36:29Z","permalink":"https://dccmmtop.github.io/posts/%E5%8F%98%E9%87%8F/","section":"posts","tags":["go"],"title":"变量"},{"categories":null,"contents":"数组 初始化 指定长度 a := [2]int{1,2} // 指定长度和字面量 不指定长度 a := [...]int{1,2} //不指定长度，有后面的列表来确定其长度 指定总长度，通过索引初始化, 没有初始化的位置使用默认值 a := [3]int{1:1, 2:3} 不指定总长度，通过索引初始化, 最后一个索引为总长度,没有初始化的位置使用默认值 a := [...]int{1:1, 2:3, 5:9} 特点 长度固定,不可以追加元素 是值类型，赋值或作为函数参数，都是值拷贝 数组长度是类型的一部分 [10]int 和 [20]int 是不同类型 可以根据数组创建切片 切片 创建切片 由数组创建 array[b:e] array 表示数组名，b表示开始索引，可以不指定，默认是0， e表示结束索引，可以不指定，默认是数组长度len(array) array[b:e] 表示创建 e-b 个元素的切片，第一个元素是 array[b],最后一个元素是 array[e-1] 由 make 函数创建\n// len = 10 cap = 10 a := make([]int, 10) //len = 10 cap = 15 b := make([]int, 10, 15) 切片操作 len() 返回长度 cap() 返回底层数组容量 append() 追加元素 copy() 复制切片 字符串与切片的转换 str := \u0026#34;hello,世界\u0026#34; // 转换成字节切片 a := []byte(str) // 转换成 rune 切片 b := []rune(str) ","date":"2021-07-29T23:11:14Z","permalink":"https://dccmmtop.github.io/posts/%E6%95%B0%E7%BB%84%E4%B8%8E%E5%88%87%E7%89%87/","section":"posts","tags":null,"title":"数组与切片"},{"categories":null,"contents":"用在常量声明中，初始值为0，一组常量同时声明时，其值逐行增加\n类似枚举 const ( c0 = iota //c0 == 0 c1 = iota //c1 == 1 c2 = iota //c2 == 2 ) 简写模式\nconst ( c0 = iota // c0 == 0 c1 // c1 == 1 c2 // c2 == 2 ) 分开的const 分开的const语句， iota 的值每次都是从0开始\nconst c0 = iota // c0 == 0 const c1 = iota // c1 == 0\n","date":"2021-07-29T22:49:53Z","permalink":"https://dccmmtop.github.io/posts/iota-%E7%94%A8%E6%B3%95/","section":"posts","tags":null,"title":"iota 用法"},{"categories":null,"contents":"1.Mac下编译Linux, Windows Linux CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build filename.go Windows CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build filename.go 如: CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o helloworld-windows helloworld.go\n2.Linux下编译Mac, Windows Mac CGO_ENABLED=0 GOOS=darwin GOARCH=amd64 go build filename.go Windows CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build filename.go 3.Windows下编译Mac, Linux Mac SET CGO_ENABLED=0 SET GOOS=darwin SET GOARCH=amd64 go build filename.go Linux SET CGO_ENABLED=0 SET GOOS=linux SET GOARCH=amd64 go build filename.go\n","date":"2021-07-25T10:24:03Z","permalink":"https://dccmmtop.github.io/posts/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/","section":"posts","tags":null,"title":"交叉编译"},{"categories":null,"contents":" ps -ef | grep nginx | awk -F ' ' '{print $2}'| xargs\n","date":"2021-07-10T16:58:13Z","permalink":"https://dccmmtop.github.io/posts/%E5%A4%9A%E8%A1%8C%E5%8F%98%E5%8D%95%E8%A1%8C/","section":"posts","tags":null,"title":"多行变单行"},{"categories":null,"contents":"有时需要根据自己的工作场合去扩展 git 命令,比如\n推送到仓库后自动打开浏览器跳转到发起合并求页面 分支命名比较长其相似度比较大时，自动补全不那么有效率，给每个分支编号，输入指定编号即可切换对应的分支 我最推荐的一种方式是利用 shell 脚本的特性，将脚本命名为 git-xxxx 方式，在终端就可以通过 git xxx 的方式运行该命令\n下面是两个例子：\n给每个分支编码 在 /usr/lcoal/bin 先新建 git-brr 文件,输入下面脚本：\n#!/bin/bash git branch --no-color | cat -n | sed \u0026#39;s/*/ /\u0026#39; | awk \u0026#39;{print $2 \u0026#34; (\u0026#34;$1\u0026#34;)\u0026#34;}\u0026#39; sudo chomd +x ./git-brr 赋予可执行权限\n然后到一个项目下执行 git brr 指定编号切换分支 在 /usr/lcoal/bin 先新建 git-coo 文件,输入下面脚本\n#!/bin/bash git checkout $( git brr | egrep \u0026#34;\\($1)$\u0026#34; | egrep -o \u0026#39;.+ \u0026#39;) 然后 sudo chomd +x ./git-coo 赋予可执行权限\n此时就可已通过 git coo 4 切换对应的分支了\n","date":"2021-06-10T17:17:09Z","permalink":"https://dccmmtop.github.io/posts/%E6%89%A9%E5%B1%95git%E5%8A%9F%E8%83%BD%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F/","section":"posts","tags":["git"],"title":"扩展git功能的一种方式"},{"categories":null,"contents":"基本类型以及范围 选择优化的数据类型 MySQL 支持的数据类型非常多，选择正确的数据类型对于获得高性能至关重要。不管 存储哪种类型的数据，下面几个简单的原则都有助于做出更好的选择。\n更小的通常更好。 一般情况下,应该尽量使用可以正确存储数据的最小数据类型二。更小的数据类型通 常更快，因为它们占用更少的磁盘、内存和 CPU 缓存，并且处理时需要的 CPU 周 期也更少。\n但是要确保没有低估需要存储的值的范围，因为在 schema 中的多个地方增加数据类 型的范围是一个非常耗时和痛苦的操作。如果无法确定哪个数据类型是最好的，就 选择你认为不会超过范围的最小类型。(如果系统不是很忙或者存储的数据量不多， 或者是在可以轻易修改设计的早期阶段，那之后修改数据类型也比较容易) 。\n简单就好 简单数据类型的操作通常需要更少的 CPU 周期。例如，整型比字符操作代价更低， 因为字符集和校对规则 (排序规则)使字符比较比整型比较更复杂。这里有两个例子: 一个是应该使用 MySQL 内建的类型圭 而不是字符串来存储日期和时间，另外一个 是应该用整型存储 卫 地址。稍后我们将专门讨论这个话题。\n尽量避免 NULL 很多表都包含可为 NULL (空值) 的列，即使应用程序并不需要保存 NULL 也是如此， 这是因为可为 NULL 是列的默认属性主:。通常情况下最好指定列为 NOT NULL, 除非真 的需要存储 NULL值。 如果查询中包含可为 NULL 的列，对 MySQL 来说更难优化，因为可 NULL 的列使 得索引、索引统计和值比较都更复杂。可为 NULL 的列会使用更多的存储空间，在 MySQL 里也需要特殊处理。当可为 NULL 的列被索引时，每个索引记录需要一个额 外的字节，在 MyISAM 里甚至还可能导致固定大小的索引(例如只有一个整数列的 索引) 变成可变大小的索引。 通常把可为 NULL 的列改为 NOT NULL 带来的性能提升比较小，所以 (调优时) 没有 必要首先在现有 schema 中查找并修改掉这种情况，除非确定这会导致问题。但是， 如果计划在列上建索引，就应该尽量避免设计成可为 NULL 的列。\n整数类型 类型 存储空间 TINYINT 8 SMALLINT 16 MEDIUMINT 24 INT 32 BIGINT 64 字符串 VARCHAR VARCHAR 类型用于存储可变长字符串，是最常见的字符串数据类型。它比定长类型 更节省空间，因为它仅使用必要的空间 (例如，越短的字符串使用越少的空间) 。有 一种情况例外，如果 MySQL 表使用 ROW_FORMAT=FIXED 创建的话，每一行都会使用 定长存储，这会很浪费空间。\nVARCHAR需要使用 1 或 2 个额外字节记录字符串的长度 : 如果列的最大长度小于或 等于 255 字节, 则只使用 1 个字节表示, 否则使用 2 个字节。假设采用 latinl 字符集， 一个 VARCHAR(16) 的列需要 11 个字节的存储空间。VARCHAR(1069) 的列则需要 1002 个字节，因为需要 2 个字节存储长度信息。\nVARCHAR节省了存储空间，所以对性能也有帮助。但是，由手答是变长的，在 UPDATE 时可能使行变得比原来更长，这就导致需要做额外的工作。如果一个行占用 的空间增长，并且在页内没有更多的空间可以存储，在这种情况下，不同的存储引 人擎的处理方式是不一样的。例如，MyISAM 会将行拆成不同的片段存储，InnoDB 则需要分裂页来使行可以放进页内。其他一些存储引擎也许从不在原数据位置更新 数据。\n下面这些情况下使用VARCHAR是合适的 : 字符串列的最大长度比平均长度大很多 ， 列的更新很少，所以碎片不是问题 ， 使用了像 UTF-8 这样复杂的字符集，每个字符 都使用不同的字节数进行存储。\nCHAR CHAR 类型是定长的 : MySQL 总是根据定义的字符串长度分配足够的空间。当存储 [CHAR值时全MYSQI研删除所有的末尾空格上(在 MySQL 4.1 和更老版本中 VARCHAR 也是这样实现的一 也就是说这些版本中 CHAR 和 VARCHAR 在逻辑上是一样的，区 别只是在存储格式上) 。 CHAR值佐根据需要采用空格进行填充以方便比较 EEAR适咎存储很知的字符十|或者所有值都接近同一个长度。例如，CHAR 非常适\n合存储密码的 MD5 值，因为这是一个定长的值。对于经常变更的数据，CHAR也比 VARCHAR更好，因为定长的 CHAR 类型不容易产生碎片。对于非常短的列，CHAR 比 VARCHAR 在存储空间上也更有效率。例如用 CHAR(1) 来存储只有 Y 和N的值，如果 采用单字节字符集二 只需要一个字节，但是 VARCHAR(1) 却需要两个字节，因为还有 一个记录长度的额外字节。\n慷慨是不明智的 使用 VARCHAR(5) 和 VARCHAR(290) 存储“heLLo\u0026rsquo; 的空间开销是一样的。那么使用更 短的列有什么优势吗?\n事实证明有很大的优势。更长的列会消耗更多的内存，因为 MYSQL 通常会分配固 定大小的内存块来保存内部值。尤其是使用内存临时表进行排序或操作时会特别粳 粒。在利用磁盘临时表进行排序时也同样糟楼。\n所以最好的策略是只分配真正需要的空间。\n日期和时间类型 MySQL 可以使用许多类型来保存日期和时间值，例如 YEAR 和 DATE。MySQL 能存储的 最小时间粒度为秒 (MariaDB 支持微秒级别的时间类型) 。但是 MySQL 也可以使用微秒 级的粒度进行临时运算，我们会展示怎么绕开这种存储限制。\n大部分时间类型都没有替代品，因此没有什么是最佳选择的问题。唯一的问题是保 存日期和时间的时候需要做什么。MySQL 提供两种相似的日期类型 : DATETIME 和 TIMESTAMP。对于很多应用程序，它们都能工作，但是在某些场景，一个比另一个工作 得好。让我们来看一下。\nDATETIME 这个类型能保存大范围的值，从 1001 年到 9999 年，精度为秒。它把日期和时间封 装到格式为YYYYMMDDHHMMSS 的整数中，与时区无关。使用8 个字节的存储 空间。 默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME值，例如 “2008-01-16 22:37:08\u0026quot;。这是 ANSI 标准定义的日期和时间表示方法。\nTIMESTAMP 就像它的名字一样，TIMETAMP 类型保存了从 1970 年 1 月 1 日午夜 格林尼治标准 时间) 以来的秒数,它和 UNIX 时间戳相同。TIMESTAMP 只使用 4 个字节的存储空间， 因此它的范围比 DATETIME 小得多 : 只能表示从 1970 年到 2038 年。MySQL 提供了 FROM_UNIXTINME() 函数把 Unix 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函 数把日期转换为 Unix 时间发。 MySQL 4.1 以及更新的版本按照 DATETIME 的方式格式化 TIMESTAMP 的值，但是 MySQL 4.0 以及更老的版本不会在各个部分之间显示任何标点符号。这仅仅是显示 格式上的区别，TIMESTAMP 的存储格式在各个版本都是一样的。 TIMESTAMP 显示的值也依赖于时区。MyYSQL 服务器、操作系统，以及客户端连接都 有时区设置。 因此，存储值为 0 的 TIMESTAMP 在美国东部时区显示 “1969-12-31 19:00:00\u0026quot;, 45 格林尼治时间差 5 个小时。有必要强调一下这个区别 : 如果在多个时区存储或访问 数据，TIMESTAMP 和 DATETIME 的行为将很不一样。前者提供的值与时区有关系，后 者则保留文本表示的日期和时间。 TIMESTAMP 也有 DATETIME 没有的特殊属性。默认情况下，如果插和人时没有指定第一 个TIMESTAMP 列的值,MySQL 则设置这个列的值为当前时间\u0026quot;。 在插入一行记录时， MySQL 默认也会更新第一个 TIMESTAMP 列的值 (除非在 UPDATE 语句中明确指定了 值)。你可以配置任何 TIMESTAMP 列的插入和更新行为。最后，TIMESTAMP 列默认为 NOT NULL，这也和其他的数据类型不一样。\n除了特殊行为之外, 通常也应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。 有时候人们会将 Unix 时间截存储为整数值，但这不会带来任何收益。用整数保存时间 截的格式通常不方便处理，所以我们不推荐这样做。\n如果需要存储比秒更小粒度的日期和时间值怎么办? MySQL 目前没有提供合适的数据 类型，但是可以使用自己的存储格式 : 可以使用 BIGINT 类型存储微秒级别的时间截，或 者使用DOUBLE 存储秒之后的小数部分。这两种方式都可以，或者也可以使用 MariaDB\n","date":"2021-04-13T23:59:32Z","permalink":"https://dccmmtop.github.io/posts/%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E4%BB%A5%E5%8F%8A%E8%8C%83%E5%9B%B4/","section":"posts","tags":["mysql"],"title":"基本类型以及范围"},{"categories":null,"contents":"先sort排序，再去重 :sort //直接排序 :g/^\\(.*\\)$\\n\\1$/d //去除重复行 :g/\\%(^\\1$\\n\\)\\@\u0026lt;=\\(.*\\)$/d //功能同上，也是去除重复行 :g/\\%(^\\1\\\u0026gt;.*$\\n\\)\\@\u0026lt;=\\(\\k\\+\\).*$/d //功能同上，也是去除重复行 使用awk awk '!a[$0]++' file\n解析：\nawk流程是逐行处理的，默认从文件的第一行一直处理到文件最后一行 还要知道awk的基本命令格式是'pattern{action}'先匹配各种各样的样式，然后大括号里处理如何打印输出， 默认的只要匹配了pattern就{print $0}，如果pattern未命中其判断值为假（0）那么就不会再去处理{action}了 pattern命中则为判断值为真（非0）就去处理{action}。 举个最简单的例子：awk '1' file和awk '{print $0}' file 是一个道理，都是从头到尾依次打印文件的每一行。\n'!a[$0]++' 分成几个部分简单解释下吧。 这个命令没有{action}也就是说，只要pattern部分判断值为真（非0）就打印该行，否则就跳过不打印 ！在awk是取相反的意思，就是把对的变成错的把真的变成假的，放在这个命令中是神马作用一会解释； a[$0] 这个非常好理解，建立数组a，其变量是文本中的每一行，awk里$1是第一列，$2是第二列，以此类推$NF是最后一列，而$0是代表所有列及分隔符，也就是一整行，这样如果pattern是真的那就打印一整行 ++的意思是a数组取变量完毕后，对该数组值+1 找个最简单的文档来解释一下\ncat file xxx yyy xxx zzz 这个文件有4行，其中第一、三行是重复的。套用这个命令处理流程如下： 获取第一行a[xxx]，因为这是第一行，数组a里从没见过xxx这个变量，那么自然他的值就是假（0）也就是说a[xxx]=0，这个时候！就有大作用了，他把a[xxx]假（0）变成了a[xxx]为真（!0）这个时候原本不该打印的第一行就变成了应该打印了，取逻辑反后对a[xxx]的值+1然后处理第二行 第二行a[yyy]这个情况跟刚才第一行的a[xxx]一样，也应该打印他 到第三行的时候情况变了，因为第一行已经出现过a[xxx]并且已经++过了，他的值已经是非0而不是前两行的0了，本应打印但这时候再由！取逻辑反就不必打印了 第四行a[zzz]就又和第一、二两行一样了。 所以执行完就是这个结果\nawk \u0026#39;!a[$0]++\u0026#39; file xxx yyy zzz 再把file搞稍微复杂点\nawk \u0026#39;{print NR,$0}\u0026#39; file 1 xxx 2 yyy 3 zzz 4 xxx 5 yyy 6 zzz 7 xxx 8 yyy 9 zzz 一共9行文本，3行一次重复。为了看得更清楚，本来默认的{print $0}稍微改下，变成{print NR,$0} NR表示行号。 那么现在来执行下刚才讲的试试看\nawk \u0026#39;!a[$0]++{print NR,$0}\u0026#39; file 1 xxx 2 yyy 3 zzz awk \u0026#39;a[$0]++{print NR,$0}\u0026#39; file 4 xxx 5 yyy 6 zzz 7 xxx 8 yyy 9 zzz ","date":"2019-11-09T18:05:59Z","permalink":"https://dccmmtop.github.io/posts/%E5%8E%BB%E9%99%A4%E9%87%8D%E5%A4%8D%E8%A1%8C/","section":"posts","tags":["linux"],"title":"去除重复行"},{"categories":null,"contents":"根据项目的进展，我们需要实现后台进行定时读取信息的功能，而最关键的实现部分是周期性功能，根据调研，决定使用whenever来实现这一功能。\ngithub：https://github.com/javan/whenever\n开发前需要明确的问题 whenever是怎样一种周期性机制？ whenever能为我们提供什么功能？ whenever为周期性任务提供了哪些控制方式？ 问题解决\nwhenever周期性机制\n我们来看一下github上面是怎么说的：\nWhenever is a Ruby gem that provides a clear syntax for writing and deploying cron jobs.\n意思就是说，whenever是一个ruby gem，但同时它是基于cron jobs的。\n那么什么是cron jobs呢？我们来看一下维基百科的定义：\nCron crontab命令常见于Unix和类Unix的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。该词来源于希腊语chronos（χρόνος），原意是时间。\n通常，crontab储存的指令被守护进程激活，crond常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。\n也就是说，crontab是在unix和类unix系统中用来实现周期性功能的指令。在网上搜一下，我们就会看到很多crontab指令相关的语法。\n根据上述的分析，我们可以得出这样的结论：\nwhenever事实上是一个cron翻译器，它将rails中的ruby代码翻译成cron脚本，从而将周期性的任务交给cron来执行。 这样，通过whenever我们可以使用ruby语言来写周期性任务代码，在ruby层控制代码，而不需要与shell脚本进行切换；另一方面，我们会发现，由于cron命令的强大，它的语法也因此变得很复杂，通过whenever，我们可以很方便的实现周期性任务。\n一个十分简单的demo 1.添加whenever(Gemfile)\ngem 'whenever', :require =\u0026gt;false\n2.生成config/schedule.rb文件\n执行命令：\nwheneverize\n3.添加自己的周期性任务\n在config/schedule.rb文件中添加：\nset :environment, :development every 2.minutes do runner \u0026#34;Timetest.mytime\u0026#34; end 其中，set :environment, :development是设置执行任务时的环境，默认情况下环境为production\n上述代码实现的是每两分钟读取当前时间并存入到数据库的功能。其中，runner方法执行的方法如下：\nclassTimetest \u0026lt; ApplicationRecord def self.mytime a = Timetest.new a.time_now = Time.now a.save end end 这样，在rails中实现whenever的代码就算是写完了，真的是简单到不行啊！（实在忍不住感慨一句）\n下面就要执行周期性任务了。\n4.执行周期性任务\n在rails工程文件夹下进行一下操作\n更新schedule.rb中的任务到cronjob中 whenever -i\n可以看到这样的打印结果：\n[write]crontabfileupdated\n执行周期性任务 whenever -w\n可以看到：\n[write]crontabfilewritten\n此时我们的周期性任务便在后台运行了，此时查看我们的任务：\ncrontab -l\n可以看到以下打印：\n# Begin Whenever generated tasks for:\n/home/vito/rails/test_of_rails/test_rails/config/schedule.rb0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58 * * * * /bin/bash -l -c \u0026#39;cd /home/vito/rails/test_of_rails/test_rails \u0026amp;\u0026amp; bundle exec bin/rails runner -e development \u0026#39;\\\u0026#39;\u0026#39;Timetest.mytime\u0026#39;\\\u0026#39;\u0026#39;\u0026#39;# End Whenever generated tasks for: /home/vito/rails/test_of_rails/test_rails/config/schedule.rb 这样，我们的周期性任务就算是在顺利执行了。\n需要注意的一点是运行时crontab的环境（rails和crontab环境不匹配时whenever无法执行），一般调试时多使用的是development环境，而不设置时默认的是production环境，如果你使用crontab -l发现是production环境，可以使用\ncrontab -e\n直接修改为development，或者直接将-e production删掉即可。\n经过上述流程，我们便可以成功地实现周期性任务了。如果此时你发现自己的周期性任务还是没有执行，那你就得好好看看你自己的任务代码了，很可能是执行的任务代码本身有问题，而与whenever的实现没有太大的关系了。\n","date":"2019-10-31T22:04:20Z","permalink":"https://dccmmtop.github.io/posts/whenever%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F/","section":"posts","tags":["ruby"],"title":"whenever使用方式"},{"categories":null,"contents":"以前就遇到过的问题。有如下情景：\n1.假设现在我要将文件a的部分内容复制到文件b中，一般情况，我会用vs或者sp命令打开这两个文件然后用y和p进行复制粘贴。但是如果分别用vim打开这两个文件就不能完成上述动作。 2.假设我先在要把vim打开的源代码中的部分内容复制到博客中，一般我会用vim编辑好以后，退出用gedit打开，或者cat一下，再复制到系统剪切板，再粘贴。\n今天，对于vim这个没办法跟“外界”交流的特性忍够了，决定解决一下。\n1.首先，查看vim版本是否支持clipboard vim --version | grep \u0026#34;clipboard\u0026#34;1 结果如下：\nclipboard 前面有一个小小的减号，说明不支持。\n2.如果不支持的话，需要安装图形化界面的vim，或者重新编译vim sudo apt-get install vim-gnome1 安装完成后再次执行：\nvim --version | grep \u0026#34;clipboard\u0026#34;1 发现已经支持clipboard\n3.vim的寄存器 打开vim输入:reg查看vim的寄存器，当支持clipboard之后，会多出\u0026quot;+寄存器，表示系统剪切板，在vim中进入visual视图后使用\u0026quot;Ny(N表示特定寄存器编好)，将内容复制到特定的剪切板，那么我们的目的是要复制到系统剪切板则需要选中内容后输入命令：\n\u0026#34;+y1 粘贴到特定的寄存器也是同理。例如\u0026quot;+p将系统剪切板的内容拷贝到vim中（非编辑模式下）。\n4. 映射\n\u0026#34; 从系统剪切板粘贴 nnoremap P \u0026#34;+p \u0026#34; 复制到系统剪切板 vmap Y \u0026#34;+y ","date":"2019-10-31T22:01:14Z","permalink":"https://dccmmtop.github.io/posts/vim%E4%B8%AD%E4%B8%8E%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%B2%98%E8%B4%B4%E5%92%8C%E5%A4%8D%E5%88%B6/","section":"posts","tags":["vim"],"title":"vim中与系统的粘贴和复制"},{"categories":null,"contents":"自定义脚本 编写自己的需要开启自启的脚本，如下：\n#!/usr/bin/env ruby `cd /home/mc/code/rails_app/master_crawler_trm \u0026amp;\u0026amp; nohup rake real_time_extract_word \u0026gt; /dev/null 2\u0026amp;\u0026gt;1` 启动脚本 #!/usr/bin/env ruby start = \u0026#34; sudo -u mc /home/mc/bin/real_time_extract_words \u0026#34; stop = \u0026#34;ps -ef | grep real_time_extract_word | grep -v grep | cut -c 9-15 | xargs kill -9\u0026#34; action = ARGV[0] if action == \u0026#39;start\u0026#39; system(start) puts \u0026#34;启动成功\u0026#34; elsif action == \u0026#39;stop\u0026#39; system(stop) puts \u0026#34;已停止\u0026#34; elsif action == \u0026#39;restart\u0026#39; system(stop) system(start) puts \u0026#34;已重启\u0026#34; end 这个文件可以被 ubuntu 下的服务读取并执行，保存该文件为 custom_sh 然后 sudo chmod +x custom_sh赋予可执行权限\n要注意自己的脚本需要在哪个用户下执行\n操作该服务 sudo systemctl daemon-reload # 添加新的 或者改动 服务之后要刷新 sudo service custom_sh start # 启动 sudo service custom_sh stop # 停止 sudo service custom_sh status # 查看状态 开启自启 sudo update-rc.d custom_sh defaults ","date":"2019-04-28T17:59:37Z","permalink":"https://dccmmtop.github.io/posts/ubuntu%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%9C%8D%E5%8A%A1%E4%BB%A5%E5%8F%8A%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/","section":"posts","tags":["linux"],"title":"ubuntu中添加服务以及开机自启"},{"categories":null,"contents":"假如已经连接的的数据库名是 word_development, 现在需要添加一个名为 trademark_development 和 trademark_test 的本地数据库\n步骤如下:\n添加配置信息 在 config/database.yml 添加如下信息\ntrademark_default: \u0026amp;trademark_default adapter: postgresql encoding: unicode pool: \u0026lt;%= ENV.fetch(\u0026#34;RAILS_MAX_THREADS\u0026#34;) { 5 } %\u0026gt; trademark_development: \u0026lt;\u0026lt;: *trademark_default database: trademark_development # 如果是远程的数据库添加如下信息 #username: username #password: 123456 #host: xxxx #port: xxx trademark_test: \u0026lt;\u0026lt;: *trademark_default database: trademark_test 关联 model 模仿 ApplicationRecord 这个抽象类，新建一个 TrademarkBase 抽象类，然后指明这个类连接的数据库的配置\nclass TrademarkBase \u0026lt; ActiveRecord::Base self.abstract_class = true establish_connection \u0026#34;trademark_#{Rails.env}\u0026#34;.to_sym end 继承抽象类 新建的 model 继承该类，然后指定表名\nclass TrademarkUser \u0026lt; TrademarkBase self.table_name = :users end 测试 进入控制台中\npry(main)\u0026gt; TrademarkUser =\u0026gt; TrademarkUser (call \u0026#39;TrademarkUser.connection\u0026#39; to establish a connection) 可以看到成功关联另一个数据库中的表对象\n","date":"2019-04-03T11:48:31Z","permalink":"https://dccmmtop.github.io/posts/rails%E8%BF%9E%E6%8E%A5%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BA%93/","section":"posts","tags":["rails"],"title":"Rails连接多个数据库"},{"categories":null,"contents":" 摘抄自 《posrgreSQL 修炼之道 从小工到专家》\n触发器（trigger）是一种由事件自动触发执行的特殊的存储过程，这些事件可以是对一个表进行 INSERT,UPDATE,DELETE 等操作\n触发器经常用于加强数据库的完整性约束和业务规则上的约束等\n创建触发器 创建触发器的语法如下\nCREATE [ CONSTRAINT ] TRIGGER name { BEFORE | AFTER | INSTEAD OF } { event [ OR ... ] } ON table_name [ FROM referenced_table_name ] [ NOT DEFERRABLE | [ DEFERRABLE ] [ INITIALLY IMMEDIATE | INITIALLY DEFERRED ] ] [ REFERENCING { { OLD | NEW } TABLE [ AS ] transition_relation_name } [ ... ] ] [ FOR [ EACH ] { ROW | STATEMENT } ] [ WHEN ( condition ) ] EXECUTE PROCEDURE function_name ( arguments ) 创建触发器的步骤 创建执行函数 先为触发器创建一个执行函数，此函数的返回类型为触发器类型，然后即可创建相应的触发器\n下面使用一个例子来讲解触发器的使用，假设有一张学生表（student），和一张考试成绩表（score）\nCREATE TABLE student( student_no int primary key, student_name varchar(40), age int ) CREATE TABLE score( student_no int, chinese_no int, math_score int, test_date date ) 如果想删除学生表的一条记录时，把这个学生在成绩表中的成绩也删除掉，这时就可以使用触发器。先建触发器的执行函数：\nCREATE OR REPLACE FUNCTION student_delete_trigger() RETURNS TRIGGER AS $$ BEGIN DELETE FROM score WHERE student_no = OLD.student_no; RETURN OLD; END; $$ LANGUAGE plpgsql; 创建触发器 CREATE TRIGGER delete_student_trigger AFTER DELETE ON student FOR EACH ROW EXECUTE PROCEDURE student_delete_trigger(); 测试 按照上面的语句创建好触发器后还需要相应的测试，先插入一些测试数据：\nINSERT INTO student VALUES(1, \u0026#39;张三\u0026#39;, 14); INSERT INTO student VALUES(2, \u0026#39;李四\u0026#39;, 13); INSERT INTO student VALUES(3, \u0026#39;王二\u0026#39;, 15); INSERT INTO score VALUES(1, 85, 75, date \u0026#39;2013-05-23\u0026#39;); INSERT INTO score VALUES(1, 89, 73, date \u0026#39;2013-09-18\u0026#39;); INSERT INTO score VALUES(2, 68, 83, date \u0026#39;2013-05-23\u0026#39;); INSERT INTO score VALUES(2, 73, 85, date \u0026#39;2013-09-18\u0026#39;); INSERT INTO score VALUES(3, 72, 79, date \u0026#39;2013-05-23\u0026#39;); INSERT INTO score VALUES(3, 78, 82, date \u0026#39;2013-05-23\u0026#39;); 现在把学好为 3 的学生 “王二” 从表 \u0026ldquo;student\u0026rdquo; 删掉：\nDELETE FROM stduent WHERE student_no = 3; 这时可以查询成绩表 \u0026lsquo;score\u0026rsquo; 可以发现学号（student_no ） 为 3 的学生成绩记录也被删除掉了\n语句级触发器与行级触发器 语句级触发器 语句级触发器是指执行每个 SQL 时，只执行一次，行级触发器是指每行就会执行一次。一个修改 0 行的操作任然会导致合适的语句级触发器被执行。下面来看看相应的示例。\n假设对 student 的更新情况记录 log。可以为 student 建一张 log 表，如下：\nCREATE TABLE log_student( update_time timetamp, --操作的时间 db_user varchar(40), --操作的数据库用户名 opr_type varchar(6), --操作类型：insert delete udate ); 创建记录 log 的触发器函数：\nCREATE FUNCTION log_student_trigger() RETURNS trigger AS $$ BEGIN INSERT INTO log_student values(now(), user, TG_OP); RETURN NULL; END; $$ LANGUAGE \u0026#34;plpgsql\u0026#34;; 上面函数中的 \u0026ldquo;TG_OP\u0026rdquo; 是触发器中的特殊变量，代表 DML 操作类型。\n然后在 student 表上创建一个语句级触发器：\nCREATE TRIGGER log_student_trigger AFTER INSERT OR UPDATE OR DELETE ON student FOR STATEMENT EXECUTE PROCEDURE log_student_trigger(); 删除触发器 drop trigger log_student_log on student; 语句级触发器即使在没有更新到数据时，也会被触发\n行级触发器 CREATE TRIGGER log_student_trigger_2 AFTER INSERT OR UPDATE OR DELETE ON student FOR ROW EXECUTE PROCEDURE log_student_trigger(); 行级触发器即使在没有更新到数据时，不会被触发\nBEFORE 触发器和 AFTER 触发器 通常，语句级别的 \u0026ldquo;before\u0026rdquo; 触发器是在语句开始做任何事之前被触发，而语句级别的\u0026quot;after\u0026quot; 触发器是在语句结束时才触发的。行级别的\u0026quot;before\u0026quot; 触发器是在对特定行进行操作之前触发的，而行级别的 \u0026ldquo;after\u0026rdquo; 触发器是在语句结束时才触发的，但是它会在任何语句级别的 \u0026ldquo;after\u0026rdquo; 触发器被触发之前触发\nBEFORE 触发器可以直接修改 \u0026ldquo;NEW\u0026rdquo; 值以改变实际更新的值，具体例子如下：\n先建一个触发器函数：\nCREATE FUNCTION student_new_name_trigger() RETURNS trigger AS \u0026#39; BEGIN NEW.student_name = NEW.student_name || NEW.student_no; RETURN NEW; END; \u0026#39; LANGUAGE \u0026#34;plpgsql\u0026#34;; 这个函数的作用是，插入或者更新时，在 \u0026ldquo;student_name\u0026rdquo; 后面加上 \u0026ldquo;student_no\u0026rdquo; 学号。也就是直接修改 \u0026ldquo;NEW.student_name\u0026rdquo; ,语句如下：\nNEW.student_name = NEW.student_name||NEW.student_no 在这中情况下只能使用 BEFORE 触发器，因为 BEFORE 触发器是在更新数据之前触发的，所以这时修改了\u0026quot;NEW.student_name\u0026quot;, 后面实际更新到数据库中的值就变成了 \u0026ldquo;student_name||student_no\u0026rdquo;\n如果使用了 AFTER ，则修改 \u0026ldquo;NEW\u0026rdquo; 是没用的\n删除触发器 删除触发器的语法如下：\nDROP TRIGGER [ IF EXISTS ] name ON table [CASCADE | RESTRICT ];\n其中的语法说明如下。\nIF EXISTS: 如果指定的触发器不存在，那么发出一个 notice 而不是跑出一个错误 CASCADE: 级联删除依赖此触发器的对象 RESTRICT: 这是默认值，如果有任何依赖对象存在，那么拒绝删除 **在 PostgresSQL 中要在删除触发器的语法中指定 \u0026ldquo;ON table\u0026rdquo;，而在其他一些数据库的语发可能直接是 \u0026ldquo;DROP TRIGGER name\u0026rdquo; **\n删除触发器时，触发器的函数不会被删除。不过，当表删除时，表上的触发器也会被删除\n触发器的行为 触发器函数与返回值。语句级触发器总是返回 NULL。 即必须显式的在触发器函数中写上 \u0026ldquo;RETURN NULL\u0026rdquo;, 如果没有写，将导致出错。\n对于 \u0026ldquo;BEFORE\u0026rdquo; 和 \u0026ldquo;INSTEAD OF\u0026rdquo; 这类行级触发器函数来说，如果返回的是 NULL， 则表示忽略对当前行的操作，如果返回的是非 NULL 行，对与 INSERT 和 UPDATE 来说，返回的行将成为被插入的行或将要是更新的行。\n对于　AFTER 这类行级触发器来说，其返回值将会被忽略。\n如果同一时间上有多个触发器，则将按触发器名字的顺序来触发。　如果是　\u0026ldquo;BEFORE\u0026rdquo; 和　\u0026ldquo;INTEAD OF\u0026rdquo; 行级触发器，每个触发器所返回的行（可能已经被修改）将成为下一个触发器的输入，如果\u0026quot;BEFORE\u0026quot; 和　\u0026ldquo;INSTEAD OF\u0026rdquo; 行级触发器返回的内容为空，那么该行上的其他行级触发器也不会被触发。\n触发器函数中的特殊变量 当把一个 PL/pgSQL 函数当做触发器函数调用的时候，系统会在顶层生命字段里自动创建几个特殊变量，比如在之前的几个例子当中　\u0026ldquo;NEW\u0026rdquo;, \u0026ldquo;OLD\u0026rdquo;, \u0026ldquo;TG_OP\u0026rdquo;, 变量等。可以使用的变量如下这些：\nNEW: 该变量为 INSERT/UPDATE 操作触发的行级触发器中存储的新数据行，数据类型是　\u0026ldquo;RECORD\u0026rdquo; ,在语句级别的触发器里没有分配次变量，　DELETE 操作触发的行级触发器中也没有分配此变量\nOLD：数据类型是 record。在 update、delete 操作触发时存储旧的数据行。\nTG_NAME：数据类型是 name。触发器名称。\nTG_WHEN：内容为\u0026quot;BEFORE\u0026quot;或“AFTER”，可以用来判断是 BEFORE 触发器还是 AFTER 触发器。\nTG_LEVEL：内容为“ROW”或“STATEMENT”，可以用来判断是语句级触发器还是行级触发器。\nTG_OP：内容为“INSERT”、“UPDATE”、“DELETE”、“TRUNCATE”，用于指定 DML 语句类型。\nTG_RELID：触发器所在表的 oid。\nTG_TABLE_NAME：触发器所在表的表名称。\nTG_SCHEMA_NAME：触发器所在表的模式。\nTG_NARGS：在创建触发器语句中赋予触发器过程的参数个数。\nTG_ARGV[]：text 类型的一个数组。创建触发器语句中指定的参数。\n","date":"2019-03-30T09:15:25Z","permalink":"https://dccmmtop.github.io/posts/%E8%A7%A6%E5%8F%91%E5%99%A8%E5%88%9B%E5%BB%BA%E7%A4%BA%E4%BE%8B/","section":"posts","tags":["SQL","触发器"],"title":"触发器创建示例"},{"categories":null,"contents":" 翻译并整理： https://collectiveidea.com/blog/archives/2016/07/22/solutions-to-potential-upgrade-problems-in-rails-5 https://blog.bigbinary.com/2016/08/29/rails-5-disables-autoloading-after-booting-the-app-in-production.html\nautoload和eager_load autoload: 在常量使用之前不会加载，只有当使用一个当前不存在常量时，会在 autoload_paths 寻找，然后加载它，当在给定的目录中找不到这个常量时，就会触发错误。并且 autoload 是非线程安全的\neager_load 在使用常量之前，加载 eager_load_paths 中的所有常量\nRails5的变化 在 Rails5 中的生产环境下， autoload 默认是被禁用的， 有三种办法解决这个问题\n添加路径到 eager_load_paths 假如自己写的类在lib下\n在 config/application.rb 中\nconfig.eager_load_paths \u0026lt;\u0026lt; Rails.root.join(\u0026#39;lib\u0026#39;) 重新启用 autoload 你可以在任何环境中重新启用 autoload, 但是这种方法在高版本中可能被弃用\nconfig.enable_dependency_loading = true 把代码移动到 app/ 目录下 Rails 在默认的情况下会 autolaod 和 eager_load app/ 目录下的所有内容。这样可以减少你额外的配置。例如把 lib 目录 移动到 app/lib/\n译注： 这种方式使 rails 的目录变得混乱， 不建议\n","date":"2019-01-19T10:21:48Z","permalink":"https://dccmmtop.github.io/posts/rails5%E4%B8%AD%E7%9A%84autoload%E5%92%8Ceager_load/","section":"posts","tags":["rails"],"title":"Rails5中的autoload和eager_load"},{"categories":null,"contents":" 翻译自 https://medium.com/@florenceliang/some-notes-about-using-hash-sort-by-in-ruby-f4b3a700fc33\nruby中的sort和sort_by 是两个非常有用的工具，我们可以按照自己的想法排序。比如我们可以按照名字字母表或者名字的长短进行排序。但是ruby文档中却没有过多的说明，因此本文主要说明一下关于Hash的sort和sort_by的用法\n返回数组 首先要知道的是 Hash 对象调用 sort 或者 sort_by 后，会返回一个数组，而不是Hash。如果想得到 Hash，需要使用 to_h 方法\n假如有下面的 Hash\nhash = {a:1, b:2, c:4, d:3, e:2} 调用sort方法，将会返回一个嵌套数组\nhash.sort # 没有指定按什么值进行排序，所顺序不会发生变化 =\u0026gt; [[:a, 1], [:b, 2], [:c, 4], [:d, 3], [:e 2]] 按照 hash 的值排序 hash.sort_by {|k, v| v} =\u0026gt; [[:a, 1], [:b, 2], [:e, 2], [:d, 3], [:c, 4]] 按照 hash 的值倒序 hash.sort_by {|k, v| -v} =\u0026gt; [[:c, 4], [:d, 3], [:b, 2], [:e, 2], [:a, 1]] 先按值倒序再按 key 正序 hash = {“w”=\u0026gt;2, “k”=\u0026gt;1, “l”=\u0026gt;2, “v”=\u0026gt;5, “d”=\u0026gt;2, “h”=\u0026gt;4, “f”=\u0026gt;1, “u”=\u0026gt;1, “p”=\u0026gt;1, “j”=\u0026gt;1} hash.sort_by {|k, v| [-v, k]} =\u0026gt; [[“v”, 5], [“h”, 4], [“d”, 2], [“l”, 2], [“w”, 2], [“f”, 1], [“j”, 1], [“k”, 1], [“p”, 1], [“u”, 1]] ","date":"2019-01-15T11:45:27Z","permalink":"https://dccmmtop.github.io/posts/ruby%E4%B8%ADhash%E7%9A%84%E6%8E%92%E5%BA%8F/","section":"posts","tags":["ruby"],"title":"ruby中hash的排序"},{"categories":null,"contents":":g/xxx/d，删除包含xxx的行 :v/xxx/d，删除不含xxx的行 :%s/xxx//gn，统计xxx个数，n表示只报告匹配的个数而不进行实际的替换。 详见「:help :v」或「help :g」\n","date":"2019-01-09T15:25:29Z","permalink":"https://dccmmtop.github.io/posts/vim%E5%88%A0%E9%99%A4%E4%B8%8D%E5%8C%85%E5%90%AB%E7%89%B9%E5%AE%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E8%A1%8C/","section":"posts","tags":["vim"],"title":"vim删除不包含特定字符串的行"},{"categories":null,"contents":"反引号 返回标准输出 output = `ls` puts \u0026#34;output is #{output}\u0026#34; Result of above code is\n$ ruby main.rb output is lab.rb 反引号执行系统命令时，会把异常抛给主线程 反引号会从主进程新开一个进程执行命令，如果子进程发生异常，会传递给主进程，如果主进程没有对异常进行处理，主进程就会终止。\n下面的例子中 执行一个‘xxxxx’非法的命令\noutput = `xxxx` puts \u0026#34;output is #{output}\u0026#34; 执行结果：\n$ ruby main.rb main.rb:1:in ``\u0026#39;: No such file or directory - xxxxxxx (Errno::ENOENT) from main.rb:1:in `\u0026lt;main\u0026gt;\u0026#39; 阻塞进程 主进程会一直等待反引号中的子进程结束\n检查命令的执行状态 使用 $?.success? 来检查命令的执行状态\noutput = `ls` puts \u0026#34;output is #{output}\u0026#34; puts $?.success? 结果:\n$ ruby main.rb output is lab.rb main.rb true 允许使用字符串插值 例子：\ncmd = \u0026#39;ls\u0026#39; `#{cmd}` x% x% 和反引号一样，它可以使用不同的分隔符\noutput = %x[ ls ] output = %x{ ls } system system 也可以执行系统命令，他和反引号有点相像。\n阻塞进程\n隐藏异常\nsystem 不会向主进程传递异常。\noutput = system(\u0026#39;xxxxxxx\u0026#39;) puts \u0026#34;output is #{output}\u0026#34; 结果：\n$ ruby main.rb output is 检查命令的执行状态 如果命令成功执行，则系统返回true（退出状态为零）。对于非零退出状态，它返回false, 如果命令执行失败，返回nil\nsystem(\u0026#34;command that does not exist\u0026#34;) #=\u0026gt; nil system(\u0026#34;ls\u0026#34;) #=\u0026gt; true system(\u0026#34;ls | grep foo\u0026#34;) #=\u0026gt; false exec exec 会替换掉当前进程，请看下面的例子：\n在 irb 中执行exec(\u0026rsquo;ls\u0026rsquo;)：\n$ irb e1.9.3-p194 :001 \u0026gt; exec(\u0026#39;ls\u0026#39;) lab.rb main.rb nsingh ~/dev/lab 1.9.3 $ 可以发现，执行完exec(\u0026ldquo;ls\u0026rdquo;)命令以后，已经退出irb，回到shell。\n由于exec替换了当前进程，因此如果操作成功，则不会返回任何内容。如果操作失败，则引发SystemCallError\nsh sh实际上是在呼叫系统, FileUtils在rake中添加了此方法。它允许以简单的方式检查命令的退出状态。\nrequire \u0026#39;rake\u0026#39; sh %w(xxxxx) do |ok, res| if !ok abort \u0026#39;the operation failed\u0026#39; end end popen3 如果你要捕获stdout和stderr，那么你应该使用popen3，因为这个方法允许你与stdin，stdout和stderr进行交互。\n我想以编程方式执行git push heroku master，我想捕获输出。这是我的代码。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;git push heroku master\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| puts \u0026#34;stdout is:\u0026#34; + stdout.read puts \u0026#34;stderr is:\u0026#34; + stderr.read end 输出：\nstdout is: stderr is: -----\u0026gt; Heroku receiving push -----\u0026gt; Ruby/Rails app detected -----\u0026gt; Installing dependencies using Bundler version 1.2.1 这里需要注意的重要一点是，当我执行程序ruby lab.rb时，我的终端在前10秒内没有看到任何输出。然后我将整个输出视为一个转储。 另外需要注意的是，heroku正在将所有这些输出写入stderr而不是stdout。\n所以我们应该捕获来自heroku的输出，因为它正在流式传输而不是在处理结束时将整个输出转储为一个单个的字符串块。\n这是修改后的代码。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;git push heroku master\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| while line = stderr.gets puts line end end 现在，当我使用ruby lab.rb执行上述命令时，我会逐步获得终端输出，就好像我输入了git push heroku master一样。 这是捕获流输出的另一个例子。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;ping www.google.com\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| while line = stdout.gets puts line end end 在上面的例子中，您将在终端上获得ping的输出，就像您在终端上键入ping www.google.com一样。\n现在让我们看看如何检查命令是否成功\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;ping www.google.com\u0026#39; Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr| exit_status = wait_thr.value unless exit_status.success? abort \u0026#34;FAILED !!! #{cmd}\u0026#34; end end popen2e popen2e类似于popen3，但合并了标准输出和标准错误。\nrequire \u0026#39;open3\u0026#39; cmd = \u0026#39;ping www.google.com\u0026#39; Open3.popen2e(cmd) do |stdin, stdout_err, wait_thr| while line = stdout_err.gets puts line end exit_status = wait_thr.value unless exit_status.success? abort \u0026#34;FAILED !!! #{cmd}\u0026#34; end end 在所有其他领域，此方法与popen3类似。\nProcess.spawn Kernel.spawn在子shell中执行给定的命令。它会立即返回进程ID。\nirb(main)\u0026gt; pid = Process.spawn(\u0026#34;ls -al\u0026#34;) =\u0026gt; 81001 ","date":"2019-01-03T09:50:00Z","permalink":"https://dccmmtop.github.io/posts/%E7%94%A8ruby%E6%89%A7%E8%A1%8C%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4/","section":"posts","tags":["ruby"],"title":"用ruby执行系统命令"},{"categories":null,"contents":"整理自https://devhints.io/xpath#prefixes\nDescendant selectors h1 //h1 div p //div//p ul \u0026gt; li //ul/li ul \u0026gt; li \u0026gt; a //ul/li/a div \u0026gt; * //div/* :root / :root \u0026gt; body /body Attribute selectors #id //*[@id=\u0026quot;id\u0026quot;] .class //*[@class=\u0026quot;class\u0026quot;] …kinda input[type=\u0026quot;submit\u0026quot;] //input[@type=\u0026quot;submit\u0026quot;] a#abc[for=\u0026quot;xyz\u0026quot;] //a[@id=\u0026quot;abc\u0026quot;][@for=\u0026quot;xyz\u0026quot;] a[rel] //a[@rel] a[href^='/'] //a[starts-with(@href, '/')] a[href$='pdf'] //a[ends-with(@href, '.pdf')] a[href*='://'] //a[contains(@href, '://')] a[rel~='help'] //a[contains(@rel, 'help')] …kinda Order selectors ul \u0026gt; li:first-child //ul/li[1] ul \u0026gt; li:nth-child(2) //ul/li[2] ul \u0026gt; li:last-child //ul/li[last()] li#id:first-child //li[@id=\u0026quot;id\u0026quot;][1] a:first-child //a[1] a:last-child //a[last()] Siblings h1 ~ ul //h1/following-sibling::ul h1 + ul //h1/following-sibling::ul[1] h1 ~ #id //h1/following-sibling::[@id=\u0026quot;id\u0026quot;] jQuery $('ul \u0026gt; li').parent() //ul/li/.. $('li').closest('section') //li/ancestor-or-self::section $('a').attr('href') //a/@href $('span').text() //span/text() Other things h1:not([id]) //h1[not(@id)] Text match //button[text()=\u0026quot;Submit\u0026quot;] Text match (substring) //button[contains(text(),\u0026quot;Go\u0026quot;)] Arithmetic //product[@price \u0026gt; 2.50] Has children //ul[*] Has children (specific) //ul[li] Or logic //a[@name or @href] Union (joins results) `//a Class check //div[contains(concat(\u0026#39; \u0026#39;,normalize-space(@class),\u0026#39; \u0026#39;),\u0026#39; foobar \u0026#39;)] Xpath doesn’t have the “check if part of space-separated list” operator, so this is the workaround (source).\n#Expressions Steps and axes // ul / a[@id='link'] Axis Step Axis Step Prefixes Prefix Example What // //hr[@class='edge'] Anywhere ./ ./a Relative / /html/body/div Root Begin your expression with any of these.\nAxes Axis Example What / //ul/li/a Child // //[@id=\u0026quot;list\u0026quot;]//a Descendant Separate your steps with /. Use two (//) if you don’t want to select direct children.\nSteps //div //div[@name=\u0026#39;box\u0026#39;] //[@id=\u0026#39;link\u0026#39;] A step may have an element name (div) and predicates ([...]). Both are optional. They can also be these other things:\n//a/text() #=\u0026gt; \u0026#34;Go home\u0026#34; //a/@href #=\u0026gt; \u0026#34;index.html\u0026#34; //a/* #=\u0026gt; All a\u0026#39;s child elements #Predicates Predicates //div[true()] //div[@class=\u0026#34;head\u0026#34;] //div[@class=\u0026#34;head\u0026#34;][@id=\u0026#34;top\u0026#34;] Restricts a nodeset only if some condition is true. They can be chained.\nOperators # Comparison //a[@id = \u0026#34;xyz\u0026#34;] //a[@id != \u0026#34;xyz\u0026#34;] //a[@price \u0026gt; 25] # Logic (and/or) //div[@id=\u0026#34;head\u0026#34; and position()=2] //div[(x and y) or not(z)] Use comparison and logic operators to make conditionals.\nUsing nodes # Use them inside functions //ul[count(li) \u0026gt; 2] //ul[count(li[@class=\u0026#39;hide\u0026#39;]) \u0026gt; 0] # This returns `\u0026lt;ul\u0026gt;` that has a `\u0026lt;li\u0026gt;` child //ul[li] You can use nodes inside predicates.\nIndexing //a[1] # first \u0026lt;a\u0026gt; //a[last()] # last \u0026lt;a\u0026gt; //ol/li[2] # second \u0026lt;li\u0026gt; //ol/li[position()=2] # same as above //ol/li[position()\u0026gt;1] # :not(:first-child) Use [] with a number, or last() or position().\nChaining order a[1][@href=\u0026#39;/\u0026#39;] a[@href=\u0026#39;/\u0026#39;][1] Order is significant, these two are different.\nNesting predicates //section[//h1[@id=\u0026#39;hi\u0026#39;]] This returns \u0026lt;section\u0026gt; if it has an \u0026lt;h1\u0026gt; descendant with id='hi'.\n#Functions Node functions name() # //[starts-with(name(), \u0026#39;h\u0026#39;)] text() # //button[text()=\u0026#34;Submit\u0026#34;] # //button/text() lang(str) namespace-uri() count() # //table[count(tr)=1] position() # //ol/li[position()=2] Boolean functions not(expr) # button[not(starts-with(text(),\u0026#34;Submit\u0026#34;))] String functions contains() # font[contains(@class,\u0026#34;head\u0026#34;)] starts-with() # font[starts-with(@class,\u0026#34;head\u0026#34;)] ends-with() # font[ends-with(@class,\u0026#34;head\u0026#34;)] concat(x,y) substring(str, start, len) substring-before(\u0026#34;01/02\u0026#34;, \u0026#34;/\u0026#34;) #=\u0026gt; 01 substring-after(\u0026#34;01/02\u0026#34;, \u0026#34;/\u0026#34;) #=\u0026gt; 02 translate() normalize-space() string-length() Type conversion string() number() boolean() #Axes Using axes //ul/li # ul \u0026gt; li //ul/child::li # ul \u0026gt; li (same) //ul/following-sibling::li # ul ~ li //ul/descendant-or-self::li # ul li //ul/ancestor-or-self::li # $(\u0026#39;ul\u0026#39;).closest(\u0026#39;li\u0026#39;) Steps of an expression are separated by /, usually used to pick child nodes. That’s not always true: you can specify a different “axis” with ::.\n// ul /child:: li Axis Step Axis Step Child axis # both the same //ul/li/a //child::ul/child::li/child::a child:: is the default axis. This makes //a/b/c work.\n# both the same # this works because `child::li` is truthy, so the predicate succeeds //ul[li] //ul[child::li] # both the same //ul[count(li) \u0026gt; 2] //ul[count(child::li) \u0026gt; 2] Descendant-or-self axis # both the same //div//h4 //div/descendant-or-self::h4 // is short for the descendant-or-self:: axis.\n# both the same //ul//[last()] //ul/descendant-or-self::[last()] Other axes Axis Abbrev Notes ancestor ancestor-or-self attribute @ @href is short for attribute::href child div is short for child::div descendant descendant-or-self // // is short for /descendant-or-self::node()/ namespace self . . is short for self::node() parent .. .. is short for parent::node() following following-sibling preceding preceding-sibling There are other axes you can use.\nUnions //a | //span Use | to join two expressions.\n#More examples Examples //* # all elements count(//*) # count all elements (//h1)[1]/text() # text of the first h1 heading //li[span] # find a \u0026lt;li\u0026gt; with an \u0026lt;span\u0026gt; inside it # ...expands to //li[child::span] //ul/li/.. # use .. to select a parent Find a parent //section[h1[@id=\u0026#39;section-name\u0026#39;]] Finds a \u0026lt;section\u0026gt; that directly contains h1#section-name\n//section[//h1[@id=\u0026#39;section-name\u0026#39;]] Finds a \u0026lt;section\u0026gt; that contains h1#section-name. (Same as above, but uses descendant-or-self instead of child)\nClosest ./ancestor-or-self::[@class=\u0026#34;box\u0026#34;] Works like jQuery’s $().closest('.box').\nAttributes //item[@price \u0026gt; 2*@discount] Finds \u0026lt;item\u0026gt; and check its attributes\n","date":"2019-01-02T17:56:48Z","permalink":"https://dccmmtop.github.io/posts/xpath%E7%94%A8%E6%B3%95/","section":"posts","tags":["xpath"],"title":"xpath用法"},{"categories":null,"contents":" 打开管理配置， \u0026ndash;\u0026gt; appearance \u0026ndash;\u0026gt; new 给新的 color theme 命名(gruvbox)，然后点击应用，保存。\n打开 .kde/share/apps/konsole/gruvbox.colortheme,清空文件内容\n在 manjaro 中， 该文件位于 ~/.local/share/konsole/ 目录下\n复制 https://github.com/morhetz/gruvbox-contrib/blob/master/konsole/Gruvbox_dark.colorscheme, 到 gruvbox.colorscheme.\nGruvbox_dark.colorscheme\n[Background] Color=40,40,40 [BackgroundIntense] Color=40,40,40 [Color0] Color=40,40,40 [Color0Intense] Color=146,131,116 [Color1] Color=204,36,29 [Color1Intense] Color=251,73,52 [Color2] Color=152,151,26 [Color2Intense] Color=184,187,38 [Color3] Color=215,153,33 [Color3Intense] Color=250,189,47 [Color4] Color=69,133,136 [Color4Intense] Color=131,165,152 [Color5] Color=177,98,134 [Color5Intense] Color=211,134,155 [Color6] Color=104,157,106 [Color6Intense] Color=142,192,124 [Color7] Color=168,153,132 [Color7Intense] Color=235,219,178 [Foreground] Color=235,219,178 [ForegroundIntense] Color=235,219,178 [General] Description=Gruvbox Opacity=1 Wallpaper= ","date":"2018-12-28T11:31:57Z","permalink":"https://dccmmtop.github.io/posts/yakuake%E9%85%8D%E7%BD%AEcolor-theme/","section":"posts","tags":["yakuake","color"],"title":"yakuake配置color-theme"},{"categories":null,"contents":"rails 中使用邮件服务是非常方便的，直接加配置文件就可以，参考指南\nqq 邮箱正确的配置如下\nproduction.rb / development.rb\nActionMailer::Base.delivery_method = :smtp config.action_mailer.perform_deliveries = true config.action_mailer.raise_delivery_errors = true config.action_mailer.default :charset =\u0026gt; \u0026#34;utf-8\u0026#34; ActionMailer::Base.smtp_settings = { :address =\u0026gt; \u0026#39;smtp.qq.com\u0026#39;, :port =\u0026gt; 465, :domain =\u0026gt; \u0026#39;qq.com\u0026#39;, :user_name =\u0026gt; ENV[\u0026#39;qq_mail_address\u0026#39;] # 授权码 :password =\u0026gt; ENV[\u0026#39;qq_mail_address\u0026#39;] :authentication =\u0026gt; \u0026#39;plain\u0026#39;, :ssl =\u0026gt; true, :enable_starttls_auto =\u0026gt; true } 切记，qq 邮箱后台要开启 POP3/SMTP 服务，开启的时候需要通过发送短信息启用，启用的时候会生成一个授权码，配置文件的 password 只需要填写授权码即可。\n","date":"2018-11-28T17:26:31Z","permalink":"https://dccmmtop.github.io/posts/rails%E5%8F%91%E9%80%81qq%E9%82%AE%E4%BB%B6%E7%9A%84%E9%85%8D%E7%BD%AE/","section":"posts","tags":["rails"],"title":"rails发送qq邮件的配置"},{"categories":null,"contents":"在初始化数据库系统时，有一个预定义的超级用户，这用户的名称与初始化该数据库的操作系统用户名相同，默认是 postgres，在这个超级用户连接数据库，然后创建出更多的用户。\n创建用户和角色 创建用户与角色的语法如下：\nCREATE ROLE name [ [ WITH] option [ ... ] ] // 或 CREATE ROLE name [ [ WITH] option [ ... ] ] 在 postgres 中，用户与角色是没有区别的，除了 \u0026ldquo;CREATE USER\u0026rdquo; 默认创建出来的用户具有登录（LOGIN）权限，而 \u0026ldquo;CREATE ROLE\u0026quot;创建出来的用户默认没有登录权限之外，没有任何不同。\n上面的 option 可以是以下内容：\nSUPERUSER | NOSUPERUSER: 表示创建出来的用户是否是超级用户，只能是超级用户才能创建超级用户。 CREATEDB | NOCREATEDB: 指定创建出来的用户是否具有执行 \u0026ldquo;CREATE DATABASE\u0026quot;的权限 CREATEROLE | NOCREATEROLE: 指定创建出来的用户是否具有创建其他角色的权限 CREATEUSER | NOCREATEUSER: 指定创建出来的用户是否具有创建其他用户的权限 INHERIT | NOINHERIT: 如果创建的用户拥有某一个或者某几个角色，这是若是指定 INHERIT，则表示用户自动拥有相应角色的权限，否则这个用户没有该角色的权限。 LOGIN|NOLOGIN：指定创建出来的用户是否有“LOGIN 的权限，可以临时地禁止一个用户的“LOGIN”权限，这时这个用户就不能连接到数据库了。 CONNECTION LIMIT connlimit：指定该用户可以使用的并发连接数量。默认值是-1， 表示没有限制 [ENCRYPTED | UNENCRYPTED] PASSWORD \u0026lsquo;password\u0026rsquo;：用于控制存储在系统表里面的口令是否加密。 VALID UNTIL \u0026rsquo;timestamp\u0026rsquo;：密码失效时间，如果不指定这个子句，那么口令将永远有效。 IN ROLE rolename[，\u0026hellip;]：指定用户成为哪些角色的成员，请注意没有任何选项可以把新角色添加为管理员，必须使用独立的 GRANT 命令来做这件事情。 ROLE rolename[，\u0026hellip;]：rolename 将成为这个新建的角色的成员。 ADMIN rolename[,．．．]：rolename 将有这个新建角色的 WITH ADMIN OPTION 权限。 权限的管理 在数据库中，每个数据库的逻辑结构对象（包括数据库）都有一个所有者， 也就是说任何数据库对象都是属于某个用户的，所有者默认就拥有所有权限。所以不需要把 对象的权限再赋给所有者。这也很好理解，自己创建的数据库对象，自己当然有全部的权限 了。当然，所有者出于安全考虑也可以选择废弃一些自己的权限。在 PostsgreSQL 数据库中， 删除一个对象及任意修改它的权力都不能赋予别人，它是所有者固有的，不能被赋予或撤销。 所有者也隐含地拥有把操作该对象的权限赋给别人的权利。\n一个用户的权限分为两类，一类是在创建用户时就指定的权限，这些权限如下：\n超级用户的权限 创建数据库的权限 是否允许 LOGIN 的权限 这些权限是创建用户时指定的，后面可使用 ALTER ROLE 命令来修改。 还有一类权限，是由命令 GRANT 和 REVOKE 来管理的，这些权限如下：\n在数据库中创建模式(SCHEMA) 允许在指定的数据库中创建临时表 连接某个数据库 在模式中创建数据库对象，如创建表、视图、函数等 在一些表中做 SELECT、UPDATE、INSERT、DELETE 等操作 在一张表的具体列上进行 SELECT、UPDATE、INSERT 操作 对序列进行查询（执行序列的 currval 函数）、使用（执行序列的 currval 函数和 nextval 函数）、更新等操作 在声明表上创建触发器 可以把表、索引等建到指定的表空间 在使用时，需要分清楚上述两类权限，如果要给用户赋予创建数据库的权限，则需要使用“ALTER ROLE”命令，而要给用户赋予创建模式的权限时，需要使用“GRANT”命令。\n“ALTER ROLE”命令的格式如下： ALTER ROLE name [ [ WITH ] option] 命令中的\u0026quot;option\u0026rsquo;与“CREATEROLE”中的含义相同，这里就不再重复叙述了。\n总结 PostgreSQL 中的权限是按以下几个层次进行管理的：\n首先管理赋在用户特殊属性上的权限，如超级用户的权限、创建数据库的权限、创建 用户的权限、Login 的权限，等等。 然后是在数据库中创建模式的权限。 接着是在模式中创建数据库对象的权限，如创建表、创建索引，等等。 之后是查询表、往表中插人数据、更新表、删除表中数据的权限。 最后是操作表中某些字段的权限。 ","date":"2018-11-25T22:58:15Z","permalink":"https://dccmmtop.github.io/posts/postgres%E7%B3%BB%E5%88%97%E4%B9%8B%E7%94%A8%E6%88%B7%E5%8F%8A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/","section":"posts","tags":["database"],"title":"postgres系列之用户及权限管理"},{"categories":null,"contents":"整理自ruby-china 基础的用法 def print_heredoc puts \u0026lt;\u0026lt;EOF this is the first line this is the second line EOF end print_heredoc 输出：\nthis is the first line this is the second line 如果你觉得代码太难看（这根本不符合 Ruby 的风格），你可能会这样写：\ndef print_heredoc puts \u0026lt;\u0026lt;EOF this is the first line this is the second line EOF end print_heredoc 你会发现高亮显示已经不对了，它还会报这样的一个错误：\ntest.rb:6: can\u0026#39;t find string \u0026#34;EOF\u0026#34; anywhere before EOF test.rb:2: syntax error, unexpected end-of-input, expecting tSTRING_CONTENT or tSTRING_DBEG or tSTRING_DVAR or tSTRING_END 可以缩进的用法 希望代码写的漂亮一点的话，就得多做点工作，在第一个 EOF 前加上一个减号就 OK 了：\ndef print_heredoc puts \u0026lt;\u0026lt;-EOF this is the first line this is the second line EOF end print_heredoc 输出：\nthis is the first line this is the second line ruby 2.3 引入一个新的语法 def hello puts \u0026lt;\u0026lt;~HEREDOC I know I know You will like it. HEREDOC end hello 完美输出:\nI know I know You will like it. heredoc 的本质 有下面一个方法：\ndef a_method(string, integer) puts \u0026#34;the string is #{string} and the integer is #{integer}\u0026#34; end 一般这么用这个方法：\na_method \u0026#34;the string\u0026#34;, 1 如果想用 heredoc 呢？这里就需要说下那个\u0026lt;\u0026lt;-EOF到底是什么东西？其实\u0026lt;\u0026lt;-EOF只是个占位符，写上它以后，它就代表将要输入的字符串，这个字符串的判断是从\u0026lt;\u0026lt;-EOF的下一行开始计算，一直碰到只有EOF的一行（这一行只有一个EOF），这个字符串就这样计算出来的。如果上面的这个方法要用 heredoc 的话，就可以这样写：\na_method \u0026lt;\u0026lt;-EOF, 1 this is the first line this is the second line EOF 输出：\nthe string is this is the first line this is the second line and the integer is 1 可以看到，这个\u0026lt;\u0026lt;-EOF就好像一个实参一样，我们也可以把他完全当个实参来对待，它是个字符串，那么就可以调用字符串的方法，像这样：\na_method \u0026lt;\u0026lt;-EOF.gsub(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;), 1 this is the first line this is the second line EOF 输出：\nthe string is this is the first linethis is the second line and the integer is 1 我们把它掰直了，哈哈。换个写法更能体现 heredoc 的本质：\na_method(\u0026lt;\u0026lt;-EOF.gsub(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;), 1) this is the first line this is the second line EOF heredoc 的小技巧 那个\u0026lt;\u0026lt;-EOF为什么叫EOF，为什么不叫ABC，你可以试试，叫ABC也可以，但是末尾那个也要写 ABC，要保持配对。在有些编辑器（Atom，RubyMine）中甚至会根据占位符将 heredoc 中的内容高亮显示，比如可以写\u0026lt;\u0026lt;-RUBY, \u0026lt;\u0026lt;-HTML等等。\n假如前面的那个方法要传入两个字符串该怎么写呢？很简单：\na_method \u0026lt;\u0026lt;-STR1, \u0026lt;\u0026lt;-STR2 this is for STR1 STR1 this is for STR2 STR2 输出：\nthe string is this is for STR1 and the integer is this is for STR2 有一点点晕，解释一下。\u0026lt;\u0026lt;-STR1在前面，它就会一直找到只包含STR1的那一行，并把其中的内容替换掉\u0026lt;\u0026lt;-STR1，而\u0026lt;\u0026lt;-STR2在后面，它不会包括\u0026lt;\u0026lt;-STR1和STR1中的部分， 会一直找到只包含STR2的那一行，然后替换\u0026lt;\u0026lt;-STR2。只需记住是只包含占位符的一行，像this is for STR1中虽然包含STR1，但是这一行还有其他字符，heredoc 就不会在这一行结束，而是接着往下找。同时需要知道，虽然是占位符，但是可以调用字符串的方法，在 Rails5 中最近的一个 pull request 中 DHH 就用了很多 heredoc。不过我找了半天没找到，等以后找到再把链接付在这里。\n前面的输出结果中，大家一定发现一个问题，就是字符串的换行和行首的缩进，有时候你想要他们，有时候可不是，我们可以用gsub来替换。在 Rails 中已经有一个好用的方法了：\n2.times do puts \u0026lt;\u0026lt;-STR.strip_heredoc this is the first line this is the second line STR end 输出：\nthis is the first line this is the second line this is the first line this is the second line 注意行首是没有空格的，和输入的格式保持了一致，很方便，实现也是很简单的，参考 github 吧，strip.rb\n那些太奇怪的写法 其实还有很多奇怪的写法，比如下面这个：\nputs \u0026lt;\u0026lt;-\u0026#34;I am the content\u0026#34; line 1 line 2 line 3 I am the content 这是合法语法，但估计不会有人这么写，甚至有的编辑器都不能正常高亮显示它.\n","date":"2018-11-22T09:23:36Z","permalink":"https://dccmmtop.github.io/posts/ruby_heredoc%E7%9A%84%E7%94%A8%E6%B3%95/","section":"posts","tags":["ruby"],"title":"ruby_heredoc的用法"},{"categories":null,"contents":"什么是多态关联 假如有三个模型，分别是 用户， 产品， 图片。图片为用户所有，也为产品所有。我们可以创建两个 picture 的模型，如下\nrails g modle picture_user user_id:integer name:string url:string\nrails g modle picture_product product_id:integer name:string url:string\nclass PictureUser \u0026lt; ApplicationRecord belongs_to :user end class PictureProduct \u0026lt; ApplicationRecord belongs_to :product end 这样我们就可以使用user.prictures 和 product.pictures来分别获得用户下和产品下的图片了。但是我们发现，两个图片模型除了外键不一样，其他字段都是一样的，那么有没有一种办法只创建一个 picture 模型，同时属于 user 和 product 呢，这种既属于一个模型又属于另外一个模型（可以是很多个）的关联就是多态关联。\n多态关联的实现 为了能同时使用 user.pictures 和 product.pictures 来获得各自的图片，我们就需要对 picture 模型做一些修改，使其能够标识一张图片是属于 user 还是属于 product ，当然外键是必不可少的。我们还需要一个外键对应类的名称，如下：\nrails g modle picture pictureable_id:integer pictureable_type:string name:string url:string\nclass Picture \u0026lt; ApplicationRecord belongs_to :pictureable, polymorphic: true end class User \u0026lt; ApplicationRecord has_many :pictures, as: :pictureable end class Product \u0026lt; ApplicationRecord has_many :pictures, as: :pictureable end pictureable 相当于一个接口，凡是拥有图片的模型都可以像 User 那样使用关联。\n可以使用user.pictures.create(name: 'user_0', url: 'https://dcc.com')来创建一条关联对象，创建之后我们发现在 picture 表中多了一条记录：\nid: 1, pictureable_type: \u0026#39;User\u0026#39;, pictureable_id: 1, name:\u0026#34;user_0\u0026#34;, url:\u0026#39;https://dcc.com\u0026#39; pictureable_type: \u0026lsquo;User\u0026rsquo; 就是所属对象的标识，这样才可以使用 user.pictures 进行查询。由此我们知道，多态关联中，xxxable_type, xxxable_id字段是必不可少的。\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\n下面是关于多态 view 页面使用的讲解原文\n什么是多态 Rails 模型中的关系有一对一，一对多还有多对多，这些关联关系都比较直观，除此之外 Rails 还支持多态关联，所谓的多态关联其实可以概括为一个模型同时与多个其它模型之间发生一对多的关联。并且在实际的应用中这种关系也十分普遍，比如可以应用到站内消息模块，评论模块，标签模块等地方，下图就是多态关系下的评论模块的 E-R 图。\n通过 E-R 图，我们能直观的看到系统中的事件，文章以及照片都可以被用户评论，并且这些评论都被存储在一张叫 comments 表中。Ok，现在我们已经搞清楚了多态的含义，下面继续看下 Rails 中是如何实现多态关联的。\nRails 中实现多态的步骤 这里我们通过将 Rails Guides 中给出的例子线性化(转化为详细步骤)来说明这个问题。\nStep 1： 通过 Migration 创建表 执行下面命令来生成 Migration 文件\nrails g model picture name:string imageable_id:integer imageable_type:string 生成的 Migration 文件如下:\nclass CreatePictures \u0026lt; ActiveRecord::Migration def change create_table :pictures do |t| t.string :name t.integer :imageable_id t.string :imageable_type t.timestamps null: false end add_index :pictures, :imageable_id end end 其中需要特别关注， imageable_id 与 imageable_type 两个字段，前者用来存储相关联内容的外键键值，后者则用来存储相关联内容的类型名。后面在通过模型查找关联内容的时候，可以通过这两个值来定位到要找到的内容。特别是后者 imageable_type 的存在是多态实现的关键。\nStep 2: 修改各 Model 得关联关系 按照上面 E-R 图和代码修改模型结构，因为 Employee，Product 分别与 Picture 是一对多的关系，所以用到了 has_many 与 belongs_to 方法，再使用 polymorphic 与 as 来指明是多态关联。\nStep 3: Controller 中应用 上面的两步完成后就能在 Controller 中通过多态关联关系进行相互访问了，并且通过关联关系创建的新评论 Rails 也会自动帮你设置 commentable_id 与 commentable_type 两个字段的值。\nevent = Event.create name: \u0026#34;event1\u0026#34;\revent1= event.comments.create content: “comment1”\revent1.commentable_type #=\u0026gt; “Event” Done！到此就算完整应用到了多态关联关系，后续需要处理的就是如何来组织代码让多态关系更加灵活便捷的被你操作，不过这个就应该是另一篇文章的内容了。:)\n刚开始看 Rails Guide 的时候对多态的表关联真的是一头雾水。后来自己写了一个博客应用的时候用到了 acts_as_commentable 这个 gem，它就是用到了多态表的关联，然后我又看了 Terry 在 railscasts china 上的 视频 ，对多态的理解就深了很多。\n理解什么是多态 一般表的关联有一对一，一对多，多对多，这些都是非常好理解的，然后对于多态的表关联可能稍微有点不好理解。其实多态关键就是一个表关联到多个表上。就如 Comment（评论）表吧，一个 Topic 应该有 Comment（一个帖子应该有许多的评论），除此之外 Micropost（微博）也可能有很多的 Comment。然后一个网站中既有 Topic 的论坛功能，又有 Micropost 的功能，我们怎么处理 Comment 表呢？当然我们可以建两个独立的表比如 TopicComment 和 MicropostComment，再分别关联到 Topic 和 Micropost 上，但这不是一种好的选择，我们可以只建一个表，然后去关联这两个表，甚至多个表。这也就实现了多态的能力。\n一个例子 1.首先我们先生成一个 Comment 的 model，假设已经有 Topic 和 Micropost 这两个 model 了\nrails g model comment content:text commentable_id:integer comment_type:string 2.然后我们 会得到一个 migration\nclass CreateComments \u0026lt; ActiveRecord::Migration def change create_table :comments do |t| t.text :content t.integer :commentable_id t.string :commentable_type t.timestamps end end end 也可以通过 t.references 来简化上面的\nclass CreateComments \u0026lt; ActiveRecord::Migration def change create_table :comments do |t| t.text :content t.references :commentable, :polymorphic =\u0026gt; true #这里指明了多态，这样会生成comment_id和comment_type这两个字段的，如上 t.timestamps end end end 多态魔法就在这里，commentable_typle 字段用于指明 comment 所关联的表的类型，如 topic 或 micropost 等，而 comment_id 用于指定那个关联表的类型对象的 id。如：可以把一个 comment 关联到第一篇 topic 上，那么 comment_type 字段为 topic，而 comment_id 为对应 topic 对象的 id 1,同理这样就可以关联到不同表了，从而实现多态的关联。\n3,数据迁移 rake db:migrate 就能生成我们要的表了\n4,对 model 进行操作从而现实表的关联\n####comment model class Comment \u0026lt; ActiveRecord::Base belongs_to :commentable, :polymorphic =\u0026gt; true end 看到没有，这里的 comment belongs_to 没有写 topic，micropost 等，而写了 commentable,因为 commentable 中有 type 和 id 两个字段，可以指定任何其他 model 对象的，从而才能实现多态，如果这里写 belongs_to topic 的话就没办法实现多态了。然后我们看看 topic 和 mocropost 的 model 该如何写。\nclass Topic \u0026lt; ActiveRecord::Base has_many :comments, :as =\u0026gt; :commentable end class Micropost \u0026lt; ActiveRecord::Base has_many :comments, :as =\u0026gt; :commentable end 看到这里的 as 了吗？as 在这我们可以解释为：作为（我的理解，可能这种理解补科学，哈哈），也就是说 Topic 有许多的 comments，但是它是通过将自己作为 commentable，实现的。Micropost 同理。\n然后就是 controller 和 views 中（如 form 表单）的设计了，这也是我刚学的时候，最头疼这个了，因为对 params 参数通过表单到 controller 的传递没掌握好。\n在写这些之前，我们先看看如何写路由吧，因为一个 topic 有多个 comments，Micropost 同理。所以我们可以这样写\nresources :topics do resources :comments end resources :microposts do resources :comments end 然后我们通过命令 rake routes 就可以得到相应的路由了如：\ntopic_comments GET /topics/:topic_id/comments(.:format) comments#index POST /topics/:topic_id/comments(.:format) comments#create new_topic_comment GET /topics/:topic_id/comments/new(.:format) comments#new edit_topic_comment GET /topics/:topic_id/comments/:id/edit(.:format) comments#edit topic_comment GET /topics/:topic_id/comments/:id(.:format) comments#show PUT /topics/:topic_id/comments/:id(.:format) comments#update DELETE /topics/:topic_id/comments/:id(.:format) comments#destroy 这些待会我们会用到。\n然后我们再来分析 controller 和 views 之间的参数传递。我们通过完整的创建 comment 的过程进行说明\n(1)首先页面上肯定有一个创建 comment 的连接或按钮（假设创建 comment 的表单和 topic show 页面不在统一页面上），代码应该是这样的：\n\u0026lt;%= link_to \u0026#34;发表评论\u0026#34;, new_topic_comment_path%\u0026gt; (2)点击这个链接后，通过路由来到 controller 中的 new 方法(同时会将对应的 topic 相关的参数传给 controller)\ndef new @topic = Topic.find(parmas[:id]) #找到comment属于的topic @comment = @topic.comments.build #建立这个关系 end (3)经过这个方法（action）后，页面来到了 comments/new.html.erb,在这个页面中有一个评论的表单，大概是这样的\n\u0026lt;%= form_for([@comment.commentable, @comment]) do |f| %\u0026gt;\r......\r\u0026lt;%end%\u0026gt; 这个表的参数是一个数组，[email protected]\n@comment，如果没有关联的化，[email protected]，\n[email protected]\n��，还有一个就是 commentable，这里也就是 topic。\n还记得 new 中的 @comment = @topic.comments.build 的吗，这里就暂时将对应的 topic 对象写入 commentable（注意：只是暂时建立关系，还没有写入数据库），[email protected]\n@topic。\n(4)然后你填完表单后，按提交按钮后，表单中的参数（包括 commentable，@post 的 id 等信息），一起来到 controller 的 create 方法中\ndef create Topic.find(parmas[:topic_id]).comments.create(parmas[:comment]) ...... end 这样就真正创建了一个新的 comment。micropost 同理。\n其实多态讲的也差不多了，但在提一个地方\n**重要知识点:**假设一个 comment 已经建立了，它的 commentable_type 是:topic.comment_id 是 1。如果我们得到了这个 id 为 1 的 topic，@topic，那么我们怎么得到它的 comments 呢？是的很简单，直接 @topic.comments 就 ok 了。但是反过来呢，我们得到了这个 comment，@comment，我们如何得到对应的 topic 的信息呢？我以前刚学的时候，就用了@comment.topic ，呵呵，没错，得到的是一串错误，正确一概是 @comment.commentable\n关于多态我们已经讲的差不多了。\n补充：上面的例子 comment 的表单是独立在 comments/new.html.erb 中的，但是一般的应用 comment 的表单是在 topics/show.html.erb 中，也就是上面一个 topic，topic 下有一个 comment 表单。这样的话在 controller 中我们就不需要 new 这个方法了，那么我们在哪建立关系呢？\n@comment = @topic.comments.build #建立这个关系 我们就在表单的 \u0026lt;%= form_for ...%\u0026gt; 前面写 \u0026lt;@comment = @topic.comments.build\u0026gt;\n","date":"2018-11-21T15:35:41Z","permalink":"https://dccmmtop.github.io/posts/rails_%E5%A4%9A%E6%80%81%E5%85%B3%E8%81%94/","section":"posts","tags":["rails"],"title":"rails_多态关联"},{"categories":null,"contents":"安装 redis gem redis bundle install 修改 cable.yml development: adapter: redis 生成订阅 rails g channel block speak\n连接设置 连接是客户端-服务器通信的基础。每当服务器接受一个 WebSocket，就会实例化一个连接对象。所有频道订阅（channel subscription）都是在继承连接对象的基础上创建的。连接本身并不处理身份验证和授权之外的任何应用逻辑。WebSocket 连接的客户端被称为连接用户（connection consumer）。每当用户新打开一个浏览器标签、窗口或设备，对应地都会新建一个用户-连接对（consumer-connection pair）。 连接是 ApplicationCable::Connection 类的实例。对连接的授权就是在这个类中完成的，对于能够识别的用户，才会继续建立连接。\n例子：\nmodule ApplicationCable class Connection \u0026lt; ActionCable::Connection::Base identified_by :current_user def connect self.current_user = find_verified_user end private def find_verified_user if cu_user = User.find_by(id: cookies.signed[:user_id]) cu_user else reject_unauthorized_connection end end end end action cable 中不能使用 session，我们可以用 cookie 来验证用户。下面在 SessionHelper 中加入 cookie，方便 action cable 使用\nmodule SessionsHelper def log_in(user) session[:user_address] = user.address user = User.find_by(address: session[:user_address]) if user cookies.permanent.signed[:user_id] = user.id end end def current_user @current_user ||= User.find_by(address: session[:user_address]) end def logged_in? !current_user.nil? end def log_out session.delete(:user_address) @current_user = nil cookies.delete(:user_id) end end 订阅 App.block = App.cable.subscriptions.create channel: \u0026#34;BlockChannel\u0026#34;, connected: -\u0026gt; # Called when the subscription is ready for use on the server disconnected: -\u0026gt; # Called when the subscription has been terminated by the server received: (data) -\u0026gt; alert(data) 上述channel: 'BlockChannel'是必须的，声明像哪个频道订阅\n处理订阅 class BlockChannel \u0026lt; ApplicationCable::Channel def subscribed # 设置可以向哪些订阅者发布信息 stream_from current_user.address end def unsubscribed # Any cleanup needed when channel is unsubscribed end 测试 在 rails console 中\n# 向用户22发送一条通知 ActionCable.server.broadcast User.find(22).address, data: 22 关于 action cable 的部署 cable.yml action cable 默认为“异步”适配器，当涉及多个进程时，它不起作用。因此，您需要配置 Action Cable 以使用其他适配器，例如 Redis 或 PostgreSQL。\nproduction: adapter: redis url: redis://localhost:6379 staging: adapter: redis url: redis://localhost:6379 local: \u0026amp;local adapter: redis url: redis://localhost:6379 development: *local test: *local 不要忘记启动 redis\n在和 rails 相同的主机和端口使用 action cable 下面是 Rails 推荐的默认设置，也是最简单的设置.它的工作原理是将 ActionCable.server 挂载到 config / routes.rb 中的某个路径。这样，您的 Action Cable 服务器将在与您的应用程序相同的主机和端口上运行，但在子 URI 下运行。\n在 router.rb 文件中\nmount ActionCable.server =\u0026gt; \u0026#39;/cable\u0026#39; 你需要配置一个 location块 配置 cable 的请求,像下面这样：\nserver { listen 80; server_name www.foo.com; root /path-to-your-app/public; passenger_enabled on; ### INSERT THIS!!! ### location /cable { passenger_force_max_concurrent_requests_per_process 0; } } 为了应用的性能，必须添加passenger_force_max_concurrent_requests_per_process 0的配置，关于这个配置的详解请看 文档\n本文内容整理自 ruby-china 和 passenger 的配置文档\n","date":"2018-11-18T17:24:00Z","permalink":"https://dccmmtop.github.io/posts/rails_action_cable%E4%BD%BF%E7%94%A8/","section":"posts","tags":["rails"],"title":"rails_action_cable使用"},{"categories":null,"contents":"日积月累，自己写的 vim 脚本越来越多，大大的方便了日常编写任务，但是这些脚本没有做成插件的形式，导致换一台新机器时，不方便下载使用，下面就介绍一下如何把 自己写的脚本做成一个插件，可以在vimrc中使用Plug xxx安装。\nbegin 新建文件夹，命名为vim_script 进入文件价，执行 git init初始化一个仓库 去 github 新建一个仓库，vim_scipt 设置本地仓库的 remote 信息 在 vim_script 下新建 autoload 文件夹，把自己写的 vim 脚本放到 autoload 下 在 vim_script 下新建 plugin 文件夹，新建script.vim(名字随意)，在该文件内设置脚本执行的命令，或者设置执行脚本的快捷键 如图：\n","date":"2018-10-28T15:48:33Z","permalink":"https://dccmmtop.github.io/posts/vim%E8%84%9A%E6%9C%AC%E6%8F%92%E4%BB%B6%E5%8C%96/","section":"posts","tags":["vim"],"title":"vim脚本插件化"},{"categories":null,"contents":"本文章为转载内容，点击查看原文章https://zhuanlan.zhihu.com/p/27389503\n使用脚本语言，可以更灵活地定制编辑器以完成复杂的任务。\n自定义命令 Vim 编辑器允许定义自己的命令，我们可以像执行内置命令一样来执行我们自己定义的命令。\n使用以下:command 命令自定义命令：\n:command Delete_first :1delete 注意自定义命令的名称，必须以大写字母开头，而且不能包含下划线；如果我们执行:Delete_first 自定义命令，那么 Vim 就会执行:1delete 命令，从而删除第一行。\n可以使用!来强制重新定义同名的自定义命令：\n:command! -nargs=+ Say :echo \u0026lt;args\u0026gt; 用户定义的命令可以指定一系列的参数，参数的个数由-nargs 选项在命令行中指定。例如定义 Delete_one 命令没有参数：\n:command Delete_one -nargs=0 1delete 默认情况下-nargs=0，所以可以省略。其他-nargs 选项值如下：\n-nargs=0 没有参数\n-nargs=1 1 个参数\n-nargs=* 任何个数的参数\n-nargs=? 零个或是一个参数\n-nargs=+ 一个或是更多个参数\n在命令定义中，参数是由关键字指定的：\n:command -nargs=+ Say :echo \u0026#34;\u0026lt;args\u0026gt;\u0026#34; 输入以下自定义命令：\n:Say Hello World 命令的执行结果显示：\nHello World 使用-range 选项，可以指定一个范围作为自定义命令的参数。-range 选项值如下：\n-range 允许范围，默认为当前行\n-range=%允许范围，默认为当前文件(while file)\n-range=count 允许范围，单一的数字\n当指定范围之后，就可以用关键字和得到这个范围的第一行和最后一行。\n例如以下定义了 SaveIt 命令，用于将指定范围的文件写入文件 save_file：\n:command -range=% SaveIt :\u0026lt;line1\u0026gt;,\u0026lt;line2\u0026gt;write! save_file 关键字含有与关键字相同的信息，所不同的是它用于调用函数。例如以下自定义命令：\n:command -nargs=* DoIt :call AFunction(\u0026lt;f-args\u0026gt;) 执行自定义命令：\n:DoIt a b c 将会传递参数给调用的函数：\n:call AFunction(\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;) 其他选项和关键字包括：\n-count=number 指定数量保存在关键字中\n-bang 指定!修饰符存放在关键字中\n-register 指定寄存器，默认为未命名寄存器，寄存器的定义保存在关键字中\n-bar 其他命令可以用|跟随在此命令之后\n-buffer 命令仅对当前缓冲区有效\n使用以下命令，首先分别创建一个用户自定义命令，然后再将两个命令组合起来。\ncommand! -bar DelTab %s/\t// command! DelLF %s/\\n// command! FmtCode DelTab|DelLF view raw\nScriptCommandMulti.vim\nhosted with ❤ by\nGitHub\n列示自定义命令 使用以下命令，可以列出用户定义的命令：\n:command 删除自定义命令 使用以下:delcommand 命令，可以删除用户定义的命令：\n:delcommand Delete_one 使用以下命令，清除所有的用户定义的命令：\n:comclear ","date":"2018-10-23T14:29:42Z","permalink":"https://dccmmtop.github.io/posts/vim%E8%87%AA%E5%AE%9A%E4%B9%89%E5%91%BD%E4%BB%A4/","section":"posts","tags":["vim"],"title":"vim自定义命令"},{"categories":null,"contents":"需求 给一段文字自动添加序号，要求本行的序号可以根据上一行的序号自动增一，若上一行没有序号，则从 1 开始\n实现 用 ruby 编写 vim 脚本非常容易实现\n\u0026#34; 每行的前面添加序号，根据上一行序号自动递增，若上一行没有序号，则从1开始 function! num#add_num() ruby \u0026lt;\u0026lt; EOF def get_current_line() count = 0 \u0026#34; 得到当前缓冲区 cb = Vim::Buffer.current \u0026#34; 得到上一行的行号 previousLine = cb.line_number - 1 \u0026#34; 如果行号存在，并且以数字开头 if previousLine \u0026gt;= 1 \u0026amp;\u0026amp; cb[previousLine] =~ /^\\d+/ \u0026#34; 得到上一行的序号 count = $\u0026amp;.to_i end \u0026#34; 修改本行内容 cb.line = \u0026#34;#{count + 1}. #{line}\u0026#34; end get_current_line() EOF endfunction 添加自定义命令 在.vimrc中，添加如下一行\ncommand! -range=% AddNum :\u0026lt;line1\u0026gt;,\u0026lt;line2\u0026gt; cal num#add_num() 关于自定义命令请查看这篇文章：vim 添加自定义命令\n演示 ","date":"2018-10-23T12:58:16Z","permalink":"https://dccmmtop.github.io/posts/%E8%87%AA%E5%8A%A8%E6%B7%BB%E5%8A%A0%E5%BA%8F%E5%8F%B7/","section":"posts","tags":["vim"],"title":"自动添加序号"},{"categories":null,"contents":"ruby 中有个 range 对象，可以自动推测范围内的数据，比如：\n(1..100).each do |i| puts i end 会输出 1 到 100 内的所有数字\n自定义 如果我们有一个自定义的对象，假如名字为Ym\nclass Ym attr_accessor :year, :month def initialize @year, @month = year, month end end 若是想在Ym上使用((Ym.new(2009,1))..(Ym.new(2010,1))).each {|i| puts i},输出的结果按照正常的年月逻辑来显示，该如何实现呢？\n其实要实现类似(1..100)的方法很容易，只需在该类中include Compareable然后实现 succ和\u0026lt;=\u0026gt;方法就行了。\nclass Ym include Comparable attr_accessor :year, :month def initialize(year, month) @year, @month = year, month end def succ #如果月份满12，则年份增加一，月份再从一开始。 # 可以按需求定制更复杂的推测方法 yyy, mmm = @month == 12 ? [@year + 1, 1] : [@year, @month + 1] Ym.new(yyy, mmm) end def \u0026lt;=\u0026gt;(other) # 定义大小规则 (@year * 12 + @month) \u0026lt;=\u0026gt; (other.year * 12 + other.month) end def to_s sprintf \u0026#34;%4d-%02d\u0026#34;, @year, @month end end (Ym.new(2008,8)..(Ym.new(2019,9))).each do |y| puts y end 结果如图:\n","date":"2018-09-27T09:10:50Z","permalink":"https://dccmmtop.github.io/posts/%E8%87%AA%E5%AE%9A%E4%B9%89range%E5%AF%B9%E8%B1%A1/","section":"posts","tags":["ruby"],"title":"自定义range对象"},{"categories":null,"contents":"首先，在哪些情况下会用到正则表达式？\n使用正则表达式的命令最常见的就是 / 和 ? 命令。其格式如下：\n/正则表达式\r?正则表达式 另一个很有用的命令就是 :s（替换）命令，将第一个//之间的正则表达式替换成第二个//之间的字符串。\n:s/正则表达式/替换字符串/选项 在学习正则表达式时可以利用 / 命令来练习。\n元字符 元字符 说明 . 匹配任意字符 [abc] 匹配方括号中的任意一个字符。可以使用 - 表示字符范围，如[a-z0-9]匹 配小写字母和阿拉伯数字。 \\d 匹配阿拉伯数字，等同于[0-9]。 [^abc] 在方括号内开头使用^符号，表示匹配除方括号中字符之外的任意字符。 \\d 匹配阿拉伯数字，等同于[0-9]。 \\D 匹配阿拉伯数字之外的任意字符，等同于[^0-9]。 \\x 匹配十六进制数字，等同于[0-9A-Fa-f]。 \\X 匹配十六进制数字之外的任意字符，等同于[^0-9a-fa-f]。 \\w 匹配单词字母，等同于[0-9A-Za-z_]。 \\W 匹配单词字母之外的任意字符，等同于[^0-9a-za-z_]。 \\t 匹配字符。 \\s 匹配空白字符，等同于[ \\t]。 \\S 匹配非空白字符，等同于[^ \\t]。 如果需要查找一些特殊字符，如 *、.、/ 等，可以在这些字符前面添加 \\，表示这些不是元字符，而是普通字符。比如：\\/d 匹配的是 /d这两个字符，而不是匹配任意数字。\n表示数量的元字符\n元字符 说明 * 匹配 0-任意个 \\+ 匹配 1-任意个 \\? 匹配 0-1 个 \\{n,m} 匹配 n-m 个 \\{n} 匹配 n 个 \\{n,} 匹配 n-任意个 \\{,m} 匹配 0-m 个 表示位置的符号\n元字符 说明 $ 匹配行尾 ^ 匹配行首 \\\u0026lt; 匹配单词词首 \\\u0026gt; 匹配单词词尾 使用示例\n命令 描述 /char\\s\\+[A-Za-z_]\\w*; 查找所有以 char 开头，之后是一个以上的空白，最后是一个标识符和分号 /\\d\\d:\\d\\d:\\d\\d 查找如 17:37:01 格式的时间字符 :g/^\\s*$/d 删除只有空白的行 :s/\\\u0026lt;four\\\u0026gt;/4/g 将所有的 four 替换成 4，但是 fourteen 中的 four 不替换 替换变量 在正规表达式中使用 \\( 和 \\) 符号括起正规表达式，即可在后面使用\\1、\\2 等变量来访问 \\(和 \\) 中的内容。 使用示例\n命令 描述 /\\(a\\+\\)[^a]\\+\\1 查找开头和结尾处 a 的个数相同的字符串，如 aabbbaa，aaacccaaa，但是不匹配 abbbaa :s/\\(http:\\/\\/[-a-z\\._~\\+%\\/]\\+\\)/\u0026lt;a href=\u0026quot;\\1\u0026quot;\u0026gt;\\1\u0026lt;\\/a\u0026gt;/ 将 url 替换为http://url的格式 :s/\\(\\w\\+\\)\\s\\+\\(\\w\\+\\)/\\2\\t\\1 将 data1 data2 修改为 data2 data1 函数式 在替换命令 :s/{pattern}/{string}/[flags] 中可以使用函数表达式来书写替换内容，格式为\n:s/替换字符串/\\=函数式 在函数式中可以使用 submatch(1)、submatch(2) 等来引用 \\1、\\2 等的内容，而submatch(0)可以引用匹配的整个内容。\n使用例\n:%s/\\\u0026lt;id\\\u0026gt;/\\=line(\u0026#34;.\u0026#34;) 将各行的 id 字符串替换为行号\n:%s/^\\\u0026lt;\\w\\+\\\u0026gt;/\\=(line(\u0026#34;.\u0026#34;)-10) .\u0026#34;.\u0026#34;. submatch(1) 将每行开头的单词替换为 (行号-10).单词 的格式，如第 11 行的 word 替换成 1. word\n与 Perl 正则表达式的区别 Vim 语法 Perl 语法 含义 \\+ + 1-任意个 \\? ? 0-1 个 \\{n,m} {n,m} n-m 个 \\( 和 \\) ( 和 ) 分组 贪婪模式和非贪婪模式 在 Vim 里，默认是贪婪模式，即 a.*b 会尽可能多滴匹配字符，在 ahdbjkbkls 中匹配 ahdbjkb 而不是 ahdb。 如果是非贪婪的，可以使用 \\{-} 代替 *，即 a.\\{-}b 匹配 ahdb 而不是 ahdbjkb。\n作者：SpaceVim 链接：https://www.jianshu.com/p/03770041397c 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 ","date":"2018-09-21T14:28:51Z","permalink":"https://dccmmtop.github.io/posts/vim%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","section":"posts","tags":["vim"],"title":"vim中的正则表达式"},{"categories":null,"contents":"什么是跨域 理解跨域首先必须要了解同源策略。同源策略是浏览器上为安全性考虑实施的非常重要的安全策略。 那么什么是同源？我们知道，URL 由协议、域名、端口和路径组成，如果两个 URL 的协议、域名和端口相同，则表示他们同源。 我们用一个例子来说明： URL: http://www.example.com:8080/script/jquery.js\n在这个 url 中，各个字段分别代表的含义：\nhttp://——协议\nwww——子域名\nexample.com——主域名\n8080——端口号\nscript/jquery.js——请求的地址\n当协议、子域名、主域名、端口号中任意一各不相同时，都算不同的“域”。不同的域之间相互请求资源，就叫跨域。 这里要注意，如果只是通过 AJAX 向另一个服务器发送请求而不要求数据返回，是不受跨域限制的。浏览器只是限制不能访问另一个域的数据，即不能访问返回的数据，并不限制发送请求。\n事实上，为了解决因同源策略而导致的跨域请求问题，解决方法有五种：\ndocument.domain Cross-Origin Resource Sharing(CORS) Cross-document messaging JSONP WebSockets 什么是 CORS(跨域资源共享，Cross-Origin Resource Sharing)？ 我们先来看看 wiki 上的定义：\n跨来源资源共享（CORS）是一份浏览器技术的规范，提供了 Web 服务从不同网域传来沙盒脚本的方法，以避开浏览器的同源策略，是 JSONP 模式的现代版。与 JSONP 不同，CORS 除了 GET 要求方法以外也支持其他的 HTTP 要求。用 CORS 可以让网页设计师用一般的 XMLHttpRequest，这种方式的错误处理比 JSONP 要来的好。另一方面，JSONP 可以在不支持 CORS 的老旧浏览器上运作。现代的浏览器都支持 CORS。\n由此我们可以知道， CORS 定义一种跨域访问的机制，可以让 AJAX 实现跨域访问。CORS 允许一个域上的网络应用向另一个域提交跨域 AJAX 请求。对于 CORS 来说，实现此功能非常简单，只需由服务器发送一个响应标头即可。服务器端对于 CORS 的支持，主要就是通过设置 Access-Control-Allow-Origin 来进行的。具体的关于 CORS 原理性的知识此处不再进行介绍，只在此对 CORS 和 JSONP 进行简单的比较.\nCORS 与 JSONP 比较 CORS 与 JSONP 相比，更为先进、方便和可靠。\nJSONP 只能实现 GET 请求，而 CORS 支持所有类型的 HTTP 请求。 使用 CORS，开发者可以使用普通的 XMLHttpRequest 发起请求和获得数据，比起 JSONP 有更好的错误处理。 JSONP 主要被老的浏览器支持，它们往往不支持 CORS，而绝大多数现代浏览器都已经支持了 CORS。 rack-cors 怎样解决跨域问题？ Rack Middleware for handling Cross-Origin Resource Sharing (CORS), which makes cross-origin AJAX possible.\n也就是说，这个 gem 是基于 CORS 来实现 Ajax 的跨域请求功能的，我们可以添加这个 gem 来解决我们项目中遇到的问题。 我们看到 gem 中给出的配置接口：\nRack In config.ru, configure Rack::Cors by passing a block to the use command:\nuse Rack::Cors do allow do origins \u0026#39;localhost:3000\u0026#39;, \u0026#39;127.0.0.1:3000\u0026#39;, /\\Ahttp:\\/\\/192\\.168\\.0\\.\\d{1,3}(:\\d+)?\\z/ # regular expressions can be used here resource \u0026#39;/file/list_all/\u0026#39;, :headers =\u0026gt; \u0026#39;x-domain-token\u0026#39; resource \u0026#39;/file/at/_\u0026#39;, :methods =\u0026gt; [:get, :post, :delete, :put, :patch, :options, :head], :headers =\u0026gt; \u0026#39;x-domain-token\u0026#39;, :expose =\u0026gt; [\u0026#39;Some-Custom-Response-Header\u0026#39;], :max_age =\u0026gt; 600 # headers to expose end allow do origins \u0026#39;_\u0026#39; resource \u0026#39;/public/\\*\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; :get end end Rails\nPut something like the code below in config/application.rb of your Rails application. For example, this will allow GET, POST or OPTIONS requests from any origin on any resource.\nmodule YourApp class Application \u0026lt; Rails::Application # ... # Rails 3/4 config.middleware.insert_before 0, \u0026#34;Rack::Cors\u0026#34; do allow do origins \u0026#39;_\u0026#39; resource \u0026#39;_\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; [:get, :post, :options] end end # Rails 5 config.middleware.insert_before 0, Rack::Cors do allow do origins \u0026#39;_\u0026#39; resource \u0026#39;_\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; [:get, :post, :options] end end end end 可以看出，rack-cors 实际上直接给出了借口，我们在 bundle 这个 gem 后，直接在 config/application.rb 文件中添加配置信息即可，而无需自己在代码中添加有关跨域资源的策略信息。\n实际应用 这个 gem 是用在被访问的资源服务器上的，用来定义哪些域可以访问资源以及可以访问自己的哪些资源等策略信息。这个 gem 可以很轻松很方便地解决 ajax 跨域问题。\n安装 gem gem 'rack-cors', :require =\u0026gt; 'rack/cors' 修改 config/application.rb 我们使用的是 rails，因此只需要做以下修改即可： config.middleware.insert*before 0, Rack::Cors do allow do origins \u0026#39;*\u0026#39; resource \u0026#39;\\_\u0026#39;, :headers =\u0026gt; :any, :methods =\u0026gt; [:get, :post, :options] end end 其中，origins 用来配置可以请求自己资源的域，*表示任何域都可以请求；resource 用来配置自己的哪些资源可以被请求，*代表所有资源都可以被请求，methods 代表可以被请求的方法。\n做完这两部，我们就可以实现跨域请求资源了。此时重启服务器，本地再次请求资源就会成功，同时我们可以看到自己的请求中多了类似下面的一些信息：\nAccess-Control-Allow-Origin: http://localhost:3000\rAccess-Control-Allow-Methods: GET, POST, OPTIONS\rAccess-Control-Max-Age: 1728000\rAccess-Control-Allow-Credentials: true 这样，我们便在 rails 中解决了 Ajax 的跨域请求资源的问题，项目也可以继续向前开发了。\n作者：vito1994 链接：https://www.jianshu.com/p/c54a1dbaab24 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 ","date":"2018-09-19T09:25:49Z","permalink":"https://dccmmtop.github.io/posts/rack-cors%E8%A7%A3%E5%86%B3ajax%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98-cors/","section":"posts","tags":["rails"],"title":"rack-cors解决Ajax跨域问题-CORS"},{"categories":null,"contents":"在开始编写插件之前，你需要确认 Vim 是否支持 Ruby，通过以下命令来判别：\n$ vim --version | grep +ruby 如果输出为空，则表示你当前的 vim 不支持 Ruby，需要重新编译一下，并启用对 Ruby 的支持。 如果没有问题那就开始吧！\n下面的示例是我用来把本地图片上传到七牛云图床。\n新建 在.vim/autoload中新建test.vim,复制以下代码\nfunction! qiniu#get_picture_url() ruby \u0026lt;\u0026lt; EOF class Qiniu def initialize @buffer = Vim::Buffer.current end def get_current_line s = @buffer.line # gets the current line # qiniu 是七牛云的一个gem，修改了部分代码，并重命名。 real_link = `qiniu #{s}` real_link = real_link.split(/\\n/).last Vim::Buffer.current.line = \u0026#34;![](#{real_link})\u0026#34; # sets the current line number end end gem = Qiniu.new gem.get_current_line EOF endfunction 绑定快捷键 在.vimrc中，映射快捷键\nnoremap \u0026lt;leader\u0026gt;qn :cal qiniu#get_picture_url()\u0026lt;cr\u0026gt;\u0026lt;CR\u0026gt; [\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;]\n我把图片的本地地址粘贴到 vim 中，然后使用qn快捷键，本地地址就会被修改成该图片在七牛云的外链，经常写博客用到\nvim API 在 vim 中执行:h ruby 查看 vim 提供 ruby 的 API\n演示 ","date":"2018-08-30T17:36:12Z","permalink":"https://dccmmtop.github.io/posts/%E7%94%A8ruby%E7%BC%96%E5%86%99vim%E8%84%9A%E6%9C%AC/","section":"posts","tags":["vim"],"title":"用ruby编写vim脚本"},{"categories":null,"contents":"在 rails 中,model 的属性是默认的可读可写的，有时我们需要重写某个字段的访问器。当查询某个字段的值时，需要进行其他操作；\n如： 当查询recommand_code的值时，若存在，则返回，若不存在则创建一个包含大写字母和数字的 6 为随机字符串\n主要是 read_attribute() 和 write_attribute() 的用法\ndef recommand_code # 重写 recommand_code 字段 _code = read_attribute(:recommand_code) # _code = self.recommand_code 错误，会引起无限递归 if self.block.empty? self.recommand_code=nil; return end return _code if _code loop do _code = ((\u0026#34;0\u0026#34;..\u0026#34;9\u0026#34;).to_a + (\u0026#34;A\u0026#34;..\u0026#34;Z\u0026#34;).to_a).sample(6).join break if User.find_by_recommand_code(_code).nil? end self.recommand_code = nil; return _code end def recommand_code=(value) write_attribute(:recommand_code,value) end ","date":"2018-08-20T18:09:07Z","permalink":"https://dccmmtop.github.io/posts/rails%E9%87%8D%E5%86%99%E5%AD%97%E6%AE%B5/","section":"posts","tags":["rails"],"title":"rails重写字段"},{"categories":null,"contents":"在开发微信公众号或小程序的时候，由于微信平台规则的限制，部分接口需要通过线上域名才能正常访问。但我们一般都会在本地开发，因为这能快速的看到源码修改后的运行结果。但当涉及到需要调用微信接口时，由于不和你在同一个局域网中的用户是无法访问你的本地开发机的，就必须把修改后的代码重新发布到线上域名所在的服务器才能去验证结果。每次修改都重新发布很繁琐也很浪费时间。\n本文将教你如何通过 SSH 隧道把本地服务映射到外网，以方便调试，通常把这种方法叫内网穿透。\n阅读完本文后，你能解决以下常见问题：\n开发微信公众号等应用时把本地服务映射到外网，加速调试流程； 把你正在开发的本地服务分享给互联网上其它人访问体验； 在任何地方通过互联网控制你家中在局域网里的电脑； 最终目的 把运行在本地开发机上的 HTTP 服务映射到外网，让全世界都能通过外网 IP 服务到你本地开发机上的 HTTP 服务。例如你本地的 HTTP 服务监听在 127.0.0.1:8080，你有一台公网 IP 为 12.34.56.78 的服务器，通过本文介绍的方法，可以让全世界的用户通过 http://12.34.56.78:8080 访问到你本地开发机上的 HTTP 服务。\n总结成一句话就是：把内网端口映射到外网。\n前提条件 为了把内网服务映射到外网，以下资源为必须的：\n一台有外网 IP 的服务器； 能在本地开发机上通过 ssh 登入到外网服务器。 要满足以上条件很简单：\n对于条件 1：购买一台低配 Linux 服务器，推荐国外的 DigitalOcean； 对于条件 2：对于 Mac、Linux 开发机是内置了 ssh 客户端的，对于 Windows 可以安装 Cygwin。 实现原理 要实现把内网端口映射到外网，最简单的方式就是通过 SSH 隧道。\nSSH 隧道就像一根管道，能把任何 2 台机器连接在一起，把发送到其中一台机器的数据通过管道传输到另一台机器。假如已经通过 SSH 隧道把本地开发机和外网服务器连接在了一起，外网服务器端监听在 12.34.56.78:8080，那么所有发给 12.34.56.78:8080 的数据都会通过 SSH 隧道原封不动地传输给本地开发机的 127.0.0.1:8080，如图所示：\n也就是说，去访问 12.34.56.78:8080 就像是访问本地开发机的 127.0.0.1:8080，本地开发机上的 8080 端口被映射到了外网服务器上的 8080 端口。\n如果你的外网服务器 IP 配置了域名解析，例如 yourdomin.com 会通过 DNS 解析为 12.34.56.78，那么也可以通过 yourdomin.com:8080 去访问本地开发机上的服务。\n这样就做到了访问外网地址时其实是本地服务返回的结果。\n通过 SSH 隧道传输数据时，数据会被加密，就算中间被劫持，黑客也无法得到数据的原内容。\n所以 SSH 隧道还有一个功能就是保证数据传输的安全性。\n实现步骤 把本地开机和外网服务器通过 SSH 隧道连接起来就和在本地开发机 SSH 登入远程登入到外网服务器一样简单。\n先来回顾以下 SSH 远程登入命令，假如想在本地远程登入到 12.34.56.78，可以在本地开发机上执行以下命令：\nssh username@12.34.56.78\n而实现 SSH 隧道只需在本地开发机上执行：\nssh -R 8080:127.0.0.1:8080 username@12.34.56.78\n可以看出实现 SSH 隧道的命令相对于 SSH 登入多出来 -R 8080:127.0.0.1:8080，多出的这部分的含义是：\n在远程机器(12.34.56.78)上启动 TCP 8080 端口监听着，再把远程机器(12.34.56.78)上 8080 端口映射到本地的 127.0.0.1:8080。\n执行完以上命令后，就可以通过 12.34.56.78:8080 去访问本地的 127.0.0.1:8080 了。\n通常把这种技术叫做 SSH 远程端口转发(remote forwarding)。\n其实不限于只能把本地开发机上运行的服务映射到外网服务器上去，还可以把任何本地开发机可以访问的服务映射到外网服务器上去。例如在本地开发机上能访问 github.com:80，在本地开发机上执行：\nssh -R 8080:github.com:80 username@12.34.56.78\n就能通过 12.34.56.78:8080 去访问 github.com:80 了。\n保持运行\n在执行完上面介绍的 SSH 隧道命令后，你会发现登入到了外网服务器上去了，如果你登出外网服务器，就会发现 12.34.56.78:8080 无法访问了。导致这个问题的原因是你登出外网服务器时，在外网服务器上本次操作对应的 SSH 进程也跟着退出了，而这个退出的进程曾负责监听在 8080 端口进行转发操作。\n为了让 SSH 隧道一直保持在后台执行，有以下方法。\n通过 SSH 自带的参数\nSSH 还支持这些参数：\nN 参数：表示只连接远程主机，不打开远程 shell； T 参数：表示不为这个连接分配 TTY； f 参数：表示连接成功后，转入后台运行； 因此要让 SSH 隧道一直保持在后台执行，可以通过以下命令：\nssh -NTf -R 8080:127.0.0.1:8080 username@12.34.56.78\n通过 AutoSSH SSH 隧道是不稳定的，在网络恶劣的情况下可能随时断开。如果断开就需要手动去本地开发机再次向外网服务器发起连接。\nAutoSSH 能让 SSH 隧道一直保持执行，他会启动一个 SSH 进程，并监控该进程的健康状况；当 SSH 进程崩溃或停止通信时，AutoSSH 将重启动 SSH 进程。\n使用 AutoSSH 只需在本地开发机上安装 AutoSSH ，方法如下：\nMac 系统：brew install autossh； Linux 系统：apt-get install autossh； 安装成功后，在本地开发机上执行：\nautossh -N -R 8080:127.0.0.1:8080 username@12.34.56.78\n就能完成和上面一样的效果，但本方法能保持 SSH 隧道一直运行。\n可以看出这行命令和上面的区别在于把 ssh 换成了 autossh，并且少了 -f 参数，原因是 autossh 默认会转入后台运行。\n常见问题\n如果你遇到通过以上方法成功启动 SSH 隧道后，还是无法访问 12.34.56.78:8080，那么很有可能是外网服务器上的 SSH 没有配置对。为此你需要去外网服务器上修改 /etc/ssh/sshd_config 文件如下：\nGatewayPorts yes\n这个选项的意思是，SSH 隧道监听的服务的 IP 是对外开放的 0.0.0.0，而不是只对本机的 127.0.0.1。不开 GatewayPorts 的后果是不能通过 12.34.56.78:8080 访问，只能在外网服务器上通过 127.0.0.1:8080 服务到本地开发机的服务。\n修改好配置文件后，你还需要重启 sshd 服务来加载新的配置，命令如下：\nservice sshd restart\n如果使用以上方法还是无法访问 12.34.56.78:8080，请检查你外网服务器的防火墙配置，确保 8080 端口是对外开放的。\n其它代替方案\n除了 SSH 隧道能实现内网穿透外，还有以下常用方法。\nfrp frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。\nfrp 有以下特性：\nfrp 比 SSH 隧道功能更多，配置项更多； frp 也需要一台外网服务器，并且需要在外网服务器上安装 frps，在本地开发机上安装 frpc； ngrok ngrok 是一个商用的内网穿透工具，它有以下特点：\n不需要有外网服务器，因为 ngrok 会为你提供； 只需要在本地开发机安装 ngrok 客户端，和注册 ngrok 账户； 按照服务收费； 这些代替方案的缺点在于都需要再额外安装其它工具，没有 SSH 隧道来的直接。\n想了解更多可以访问它们的主页。\n阅读原文\n","date":"2018-08-19T17:04:28Z","permalink":"https://dccmmtop.github.io/posts/ssh%E9%9A%A7%E9%81%93/","section":"posts","tags":["linux"],"title":"SSH隧道"},{"categories":null,"contents":"关于 Rails 的模型自关联有一个非常有意思的题目，大概是这样的：\nlisa = Person.create(name:\u0026#39;Lisa\u0026#39;) tom = Person.create(name:\u0026#39;Tom\u0026#39;,parent_id:lisa.id) andy = Person.create(name:\u0026#39;Andy\u0026#39;,parent_id:lisa.id) tom.parent.name =\u0026gt; \u0026#39;Lisa\u0026#39; lisa.children.map(\u0026amp;:name) =\u0026gt; [\u0026#39;Tom\u0026#39;,\u0026#39;Andy\u0026#39;] thomas = Person.create(name: \u0026#39;Thomas\u0026#39;,parent_id: tom.id) peter = Person.create(name:\u0026#39;Peter\u0026#39;,parent_id:tom.id) gavin = Person.create(name:\u0026#39;Gavin\u0026#39;, parent_id: andy.id) lisa.grandchildren.map(\u0026amp;:name) =\u0026gt; [\u0026#39;Thomas\u0026#39;,\u0026#39;Peter\u0026#39;,\u0026#39;Gavin\u0026#39;] 问如何定义 Person 模型来满足以上需求？\n题目考察了对模型自关联的理解，通过审题我们可以得出以下几点：\nPerson 对象的 Parent 同样是 Person 对象（自关联） Person 对象对 Parent 是多对一关系 Person 对象对 Children 是一对多关系 Person 对象通过 Children 与 GrandChildren 建立了一对多关系 在不考虑 GrandChildren 时，不难得出模型定义如下： class Person \u0026lt; ActiveRecord::Base belongs_to :parent, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; has_many :children, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; end 其中 Person 包含两个自关联关系：\n第一个就是非常常见的从子到父的关系，在 Person 对象创建时指定 parent_id 来指向父对象； 第二个关系用来指定 Person 对象对应的所有子对象 接下来更近一步，我们要找到 Person 对象子对象的子对象，换句话说：孙子对象。 如我们上面的分析，Person 对象通过 Children 与 GrandChildren 建立了一对多关系，其代码表现为：\nhas_many :grandchildren, :through =\u0026gt; :children, :source =\u0026gt; :children :source 选项的官方文档说明如下：\nThe :source option specifies the source association name for a has_many :through association. You only need to use this option if the name of the source association cannot be automatically inferred from the association name. —— rails guide\n在这里我们通过:source 选项告诉 Rails 在 children 对象上查找 children 关联关系。 于是该题目完整的模型定义如下：\nclass Person \u0026lt; ActiveRecord::Base belongs_to :parent, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; has_many :children, class_name: \u0026#39;Person\u0026#39;, foreign_key: \u0026#39;parent_id\u0026#39; has_many :grandchildren, :through =\u0026gt; :children, :source =\u0026gt; :children end 作者：李小西 033\n链接：https://www.jianshu.com/p/076b5fec4dad\n來源：简书\n简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。\n","date":"2018-08-15T12:22:06Z","permalink":"https://dccmmtop.github.io/posts/rails-%E8%87%AA%E5%85%B3%E8%81%94/","section":"posts","tags":["rails"],"title":"rails-自关联"},{"categories":null,"contents":"shuffle ((\u0026#34;0\u0026#34;..\u0026#34;9\u0026#34;).to_a + (\u0026#34;A\u0026#34;..\u0026#34;Z\u0026#34;).to_a).shuffle[0..6].to_a.join shuffle: 随机排列，中文名称是洗牌\nsample ((\u0026#34;0\u0026#34;..\u0026#34;9\u0026#34;).to_a + (\u0026#34;A\u0026#34;..\u0026#34;Z\u0026#34;).to_a).sample(6).join * [*\u0026#39;0\u0026#39;..\u0026#39;9\u0026#39;,*\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;].sample(6).join *的意思是将范围展开\n","date":"2018-08-12T10:23:46Z","permalink":"https://dccmmtop.github.io/posts/ruby%E9%9A%8F%E6%9C%BA%E7%94%9F%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/","section":"posts","tags":["ruby"],"title":"ruby随机生成字符串"},{"categories":null,"contents":"用 app 来调用 routes，比如 app.posts_path, app.topic_path(1)\nirb \u0026gt; app.topics_path =\u0026gt; \u0026#34;/topics\u0026#34; irb \u0026gt; app.get(app.root_path) ...... =\u0026gt; 200 用 helper 来调用 Helper 方法，比如:\nirb \u0026gt; helper.link_to(\u0026#34;Ruby China\u0026#34;, \u0026#34;http://ruby-china.org\u0026#34;) =\u0026gt; \u0026#34;\u0026lt;a href=\\\u0026#34;http://ruby-china.org\\\u0026#34;\u0026gt;Ruby China\u0026lt;/a\u0026gt;\u0026#34; irb \u0026gt; helper.truncate(\u0026#34;Here is Ruby China.\u0026#34;, length: 15) =\u0026gt; \u0026#34;Here is Ruby...\u0026#34; 使用 source_location 方法查看方法在那里定义的, 比如:\nirb \u0026gt;Topic.instance_method(:destroy).source_location =\u0026gt; [\u0026#34;/Users/jason/.rvm/gems/ruby-1.9.3-p0/gems/mongoid-2.4.8/lib/mongoid/persistence.rb\u0026#34;, 30] irb \u0026gt;Topic.method(:destroy_all).source_location =\u0026gt; [\u0026#34;/Users/jason/.rvm/gems/ruby-1.9.3-p0/gems/mongoid-2.4.8/lib/mongoid/persistence.rb\u0026#34;, 239] ","date":"2018-08-07T23:58:40Z","permalink":"https://dccmmtop.github.io/posts/rails_console%E5%A5%BD%E7%94%A8%E7%9A%84%E6%8A%80%E5%B7%A7/","section":"posts","tags":["rails"],"title":"rails_console好用的技巧"},{"categories":null,"contents":"转载 http://api.rubyonrails.org/classes/ActiveRecord/Store.html\n阅读 http://api.rubyonrails.org 相关的笔记\n使用 Model 里面的一个字段作为一个序列化的封装，用来存储一个 key/value\n文档里面提到，对应的存储字段的类型最好是 text， 以便确保有足够的存储空间\nMake sure that you declare the database column used for the serialized store as a text, so there\u0026#39;s plenty of room. 假设 Model 里面有一个字段 body\nclass CreatePosts \u0026lt; ActiveRecord::Migration[5.0] def change create_table :posts do |t| t.string :title t.text :body # 作为store序列化的字段 t.boolean :published t.integer :status t.timestamps end end end 接着设置对应的序列化属性\nclass Post \u0026lt; ApplicationRecord # enum status: [ :active, :archived ] # 这里使用数组 与之对应的数字从0依次增加 enum status: { active: 10, archived: 20 } # 明确指定对应的数字 store :body, accessors: [ :color, :homepage, :email ], coder: JSON # 序列化属性 end 这样设置后，在 body 这一个字段上就可以存储多个 key/value 了\nirb(main):001:0\u0026gt; p = Post.create (0.1ms) begin transaction SQL (1.2ms) INSERT INTO \u0026#34;posts\u0026#34; (\u0026#34;created_at\u0026#34;, \u0026#34;updated_at\u0026#34;) VALUES (?, ?) [[\u0026#34;created_at\u0026#34;, 2017-02-16 07:32:44 UTC], [\u0026#34;updated_at\u0026#34;, 2017-02-16 07:32:44 UTC]] (1.9ms) commit transaction =\u0026gt; #\u0026lt;Post id: 4, title: nil, body: {}, published: nil, status: nil, created_at: \u0026#34;2017-02-16 07:32:44\u0026#34;, updated_at: \u0026#34;2017-02-16 07:32:44\u0026#34;\u0026gt; irb(main):002:0\u0026gt; p.body =\u0026gt; {} irb(main):003:0\u0026gt; p.body.class =\u0026gt; ActiveSupport::HashWithIndifferentAccess irb(main):004:0\u0026gt; p.body[:color] = \u0026#34;red\u0026#34; =\u0026gt; \u0026#34;red\u0026#34; irb(main):005:0\u0026gt; p.body[:email] = \u0026#34;hello@126.com\u0026#34; =\u0026gt; \u0026#34;hello@126.com\u0026#34; irb(main):006:0\u0026gt; p.color =\u0026gt; \u0026#34;red\u0026#34; irb(main):007:0\u0026gt; p.email =\u0026gt; \u0026#34;hello@126.com\u0026#34; irb(main):008:0\u0026gt; p.body[:no_set] = \u0026#34;这个属性没有在model声明\u0026#34; =\u0026gt; \u0026#34;这个属性没有在model声明\u0026#34; irb(main):009:0\u0026gt; p.body[:no_set] =\u0026gt; \u0026#34;这个属性没有在model声明\u0026#34; irb(main):010:0\u0026gt; p.no_set #这个会报错 ","date":"2018-08-07T22:53:19Z","permalink":"https://dccmmtop.github.io/posts/activerecord_store%E7%94%A8%E6%B3%95%E7%A4%BA%E4%BE%8B/","section":"posts","tags":["rails"],"title":"ActiveRecord_Store用法示例"},{"categories":null,"contents":"这个命令可以以递归的方式下载整站，并可以将下载的页面中的链接转换为本地链接。\nwget 加上参数之后，即可成为相当强大的下载工具。\nwget -r -p -np -k http://xxx.com/abc/ -r, \u0026ndash;recursive（递归） specify recursive download.（指定递归下载） -k, \u0026ndash;convert-links（转换链接） make links in downloaded HTML point to local files.（将下载的 HTML 页面中的链接转换为相对链接即本地链接） -p, \u0026ndash;page-requisites（页面必需元素） get all images, etc. needed to display HTML page.（下载所有的图片等页面显示所需的内容） -np, \u0026ndash;no-parent（不追溯至父级） don\u0026rsquo;t ascend to the parent directory.\n另外断点续传用-nc 参数 日志 用-o 参数 ​ 熟练掌握 wget 命令，可以帮助你方便的使用 linux。\n命令用法详解 http://www.cnblogs.com/peida/archive/2013/03/18/2965369.html\nLinux 系统中的 wget 是一个下载文件的工具，它用在命令行下。对于 Linux 用户是必不可少的工具，我们经常要下载一些软件或从远程服务器恢复备份到本地服务器。wget 支持 HTTP，HTTPS 和 FTP 协议，可以使用 HTTP 代理。所谓的自动下载是指，wget 可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个 wget 下载任务，然后退出系统，wget 将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。\nwget 可以跟踪 HTML 页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循 Robot Exclusion 标准(/robots.txt). wget 可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。\nwget 非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget 会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。\n1．命令格式：\nwget [参数][url地址]\n2．命令功能：\n用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget 虽然功能强大，但是使用起来还是比较简单：\n1）支持断点下传功能；这一点，也是网络蚂蚁和 FlashGet 当年最大的卖点，现在，Wget 也可以使用此功能，那些网络不是太好的用户可以放心了；\n2）同时支持 FTP 和 HTTP 下载方式；尽管现在大部分软件可以使用 HTTP 方式下载，但是，有些时候，仍然需要使用 FTP 方式下载软件；\n3）支持代理服务器；对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能；\n4）设置方便简单；可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标；\n5）程序小，完全免费；程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。\n3．命令参数：\n启动参数：\n-V, –version 显示 wget 的版本后退出 -h, –help 打印语法帮助 -b, –background 启动后转入后台执行 -e, –execute=COMMAND 执行.wgetrc格式的命令，wgetrc 格式参见/etc/wgetrc 或~/.wgetrc 记录和输入文件参数：\n-o, –output-file=FILE 把记录写到 FILE 文件中 -a, –append-output=FILE 把记录追加到 FILE 文件中 -d, –debug 打印调试输出 -q, –quiet 安静模式(没有输出) -v, –verbose 冗长模式(这是缺省设置) -nv, –non-verbose 关掉冗长模式，但不是安静模式 -i, –input-file=FILE 下载在 FILE 文件中出现的 URLs -F, –force-html 把输入文件当作 HTML 格式文件对待 -B, –base=URL 将 URL 作为在-F -i 参数指定的文件中出现的相对链接的前缀 –sslcertfile=FILE 可选客户端证书 –sslcertkey=KEYFILE 可选客户端证书的 KEYFILE –egd-file=FILE 指定 EGD socket 的文件名 下载参数：\n–bind-address=ADDRESS 指定本地使用地址(主机名或 IP，当本地有多个 IP 或名字时使用) -t, –tries=NUMBER 设定最大尝试链接次数(0 表示无限制). -O –output-document=FILE 把文档写到 FILE 文件中 -nc, –no-clobber 不要覆盖存在的文件或使用.#前缀 -c, –continue 接着下载没下载完的文件 –progress=TYPE 设定进程条标记 -N, –timestamping 不要重新下载文件除非比本地文件新 -S, –server-response 打印服务器的回应 –spider 不下载任何东西 -T, –timeout=SECONDS 设定响应超时的秒数 -w, –wait=SECONDS 两次尝试之间间隔 SECONDS 秒 –waitretry=SECONDS 在重新链接之间等待 1…SECONDS 秒 –random-wait 在下载之间等待 0…2*WAIT 秒 -Y, –proxy=on/off 打开或关闭代理 -Q, –quota=NUMBER 设置下载的容量限制 –limit-rate=RATE 限定下载输率 目录参数：\n-nd –no-directories 不创建目录 -x, –force-directories 强制创建目录 -nH, –no-host-directories 不创建主机目录 -P, –directory-prefix=PREFIX 将文件保存到目录 PREFIX/… –cut-dirs=NUMBER 忽略 NUMBER 层远程目录 HTTP 选项参数：\n–http-user=USER 设定 HTTP 用户名为 USER. –http-passwd=PASS 设定 http 密码为 PASS -C, –cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许) -E, –html-extension 将所有 text/html 文档以.html 扩展名保存 –ignore-length 忽略 Content-Length头域 –header=STRING 在 headers 中插入字符串 STRING –proxy-user=USER 设定代理的用户名为 USER –proxy-passwd=PASS 设定代理的密码为 PASS –referer=URL 在 HTTP 请求中包含 Referer: URL头 -s, –save-headers 保存 HTTP 头到文件 -U, –user-agent=AGENT 设定代理的名称为 AGENT 而不是 Wget/VERSION –no-http-keep-alive 关闭 HTTP 活动链接 (永远链接) –cookies=off 不使用 cookies –load-cookies=FILE 在开始会话前从文件 FILE 中加载 cookie –save-cookies=FILE 在会话结束后将 cookies 保存到 FILE 文件中 FTP 选项参数：\n-nr, –dont-remove-listing 不移走 .listing文件 -g, –glob=on/off 打开或关闭文件名的 globbing 机制 –passive-ftp 使用被动传输模式 (缺省值). –active-ftp 使用主动传输模式 –retr-symlinks 在递归的时候，将链接指向文件(而不是目录) 递归下载参数：\n-r, –recursive 递归下载－－慎用! -l, –level=NUMBER 最大递归深度 (inf 或 0 代表无穷) –delete-after 在现在完毕后局部删除文件 -k, –convert-links 转换非相对链接为相对链接 -K, –backup-converted 在转换文件 X 之前，将之备份为 X.orig -m, –mirror 等价于 -r -N -l inf -nr -p, –page-requisites 下载显示 HTML 文件的所有图片 递归下载中的包含和不包含(accept/reject)：\n-A, –accept=LIST 分号分隔的被接受扩展名的列表 -R, –reject=LIST 分号分隔的不被接受的扩展名的列表 -D, –domains=LIST 分号分隔的被接受域的列表 –exclude-domains=LIST 分号分隔的不被接受的域的列表 –follow-ftp 跟踪 HTML 文档中的 FTP 链接 –follow-tags=LIST 分号分隔的被跟踪的 HTML 标签的列表 -G, –ignore-tags=LIST 分号分隔的被忽略的 HTML 标签的列表 -H, –span-hosts 当递归时转到外部主机 -L, –relative 仅仅跟踪相对链接 -I, –include-directories=LIST 允许目录的列表 -X, –exclude-directories=LIST 不被包含目录的列表 -np, –no-parent 不要追溯到父目录 wget -S –spider url 不下载只显示过程 4．使用实例：\n实例 1：使用 wget 下载单个文件\n命令：\nwget http://www.linuxidc.com/linuxidc.zip\n说明：\n以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。\n实例 2：使用 wget -O 下载并以不同的文件名保存\n命令：\nwget -O wordpress.zip http://www.linuxidc.com/download.aspx?id=1080\n说明：\nwget 默认会以最后一个符合”/”的后面的字符来命令，对于动态链接的下载通常文件名会不正确。\n错误：下面的例子会下载一个文件并以名称 download.aspx?id=1080 保存\nwget http://www.linuxidc.com/download?id=1\n即使下载的文件是 zip 格式，它仍然以 download.php?id=1080 命令。\n正确：为了解决这个问题，我们可以使用参数-O 来指定一个文件名：\nwget -O wordpress.zip http://www.linuxidc.com/download.aspx?id=1080\n实例 3：使用 wget –limit -rate 限速下载\n命令：\nwget \u0026ndash;limit-rate=300k http://www.linuxidc.com/linuxidc.zip\n说明：\n当你执行 wget 的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。\n实例 4：使用 wget -c 断点续传\n命令：\nwget -c http://www.linuxidc.com/linuxidc.zip\n说明：\n使用 wget -c 重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c 参数。\n实例 5：使用 wget -b 后台下载\n命令：\nwget -b http://www.linuxidc.com/linuxidc.zip\n说明：\n对于下载非常大的文件的时候，我们可以使用参数-b 进行后台下载。\nwget -b http://www.linuxidc.com/linuxidc.zip\nContinuing in background, pid 1840.\nOutput will be written to wget-log.\n你可以使用以下命令来察看下载进度：\ntail -f wget-log\n实例 6：伪装代理名称下载\n命令：\nwget \u0026ndash;user-agent=\u0026ldquo;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16\u0026rdquo; http://www.linuxidc.com/linuxidc.zip\n说明：\n有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过–user-agent 参数伪装。\n实例 7：使用 wget –spider 测试下载链接\n命令：\nwget \u0026ndash;spider URL\n说明：\n当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加–spider 参数进行检查。\nwget \u0026ndash;spider URL\n如果下载链接正确，将会显示\nwget --spider URL\rSpider mode enabled. Check if remote file exists.\rHTTP request sent, awaiting response... 200 OK\rLength: unspecified [text/html]\rRemote file exists and could contain further links,\rbut recursion is disabled -- not retrieving.\r这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误\rwget --spider url\rSpider mode enabled. Check if remote file exists.\rHTTP request sent, awaiting response... 404 Not Found\rRemote file does not exist -- broken link!!! 你可以在以下几种情况下使用 spider 参数：\n定时下载之前进行检查\n间隔检测网站是否可用\n检查网站页面的死链接\n实例 8：使用 wget –tries 增加重试次数\n命令：\nwget \u0026ndash;tries=40 URL\n说明：\n如果网络有问题或下载一个大文件也有可能失败。wget 默认重试 20 次连接下载文件。如果需要，你可以使用–tries 增加重试次数。\n实例 9：使用 wget -i 下载多个文件\n命令：\nwget -i filelist.txt\n说明：\n首先，保存一份下载链接文件\ncat \u0026gt; filelist.txt\nurl1\nurl2\nurl3\nurl4\n接着使用这个文件和参数-i 下载\n实例 10：使用 wget –mirror 镜像网站\n命令：\nwget \u0026ndash;mirror -p \u0026ndash;convert-links -P ./LOCAL URL\n说明：\n下载整个网站到本地。\n–miror:开户镜像下载\n-p:下载所有为了 html 页面显示正常的文件\n–convert-links:下载后，转换成本地的链接\n-P ./LOCAL：保存所有文件和目录到本地指定目录\n实例 11：使用 wget –reject 过滤指定格式下载\n命令： wget \u0026ndash;reject=gif ur\n说明：\n下载一个网站，但你不希望下载图片，可以使用以下命令。\n实例 12：使用 wget -o 把下载信息存入日志文件\n命令：\nwget -o download.log URL\n说明：\n不希望下载信息直接显示在终端而是在一个日志文件，可以使用\n实例 13：使用 wget -Q 限制总下载文件大小\n命令：\nwget -Q5m -i filelist.txt\n说明：\n当你想要下载的文件超过 5M 而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。\n实例 14：使用 wget -r -A 下载指定格式文件\n命令：\nwget -r -A.pdf url\n说明：\n可以在以下情况使用该功能：\n下载一个网站的所有图片\n下载一个网站的所有视频\n下载一个网站的所有 PDF 文件\n实例 15：使用 wget FTP 下载\n命令：\nwget ftp-url\nwget \u0026ndash;ftp-user=USERNAME \u0026ndash;ftp-password=PASSWORD url\n说明：\n可以使用 wget 来完成 ftp 链接的下载。\n使用 wget 匿名 ftp 下载：\nwget ftp-url\n使用 wget 用户名和密码认证的 ftp 下载\nwget \u0026ndash;ftp-user=USERNAME \u0026ndash;ftp-password=PASSWORD url\n备注：编译安装\n使用如下命令编译安装：\ntar zxvf wget-1.9.1.tar.gz cd wget-1.9.1 ./configure make make install ","date":"2018-06-22T10:56:46Z","permalink":"https://dccmmtop.github.io/posts/wget%E6%95%B4%E7%AB%99%E4%B8%8B%E8%BD%BD/","section":"posts","tags":["linux"],"title":"wget整站下载"},{"categories":null,"contents":"添加新用户 在服务器添加一个新的用户，用户名为 deploy教程\n执行命令sudo adduser 用户名\n按提示输入密码\n设置一些个人信息，可以直接按 enter 键，设为空\n添加权限\n在 root 用户下，打开/etc/sudoers文件\n# # This file MUST be edited with the \u0026#39;visudo\u0026#39; command as root. # # Please consider adding local content in /etc/sudoers.d/ instead of # directly modifying this file. # # See the man page for details on how to write a sudoers file. # Defaults env_reset Defaults mail_badpass Defaults secure_path=\u0026#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\u0026#34; # Host alias specification # User alias specification # Cmnd alias specification # User privilege specification root ALL=(ALL:ALL) ALL deploy ALL=(ALL:ALL) ALL # 添加这一行，使deploy具有使用sudo的权限 # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL # Allow members of group sudo to execute any command %sudo ALL=(ALL:ALL) ALL # See sudoers(5) for more information on \u0026#34;#include\u0026#34; directives: #includedir /etc/sudoers.d ruby 安装 安装rbenv 教程来源 sudo deploy回到 deploy 下\ngit clone https://github.com/rbenv/rbenv.git ~/.rbenv # 用来编译安装 ruby git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build # 用来管理 gemset, 可选, 因为有 bundler 也没什么必要 git clone git://github.com/jamis/rbenv-gemset.git ~/.rbenv/plugins/rbenv-gemset # 通过 rbenv update 命令来更新 rbenv 以及所有插件, 推荐 git clone git://github.com/rkh/rbenv-update.git ~/.rbenv/plugins/rbenv-update # 使用 Ruby China 的镜像安装 Ruby, 国内用户推荐 git clone git://github.com/AndorChen/rbenv-china-mirror.git ~/.rbenv/plugins/rbenv-china-mirror 然后把下面的代码放到 ~/.bashrc 里\nexport PATH=\u0026#34;$HOME/.rbenv/bin:$PATH\u0026#34; eval \u0026#34;$(rbenv init -)\u0026#34; 然后重开一个终端就可以执行 rbenv 了.\n安装 ruby\nrbenv install --list # 列出所有 ruby 版本 rbenv install 2.5.0 # 安装 2.5.0 安转过程可能出现缺少依赖的错误，可参考这篇文章解决 一般解决办法:\nsudo apt-get install autoconf bison build-essential libssl-dev libyaml-dev libreadline6 libreadline6-dev zlib1g zlib1g-dev 验证安装是否成功\nrbenv versions # 列出安装的版本 rbenv version # 列出正在使用的版本 设置版本\nrbenv global 2.5.0 # 默认使用 2.5.0 rbenv shell 2.5.0 # 当前的 shell 使用 2.5.0, 会设置一个 `RBENV_VERSION` 环境变量 rbenv local jruby-1.7.3 # 当前目录使用 jruby-1.7.3, 会生成一个 `.rbenv-version` 文件 last\nrbenv rehash # 每当切换 ruby 版本和执行 bundle install 之后必须执行这个命令 rbenv which irb # 列出 irb 这个命令的完整路径 rbenv whence irb # 列出包含 irb 这个命令的版本 安装bundle\ngem install bundle 安装rails\ngem install rails 安装 nodejs\ncurl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash - sudo apt-get install -y nodejs 数据库 使用 postgresql 数据库教程来源\nsudo apt-get install postgresql 新建数据库用户\nsudo -i -u postgres //切换到数据库的超级管理员 psql //进入数据库控制台 create user deploy with password \u0026#39;xxxx\u0026#39;; //新建一个deploy用户，密码是xxx alter role deploy with createdb; //使deploy用户具有创建数据库的权限 alter role deploy with login；//使deploy用户具有登录数据库的权限 注意：\n在后面安装 pg gem 时，可能会出现You need to install postgresql-server-dev-X.Y for building a server-side extension or libpq-dev for building a client-side applic ation错误,依次执行：\nsudo apt-get install python-psycopg2 sudo apt-get install libpq-dev nginx passenger 安装 这里很详细了\nsudo apt-get install -y dirmngr gnupg sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7 sudo apt-get install -y apt-transport-https ca-certificates sudo sh -c \u0026#39;echo deb https://oss-binaries.phusionpassenger.com/apt/passenger xenial main \u0026gt; /etc/apt/sources.list.d/passenger.list\u0026#39; sudo apt-get update sudo apt-get install -y nginx-extras passenger passenger 的配置\nnginx 安装以后，打开/etc/nginx/passenger.conf会看到\npassenger_root /usr/lib/ruby/vendor_ruby/phusion_passenger/locations.ini;\rpassenger_ruby /home/deploy/.rbenv/shims/ruby; //这里需要修改ruby的安装路径 which ruby 可以查看 ruby 的路径\nCapistrano 配置原文教程 安装必要的包\ngroup :development do gem \u0026#39;capistrano\u0026#39; gem \u0026#39;capistrano-bundler\u0026#39; gem \u0026#39;capistrano-rails\u0026#39; gem \u0026#39;capistrano-rbenv\u0026#39; # Add this if you\u0026#39;re using rvm # gem \u0026#39;capistrano-rvm\u0026#39; end cap install\n我的 capfile 文件\n# Load DSL and set up stages require \u0026#34;capistrano/setup\u0026#34; # Include default deployment tasks require \u0026#34;capistrano/deploy\u0026#34; # Load the SCM plugin appropriate to your project: # # require \u0026#34;capistrano/scm/hg\u0026#34; # install_plugin Capistrano::SCM::Hg # or # require \u0026#34;capistrano/scm/svn\u0026#34; # install_plugin Capistrano::SCM::Svn # or require \u0026#34;capistrano/scm/git\u0026#34; install_plugin Capistrano::SCM::Git # Include tasks from other gems included in your Gemfile # # For documentation on these, see for example: # # https://github.com/capistrano/rvm # https://github.com/capistrano/rbenv # https://github.com/capistrano/chruby # https://github.com/capistrano/bundler # https://github.com/capistrano/rails # https://github.com/capistrano/passenger # # require \u0026#34;capistrano/rvm\u0026#34; require \u0026#34;capistrano/rbenv\u0026#34; # require \u0026#34;capistrano/chruby\u0026#34; require \u0026#34;capistrano/bundler\u0026#34; require \u0026#34;capistrano/rails/assets\u0026#34; require \u0026#34;capistrano/rails/migrations\u0026#34; require \u0026#34;capistrano/passenger\u0026#34; set :rbenv_type, :user set :rbenv_ruby, \u0026#39;2.5.0\u0026#39; # Load custom tasks from `lib/capistrano/tasks` if you have any defined Dir.glob(\u0026#34;lib/capistrano/tasks/*.rake\u0026#34;).each { |r| import r } 我的 deploy.rb 文件\n# config valid for current version and patch releases of Capistrano lock \u0026#34;~\u0026gt; 3.10.2\u0026#34; set :application, \u0026#34;script_blog\u0026#34; set :repo_url, \u0026#34;https://github.com/dccmmtop/script_blog.git\u0026#34; # Default branch is :master # ask :branch, `git rev-parse --abbrev-ref HEAD`.chomp # Default deploy_to directory is /var/www/my_app_name set :deploy_to, \u0026#34;/home/deploy/scrit_blog\u0026#34; # Default value for :format is :airbrussh. # set :format, :airbrussh # You can configure the Airbrussh format using :format_options. # These are the defaults. # set :format_options, command_output: true, log_file: \u0026#34;log/capistrano.log\u0026#34;, color: :auto, truncate: :auto # Default value for :pty is false # set :pty, true # Default value for :linked_files is [] # 在服务器\u0026lt;project-name\u0026gt;/share/config/ 下，要手动新建这两个文件， append :linked_files, \u0026#34;config/database.yml\u0026#34;,\u0026#34;config/secrets.yml\u0026#34; # Default value for linked_dirs is [] append :linked_dirs, \u0026#34;log\u0026#34;, \u0026#34;tmp/pids\u0026#34;, \u0026#34;tmp/cache\u0026#34;, \u0026#34;tmp/sockets\u0026#34;, \u0026#34;public/system\u0026#34; # Default value for default_env is {} # set :default_env, { path: \u0026#34;/opt/ruby/bin:$PATH\u0026#34; } # Default value for local_user is ENV[\u0026#39;USER\u0026#39;] # set :local_user, -\u0026gt; { `git config user.name`.chomp } # Default value for keep_releases is 5 # set :keep_releases, 5 # Uncomment the following to require manually verifying the host key before first deploy. # set :ssh_options, verify_host_key: :secure 注意append :linked_files, \u0026quot;config/database.yml\u0026quot;,\u0026quot;config/secrets.yml\u0026quot;\ndatabase.yml和secrets.yml是手动在,share/config/目录下新建的，一个是连接数据库的相关信息，一个是安全验证相关信息。我的部署目录是scriot_blog/ 就新建 script_blog/share/config/ 目录\n同时新建以上两个文件。\ndatabase.yml\nproduction: adapter: postgresql pool: \u0026lt;%= ENV.fetch(\u0026#34;RAILS_MAX_THREADS\u0026#34;) { 5 } %\u0026gt; timeout: 5000 database: production_blog username: \u0026#39;xxx\u0026#39; password: \u0026#39;xxx\u0026#39; secrets.yml\nproduction: secret_key_base: xxxxxx 其中secret_key_base的值是在本地项目下 执行rake secret 命令生成的。\ndeploy/production.rb\n# server-based syntax # ====================== # Defines a single server with a list of roles and multiple properties. # You can define all roles on a single server, or split them: # server \u0026#34;39.108.138.149\u0026#34;, user: \u0026#34;root\u0026#34;, roles: %w{app db web}, my_property: :my_value server \u0026#34;xxxx服务器的ip\u0026#34;, user: \u0026#34;deploy\u0026#34;, roles: %w{app db web} # server \u0026#34;example.com\u0026#34;, user: \u0026#34;deploy\u0026#34;, roles: %w{app web}, other_property: :other_value # server \u0026#34;db.example.com\u0026#34;, user: \u0026#34;deploy\u0026#34;, roles: %w{db} # role-based syntax # ================== # Defines a role with one or multiple servers. The primary server in each # group is considered to be the first unless any hosts have the primary # property set. Specify the username and a domain or IP for the server. # Don\u0026#39;t use `:all`, it\u0026#39;s a meta role. # role :app, %w{deploy@example.com}, my_property: :my_value # role :web, %w{user1@primary.com user2@additional.com}, other_property: :other_value # role :db, %w{deploy@example.com} # Configuration # ============= # You can set any configuration variable like in config/deploy.rb # These variables are then only loaded and set in this stage. # For available Capistrano configuration variables see the documentation page. # http://capistranorb.com/documentation/getting-started/configuration/ # Feel free to add new variables to customise your setup. # Custom SSH Options # ================== # You may pass any option but keep in mind that net/ssh understands a # limited set of options, consult the Net::SSH documentation. # http://net-ssh.github.io/net-ssh/classes/Net/SSH.html#method-c-start # # Global options # -------------- set :ssh_options, { keys: %w(/home/deploy/.ssh/id_rsa), port: xxx # forward_agent: false, # auth_methods: %w(password) } # # The server-based syntax can be used to override options: # ------------------------------------ # server \u0026#34;example.com\u0026#34;, # user: \u0026#34;user_name\u0026#34;, # keys: %w(/home/user_name/.ssh/id_rsa), # forward_agent: false, # auth_methods: %w(publickey password) # # password: \u0026#34;please use keys\u0026#34; # } 最后 本地执行cap production deploy\n","date":"2018-05-29T15:26:09Z","permalink":"https://dccmmtop.github.io/posts/%E9%83%A8%E7%BD%B2rails/","section":"posts","tags":["rails","部署"],"title":"部署Rails"},{"categories":null,"contents":" 生成 key 在本地执行ssh-keygen 将本地的公钥拷贝到远程服务器 ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22222 username@ip ","date":"2018-05-29T11:24:54Z","permalink":"https://dccmmtop.github.io/posts/ssh%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/","section":"posts","tags":["linux"],"title":"ssh免密码登录"},{"categories":null,"contents":"","date":"0001-01-01T00:00:00Z","permalink":"https://dccmmtop.github.io/search/","section":"","tags":null,"title":"Search Results"}]